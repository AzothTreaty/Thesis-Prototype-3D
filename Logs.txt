10/5/2016 11:02:10 PMStarting AI
Reading weights from 10/5/2016 11:02:10 PMStarting AI
Weights.txt
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
Layer 6's weights: -0.127011759170718 -0.527046723255344 -0.809567419776152 -0.658062053808399 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
Layer 8's weights: 0.391144796536028 0.669586762116017 0.582546397373737 -0.072025307192743 
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
Layer 6's weights: -0.127011759170718 -0.527046723255344 -0.809567419776152 -0.658062053808399 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
Layer 8's weights: 0.391144796536028 0.669586762116017 0.582546397373737 -0.072025307192743 
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
Layer 6's weights: -0.127011759170718 -0.527046723255344 -0.809567419776152 -0.658062053808399 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
10/5/2016 11:02:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:52 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
Modified index 9's learning in memoryPool to -0.2
Modified index 10's learning in memoryPool to -0.2
Modified index 11's learning in memoryPool to -0.2
Modified index 12's learning in memoryPool to -0.2
Modified index 13's learning in memoryPool to -0.2
Modified index 14's learning in memoryPool to -0.2
Modified index 15's learning in memoryPool to -0.2
Modified index 16's learning in memoryPool to -0.2
Modified index 17's learning in memoryPool to -0.2
Modified index 18's learning in memoryPool to -0.2
Modified index 19's learning in memoryPool to -0.2
Modified index 20's learning in memoryPool to -0.2
Modified index 21's learning in memoryPool to -0.2
Modified index 22's learning in memoryPool to -0.2
Modified index 23's learning in memoryPool to -0.2
Modified index 24's learning in memoryPool to -0.2
Modified index 25's learning in memoryPool to -0.2
Modified index 26's learning in memoryPool to -0.2
Modified index 27's learning in memoryPool to -0.2
10/5/2016 11:02:52 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 0, 0, -0.2
sum 0.0799361686402028 distri -0.348153632279741
Using diff 0.408105758759893 and condRate 0.166666666666667
Changed category 0 weights from 
-0.127011759170718 to -0.140615284665423
-0.527046723255344 to -0.540650248750049
-0.809567419776152 to -0.823170945270857
-0.658062053808399 to -0.671665579303104
Changing layer 0's weights from 
-0.568523272996696 to -0.582126798491401
-0.940449729686532 to -0.954053255181237
-1.17479899538115 to -1.18840252087585
-1.14023974550322 to -1.15384327099792
-0.392372176175865 to -0.40597570167057
-1.07501412940577 to -1.08861765490047
-0.921184405809198 to -0.934787931303903
-0.948638811832223 to -0.962242337326928
-1.18613067281798 to -1.19973419831268
-1.22988072795227 to -1.24348425344697
Changing layer 1's weights from 
-1.22478381556823 to -1.23838734106293
-0.623615130906854 to -0.637218656401559
-0.749317869668757 to -0.762921395163462
-0.75922320795611 to -0.772826733450815
-0.944275990253244 to -0.957879515747949
-1.08251382422999 to -1.09611734972469
-0.811795100694452 to -0.825398626189157
-1.1975981296356 to -1.2112016551303
-0.78593514872149 to -0.799538674216195
-1.14858201158598 to -1.16218553708068
Changing layer 2's weights from 
-0.339393779760155 to -0.35299730525486
-0.893447265153681 to -0.907050790648386
-0.608857974534783 to -0.622461500029488
-0.437657877927574 to -0.451261403422279
-0.614910587793145 to -0.62851411328785
-0.820920214181696 to -0.834523739676401
-0.302107498174461 to -0.315711023669166
-0.374529525762352 to -0.388133051257057
-1.10652782035425 to -1.12013134584895
-1.06157030177668 to -1.07517382727138
Changing layer 3's weights from 
-0.372444078450951 to -0.386047603945656
-0.635997161393914 to -0.649600686888619
-0.39338724327639 to -0.406990768771095
-1.30082647503253 to -1.31443000052723
-0.516063138967308 to -0.529666664462013
-0.646311745172296 to -0.659915270667001
-0.976496592288766 to -0.990100117783471
-0.517983719831261 to -0.531587245325966
-0.849679693704401 to -0.863283219199106
-0.376849815374169 to -0.390453340868874
Changing layer 4's weights from 
-0.476123616224083 to -0.489727141718788
-0.684956416612421 to -0.698559942107126
-0.885434016710077 to -0.899037542204782
-0.598499283319268 to -0.612102808813973
-0.837752089029108 to -0.851355614523813
-0.606286868577752 to -0.619890394072457
-0.610645398622308 to -0.624248924117013
-1.07254292083338 to -1.08614644632808
-0.478846833234581 to -0.492450358729286
-0.911148771768366 to -0.924752297263071
Changing layer 5's weights from 
-0.323343798642906 to -0.336947324137611
-0.765417322641168 to -0.779020848135873
-0.340228960042747 to -0.353832485537452
-1.27574114141419 to -1.28934466690889
-0.738472327714716 to -0.752075853209421
-1.13694027078703 to -1.15054379628173
-0.836974963670526 to -0.850578489165231
-0.764528736596857 to -0.778132262091562
-0.374926731115135 to -0.38853025660984
-0.381434843068872 to -0.395038368563577
Trying to learn from memory 1, 0, -0.2
sum 0.0801791567133419 distri -0.348227038488141
Using diff 0.408361406023148 and condRate 0.166666666666667
Changed category 0 weights from 
-0.140615284665423 to -0.154227331735696
-0.540650248750049 to -0.554262295820322
-0.823170945270857 to -0.83678299234113
-0.671665579303104 to -0.685277626373377
Changing layer 0's weights from 
-0.582126798491401 to -0.595738845561674
-0.954053255181237 to -0.96766530225151
-1.18840252087585 to -1.20201456794613
-1.15384327099792 to -1.1674553180682
-0.40597570167057 to -0.419587748740843
-1.08861765490047 to -1.10222970197075
-0.934787931303903 to -0.948399978374176
-0.962242337326928 to -0.975854384397201
-1.19973419831268 to -1.21334624538296
-1.24348425344697 to -1.25709630051725
Changing layer 1's weights from 
-1.23838734106293 to -1.25199938813321
-0.637218656401559 to -0.650830703471832
-0.762921395163462 to -0.776533442233735
-0.772826733450815 to -0.786438780521088
-0.957879515747949 to -0.971491562818222
-1.09611734972469 to -1.10972939679497
-0.825398626189157 to -0.83901067325943
-1.2112016551303 to -1.22481370220058
-0.799538674216195 to -0.813150721286468
-1.16218553708068 to -1.17579758415096
Changing layer 2's weights from 
-0.35299730525486 to -0.366609352325133
-0.907050790648386 to -0.920662837718659
-0.622461500029488 to -0.636073547099761
-0.451261403422279 to -0.464873450492552
-0.62851411328785 to -0.642126160358123
-0.834523739676401 to -0.848135786746674
-0.315711023669166 to -0.329323070739439
-0.388133051257057 to -0.40174509832733
-1.12013134584895 to -1.13374339291923
-1.07517382727138 to -1.08878587434166
Changing layer 3's weights from 
-0.386047603945656 to -0.399659651015929
-0.649600686888619 to -0.663212733958892
-0.406990768771095 to -0.420602815841368
-1.31443000052723 to -1.32804204759751
-0.529666664462013 to -0.543278711532286
-0.659915270667001 to -0.673527317737274
-0.990100117783471 to -1.00371216485374
-0.531587245325966 to -0.545199292396239
-0.863283219199106 to -0.876895266269379
-0.390453340868874 to -0.404065387939147
Changing layer 4's weights from 
-0.489727141718788 to -0.503339188789061
-0.698559942107126 to -0.712171989177399
-0.899037542204782 to -0.912649589275055
-0.612102808813973 to -0.625714855884246
-0.851355614523813 to -0.864967661594086
-0.619890394072457 to -0.63350244114273
-0.624248924117013 to -0.637860971187286
-1.08614644632808 to -1.09975849339836
-0.492450358729286 to -0.506062405799559
-0.924752297263071 to -0.938364344333344
Changing layer 5's weights from 
-0.336947324137611 to -0.350559371207884
-0.779020848135873 to -0.792632895206146
-0.353832485537452 to -0.367444532607725
-1.28934466690889 to -1.30295671397917
-0.752075853209421 to -0.765687900279694
-1.15054379628173 to -1.16415584335201
-0.850578489165231 to -0.864190536235504
-0.778132262091562 to -0.791744309161835
-0.38853025660984 to -0.402142303680113
-0.395038368563577 to -0.40865041563385
Trying to learn from memory 2, 0, -0.2
sum 0.0814113478200046 distri -0.348619370309372
Using diff 0.409677881174376 and condRate 0.166666666666667
Changed category 0 weights from 
-0.154227331735696 to -0.167883261311665
-0.554262295820322 to -0.567918225396291
-0.83678299234113 to -0.850438921917099
-0.685277626373377 to -0.698933555949346
Changing layer 0's weights from 
-0.595738845561674 to -0.609394775137643
-0.96766530225151 to -0.981321231827479
-1.20201456794613 to -1.2156704975221
-1.1674553180682 to -1.18111124764417
-0.419587748740843 to -0.433243678316812
-1.10222970197075 to -1.11588563154672
-0.948399978374176 to -0.962055907950145
-0.975854384397201 to -0.98951031397317
-1.21334624538296 to -1.22700217495893
-1.25709630051725 to -1.27075223009322
Changing layer 1's weights from 
-1.25199938813321 to -1.26565531770918
-0.650830703471832 to -0.664486633047801
-0.776533442233735 to -0.790189371809704
-0.786438780521088 to -0.800094710097057
-0.971491562818222 to -0.985147492394191
-1.10972939679497 to -1.12338532637094
-0.83901067325943 to -0.852666602835399
-1.22481370220058 to -1.23846963177655
-0.813150721286468 to -0.826806650862437
-1.17579758415096 to -1.18945351372693
Changing layer 2's weights from 
-0.366609352325133 to -0.380265281901102
-0.920662837718659 to -0.934318767294628
-0.636073547099761 to -0.64972947667573
-0.464873450492552 to -0.478529380068521
-0.642126160358123 to -0.655782089934092
-0.848135786746674 to -0.861791716322643
-0.329323070739439 to -0.342979000315408
-0.40174509832733 to -0.415401027903299
-1.13374339291923 to -1.1473993224952
-1.08878587434166 to -1.10244180391763
Changing layer 3's weights from 
-0.399659651015929 to -0.413315580591898
-0.663212733958892 to -0.676868663534861
-0.420602815841368 to -0.434258745417337
-1.32804204759751 to -1.34169797717348
-0.543278711532286 to -0.556934641108255
-0.673527317737274 to -0.687183247313243
-1.00371216485374 to -1.01736809442971
-0.545199292396239 to -0.558855221972208
-0.876895266269379 to -0.890551195845348
-0.404065387939147 to -0.417721317515116
Changing layer 4's weights from 
-0.503339188789061 to -0.51699511836503
-0.712171989177399 to -0.725827918753368
-0.912649589275055 to -0.926305518851024
-0.625714855884246 to -0.639370785460215
-0.864967661594086 to -0.878623591170055
-0.63350244114273 to -0.647158370718699
-0.637860971187286 to -0.651516900763255
-1.09975849339836 to -1.11341442297433
-0.506062405799559 to -0.519718335375528
-0.938364344333344 to -0.952020273909313
Changing layer 5's weights from 
-0.350559371207884 to -0.364215300783853
-0.792632895206146 to -0.806288824782115
-0.367444532607725 to -0.381100462183694
-1.30295671397917 to -1.31661264355514
-0.765687900279694 to -0.779343829855663
-1.16415584335201 to -1.17781177292798
-0.864190536235504 to -0.877846465811473
-0.791744309161835 to -0.805400238737804
-0.402142303680113 to -0.415798233256082
-0.40865041563385 to -0.422306345209819
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.167883261311665 to -0.181655471706134
-0.567918225396291 to -0.58169043579076
-0.850438921917099 to -0.864211132311568
-0.698933555949346 to -0.712705766343815
Changing layer 0's weights from 
-0.609394775137643 to -0.623166985532112
-0.981321231827479 to -0.995093442221948
-1.2156704975221 to -1.22944270791657
-1.18111124764417 to -1.19488345803864
-0.433243678316812 to -0.447015888711281
-1.11588563154672 to -1.12965784194119
-0.962055907950145 to -0.975828118344614
-0.98951031397317 to -1.00328252436764
-1.22700217495893 to -1.2407743853534
-1.27075223009322 to -1.28452444048769
Changing layer 1's weights from 
-1.26565531770918 to -1.27942752810365
-0.664486633047801 to -0.67825884344227
-0.790189371809704 to -0.803961582204173
-0.800094710097057 to -0.813866920491526
-0.985147492394191 to -0.99891970278866
-1.12338532637094 to -1.13715753676541
-0.852666602835399 to -0.866438813229868
-1.23846963177655 to -1.25224184217102
-0.826806650862437 to -0.840578861256906
-1.18945351372693 to -1.2032257241214
Changing layer 2's weights from 
-0.380265281901102 to -0.394037492295571
-0.934318767294628 to -0.948090977689097
-0.64972947667573 to -0.663501687070199
-0.478529380068521 to -0.49230159046299
-0.655782089934092 to -0.669554300328561
-0.861791716322643 to -0.875563926717112
-0.342979000315408 to -0.356751210709877
-0.415401027903299 to -0.429173238297768
-1.1473993224952 to -1.16117153288967
-1.10244180391763 to -1.1162140143121
Changing layer 3's weights from 
-0.413315580591898 to -0.427087790986367
-0.676868663534861 to -0.69064087392933
-0.434258745417337 to -0.448030955811806
-1.34169797717348 to -1.35547018756795
-0.556934641108255 to -0.570706851502724
-0.687183247313243 to -0.700955457707712
-1.01736809442971 to -1.03114030482418
-0.558855221972208 to -0.572627432366677
-0.890551195845348 to -0.904323406239817
-0.417721317515116 to -0.431493527909585
Changing layer 4's weights from 
-0.51699511836503 to -0.530767328759499
-0.725827918753368 to -0.739600129147837
-0.926305518851024 to -0.940077729245493
-0.639370785460215 to -0.653142995854684
-0.878623591170055 to -0.892395801564524
-0.647158370718699 to -0.660930581113168
-0.651516900763255 to -0.665289111157724
-1.11341442297433 to -1.1271866333688
-0.519718335375528 to -0.533490545769997
-0.952020273909313 to -0.965792484303782
Changing layer 5's weights from 
-0.364215300783853 to -0.377987511178322
-0.806288824782115 to -0.820061035176584
-0.381100462183694 to -0.394872672578163
-1.31661264355514 to -1.33038485394961
-0.779343829855663 to -0.793116040250132
-1.17781177292798 to -1.19158398332245
-0.877846465811473 to -0.891618676205942
-0.805400238737804 to -0.819172449132273
-0.415798233256082 to -0.429570443650551
-0.422306345209819 to -0.436078555604288
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.181655471706134 to -0.195427682100604
-0.58169043579076 to -0.59546264618523
-0.864211132311568 to -0.877983342706038
-0.712705766343815 to -0.726477976738285
Changing layer 0's weights from 
-0.623166985532112 to -0.636939195926582
-0.995093442221948 to -1.00886565261642
-1.22944270791657 to -1.24321491831104
-1.19488345803864 to -1.20865566843311
-0.447015888711281 to -0.460788099105751
-1.12965784194119 to -1.14343005233566
-0.975828118344614 to -0.989600328739084
-1.00328252436764 to -1.01705473476211
-1.2407743853534 to -1.25454659574787
-1.28452444048769 to -1.29829665088216
Changing layer 1's weights from 
-1.27942752810365 to -1.29319973849812
-0.67825884344227 to -0.69203105383674
-0.803961582204173 to -0.817733792598643
-0.813866920491526 to -0.827639130885996
-0.99891970278866 to -1.01269191318313
-1.13715753676541 to -1.15092974715988
-0.866438813229868 to -0.880211023624338
-1.25224184217102 to -1.26601405256549
-0.840578861256906 to -0.854351071651376
-1.2032257241214 to -1.21699793451587
Changing layer 2's weights from 
-0.394037492295571 to -0.407809702690041
-0.948090977689097 to -0.961863188083567
-0.663501687070199 to -0.677273897464669
-0.49230159046299 to -0.50607380085746
-0.669554300328561 to -0.683326510723031
-0.875563926717112 to -0.889336137111582
-0.356751210709877 to -0.370523421104347
-0.429173238297768 to -0.442945448692238
-1.16117153288967 to -1.17494374328414
-1.1162140143121 to -1.12998622470657
Changing layer 3's weights from 
-0.427087790986367 to -0.440860001380837
-0.69064087392933 to -0.7044130843238
-0.448030955811806 to -0.461803166206276
-1.35547018756795 to -1.36924239796242
-0.570706851502724 to -0.584479061897194
-0.700955457707712 to -0.714727668102182
-1.03114030482418 to -1.04491251521865
-0.572627432366677 to -0.586399642761147
-0.904323406239817 to -0.918095616634287
-0.431493527909585 to -0.445265738304055
Changing layer 4's weights from 
-0.530767328759499 to -0.544539539153969
-0.739600129147837 to -0.753372339542307
-0.940077729245493 to -0.953849939639963
-0.653142995854684 to -0.666915206249154
-0.892395801564524 to -0.906168011958994
-0.660930581113168 to -0.674702791507638
-0.665289111157724 to -0.679061321552194
-1.1271866333688 to -1.14095884376327
-0.533490545769997 to -0.547262756164467
-0.965792484303782 to -0.979564694698252
Changing layer 5's weights from 
-0.377987511178322 to -0.391759721572792
-0.820061035176584 to -0.833833245571054
-0.394872672578163 to -0.408644882972633
-1.33038485394961 to -1.34415706434408
-0.793116040250132 to -0.806888250644602
-1.19158398332245 to -1.20535619371692
-0.891618676205942 to -0.905390886600412
-0.819172449132273 to -0.832944659526743
-0.429570443650551 to -0.443342654045021
-0.436078555604288 to -0.449850765998758
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.195427682100604 to -0.209199892495073
-0.59546264618523 to -0.609234856579699
-0.877983342706038 to -0.891755553100507
-0.726477976738285 to -0.740250187132754
Changing layer 0's weights from 
-0.636939195926582 to -0.650711406321051
-1.00886565261642 to -1.02263786301089
-1.24321491831104 to -1.25698712870551
-1.20865566843311 to -1.22242787882758
-0.460788099105751 to -0.47456030950022
-1.14343005233566 to -1.15720226273013
-0.989600328739084 to -1.00337253913355
-1.01705473476211 to -1.03082694515658
-1.25454659574787 to -1.26831880614234
-1.29829665088216 to -1.31206886127663
Changing layer 1's weights from 
-1.29319973849812 to -1.30697194889259
-0.69203105383674 to -0.705803264231209
-0.817733792598643 to -0.831506002993112
-0.827639130885996 to -0.841411341280465
-1.01269191318313 to -1.0264641235776
-1.15092974715988 to -1.16470195755435
-0.880211023624338 to -0.893983234018807
-1.26601405256549 to -1.27978626295996
-0.854351071651376 to -0.868123282045845
-1.21699793451587 to -1.23077014491034
Changing layer 2's weights from 
-0.407809702690041 to -0.42158191308451
-0.961863188083567 to -0.975635398478036
-0.677273897464669 to -0.691046107859138
-0.50607380085746 to -0.519846011251929
-0.683326510723031 to -0.6970987211175
-0.889336137111582 to -0.903108347506051
-0.370523421104347 to -0.384295631498816
-0.442945448692238 to -0.456717659086707
-1.17494374328414 to -1.18871595367861
-1.12998622470657 to -1.14375843510104
Changing layer 3's weights from 
-0.440860001380837 to -0.454632211775306
-0.7044130843238 to -0.718185294718269
-0.461803166206276 to -0.475575376600745
-1.36924239796242 to -1.38301460835689
-0.584479061897194 to -0.598251272291663
-0.714727668102182 to -0.728499878496651
-1.04491251521865 to -1.05868472561312
-0.586399642761147 to -0.600171853155616
-0.918095616634287 to -0.931867827028756
-0.445265738304055 to -0.459037948698524
Changing layer 4's weights from 
-0.544539539153969 to -0.558311749548438
-0.753372339542307 to -0.767144549936776
-0.953849939639963 to -0.967622150034432
-0.666915206249154 to -0.680687416643623
-0.906168011958994 to -0.919940222353463
-0.674702791507638 to -0.688475001902107
-0.679061321552194 to -0.692833531946663
-1.14095884376327 to -1.15473105415774
-0.547262756164467 to -0.561034966558936
-0.979564694698252 to -0.993336905092721
Changing layer 5's weights from 
-0.391759721572792 to -0.405531931967261
-0.833833245571054 to -0.847605455965523
-0.408644882972633 to -0.422417093367102
-1.34415706434408 to -1.35792927473855
-0.806888250644602 to -0.820660461039071
-1.20535619371692 to -1.21912840411139
-0.905390886600412 to -0.919163096994881
-0.832944659526743 to -0.846716869921212
-0.443342654045021 to -0.45711486443949
-0.449850765998758 to -0.463622976393227
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.209199892495073 to -0.222972102889543
-0.609234856579699 to -0.623007066974169
-0.891755553100507 to -0.905527763494977
-0.740250187132754 to -0.754022397527224
Changing layer 0's weights from 
-0.650711406321051 to -0.664483616715521
-1.02263786301089 to -1.03641007340536
-1.25698712870551 to -1.27075933909997
-1.22242787882758 to -1.23620008922204
-0.47456030950022 to -0.48833251989469
-1.15720226273013 to -1.17097447312459
-1.00337253913355 to -1.01714474952802
-1.03082694515658 to -1.04459915555105
-1.26831880614234 to -1.2820910165368
-1.31206886127663 to -1.32584107167109
Changing layer 1's weights from 
-1.30697194889259 to -1.32074415928705
-0.705803264231209 to -0.719575474625679
-0.831506002993112 to -0.845278213387582
-0.841411341280465 to -0.855183551674935
-1.0264641235776 to -1.04023633397207
-1.16470195755435 to -1.17847416794881
-0.893983234018807 to -0.907755444413277
-1.27978626295996 to -1.29355847335442
-0.868123282045845 to -0.881895492440315
-1.23077014491034 to -1.2445423553048
Changing layer 2's weights from 
-0.42158191308451 to -0.43535412347898
-0.975635398478036 to -0.989407608872506
-0.691046107859138 to -0.704818318253608
-0.519846011251929 to -0.533618221646399
-0.6970987211175 to -0.71087093151197
-0.903108347506051 to -0.916880557900521
-0.384295631498816 to -0.398067841893286
-0.456717659086707 to -0.470489869481177
-1.18871595367861 to -1.20248816407307
-1.14375843510104 to -1.1575306454955
Changing layer 3's weights from 
-0.454632211775306 to -0.468404422169776
-0.718185294718269 to -0.731957505112739
-0.475575376600745 to -0.489347586995215
-1.38301460835689 to -1.39678681875135
-0.598251272291663 to -0.612023482686133
-0.728499878496651 to -0.742272088891121
-1.05868472561312 to -1.07245693600759
-0.600171853155616 to -0.613944063550086
-0.931867827028756 to -0.945640037423226
-0.459037948698524 to -0.472810159092994
Changing layer 4's weights from 
-0.558311749548438 to -0.572083959942908
-0.767144549936776 to -0.780916760331246
-0.967622150034432 to -0.981394360428902
-0.680687416643623 to -0.694459627038093
-0.919940222353463 to -0.933712432747933
-0.688475001902107 to -0.702247212296577
-0.692833531946663 to -0.706605742341133
-1.15473105415774 to -1.1685032645522
-0.561034966558936 to -0.574807176953406
-0.993336905092721 to -1.00710911548719
Changing layer 5's weights from 
-0.405531931967261 to -0.419304142361731
-0.847605455965523 to -0.861377666359993
-0.422417093367102 to -0.436189303761572
-1.35792927473855 to -1.37170148513301
-0.820660461039071 to -0.834432671433541
-1.21912840411139 to -1.23290061450585
-0.919163096994881 to -0.932935307389351
-0.846716869921212 to -0.860489080315682
-0.45711486443949 to -0.47088707483396
-0.463622976393227 to -0.477395186787697
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.222972102889543 to -0.236744313284013
-0.623007066974169 to -0.636779277368638
-0.905527763494977 to -0.919299973889446
-0.754022397527224 to -0.767794607921693
Changing layer 0's weights from 
-0.664483616715521 to -0.67825582710999
-1.03641007340536 to -1.05018228379983
-1.27075933909997 to -1.28453154949444
-1.23620008922204 to -1.24997229961651
-0.48833251989469 to -0.50210473028916
-1.17097447312459 to -1.18474668351906
-1.01714474952802 to -1.03091695992249
-1.04459915555105 to -1.05837136594552
-1.2820910165368 to -1.29586322693127
-1.32584107167109 to -1.33961328206556
Changing layer 1's weights from 
-1.32074415928705 to -1.33451636968152
-0.719575474625679 to -0.733347685020148
-0.845278213387582 to -0.859050423782051
-0.855183551674935 to -0.868955762069404
-1.04023633397207 to -1.05400854436654
-1.17847416794881 to -1.19224637834328
-0.907755444413277 to -0.921527654807746
-1.29355847335442 to -1.30733068374889
-0.881895492440315 to -0.895667702834784
-1.2445423553048 to -1.25831456569927
Changing layer 2's weights from 
-0.43535412347898 to -0.44912633387345
-0.989407608872506 to -1.00317981926698
-0.704818318253608 to -0.718590528648077
-0.533618221646399 to -0.547390432040868
-0.71087093151197 to -0.724643141906439
-0.916880557900521 to -0.93065276829499
-0.398067841893286 to -0.411840052287756
-0.470489869481177 to -0.484262079875647
-1.20248816407307 to -1.21626037446754
-1.1575306454955 to -1.17130285588997
Changing layer 3's weights from 
-0.468404422169776 to -0.482176632564246
-0.731957505112739 to -0.745729715507208
-0.489347586995215 to -0.503119797389685
-1.39678681875135 to -1.41055902914582
-0.612023482686133 to -0.625795693080602
-0.742272088891121 to -0.75604429928559
-1.07245693600759 to -1.08622914640206
-0.613944063550086 to -0.627716273944555
-0.945640037423226 to -0.959412247817695
-0.472810159092994 to -0.486582369487464
Changing layer 4's weights from 
-0.572083959942908 to -0.585856170337377
-0.780916760331246 to -0.794688970725715
-0.981394360428902 to -0.995166570823371
-0.694459627038093 to -0.708231837432562
-0.933712432747933 to -0.947484643142402
-0.702247212296577 to -0.716019422691046
-0.706605742341133 to -0.720377952735602
-1.1685032645522 to -1.18227547494667
-0.574807176953406 to -0.588579387347875
-1.00710911548719 to -1.02088132588166
Changing layer 5's weights from 
-0.419304142361731 to -0.433076352756201
-0.861377666359993 to -0.875149876754462
-0.436189303761572 to -0.449961514156042
-1.37170148513301 to -1.38547369552748
-0.834432671433541 to -0.84820488182801
-1.23290061450585 to -1.24667282490032
-0.932935307389351 to -0.94670751778382
-0.860489080315682 to -0.874261290710151
-0.47088707483396 to -0.48465928522843
-0.477395186787697 to -0.491167397182167
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.236744313284013 to -0.250516523678482
-0.636779277368638 to -0.650551487763108
-0.919299973889446 to -0.933072184283916
-0.767794607921693 to -0.781566818316163
Changing layer 0's weights from 
-0.67825582710999 to -0.69202803750446
-1.05018228379983 to -1.0639544941943
-1.28453154949444 to -1.29830375988891
-1.24997229961651 to -1.26374451001098
-0.50210473028916 to -0.515876940683629
-1.18474668351906 to -1.19851889391353
-1.03091695992249 to -1.04468917031696
-1.05837136594552 to -1.07214357633999
-1.29586322693127 to -1.30963543732574
-1.33961328206556 to -1.35338549246003
Changing layer 1's weights from 
-1.33451636968152 to -1.34828858007599
-0.733347685020148 to -0.747119895414618
-0.859050423782051 to -0.872822634176521
-0.868955762069404 to -0.882727972463874
-1.05400854436654 to -1.06778075476101
-1.19224637834328 to -1.20601858873775
-0.921527654807746 to -0.935299865202216
-1.30733068374889 to -1.32110289414336
-0.895667702834784 to -0.909439913229254
-1.25831456569927 to -1.27208677609374
Changing layer 2's weights from 
-0.44912633387345 to -0.462898544267919
-1.00317981926698 to -1.01695202966144
-0.718590528648077 to -0.732362739042547
-0.547390432040868 to -0.561162642435338
-0.724643141906439 to -0.738415352300909
-0.93065276829499 to -0.94442497868946
-0.411840052287756 to -0.425612262682225
-0.484262079875647 to -0.498034290270116
-1.21626037446754 to -1.23003258486201
-1.17130285588997 to -1.18507506628444
Changing layer 3's weights from 
-0.482176632564246 to -0.495948842958715
-0.745729715507208 to -0.759501925901678
-0.503119797389685 to -0.516892007784154
-1.41055902914582 to -1.42433123954029
-0.625795693080602 to -0.639567903475072
-0.75604429928559 to -0.76981650968006
-1.08622914640206 to -1.10000135679653
-0.627716273944555 to -0.641488484339025
-0.959412247817695 to -0.973184458212165
-0.486582369487464 to -0.500354579881933
Changing layer 4's weights from 
-0.585856170337377 to -0.599628380731847
-0.794688970725715 to -0.808461181120185
-0.995166570823371 to -1.00893878121784
-0.708231837432562 to -0.722004047827032
-0.947484643142402 to -0.961256853536872
-0.716019422691046 to -0.729791633085516
-0.720377952735602 to -0.734150163130072
-1.18227547494667 to -1.19604768534114
-0.588579387347875 to -0.602351597742345
-1.02088132588166 to -1.03465353627613
Changing layer 5's weights from 
-0.433076352756201 to -0.44684856315067
-0.875149876754462 to -0.888922087148932
-0.449961514156042 to -0.463733724550511
-1.38547369552748 to -1.39924590592195
-0.84820488182801 to -0.86197709222248
-1.24667282490032 to -1.26044503529479
-0.94670751778382 to -0.96047972817829
-0.874261290710151 to -0.888033501104621
-0.48465928522843 to -0.498431495622899
-0.491167397182167 to -0.504939607576636
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.250516523678482 to -0.264288734072952
-0.650551487763108 to -0.664323698157577
-0.933072184283916 to -0.946844394678385
-0.781566818316163 to -0.795339028710632
Changing layer 0's weights from 
-0.69202803750446 to -0.705800247898929
-1.0639544941943 to -1.07772670458877
-1.29830375988891 to -1.31207597028338
-1.26374451001098 to -1.27751672040545
-0.515876940683629 to -0.529649151078099
-1.19851889391353 to -1.212291104308
-1.04468917031696 to -1.05846138071143
-1.07214357633999 to -1.08591578673446
-1.30963543732574 to -1.32340764772021
-1.35338549246003 to -1.3671577028545
Changing layer 1's weights from 
-1.34828858007599 to -1.36206079047046
-0.747119895414618 to -0.760892105809087
-0.872822634176521 to -0.88659484457099
-0.882727972463874 to -0.896500182858343
-1.06778075476101 to -1.08155296515548
-1.20601858873775 to -1.21979079913222
-0.935299865202216 to -0.949072075596685
-1.32110289414336 to -1.33487510453783
-0.909439913229254 to -0.923212123623723
-1.27208677609374 to -1.28585898648821
Changing layer 2's weights from 
-0.462898544267919 to -0.476670754662389
-1.01695202966144 to -1.03072424005591
-0.732362739042547 to -0.746134949437016
-0.561162642435338 to -0.574934852829807
-0.738415352300909 to -0.752187562695378
-0.94442497868946 to -0.958197189083929
-0.425612262682225 to -0.439384473076695
-0.498034290270116 to -0.511806500664586
-1.23003258486201 to -1.24380479525648
-1.18507506628444 to -1.19884727667891
Changing layer 3's weights from 
-0.495948842958715 to -0.509721053353185
-0.759501925901678 to -0.773274136296147
-0.516892007784154 to -0.530664218178624
-1.42433123954029 to -1.43810344993476
-0.639567903475072 to -0.653340113869541
-0.76981650968006 to -0.783588720074529
-1.10000135679653 to -1.113773567191
-0.641488484339025 to -0.655260694733494
-0.973184458212165 to -0.986956668606634
-0.500354579881933 to -0.514126790276403
Changing layer 4's weights from 
-0.599628380731847 to -0.613400591126316
-0.808461181120185 to -0.822233391514654
-1.00893878121784 to -1.02271099161231
-0.722004047827032 to -0.735776258221501
-0.961256853536872 to -0.975029063931341
-0.729791633085516 to -0.743563843479985
-0.734150163130072 to -0.747922373524541
-1.19604768534114 to -1.20981989573561
-0.602351597742345 to -0.616123808136814
-1.03465353627613 to -1.0484257466706
Changing layer 5's weights from 
-0.44684856315067 to -0.46062077354514
-0.888922087148932 to -0.902694297543401
-0.463733724550511 to -0.477505934944981
-1.39924590592195 to -1.41301811631642
-0.86197709222248 to -0.875749302616949
-1.26044503529479 to -1.27421724568926
-0.96047972817829 to -0.974251938572759
-0.888033501104621 to -0.90180571149909
-0.498431495622899 to -0.512203706017369
-0.504939607576636 to -0.518711817971106
10/5/2016 11:02:53 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.264288734072952 to -0.278060944467421
-0.664323698157577 to -0.678095908552047
-0.946844394678385 to -0.960616605072855
-0.795339028710632 to -0.809111239105102
Changing layer 0's weights from 
-0.705800247898929 to -0.719572458293399
-1.07772670458877 to -1.09149891498323
-1.31207597028338 to -1.32584818067785
-1.27751672040545 to -1.29128893079992
-0.529649151078099 to -0.543421361472568
-1.212291104308 to -1.22606331470247
-1.05846138071143 to -1.0722335911059
-1.08591578673446 to -1.09968799712893
-1.32340764772021 to -1.33717985811468
-1.3671577028545 to -1.38092991324897
Changing layer 1's weights from 
-1.36206079047046 to -1.37583300086493
-0.760892105809087 to -0.774664316203557
-0.88659484457099 to -0.90036705496546
-0.896500182858343 to -0.910272393252813
-1.08155296515548 to -1.09532517554995
-1.21979079913222 to -1.23356300952669
-0.949072075596685 to -0.962844285991155
-1.33487510453783 to -1.3486473149323
-0.923212123623723 to -0.936984334018193
-1.28585898648821 to -1.29963119688268
Changing layer 2's weights from 
-0.476670754662389 to -0.490442965056858
-1.03072424005591 to -1.04449645045038
-0.746134949437016 to -0.759907159831486
-0.574934852829807 to -0.588707063224277
-0.752187562695378 to -0.765959773089848
-0.958197189083929 to -0.971969399478399
-0.439384473076695 to -0.453156683471164
-0.511806500664586 to -0.525578711059055
-1.24380479525648 to -1.25757700565095
-1.19884727667891 to -1.21261948707338
Changing layer 3's weights from 
-0.509721053353185 to -0.523493263747654
-0.773274136296147 to -0.787046346690617
-0.530664218178624 to -0.544436428573093
-1.43810344993476 to -1.45187566032923
-0.653340113869541 to -0.667112324264011
-0.783588720074529 to -0.797360930468999
-1.113773567191 to -1.12754577758547
-0.655260694733494 to -0.669032905127964
-0.986956668606634 to -1.0007288790011
-0.514126790276403 to -0.527899000670872
Changing layer 4's weights from 
-0.613400591126316 to -0.627172801520786
-0.822233391514654 to -0.836005601909124
-1.02271099161231 to -1.03648320200678
-0.735776258221501 to -0.749548468615971
-0.975029063931341 to -0.988801274325811
-0.743563843479985 to -0.757336053874455
-0.747922373524541 to -0.761694583919011
-1.20981989573561 to -1.22359210613008
-0.616123808136814 to -0.629896018531284
-1.0484257466706 to -1.06219795706507
Changing layer 5's weights from 
-0.46062077354514 to -0.474392983939609
-0.902694297543401 to -0.916466507937871
-0.477505934944981 to -0.49127814533945
-1.41301811631642 to -1.42679032671089
-0.875749302616949 to -0.889521513011419
-1.27421724568926 to -1.28798945608373
-0.974251938572759 to -0.988024148967229
-0.90180571149909 to -0.91557792189356
-0.512203706017369 to -0.525975916411838
-0.518711817971106 to -0.532484028365575
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.278060944467421 to -0.291833154861891
-0.678095908552047 to -0.691868118946516
-0.960616605072855 to -0.974388815467324
-0.809111239105102 to -0.822883449499571
Changing layer 0's weights from 
-0.719572458293399 to -0.733344668687868
-1.09149891498323 to -1.1052711253777
-1.32584818067785 to -1.33962039107232
-1.29128893079992 to -1.30506114119439
-0.543421361472568 to -0.557193571867038
-1.22606331470247 to -1.23983552509694
-1.0722335911059 to -1.08600580150037
-1.09968799712893 to -1.1134602075234
-1.33717985811468 to -1.35095206850915
-1.38092991324897 to -1.39470212364344
Changing layer 1's weights from 
-1.37583300086493 to -1.3896052112594
-0.774664316203557 to -0.788436526598026
-0.90036705496546 to -0.914139265359929
-0.910272393252813 to -0.924044603647282
-1.09532517554995 to -1.10909738594442
-1.23356300952669 to -1.24733521992116
-0.962844285991155 to -0.976616496385624
-1.3486473149323 to -1.36241952532677
-0.936984334018193 to -0.950756544412662
-1.29963119688268 to -1.31340340727715
Changing layer 2's weights from 
-0.490442965056858 to -0.504215175451328
-1.04449645045038 to -1.05826866084485
-0.759907159831486 to -0.773679370225955
-0.588707063224277 to -0.602479273618746
-0.765959773089848 to -0.779731983484317
-0.971969399478399 to -0.985741609872868
-0.453156683471164 to -0.466928893865634
-0.525578711059055 to -0.539350921453525
-1.25757700565095 to -1.27134921604542
-1.21261948707338 to -1.22639169746785
Changing layer 3's weights from 
-0.523493263747654 to -0.537265474142124
-0.787046346690617 to -0.800818557085086
-0.544436428573093 to -0.558208638967563
-1.45187566032923 to -1.4656478707237
-0.667112324264011 to -0.68088453465848
-0.797360930468999 to -0.811133140863468
-1.12754577758547 to -1.14131798797994
-0.669032905127964 to -0.682805115522433
-1.0007288790011 to -1.01450108939557
-0.527899000670872 to -0.541671211065342
Changing layer 4's weights from 
-0.627172801520786 to -0.640945011915255
-0.836005601909124 to -0.849777812303593
-1.03648320200678 to -1.05025541240125
-0.749548468615971 to -0.76332067901044
-0.988801274325811 to -1.00257348472028
-0.757336053874455 to -0.771108264268924
-0.761694583919011 to -0.77546679431348
-1.22359210613008 to -1.23736431652455
-0.629896018531284 to -0.643668228925753
-1.06219795706507 to -1.07597016745954
Changing layer 5's weights from 
-0.474392983939609 to -0.488165194334079
-0.916466507937871 to -0.93023871833234
-0.49127814533945 to -0.50505035573392
-1.42679032671089 to -1.44056253710536
-0.889521513011419 to -0.903293723405888
-1.28798945608373 to -1.3017616664782
-0.988024148967229 to -1.0017963593617
-0.91557792189356 to -0.929350132288029
-0.525975916411838 to -0.539748126806308
-0.532484028365575 to -0.546256238760045
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.291833154861891 to -0.30560536525636
-0.691868118946516 to -0.705640329340986
-0.974388815467324 to -0.988161025861794
-0.822883449499571 to -0.836655659894041
Changing layer 0's weights from 
-0.733344668687868 to -0.747116879082338
-1.1052711253777 to -1.11904333577217
-1.33962039107232 to -1.35339260146679
-1.30506114119439 to -1.31883335158886
-0.557193571867038 to -0.570965782261507
-1.23983552509694 to -1.25360773549141
-1.08600580150037 to -1.09977801189484
-1.1134602075234 to -1.12723241791787
-1.35095206850915 to -1.36472427890362
-1.39470212364344 to -1.40847433403791
Changing layer 1's weights from 
-1.3896052112594 to -1.40337742165387
-0.788436526598026 to -0.802208736992496
-0.914139265359929 to -0.927911475754399
-0.924044603647282 to -0.937816814041752
-1.10909738594442 to -1.12286959633889
-1.24733521992116 to -1.26110743031563
-0.976616496385624 to -0.990388706780094
-1.36241952532677 to -1.37619173572124
-0.950756544412662 to -0.964528754807132
-1.31340340727715 to -1.32717561767162
Changing layer 2's weights from 
-0.504215175451328 to -0.517987385845797
-1.05826866084485 to -1.07204087123932
-0.773679370225955 to -0.787451580620425
-0.602479273618746 to -0.616251484013216
-0.779731983484317 to -0.793504193878787
-0.985741609872868 to -0.999513820267338
-0.466928893865634 to -0.480701104260103
-0.539350921453525 to -0.553123131847994
-1.27134921604542 to -1.28512142643989
-1.22639169746785 to -1.24016390786232
Changing layer 3's weights from 
-0.537265474142124 to -0.551037684536593
-0.800818557085086 to -0.814590767479556
-0.558208638967563 to -0.571980849362032
-1.4656478707237 to -1.47942008111817
-0.68088453465848 to -0.69465674505295
-0.811133140863468 to -0.824905351257938
-1.14131798797994 to -1.15509019837441
-0.682805115522433 to -0.696577325916903
-1.01450108939557 to -1.02827329979004
-0.541671211065342 to -0.555443421459811
Changing layer 4's weights from 
-0.640945011915255 to -0.654717222309725
-0.849777812303593 to -0.863550022698063
-1.05025541240125 to -1.06402762279572
-0.76332067901044 to -0.77709288940491
-1.00257348472028 to -1.01634569511475
-0.771108264268924 to -0.784880474663394
-0.77546679431348 to -0.78923900470795
-1.23736431652455 to -1.25113652691902
-0.643668228925753 to -0.657440439320223
-1.07597016745954 to -1.08974237785401
Changing layer 5's weights from 
-0.488165194334079 to -0.501937404728548
-0.93023871833234 to -0.94401092872681
-0.50505035573392 to -0.518822566128389
-1.44056253710536 to -1.45433474749983
-0.903293723405888 to -0.917065933800358
-1.3017616664782 to -1.31553387687267
-1.0017963593617 to -1.01556856975617
-0.929350132288029 to -0.943122342682499
-0.539748126806308 to -0.553520337200777
-0.546256238760045 to -0.560028449154514
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.30560536525636 to -0.31937757565083
-0.705640329340986 to -0.719412539735455
-0.988161025861794 to -1.00193323625626
-0.836655659894041 to -0.85042787028851
Changing layer 0's weights from 
-0.747116879082338 to -0.760889089476807
-1.11904333577217 to -1.13281554616664
-1.35339260146679 to -1.36716481186126
-1.31883335158886 to -1.33260556198333
-0.570965782261507 to -0.584737992655977
-1.25360773549141 to -1.26737994588588
-1.09977801189484 to -1.11355022228931
-1.12723241791787 to -1.14100462831233
-1.36472427890362 to -1.37849648929809
-1.40847433403791 to -1.42224654443238
Changing layer 1's weights from 
-1.40337742165387 to -1.41714963204834
-0.802208736992496 to -0.815980947386965
-0.927911475754399 to -0.941683686148868
-0.937816814041752 to -0.951589024436221
-1.12286959633889 to -1.13664180673336
-1.26110743031563 to -1.2748796407101
-0.990388706780094 to -1.00416091717456
-1.37619173572124 to -1.38996394611571
-0.964528754807132 to -0.978300965201601
-1.32717561767162 to -1.34094782806609
Changing layer 2's weights from 
-0.517987385845797 to -0.531759596240267
-1.07204087123932 to -1.08581308163379
-0.787451580620425 to -0.801223791014894
-0.616251484013216 to -0.630023694407685
-0.793504193878787 to -0.807276404273256
-0.999513820267338 to -1.01328603066181
-0.480701104260103 to -0.494473314654573
-0.553123131847994 to -0.566895342242464
-1.28512142643989 to -1.29889363683436
-1.24016390786232 to -1.25393611825679
Changing layer 3's weights from 
-0.551037684536593 to -0.564809894931063
-0.814590767479556 to -0.828362977874025
-0.571980849362032 to -0.585753059756502
-1.47942008111817 to -1.49319229151264
-0.69465674505295 to -0.708428955447419
-0.824905351257938 to -0.838677561652407
-1.15509019837441 to -1.16886240876888
-0.696577325916903 to -0.710349536311372
-1.02827329979004 to -1.04204551018451
-0.555443421459811 to -0.569215631854281
Changing layer 4's weights from 
-0.654717222309725 to -0.668489432704194
-0.863550022698063 to -0.877322233092532
-1.06402762279572 to -1.07779983319019
-0.77709288940491 to -0.790865099799379
-1.01634569511475 to -1.03011790550922
-0.784880474663394 to -0.798652685057863
-0.78923900470795 to -0.803011215102419
-1.25113652691902 to -1.26490873731349
-0.657440439320223 to -0.671212649714692
-1.08974237785401 to -1.10351458824848
Changing layer 5's weights from 
-0.501937404728548 to -0.515709615123018
-0.94401092872681 to -0.957783139121279
-0.518822566128389 to -0.532594776522859
-1.45433474749983 to -1.4681069578943
-0.917065933800358 to -0.930838144194827
-1.31553387687267 to -1.32930608726714
-1.01556856975617 to -1.02934078015064
-0.943122342682499 to -0.956894553076968
-0.553520337200777 to -0.567292547595247
-0.560028449154514 to -0.573800659548984
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.31937757565083 to -0.3331497860453
-0.719412539735455 to -0.733184750129925
-1.00193323625626 to -1.01570544665073
-0.85042787028851 to -0.86420008068298
Changing layer 0's weights from 
-0.760889089476807 to -0.774661299871277
-1.13281554616664 to -1.14658775656111
-1.36716481186126 to -1.38093702225573
-1.33260556198333 to -1.3463777723778
-0.584737992655977 to -0.598510203050446
-1.26737994588588 to -1.28115215628035
-1.11355022228931 to -1.12732243268378
-1.14100462831233 to -1.1547768387068
-1.37849648929809 to -1.39226869969256
-1.42224654443238 to -1.43601875482685
Changing layer 1's weights from 
-1.41714963204834 to -1.43092184244281
-0.815980947386965 to -0.829753157781435
-0.941683686148868 to -0.955455896543338
-0.951589024436221 to -0.965361234830691
-1.13664180673336 to -1.15041401712783
-1.2748796407101 to -1.28865185110457
-1.00416091717456 to -1.01793312756903
-1.38996394611571 to -1.40373615651018
-0.978300965201601 to -0.992073175596071
-1.34094782806609 to -1.35472003846056
Changing layer 2's weights from 
-0.531759596240267 to -0.545531806634736
-1.08581308163379 to -1.09958529202826
-0.801223791014894 to -0.814996001409364
-0.630023694407685 to -0.643795904802155
-0.807276404273256 to -0.821048614667726
-1.01328603066181 to -1.02705824105628
-0.494473314654573 to -0.508245525049043
-0.566895342242464 to -0.580667552636933
-1.29889363683436 to -1.31266584722883
-1.25393611825679 to -1.26770832865126
Changing layer 3's weights from 
-0.564809894931063 to -0.578582105325532
-0.828362977874025 to -0.842135188268495
-0.585753059756502 to -0.599525270150971
-1.49319229151264 to -1.50696450190711
-0.708428955447419 to -0.722201165841889
-0.838677561652407 to -0.852449772046877
-1.16886240876888 to -1.18263461916335
-0.710349536311372 to -0.724121746705842
-1.04204551018451 to -1.05581772057898
-0.569215631854281 to -0.58298784224875
Changing layer 4's weights from 
-0.668489432704194 to -0.682261643098664
-0.877322233092532 to -0.891094443487002
-1.07779983319019 to -1.09157204358466
-0.790865099799379 to -0.804637310193849
-1.03011790550922 to -1.04389011590369
-0.798652685057863 to -0.812424895452333
-0.803011215102419 to -0.816783425496889
-1.26490873731349 to -1.27868094770796
-0.671212649714692 to -0.684984860109162
-1.10351458824848 to -1.11728679864295
Changing layer 5's weights from 
-0.515709615123018 to -0.529481825517487
-0.957783139121279 to -0.971555349515749
-0.532594776522859 to -0.546366986917328
-1.4681069578943 to -1.48187916828877
-0.930838144194827 to -0.944610354589297
-1.32930608726714 to -1.34307829766161
-1.02934078015064 to -1.04311299054511
-0.956894553076968 to -0.970666763471438
-0.567292547595247 to -0.581064757989716
-0.573800659548984 to -0.587572869943453
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.3331497860453 to -0.346921996439769
-0.733184750129925 to -0.746956960524394
-1.01570544665073 to -1.0294776570452
-0.86420008068298 to -0.877972291077449
Changing layer 0's weights from 
-0.774661299871277 to -0.788433510265746
-1.14658775656111 to -1.16035996695558
-1.38093702225573 to -1.3947092326502
-1.3463777723778 to -1.36014998277227
-0.598510203050446 to -0.612282413444916
-1.28115215628035 to -1.29492436667482
-1.12732243268378 to -1.14109464307825
-1.1547768387068 to -1.16854904910127
-1.39226869969256 to -1.40604091008703
-1.43601875482685 to -1.44979096522132
Changing layer 1's weights from 
-1.43092184244281 to -1.44469405283728
-0.829753157781435 to -0.843525368175905
-0.955455896543338 to -0.969228106937807
-0.965361234830691 to -0.97913344522516
-1.15041401712783 to -1.16418622752229
-1.28865185110457 to -1.30242406149904
-1.01793312756903 to -1.0317053379635
-1.40373615651018 to -1.41750836690465
-0.992073175596071 to -1.00584538599054
-1.35472003846056 to -1.36849224885503
Changing layer 2's weights from 
-0.545531806634736 to -0.559304017029206
-1.09958529202826 to -1.11335750242273
-0.814996001409364 to -0.828768211803833
-0.643795904802155 to -0.657568115196625
-0.821048614667726 to -0.834820825062195
-1.02705824105628 to -1.04083045145075
-0.508245525049043 to -0.522017735443512
-0.580667552636933 to -0.594439763031403
-1.31266584722883 to -1.3264380576233
-1.26770832865126 to -1.28148053904573
Changing layer 3's weights from 
-0.578582105325532 to -0.592354315720002
-0.842135188268495 to -0.855907398662965
-0.599525270150971 to -0.613297480545441
-1.50696450190711 to -1.52073671230158
-0.722201165841889 to -0.735973376236359
-0.852449772046877 to -0.866221982441346
-1.18263461916335 to -1.19640682955782
-0.724121746705842 to -0.737893957100311
-1.05581772057898 to -1.06958993097345
-0.58298784224875 to -0.59676005264322
Changing layer 4's weights from 
-0.682261643098664 to -0.696033853493133
-0.891094443487002 to -0.904866653881471
-1.09157204358466 to -1.10534425397913
-0.804637310193849 to -0.818409520588319
-1.04389011590369 to -1.05766232629816
-0.812424895452333 to -0.826197105846803
-0.816783425496889 to -0.830555635891359
-1.27868094770796 to -1.29245315810243
-0.684984860109162 to -0.698757070503631
-1.11728679864295 to -1.13105900903742
Changing layer 5's weights from 
-0.529481825517487 to -0.543254035911957
-0.971555349515749 to -0.985327559910218
-0.546366986917328 to -0.560139197311798
-1.48187916828877 to -1.49565137868324
-0.944610354589297 to -0.958382564983767
-1.34307829766161 to -1.35685050805608
-1.04311299054511 to -1.05688520093958
-0.970666763471438 to -0.984438973865908
-0.581064757989716 to -0.594836968384186
-0.587572869943453 to -0.601345080337923
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.346921996439769 to -0.360694206834239
-0.746956960524394 to -0.760729170918864
-1.0294776570452 to -1.04324986743967
-0.877972291077449 to -0.891744501471919
Changing layer 0's weights from 
-0.788433510265746 to -0.802205720660216
-1.16035996695558 to -1.17413217735005
-1.3947092326502 to -1.40848144304467
-1.36014998277227 to -1.37392219316674
-0.612282413444916 to -0.626054623839385
-1.29492436667482 to -1.30869657706929
-1.14109464307825 to -1.15486685347272
-1.16854904910127 to -1.18232125949574
-1.40604091008703 to -1.4198131204815
-1.44979096522132 to -1.46356317561579
Changing layer 1's weights from 
-1.44469405283728 to -1.45846626323175
-0.843525368175905 to -0.857297578570374
-0.969228106937807 to -0.983000317332277
-0.97913344522516 to -0.99290565561963
-1.16418622752229 to -1.17795843791676
-1.30242406149904 to -1.31619627189351
-1.0317053379635 to -1.04547754835797
-1.41750836690465 to -1.43128057729912
-1.00584538599054 to -1.01961759638501
-1.36849224885503 to -1.3822644592495
Changing layer 2's weights from 
-0.559304017029206 to -0.573076227423675
-1.11335750242273 to -1.1271297128172
-0.828768211803833 to -0.842540422198303
-0.657568115196625 to -0.671340325591094
-0.834820825062195 to -0.848593035456665
-1.04083045145075 to -1.05460266184522
-0.522017735443512 to -0.535789945837982
-0.594439763031403 to -0.608211973425872
-1.3264380576233 to -1.34021026801777
-1.28148053904573 to -1.2952527494402
Changing layer 3's weights from 
-0.592354315720002 to -0.606126526114471
-0.855907398662965 to -0.869679609057434
-0.613297480545441 to -0.62706969093991
-1.52073671230158 to -1.53450892269605
-0.735973376236359 to -0.749745586630828
-0.866221982441346 to -0.879994192835816
-1.19640682955782 to -1.21017903995229
-0.737893957100311 to -0.751666167494781
-1.06958993097345 to -1.08336214136792
-0.59676005264322 to -0.610532263037689
Changing layer 4's weights from 
-0.696033853493133 to -0.709806063887603
-0.904866653881471 to -0.918638864275941
-1.10534425397913 to -1.1191164643736
-0.818409520588319 to -0.832181730982788
-1.05766232629816 to -1.07143453669263
-0.826197105846803 to -0.839969316241272
-0.830555635891359 to -0.844327846285828
-1.29245315810243 to -1.3062253684969
-0.698757070503631 to -0.712529280898101
-1.13105900903742 to -1.14483121943189
Changing layer 5's weights from 
-0.543254035911957 to -0.557026246306427
-0.985327559910218 to -0.999099770304688
-0.560139197311798 to -0.573911407706267
-1.49565137868324 to -1.50942358907771
-0.958382564983767 to -0.972154775378236
-1.35685050805608 to -1.37062271845055
-1.05688520093958 to -1.07065741133405
-0.984438973865908 to -0.998211184260377
-0.594836968384186 to -0.608609178778655
-0.601345080337923 to -0.615117290732392
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.360694206834239 to -0.374466417228708
-0.760729170918864 to -0.774501381313333
-1.04324986743967 to -1.05702207783414
-0.891744501471919 to -0.905516711866388
Changing layer 0's weights from 
-0.802205720660216 to -0.815977931054686
-1.17413217735005 to -1.18790438774452
-1.40848144304467 to -1.42225365343914
-1.37392219316674 to -1.38769440356121
-0.626054623839385 to -0.639826834233855
-1.30869657706929 to -1.32246878746376
-1.15486685347272 to -1.16863906386719
-1.18232125949574 to -1.19609346989021
-1.4198131204815 to -1.43358533087597
-1.46356317561579 to -1.47733538601026
Changing layer 1's weights from 
-1.45846626323175 to -1.47223847362622
-0.857297578570374 to -0.871069788964844
-0.983000317332277 to -0.996772527726747
-0.99290565561963 to -1.0066778660141
-1.17795843791676 to -1.19173064831123
-1.31619627189351 to -1.32996848228798
-1.04547754835797 to -1.05924975875244
-1.43128057729912 to -1.44505278769359
-1.01961759638501 to -1.03338980677948
-1.3822644592495 to -1.39603666964397
Changing layer 2's weights from 
-0.573076227423675 to -0.586848437818145
-1.1271297128172 to -1.14090192321167
-0.842540422198303 to -0.856312632592772
-0.671340325591094 to -0.685112535985564
-0.848593035456665 to -0.862365245851135
-1.05460266184522 to -1.06837487223969
-0.535789945837982 to -0.549562156232451
-0.608211973425872 to -0.621984183820342
-1.34021026801777 to -1.35398247841224
-1.2952527494402 to -1.30902495983467
Changing layer 3's weights from 
-0.606126526114471 to -0.619898736508941
-0.869679609057434 to -0.883451819451904
-0.62706969093991 to -0.64084190133438
-1.53450892269605 to -1.54828113309052
-0.749745586630828 to -0.763517797025298
-0.879994192835816 to -0.893766403230285
-1.21017903995229 to -1.22395125034676
-0.751666167494781 to -0.765438377889251
-1.08336214136792 to -1.09713435176239
-0.610532263037689 to -0.624304473432159
Changing layer 4's weights from 
-0.709806063887603 to -0.723578274282072
-0.918638864275941 to -0.932411074670411
-1.1191164643736 to -1.13288867476807
-0.832181730982788 to -0.845953941377258
-1.07143453669263 to -1.0852067470871
-0.839969316241272 to -0.853741526635742
-0.844327846285828 to -0.858100056680298
-1.3062253684969 to -1.31999757889137
-0.712529280898101 to -0.72630149129257
-1.14483121943189 to -1.15860342982636
Changing layer 5's weights from 
-0.557026246306427 to -0.570798456700896
-0.999099770304688 to -1.01287198069916
-0.573911407706267 to -0.587683618100737
-1.50942358907771 to -1.52319579947218
-0.972154775378236 to -0.985926985772706
-1.37062271845055 to -1.38439492884502
-1.07065741133405 to -1.08442962172852
-0.998211184260377 to -1.01198339465485
-0.608609178778655 to -0.622381389173125
-0.615117290732392 to -0.628889501126862
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.374466417228708 to -0.388238627623178
-0.774501381313333 to -0.788273591707803
-1.05702207783414 to -1.07079428822861
-0.905516711866388 to -0.919288922260858
Changing layer 0's weights from 
-0.815977931054686 to -0.829750141449155
-1.18790438774452 to -1.20167659813899
-1.42225365343914 to -1.43602586383361
-1.38769440356121 to -1.40146661395568
-0.639826834233855 to -0.653599044628324
-1.32246878746376 to -1.33624099785823
-1.16863906386719 to -1.18241127426166
-1.19609346989021 to -1.20986568028468
-1.43358533087597 to -1.44735754127044
-1.47733538601026 to -1.49110759640473
Changing layer 1's weights from 
-1.47223847362622 to -1.48601068402069
-0.871069788964844 to -0.884841999359313
-0.996772527726747 to -1.01054473812122
-1.0066778660141 to -1.02045007640857
-1.19173064831123 to -1.2055028587057
-1.32996848228798 to -1.34374069268245
-1.05924975875244 to -1.07302196914691
-1.44505278769359 to -1.45882499808806
-1.03338980677948 to -1.04716201717395
-1.39603666964397 to -1.40980888003844
Changing layer 2's weights from 
-0.586848437818145 to -0.600620648212615
-1.14090192321167 to -1.15467413360614
-0.856312632592772 to -0.870084842987242
-0.685112535985564 to -0.698884746380033
-0.862365245851135 to -0.876137456245604
-1.06837487223969 to -1.08214708263416
-0.549562156232451 to -0.563334366626921
-0.621984183820342 to -0.635756394214811
-1.35398247841224 to -1.36775468880671
-1.30902495983467 to -1.32279717022914
Changing layer 3's weights from 
-0.619898736508941 to -0.63367094690341
-0.883451819451904 to -0.897224029846373
-0.64084190133438 to -0.654614111728849
-1.54828113309052 to -1.56205334348499
-0.763517797025298 to -0.777290007419767
-0.893766403230285 to -0.907538613624755
-1.22395125034676 to -1.23772346074123
-0.765438377889251 to -0.77921058828372
-1.09713435176239 to -1.11090656215686
-0.624304473432159 to -0.638076683826628
Changing layer 4's weights from 
-0.723578274282072 to -0.737350484676542
-0.932411074670411 to -0.94618328506488
-1.13288867476807 to -1.14666088516254
-0.845953941377258 to -0.859726151771727
-1.0852067470871 to -1.09897895748157
-0.853741526635742 to -0.867513737030211
-0.858100056680298 to -0.871872267074767
-1.31999757889137 to -1.33376978928584
-0.72630149129257 to -0.74007370168704
-1.15860342982636 to -1.17237564022083
Changing layer 5's weights from 
-0.570798456700896 to -0.584570667095366
-1.01287198069916 to -1.02664419109363
-0.587683618100737 to -0.601455828495207
-1.52319579947218 to -1.53696800986665
-0.985926985772706 to -0.999699196167175
-1.38439492884502 to -1.39816713923949
-1.08442962172852 to -1.09820183212299
-1.01198339465485 to -1.02575560504932
-0.622381389173125 to -0.636153599567594
-0.628889501126862 to -0.642661711521331
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.388238627623178 to -0.402010838017647
-0.788273591707803 to -0.802045802102273
-1.07079428822861 to -1.08456649862308
-0.919288922260858 to -0.933061132655328
Changing layer 0's weights from 
-0.829750141449155 to -0.843522351843625
-1.20167659813899 to -1.21544880853346
-1.43602586383361 to -1.44979807422808
-1.40146661395568 to -1.41523882435015
-0.653599044628324 to -0.667371255022794
-1.33624099785823 to -1.3500132082527
-1.18241127426166 to -1.19618348465613
-1.20986568028468 to -1.22363789067915
-1.44735754127044 to -1.46112975166491
-1.49110759640473 to -1.5048798067992
Changing layer 1's weights from 
-1.48601068402069 to -1.49978289441516
-0.884841999359313 to -0.898614209753783
-1.01054473812122 to -1.02431694851569
-1.02045007640857 to -1.03422228680304
-1.2055028587057 to -1.21927506910017
-1.34374069268245 to -1.35751290307692
-1.07302196914691 to -1.08679417954138
-1.45882499808806 to -1.47259720848253
-1.04716201717395 to -1.06093422756842
-1.40980888003844 to -1.42358109043291
Changing layer 2's weights from 
-0.600620648212615 to -0.614392858607084
-1.15467413360614 to -1.16844634400061
-0.870084842987242 to -0.883857053381712
-0.698884746380033 to -0.712656956774503
-0.876137456245604 to -0.889909666640074
-1.08214708263416 to -1.09591929302862
-0.563334366626921 to -0.57710657702139
-0.635756394214811 to -0.649528604609281
-1.36775468880671 to -1.38152689920118
-1.32279717022914 to -1.33656938062361
Changing layer 3's weights from 
-0.63367094690341 to -0.64744315729788
-0.897224029846373 to -0.910996240240843
-0.654614111728849 to -0.668386322123319
-1.56205334348499 to -1.57582555387946
-0.777290007419767 to -0.791062217814237
-0.907538613624755 to -0.921310824019224
-1.23772346074123 to -1.25149567113569
-0.77921058828372 to -0.79298279867819
-1.11090656215686 to -1.12467877255133
-0.638076683826628 to -0.651848894221098
Changing layer 4's weights from 
-0.737350484676542 to -0.751122695071011
-0.94618328506488 to -0.95995549545935
-1.14666088516254 to -1.16043309555701
-0.859726151771727 to -0.873498362166197
-1.09897895748157 to -1.11275116787604
-0.867513737030211 to -0.881285947424681
-0.871872267074767 to -0.885644477469237
-1.33376978928584 to -1.34754199968031
-0.74007370168704 to -0.75384591208151
-1.17237564022083 to -1.18614785061529
Changing layer 5's weights from 
-0.584570667095366 to -0.598342877489835
-1.02664419109363 to -1.0404164014881
-0.601455828495207 to -0.615228038889676
-1.53696800986665 to -1.55074022026112
-0.999699196167175 to -1.01347140656164
-1.39816713923949 to -1.41193934963396
-1.09820183212299 to -1.11197404251745
-1.02575560504932 to -1.03952781544379
-0.636153599567594 to -0.649925809962064
-0.642661711521331 to -0.656433921915801
10/5/2016 11:02:53 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.402010838017647 to -0.415783048412117
-0.802045802102273 to -0.815818012496742
-1.08456649862308 to -1.09833870901755
-0.933061132655328 to -0.946833343049797
Changing layer 0's weights from 
-0.843522351843625 to -0.857294562238094
-1.21544880853346 to -1.22922101892793
-1.44979807422808 to -1.46357028462255
-1.41523882435015 to -1.42901103474462
-0.667371255022794 to -0.681143465417263
-1.3500132082527 to -1.36378541864717
-1.19618348465613 to -1.2099556950506
-1.22363789067915 to -1.23741010107362
-1.46112975166491 to -1.47490196205938
-1.5048798067992 to -1.51865201719367
Changing layer 1's weights from 
-1.49978289441516 to -1.51355510480963
-0.898614209753783 to -0.912386420148252
-1.02431694851569 to -1.03808915891016
-1.03422228680304 to -1.04799449719751
-1.21927506910017 to -1.23304727949464
-1.35751290307692 to -1.37128511347139
-1.08679417954138 to -1.10056638993585
-1.47259720848253 to -1.486369418877
-1.06093422756842 to -1.07470643796289
-1.42358109043291 to -1.43735330082738
Changing layer 2's weights from 
-0.614392858607084 to -0.628165069001554
-1.16844634400061 to -1.18221855439508
-0.883857053381712 to -0.897629263776181
-0.712656956774503 to -0.726429167168972
-0.889909666640074 to -0.903681877034543
-1.09591929302862 to -1.10969150342309
-0.57710657702139 to -0.59087878741586
-0.649528604609281 to -0.66330081500375
-1.38152689920118 to -1.39529910959565
-1.33656938062361 to -1.35034159101808
Changing layer 3's weights from 
-0.64744315729788 to -0.661215367692349
-0.910996240240843 to -0.924768450635312
-0.668386322123319 to -0.682158532517788
-1.57582555387946 to -1.58959776427393
-0.791062217814237 to -0.804834428208706
-0.921310824019224 to -0.935083034413694
-1.25149567113569 to -1.26526788153016
-0.79298279867819 to -0.806755009072659
-1.12467877255133 to -1.1384509829458
-0.651848894221098 to -0.665621104615567
Changing layer 4's weights from 
-0.751122695071011 to -0.764894905465481
-0.95995549545935 to -0.973727705853819
-1.16043309555701 to -1.17420530595148
-0.873498362166197 to -0.887270572560666
-1.11275116787604 to -1.12652337827051
-0.881285947424681 to -0.89505815781915
-0.885644477469237 to -0.899416687863706
-1.34754199968031 to -1.36131421007478
-0.75384591208151 to -0.767618122475979
-1.18614785061529 to -1.19992006100976
Changing layer 5's weights from 
-0.598342877489835 to -0.612115087884305
-1.0404164014881 to -1.05418861188257
-0.615228038889676 to -0.629000249284146
-1.55074022026112 to -1.56451243065559
-1.01347140656164 to -1.02724361695611
-1.41193934963396 to -1.42571156002843
-1.11197404251745 to -1.12574625291192
-1.03952781544379 to -1.05330002583826
-0.649925809962064 to -0.663698020356533
-0.656433921915801 to -0.67020613231027
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.415783048412117 to -0.429555258806587
-0.815818012496742 to -0.829590222891212
-1.09833870901755 to -1.11211091941202
-0.946833343049797 to -0.960605553444267
Changing layer 0's weights from 
-0.857294562238094 to -0.871066772632564
-1.22922101892793 to -1.2429932293224
-1.46357028462255 to -1.47734249501702
-1.42901103474462 to -1.44278324513909
-0.681143465417263 to -0.694915675811733
-1.36378541864717 to -1.37755762904164
-1.2099556950506 to -1.22372790544507
-1.23741010107362 to -1.25118231146809
-1.47490196205938 to -1.48867417245385
-1.51865201719367 to -1.53242422758814
Changing layer 1's weights from 
-1.51355510480963 to -1.5273273152041
-0.912386420148252 to -0.926158630542722
-1.03808915891016 to -1.05186136930462
-1.04799449719751 to -1.06176670759198
-1.23304727949464 to -1.24681948988911
-1.37128511347139 to -1.38505732386586
-1.10056638993585 to -1.11433860033032
-1.486369418877 to -1.50014162927147
-1.07470643796289 to -1.08847864835736
-1.43735330082738 to -1.45112551122185
Changing layer 2's weights from 
-0.628165069001554 to -0.641937279396023
-1.18221855439508 to -1.19599076478955
-0.897629263776181 to -0.911401474170651
-0.726429167168972 to -0.740201377563442
-0.903681877034543 to -0.917454087429013
-1.10969150342309 to -1.12346371381756
-0.59087878741586 to -0.604650997810329
-0.66330081500375 to -0.67707302539822
-1.39529910959565 to -1.40907131999012
-1.35034159101808 to -1.36411380141255
Changing layer 3's weights from 
-0.661215367692349 to -0.674987578086819
-0.924768450635312 to -0.938540661029782
-0.682158532517788 to -0.695930742912258
-1.58959776427393 to -1.6033699746684
-0.804834428208706 to -0.818606638603176
-0.935083034413694 to -0.948855244808164
-1.26526788153016 to -1.27904009192463
-0.806755009072659 to -0.820527219467129
-1.1384509829458 to -1.15222319334027
-0.665621104615567 to -0.679393315010037
Changing layer 4's weights from 
-0.764894905465481 to -0.778667115859951
-0.973727705853819 to -0.987499916248289
-1.17420530595148 to -1.18797751634594
-0.887270572560666 to -0.901042782955136
-1.12652337827051 to -1.14029558866498
-0.89505815781915 to -0.90883036821362
-0.899416687863706 to -0.913188898258176
-1.36131421007478 to -1.37508642046925
-0.767618122475979 to -0.781390332870449
-1.19992006100976 to -1.21369227140423
Changing layer 5's weights from 
-0.612115087884305 to -0.625887298278774
-1.05418861188257 to -1.06796082227704
-0.629000249284146 to -0.642772459678615
-1.56451243065559 to -1.57828464105006
-1.02724361695611 to -1.04101582735058
-1.42571156002843 to -1.4394837704229
-1.12574625291192 to -1.13951846330639
-1.05330002583826 to -1.06707223623272
-0.663698020356533 to -0.677470230751003
-0.67020613231027 to -0.68397834270474
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.429555258806587 to -0.443327469201056
-0.829590222891212 to -0.843362433285681
-1.11211091941202 to -1.12588312980649
-0.960605553444267 to -0.974377763838736
Changing layer 0's weights from 
-0.871066772632564 to -0.884838983027033
-1.2429932293224 to -1.25676543971687
-1.47734249501702 to -1.49111470541149
-1.44278324513909 to -1.45655545553356
-0.694915675811733 to -0.708687886206202
-1.37755762904164 to -1.39132983943611
-1.22372790544507 to -1.23750011583954
-1.25118231146809 to -1.26495452186256
-1.48867417245385 to -1.50244638284832
-1.53242422758814 to -1.54619643798261
Changing layer 1's weights from 
-1.5273273152041 to -1.54109952559857
-0.926158630542722 to -0.939930840937191
-1.05186136930462 to -1.06563357969909
-1.06176670759198 to -1.07553891798645
-1.24681948988911 to -1.26059170028358
-1.38505732386586 to -1.39882953426033
-1.11433860033032 to -1.12811081072479
-1.50014162927147 to -1.51391383966594
-1.08847864835736 to -1.10225085875183
-1.45112551122185 to -1.46489772161632
Changing layer 2's weights from 
-0.641937279396023 to -0.655709489790493
-1.19599076478955 to -1.20976297518402
-0.911401474170651 to -0.92517368456512
-0.740201377563442 to -0.753973587957911
-0.917454087429013 to -0.931226297823482
-1.12346371381756 to -1.13723592421203
-0.604650997810329 to -0.618423208204799
-0.67707302539822 to -0.690845235792689
-1.40907131999012 to -1.42284353038459
-1.36411380141255 to -1.37788601180702
Changing layer 3's weights from 
-0.674987578086819 to -0.688759788481288
-0.938540661029782 to -0.952312871424251
-0.695930742912258 to -0.709702953306727
-1.6033699746684 to -1.61714218506287
-0.818606638603176 to -0.832378848997645
-0.948855244808164 to -0.962627455202633
-1.27904009192463 to -1.2928123023191
-0.820527219467129 to -0.834299429861598
-1.15222319334027 to -1.16599540373474
-0.679393315010037 to -0.693165525404506
Changing layer 4's weights from 
-0.778667115859951 to -0.79243932625442
-0.987499916248289 to -1.00127212664276
-1.18797751634594 to -1.20174972674041
-0.901042782955136 to -0.914814993349605
-1.14029558866498 to -1.15406779905945
-0.90883036821362 to -0.922602578608089
-0.913188898258176 to -0.926961108652645
-1.37508642046925 to -1.38885863086372
-0.781390332870449 to -0.795162543264918
-1.21369227140423 to -1.2274644817987
Changing layer 5's weights from 
-0.625887298278774 to -0.639659508673244
-1.06796082227704 to -1.08173303267151
-0.642772459678615 to -0.656544670073085
-1.57828464105006 to -1.59205685144453
-1.04101582735058 to -1.05478803774505
-1.4394837704229 to -1.45325598081737
-1.13951846330639 to -1.15329067370086
-1.06707223623272 to -1.08084444662719
-0.677470230751003 to -0.691242441145473
-0.68397834270474 to -0.697750553099209
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.443327469201056 to -0.457099679595526
-0.843362433285681 to -0.857134643680151
-1.12588312980649 to -1.13965534020096
-0.974377763838736 to -0.988149974233206
Changing layer 0's weights from 
-0.884838983027033 to -0.898611193421503
-1.25676543971687 to -1.27053765011134
-1.49111470541149 to -1.50488691580596
-1.45655545553356 to -1.47032766592803
-0.708687886206202 to -0.722460096600672
-1.39132983943611 to -1.40510204983058
-1.23750011583954 to -1.251272326234
-1.26495452186256 to -1.27872673225703
-1.50244638284832 to -1.51621859324279
-1.54619643798261 to -1.55996864837708
Changing layer 1's weights from 
-1.54109952559857 to -1.55487173599304
-0.939930840937191 to -0.953703051331661
-1.06563357969909 to -1.07940579009356
-1.07553891798645 to -1.08931112838092
-1.26059170028358 to -1.27436391067805
-1.39882953426033 to -1.4126017446548
-1.12811081072479 to -1.14188302111926
-1.51391383966594 to -1.52768605006041
-1.10225085875183 to -1.1160230691463
-1.46489772161632 to -1.47866993201079
Changing layer 2's weights from 
-0.655709489790493 to -0.669481700184962
-1.20976297518402 to -1.22353518557849
-0.92517368456512 to -0.93894589495959
-0.753973587957911 to -0.767745798352381
-0.931226297823482 to -0.944998508217952
-1.13723592421203 to -1.1510081346065
-0.618423208204799 to -0.632195418599268
-0.690845235792689 to -0.704617446187159
-1.42284353038459 to -1.43661574077906
-1.37788601180702 to -1.39165822220149
Changing layer 3's weights from 
-0.688759788481288 to -0.702531998875758
-0.952312871424251 to -0.966085081818721
-0.709702953306727 to -0.723475163701197
-1.61714218506287 to -1.63091439545734
-0.832378848997645 to -0.846151059392115
-0.962627455202633 to -0.976399665597103
-1.2928123023191 to -1.30658451271357
-0.834299429861598 to -0.848071640256068
-1.16599540373474 to -1.17976761412921
-0.693165525404506 to -0.706937735798976
Changing layer 4's weights from 
-0.79243932625442 to -0.80621153664889
-1.00127212664276 to -1.01504433703723
-1.20174972674041 to -1.21552193713488
-0.914814993349605 to -0.928587203744075
-1.15406779905945 to -1.16784000945391
-0.922602578608089 to -0.936374789002559
-0.926961108652645 to -0.940733319047115
-1.38885863086372 to -1.40263084125819
-0.795162543264918 to -0.808934753659388
-1.2274644817987 to -1.24123669219317
Changing layer 5's weights from 
-0.639659508673244 to -0.653431719067713
-1.08173303267151 to -1.09550524306597
-0.656544670073085 to -0.670316880467554
-1.59205685144453 to -1.605829061839
-1.05478803774505 to -1.06856024813952
-1.45325598081737 to -1.46702819121184
-1.15329067370086 to -1.16706288409533
-1.08084444662719 to -1.09461665702166
-0.691242441145473 to -0.705014651539942
-0.697750553099209 to -0.711522763493679
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.457099679595526 to -0.470871889989995
-0.857134643680151 to -0.87090685407462
-1.13965534020096 to -1.15342755059543
-0.988149974233206 to -1.00192218462768
Changing layer 0's weights from 
-0.898611193421503 to -0.912383403815972
-1.27053765011134 to -1.28430986050581
-1.50488691580596 to -1.51865912620043
-1.47032766592803 to -1.4840998763225
-0.722460096600672 to -0.736232306995141
-1.40510204983058 to -1.41887426022505
-1.251272326234 to -1.26504453662847
-1.27872673225703 to -1.2924989426515
-1.51621859324279 to -1.52999080363726
-1.55996864837708 to -1.57374085877155
Changing layer 1's weights from 
-1.55487173599304 to -1.56864394638751
-0.953703051331661 to -0.96747526172613
-1.07940579009356 to -1.09317800048803
-1.08931112838092 to -1.10308333877539
-1.27436391067805 to -1.28813612107252
-1.4126017446548 to -1.42637395504927
-1.14188302111926 to -1.15565523151373
-1.52768605006041 to -1.54145826045488
-1.1160230691463 to -1.12979527954077
-1.47866993201079 to -1.49244214240526
Changing layer 2's weights from 
-0.669481700184962 to -0.683253910579432
-1.22353518557849 to -1.23730739597296
-0.93894589495959 to -0.952718105354059
-0.767745798352381 to -0.78151800874685
-0.944998508217952 to -0.958770718612421
-1.1510081346065 to -1.16478034500097
-0.632195418599268 to -0.645967628993738
-0.704617446187159 to -0.718389656581628
-1.43661574077906 to -1.45038795117353
-1.39165822220149 to -1.40543043259596
Changing layer 3's weights from 
-0.702531998875758 to -0.716304209270228
-0.966085081818721 to -0.97985729221319
-0.723475163701197 to -0.737247374095666
-1.63091439545734 to -1.64468660585181
-0.846151059392115 to -0.859923269786584
-0.976399665597103 to -0.990171875991572
-1.30658451271357 to -1.32035672310804
-0.848071640256068 to -0.861843850650537
-1.17976761412921 to -1.19353982452368
-0.706937735798976 to -0.720709946193445
Changing layer 4's weights from 
-0.80621153664889 to -0.819983747043359
-1.01504433703723 to -1.0288165474317
-1.21552193713488 to -1.22929414752935
-0.928587203744075 to -0.942359414138544
-1.16784000945391 to -1.18161221984838
-0.936374789002559 to -0.950146999397028
-0.940733319047115 to -0.954505529441584
-1.40263084125819 to -1.41640305165266
-0.808934753659388 to -0.822706964053857
-1.24123669219317 to -1.25500890258764
Changing layer 5's weights from 
-0.653431719067713 to -0.667203929462183
-1.09550524306597 to -1.10927745346044
-0.670316880467554 to -0.684089090862024
-1.605829061839 to -1.61960127223347
-1.06856024813952 to -1.08233245853399
-1.46702819121184 to -1.48080040160631
-1.16706288409533 to -1.1808350944898
-1.09461665702166 to -1.10838886741613
-0.705014651539942 to -0.718786861934412
-0.711522763493679 to -0.725294973888148
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.470871889989995 to -0.484644100384465
-0.87090685407462 to -0.88467906446909
-1.15342755059543 to -1.1671997609899
-1.00192218462768 to -1.01569439502214
Changing layer 0's weights from 
-0.912383403815972 to -0.926155614210442
-1.28430986050581 to -1.29808207090028
-1.51865912620043 to -1.5324313365949
-1.4840998763225 to -1.49787208671697
-0.736232306995141 to -0.750004517389611
-1.41887426022505 to -1.43264647061952
-1.26504453662847 to -1.27881674702294
-1.2924989426515 to -1.30627115304597
-1.52999080363726 to -1.54376301403173
-1.57374085877155 to -1.58751306916602
Changing layer 1's weights from 
-1.56864394638751 to -1.58241615678198
-0.96747526172613 to -0.9812474721206
-1.09317800048803 to -1.1069502108825
-1.10308333877539 to -1.11685554916986
-1.28813612107252 to -1.30190833146699
-1.42637395504927 to -1.44014616544374
-1.15565523151373 to -1.1694274419082
-1.54145826045488 to -1.55523047084935
-1.12979527954077 to -1.14356748993524
-1.49244214240526 to -1.50621435279973
Changing layer 2's weights from 
-0.683253910579432 to -0.697026120973901
-1.23730739597296 to -1.25107960636743
-0.952718105354059 to -0.966490315748529
-0.78151800874685 to -0.79529021914132
-0.958770718612421 to -0.972542929006891
-1.16478034500097 to -1.17855255539544
-0.645967628993738 to -0.659739839388207
-0.718389656581628 to -0.732161866976098
-1.45038795117353 to -1.464160161568
-1.40543043259596 to -1.41920264299043
Changing layer 3's weights from 
-0.716304209270228 to -0.730076419664697
-0.97985729221319 to -0.99362950260766
-0.737247374095666 to -0.751019584490136
-1.64468660585181 to -1.65845881624628
-0.859923269786584 to -0.873695480181054
-0.990171875991572 to -1.00394408638604
-1.32035672310804 to -1.33412893350251
-0.861843850650537 to -0.875616061045007
-1.19353982452368 to -1.20731203491815
-0.720709946193445 to -0.734482156587915
Changing layer 4's weights from 
-0.819983747043359 to -0.833755957437829
-1.0288165474317 to -1.04258875782617
-1.22929414752935 to -1.24306635792382
-0.942359414138544 to -0.956131624533014
-1.18161221984838 to -1.19538443024285
-0.950146999397028 to -0.963919209791498
-0.954505529441584 to -0.968277739836054
-1.41640305165266 to -1.43017526204713
-0.822706964053857 to -0.836479174448327
-1.25500890258764 to -1.26878111298211
Changing layer 5's weights from 
-0.667203929462183 to -0.680976139856652
-1.10927745346044 to -1.12304966385491
-0.684089090862024 to -0.697861301256493
-1.61960127223347 to -1.63337348262794
-1.08233245853399 to -1.09610466892846
-1.48080040160631 to -1.49457261200078
-1.1808350944898 to -1.19460730488427
-1.10838886741613 to -1.1221610778106
-0.718786861934412 to -0.732559072328881
-0.725294973888148 to -0.739067184282618
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.484644100384465 to -0.498416310778935
-0.88467906446909 to -0.898451274863559
-1.1671997609899 to -1.18097197138437
-1.01569439502214 to -1.02946660541661
Changing layer 0's weights from 
-0.926155614210442 to -0.939927824604911
-1.29808207090028 to -1.31185428129475
-1.5324313365949 to -1.54620354698937
-1.49787208671697 to -1.51164429711144
-0.750004517389611 to -0.76377672778408
-1.43264647061952 to -1.44641868101399
-1.27881674702294 to -1.29258895741741
-1.30627115304597 to -1.32004336344044
-1.54376301403173 to -1.5575352244262
-1.58751306916602 to -1.60128527956049
Changing layer 1's weights from 
-1.58241615678198 to -1.59618836717645
-0.9812474721206 to -0.995019682515069
-1.1069502108825 to -1.12072242127697
-1.11685554916986 to -1.13062775956433
-1.30190833146699 to -1.31568054186146
-1.44014616544374 to -1.45391837583821
-1.1694274419082 to -1.18319965230267
-1.55523047084935 to -1.56900268124382
-1.14356748993524 to -1.15733970032971
-1.50621435279973 to -1.5199865631942
Changing layer 2's weights from 
-0.697026120973901 to -0.710798331368371
-1.25107960636743 to -1.2648518167619
-0.966490315748529 to -0.980262526142998
-0.79529021914132 to -0.809062429535789
-0.972542929006891 to -0.98631513940136
-1.17855255539544 to -1.19232476578991
-0.659739839388207 to -0.673512049782677
-0.732161866976098 to -0.745934077370567
-1.464160161568 to -1.47793237196247
-1.41920264299043 to -1.4329748533849
Changing layer 3's weights from 
-0.730076419664697 to -0.743848630059167
-0.99362950260766 to -1.00740171300213
-0.751019584490136 to -0.764791794884605
-1.65845881624628 to -1.67223102664075
-0.873695480181054 to -0.887467690575523
-1.00394408638604 to -1.01771629678051
-1.33412893350251 to -1.34790114389698
-0.875616061045007 to -0.889388271439476
-1.20731203491815 to -1.22108424531262
-0.734482156587915 to -0.748254366982384
Changing layer 4's weights from 
-0.833755957437829 to -0.847528167832298
-1.04258875782617 to -1.05636096822064
-1.24306635792382 to -1.25683856831829
-0.956131624533014 to -0.969903834927483
-1.19538443024285 to -1.20915664063732
-0.963919209791498 to -0.977691420185967
-0.968277739836054 to -0.982049950230523
-1.43017526204713 to -1.4439474724416
-0.836479174448327 to -0.850251384842796
-1.26878111298211 to -1.28255332337658
Changing layer 5's weights from 
-0.680976139856652 to -0.694748350251122
-1.12304966385491 to -1.13682187424938
-0.697861301256493 to -0.711633511650963
-1.63337348262794 to -1.64714569302241
-1.09610466892846 to -1.10987687932293
-1.49457261200078 to -1.50834482239525
-1.19460730488427 to -1.20837951527874
-1.1221610778106 to -1.13593328820507
-0.732559072328881 to -0.746331282723351
-0.739067184282618 to -0.752839394677087
Trying to learn from memory 3, 0, -0.2
sum 0.0831177524804297 distri -0.350827991317107
Using diff 0.413166305677429 and condRate 0.166666666666667
Changed category 0 weights from 
-0.498416310778935 to -0.512188521173404
-0.898451274863559 to -0.912223485258029
-1.18097197138437 to -1.19474418177884
-1.02946660541661 to -1.04323881581108
Changing layer 0's weights from 
-0.939927824604911 to -0.953700034999381
-1.31185428129475 to -1.32562649168922
-1.54620354698937 to -1.55997575738383
-1.51164429711144 to -1.5254165075059
-0.76377672778408 to -0.77754893817855
-1.44641868101399 to -1.46019089140845
-1.29258895741741 to -1.30636116781188
-1.32004336344044 to -1.33381557383491
-1.5575352244262 to -1.57130743482066
-1.60128527956049 to -1.61505748995495
Changing layer 1's weights from 
-1.59618836717645 to -1.60996057757091
-0.995019682515069 to -1.00879189290954
-1.12072242127697 to -1.13449463167144
-1.13062775956433 to -1.14439996995879
-1.31568054186146 to -1.32945275225593
-1.45391837583821 to -1.46769058623267
-1.18319965230267 to -1.19697186269714
-1.56900268124382 to -1.58277489163828
-1.15733970032971 to -1.17111191072417
-1.5199865631942 to -1.53375877358866
Changing layer 2's weights from 
-0.710798331368371 to -0.72457054176284
-1.2648518167619 to -1.27862402715637
-0.980262526142998 to -0.994034736537468
-0.809062429535789 to -0.822834639930259
-0.98631513940136 to -1.00008734979583
-1.19232476578991 to -1.20609697618438
-0.673512049782677 to -0.687284260177146
-0.745934077370567 to -0.759706287765037
-1.47793237196247 to -1.49170458235693
-1.4329748533849 to -1.44674706377936
Changing layer 3's weights from 
-0.743848630059167 to -0.757620840453636
-1.00740171300213 to -1.0211739233966
-0.764791794884605 to -0.778564005279075
-1.67223102664075 to -1.68600323703521
-0.887467690575523 to -0.901239900969993
-1.01771629678051 to -1.03148850717498
-1.34790114389698 to -1.36167335429145
-0.889388271439476 to -0.903160481833946
-1.22108424531262 to -1.23485645570709
-0.748254366982384 to -0.762026577376854
Changing layer 4's weights from 
-0.847528167832298 to -0.861300378226768
-1.05636096822064 to -1.07013317861511
-1.25683856831829 to -1.27061077871276
-0.969903834927483 to -0.983676045321953
-1.20915664063732 to -1.22292885103179
-0.977691420185967 to -0.991463630580437
-0.982049950230523 to -0.995822160624993
-1.4439474724416 to -1.45771968283606
-0.850251384842796 to -0.864023595237266
-1.28255332337658 to -1.29632553377105
Changing layer 5's weights from 
-0.694748350251122 to -0.708520560645591
-1.13682187424938 to -1.15059408464385
-0.711633511650963 to -0.725405722045432
-1.64714569302241 to -1.66091790341687
-1.10987687932293 to -1.1236490897174
-1.50834482239525 to -1.52211703278971
-1.20837951527874 to -1.22215172567321
-1.13593328820507 to -1.14970549859954
-0.746331282723351 to -0.76010349311782
-0.752839394677087 to -0.766611605071557
10/5/2016 11:02:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:02:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:52 PMStarting AI
Reading weights from 10/5/2016 11:03:52 PMStarting AI
Weights.txt
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
Layer 6's weights: -0.127011759170718 -0.527046723255344 -0.809567419776152 -0.658062053808399 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
Layer 8's weights: 0.391144796536028 0.669586762116017 0.582546397373737 -0.072025307192743 
Layer 0's weights: -0.953700034999381 -1.32562649168922 -1.55997575738383 -1.5254165075059 -0.77754893817855 -1.46019089140845 -1.30636116781188 -1.33381557383491 -1.57130743482066 -1.61505748995495 14 11 
Layer 1's weights: -1.60996057757091 -1.00879189290954 -1.13449463167144 -1.14439996995879 -1.32945275225593 -1.46769058623267 -1.19697186269714 -1.58277489163828 -1.17111191072417 -1.53375877358866 12 9 
Layer 2's weights: -0.72457054176284 -1.27862402715637 -0.994034736537468 -0.822834639930259 -1.00008734979583 -1.20609697618438 -0.687284260177146 -0.759706287765037 -1.49170458235693 -1.44674706377936 10 7 
Layer 3's weights: -0.757620840453636 -1.0211739233966 -0.778564005279075 -1.68600323703521 -0.901239900969993 -1.03148850717498 -1.36167335429145 -0.903160481833946 -1.23485645570709 -0.762026577376854 8 5 
Layer 4's weights: -0.861300378226768 -1.07013317861511 -1.27061077871276 -0.983676045321953 -1.22292885103179 -0.991463630580437 -0.995822160624993 -1.45771968283606 -0.864023595237266 -1.29632553377105 6 3 
Layer 5's weights: -0.708520560645591 -1.15059408464385 -0.725405722045432 -1.66091790341687 -1.1236490897174 -1.52211703278971 -1.22215172567321 -1.14970549859954 -0.76010349311782 -0.766611605071557 4 1 
Layer 6's weights: -0.512188521173404 -0.912223485258029 -1.19474418177884 -1.04323881581108 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
Layer 8's weights: 0.391144796536028 0.669586762116017 0.582546397373737 -0.072025307192743 
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
Layer 6's weights: -0.127011759170718 -0.527046723255344 -0.809567419776152 -0.658062053808399 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
10/5/2016 11:03:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:03:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:32 PMStarting AI
Reading weights from 10/5/2016 11:05:32 PMStarting AI
Weights.txt
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
Layer 6's weights: -0.127011759170718 -0.527046723255344 -0.809567419776152 -0.658062053808399 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
Layer 8's weights: 0.391144796536028 0.669586762116017 0.582546397373737 -0.072025307192743 
Layer 0's weights: -0.953700034999381 -1.32562649168922 -1.55997575738383 -1.5254165075059 -0.77754893817855 -1.46019089140845 -1.30636116781188 -1.33381557383491 -1.57130743482066 -1.61505748995495 14 11 
Layer 1's weights: -1.60996057757091 -1.00879189290954 -1.13449463167144 -1.14439996995879 -1.32945275225593 -1.46769058623267 -1.19697186269714 -1.58277489163828 -1.17111191072417 -1.53375877358866 12 9 
Layer 2's weights: -0.72457054176284 -1.27862402715637 -0.994034736537468 -0.822834639930259 -1.00008734979583 -1.20609697618438 -0.687284260177146 -0.759706287765037 -1.49170458235693 -1.44674706377936 10 7 
Layer 3's weights: -0.757620840453636 -1.0211739233966 -0.778564005279075 -1.68600323703521 -0.901239900969993 -1.03148850717498 -1.36167335429145 -0.903160481833946 -1.23485645570709 -0.762026577376854 8 5 
Layer 4's weights: -0.861300378226768 -1.07013317861511 -1.27061077871276 -0.983676045321953 -1.22292885103179 -0.991463630580437 -0.995822160624993 -1.45771968283606 -0.864023595237266 -1.29632553377105 6 3 
Layer 5's weights: -0.708520560645591 -1.15059408464385 -0.725405722045432 -1.66091790341687 -1.1236490897174 -1.52211703278971 -1.22215172567321 -1.14970549859954 -0.76010349311782 -0.766611605071557 4 1 
Layer 6's weights: -0.512188521173404 -0.912223485258029 -1.19474418177884 -1.04323881581108 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
Layer 8's weights: 0.391144796536028 0.669586762116017 0.582546397373737 -0.072025307192743 
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
Layer 6's weights: -0.127011759170718 -0.527046723255344 -0.809567419776152 -0.658062053808399 
10/5/2016 11:05:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:05:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:39 PMStarting AI
Reading weights from 10/5/2016 11:07:39 PMStarting AI
Weights.txt
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
Layer 6's weights: -0.127011759170718 -0.527046723255344 -0.809567419776152 -0.658062053808399 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
Layer 8's weights: 0.391144796536028 0.669586762116017 0.582546397373737 -0.072025307192743 
Layer 0's weights: -0.953700034999381 -1.32562649168922 -1.55997575738383 -1.5254165075059 -0.77754893817855 -1.46019089140845 -1.30636116781188 -1.33381557383491 -1.57130743482066 -1.61505748995495 14 11 
Layer 1's weights: -1.60996057757091 -1.00879189290954 -1.13449463167144 -1.14439996995879 -1.32945275225593 -1.46769058623267 -1.19697186269714 -1.58277489163828 -1.17111191072417 -1.53375877358866 12 9 
Layer 2's weights: -0.72457054176284 -1.27862402715637 -0.994034736537468 -0.822834639930259 -1.00008734979583 -1.20609697618438 -0.687284260177146 -0.759706287765037 -1.49170458235693 -1.44674706377936 10 7 
Layer 3's weights: -0.757620840453636 -1.0211739233966 -0.778564005279075 -1.68600323703521 -0.901239900969993 -1.03148850717498 -1.36167335429145 -0.903160481833946 -1.23485645570709 -0.762026577376854 8 5 
Layer 4's weights: -0.861300378226768 -1.07013317861511 -1.27061077871276 -0.983676045321953 -1.22292885103179 -0.991463630580437 -0.995822160624993 -1.45771968283606 -0.864023595237266 -1.29632553377105 6 3 
Layer 5's weights: -0.708520560645591 -1.15059408464385 -0.725405722045432 -1.66091790341687 -1.1236490897174 -1.52211703278971 -1.22215172567321 -1.14970549859954 -0.76010349311782 -0.766611605071557 4 1 
Layer 6's weights: -0.512188521173404 -0.912223485258029 -1.19474418177884 -1.04323881581108 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
Layer 8's weights: 0.391144796536028 0.669586762116017 0.582546397373737 -0.072025307192743 
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
10/5/2016 11:07:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:07:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:46 PMStarting AI
Reading weights from 10/5/2016 11:08:46 PMStarting AI
Weights.txt
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
Layer 6's weights: -0.127011759170718 -0.527046723255344 -0.809567419776152 -0.658062053808399 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
Layer 8's weights: 0.391144796536028 0.669586762116017 0.582546397373737 -0.072025307192743 
Layer 0's weights: -0.953700034999381 -1.32562649168922 -1.55997575738383 -1.5254165075059 -0.77754893817855 -1.46019089140845 -1.30636116781188 -1.33381557383491 -1.57130743482066 -1.61505748995495 14 11 
Layer 1's weights: -1.60996057757091 -1.00879189290954 -1.13449463167144 -1.14439996995879 -1.32945275225593 -1.46769058623267 -1.19697186269714 -1.58277489163828 -1.17111191072417 -1.53375877358866 12 9 
Layer 2's weights: -0.72457054176284 -1.27862402715637 -0.994034736537468 -0.822834639930259 -1.00008734979583 -1.20609697618438 -0.687284260177146 -0.759706287765037 -1.49170458235693 -1.44674706377936 10 7 
Layer 3's weights: -0.757620840453636 -1.0211739233966 -0.778564005279075 -1.68600323703521 -0.901239900969993 -1.03148850717498 -1.36167335429145 -0.903160481833946 -1.23485645570709 -0.762026577376854 8 5 
Layer 4's weights: -0.861300378226768 -1.07013317861511 -1.27061077871276 -0.983676045321953 -1.22292885103179 -0.991463630580437 -0.995822160624993 -1.45771968283606 -0.864023595237266 -1.29632553377105 6 3 
Layer 5's weights: -0.708520560645591 -1.15059408464385 -0.725405722045432 -1.66091790341687 -1.1236490897174 -1.52211703278971 -1.22215172567321 -1.14970549859954 -0.76010349311782 -0.766611605071557 4 1 
Layer 6's weights: -0.512188521173404 -0.912223485258029 -1.19474418177884 -1.04323881581108 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
Layer 8's weights: 0.391144796536028 0.669586762116017 0.582546397373737 -0.072025307192743 
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
Layer 6's weights: -0.512188521173404 -0.912223485258029 -1.19474418177884 -1.04323881581108 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
10/5/2016 11:08:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:08:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:49 PMStarting AI
Reading weights from 10/5/2016 11:12:49 PMStarting AI
Weights.txt
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
Layer 6's weights: -0.127011759170718 -0.527046723255344 -0.809567419776152 -0.658062053808399 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
Layer 8's weights: 0.391144796536028 0.669586762116017 0.582546397373737 -0.072025307192743 
Layer 0's weights: -0.953700034999381 -1.32562649168922 -1.55997575738383 -1.5254165075059 -0.77754893817855 -1.46019089140845 -1.30636116781188 -1.33381557383491 -1.57130743482066 -1.61505748995495 14 11 
Layer 1's weights: -1.60996057757091 -1.00879189290954 -1.13449463167144 -1.14439996995879 -1.32945275225593 -1.46769058623267 -1.19697186269714 -1.58277489163828 -1.17111191072417 -1.53375877358866 12 9 
Layer 2's weights: -0.72457054176284 -1.27862402715637 -0.994034736537468 -0.822834639930259 -1.00008734979583 -1.20609697618438 -0.687284260177146 -0.759706287765037 -1.49170458235693 -1.44674706377936 10 7 
Layer 3's weights: -0.757620840453636 -1.0211739233966 -0.778564005279075 -1.68600323703521 -0.901239900969993 -1.03148850717498 -1.36167335429145 -0.903160481833946 -1.23485645570709 -0.762026577376854 8 5 
Layer 4's weights: -0.861300378226768 -1.07013317861511 -1.27061077871276 -0.983676045321953 -1.22292885103179 -0.991463630580437 -0.995822160624993 -1.45771968283606 -0.864023595237266 -1.29632553377105 6 3 
Layer 5's weights: -0.708520560645591 -1.15059408464385 -0.725405722045432 -1.66091790341687 -1.1236490897174 -1.52211703278971 -1.22215172567321 -1.14970549859954 -0.76010349311782 -0.766611605071557 4 1 
Layer 6's weights: -0.512188521173404 -0.912223485258029 -1.19474418177884 -1.04323881581108 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
Layer 8's weights: 0.391144796536028 0.669586762116017 0.582546397373737 -0.072025307192743 
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
Layer 6's weights: -0.512188521173404 -0.912223485258029 -1.19474418177884 -1.04323881581108 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
10/5/2016 11:12:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:12:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:22 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
Modified index 9's learning in memoryPool to -0.2
Modified index 10's learning in memoryPool to -0.2
Modified index 11's learning in memoryPool to -0.2
Modified index 12's learning in memoryPool to -0.2
Modified index 13's learning in memoryPool to -0.2
Modified index 14's learning in memoryPool to -0.2
Modified index 15's learning in memoryPool to -0.2
Modified index 16's learning in memoryPool to -0.2
Modified index 17's learning in memoryPool to -0.2
Modified index 18's learning in memoryPool to -0.2
Modified index 19's learning in memoryPool to -0.2
Modified index 20's learning in memoryPool to -0.2
Modified index 21's learning in memoryPool to -0.2
Modified index 22's learning in memoryPool to -0.2
Modified index 23's learning in memoryPool to -0.2
Modified index 24's learning in memoryPool to -0.2
Modified index 25's learning in memoryPool to -0.2
Modified index 26's learning in memoryPool to -0.2
10/5/2016 11:13:22 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 0, 0, -0.2
sum -2.23697769807276 distri -6.88896099768593
Using diff 5.21122772413136 and condRate 0.166666666666667
Changed category 0 weights from 
-0.512188521173404 to -0.685896114566227
-0.912223485258029 to -1.08593107865085
-1.19474418177884 to -1.36845177517166
-1.04323881581108 to -1.2169464092039
Changing layer 0's weights from 
-0.953700034999381 to -1.1274076283922
-1.32562649168922 to -1.49933408508204
-1.55997575738383 to -1.73368335077665
-1.5254165075059 to -1.69912410089872
-0.77754893817855 to -0.951256531571373
-1.46019089140845 to -1.63389848480127
-1.30636116781188 to -1.4800687612047
-1.33381557383491 to -1.50752316722773
-1.57130743482066 to -1.74501502821348
-1.61505748995495 to -1.78876508334777
Changing layer 1's weights from 
-1.60996057757091 to -1.78366817096373
-1.00879189290954 to -1.18249948630236
-1.13449463167144 to -1.30820222506426
-1.14439996995879 to -1.31810756335161
-1.32945275225593 to -1.50316034564875
-1.46769058623267 to -1.64139817962549
-1.19697186269714 to -1.37067945608996
-1.58277489163828 to -1.7564824850311
-1.17111191072417 to -1.34481950411699
-1.53375877358866 to -1.70746636698148
Changing layer 2's weights from 
-0.72457054176284 to -0.898278135155663
-1.27862402715637 to -1.45233162054919
-0.994034736537468 to -1.16774232993029
-0.822834639930259 to -0.996542233323082
-1.00008734979583 to -1.17379494318865
-1.20609697618438 to -1.3798045695772
-0.687284260177146 to -0.860991853569969
-0.759706287765037 to -0.933413881157861
-1.49170458235693 to -1.66541217574975
-1.44674706377936 to -1.62045465717218
Changing layer 3's weights from 
-0.757620840453636 to -0.93132843384646
-1.0211739233966 to -1.19488151678942
-0.778564005279075 to -0.952271598671899
-1.68600323703521 to -1.85971083042803
-0.901239900969993 to -1.07494749436282
-1.03148850717498 to -1.2051961005678
-1.36167335429145 to -1.53538094768427
-0.903160481833946 to -1.07686807522677
-1.23485645570709 to -1.40856404909991
-0.762026577376854 to -0.935734170769678
Changing layer 4's weights from 
-0.861300378226768 to -1.03500797161959
-1.07013317861511 to -1.24384077200793
-1.27061077871276 to -1.44431837210558
-0.983676045321953 to -1.15738363871478
-1.22292885103179 to -1.39663644442461
-0.991463630580437 to -1.16517122397326
-0.995822160624993 to -1.16952975401782
-1.45771968283606 to -1.63142727622888
-0.864023595237266 to -1.03773118863009
-1.29632553377105 to -1.47003312716387
Changing layer 5's weights from 
-0.708520560645591 to -0.882228154038414
-1.15059408464385 to -1.32430167803667
-0.725405722045432 to -0.899113315438256
-1.66091790341687 to -1.83462549680969
-1.1236490897174 to -1.29735668311022
-1.52211703278971 to -1.69582462618253
-1.22215172567321 to -1.39585931906603
-1.14970549859954 to -1.32341309199236
-0.76010349311782 to -0.933811086510643
-0.766611605071557 to -0.940319198464381
Trying to learn from memory 1, 0, -0.2
sum -2.23789821535576 distri -6.89018537397642
Using diff 5.2117617124596 and condRate 0.166666666666667
Changed category 0 weights from 
-0.685896114566227 to -0.859621507570257
-1.08593107865085 to -1.25965647165488
-1.36845177517166 to -1.54217716817569
-1.2169464092039 to -1.39067180220793
Changing layer 0's weights from 
-1.1274076283922 to -1.30113302139623
-1.49933408508204 to -1.67305947808607
-1.73368335077665 to -1.90740874378068
-1.69912410089872 to -1.87284949390275
-0.951256531571373 to -1.1249819245754
-1.63389848480127 to -1.8076238778053
-1.4800687612047 to -1.65379415420873
-1.50752316722773 to -1.68124856023176
-1.74501502821348 to -1.91874042121751
-1.78876508334777 to -1.9624904763518
Changing layer 1's weights from 
-1.78366817096373 to -1.95739356396776
-1.18249948630236 to -1.35622487930639
-1.30820222506426 to -1.48192761806829
-1.31810756335161 to -1.49183295635564
-1.50316034564875 to -1.67688573865278
-1.64139817962549 to -1.81512357262952
-1.37067945608996 to -1.54440484909399
-1.7564824850311 to -1.93020787803513
-1.34481950411699 to -1.51854489712102
-1.70746636698148 to -1.88119175998551
Changing layer 2's weights from 
-0.898278135155663 to -1.07200352815969
-1.45233162054919 to -1.62605701355322
-1.16774232993029 to -1.34146772293432
-0.996542233323082 to -1.17026762632711
-1.17379494318865 to -1.34752033619268
-1.3798045695772 to -1.55352996258123
-0.860991853569969 to -1.034717246574
-0.933413881157861 to -1.10713927416189
-1.66541217574975 to -1.83913756875378
-1.62045465717218 to -1.79418005017621
Changing layer 3's weights from 
-0.93132843384646 to -1.10505382685049
-1.19488151678942 to -1.36860690979345
-0.952271598671899 to -1.12599699167593
-1.85971083042803 to -2.03343622343206
-1.07494749436282 to -1.24867288736685
-1.2051961005678 to -1.37892149357183
-1.53538094768427 to -1.7091063406883
-1.07686807522677 to -1.2505934682308
-1.40856404909991 to -1.58228944210394
-0.935734170769678 to -1.10945956377371
Changing layer 4's weights from 
-1.03500797161959 to -1.20873336462362
-1.24384077200793 to -1.41756616501196
-1.44431837210558 to -1.61804376510961
-1.15738363871478 to -1.33110903171881
-1.39663644442461 to -1.57036183742864
-1.16517122397326 to -1.33889661697729
-1.16952975401782 to -1.34325514702185
-1.63142727622888 to -1.80515266923291
-1.03773118863009 to -1.21145658163412
-1.47003312716387 to -1.6437585201679
Changing layer 5's weights from 
-0.882228154038414 to -1.05595354704244
-1.32430167803667 to -1.4980270710407
-0.899113315438256 to -1.07283870844229
-1.83462549680969 to -2.00835088981372
-1.29735668311022 to -1.47108207611425
-1.69582462618253 to -1.86955001918656
-1.39585931906603 to -1.56958471207006
-1.32341309199236 to -1.49713848499639
-0.933811086510643 to -1.10753647951467
-0.940319198464381 to -1.11404459146841
Trying to learn from memory 2, 2, -0.2
sum -2.22611854936902 distri 2.92999467326778
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.391144796536028 to 0.357811462705989
0.669586762116017 to 0.636253428285978
0.582546397373737 to 0.549213063543698
-0.072025307192743 to -0.105358641022782
Changing layer 0's weights from 
-1.30113302139623 to -1.33446635522627
-1.67305947808607 to -1.70639281191611
-1.90740874378068 to -1.94074207761072
-1.87284949390275 to -1.90618282773279
-1.1249819245754 to -1.15831525840544
-1.8076238778053 to -1.84095721163534
-1.65379415420873 to -1.68712748803877
-1.68124856023176 to -1.7145818940618
-1.91874042121751 to -1.95207375504755
-1.9624904763518 to -1.99582381018184
Changing layer 1's weights from 
-1.95739356396776 to -1.9907268977978
-1.35622487930639 to -1.38955821313643
-1.48192761806829 to -1.51526095189833
-1.49183295635564 to -1.52516629018568
-1.67688573865278 to -1.71021907248282
-1.81512357262952 to -1.84845690645956
-1.54440484909399 to -1.57773818292403
-1.93020787803513 to -1.96354121186517
-1.51854489712102 to -1.55187823095106
-1.88119175998551 to -1.91452509381555
Changing layer 2's weights from 
-1.07200352815969 to -1.10533686198973
-1.62605701355322 to -1.65939034738326
-1.34146772293432 to -1.37480105676436
-1.17026762632711 to -1.20360096015715
-1.34752033619268 to -1.38085367002272
-1.55352996258123 to -1.58686329641127
-1.034717246574 to -1.06805058040404
-1.10713927416189 to -1.14047260799193
-1.83913756875378 to -1.87247090258382
-1.79418005017621 to -1.82751338400625
Changing layer 3's weights from 
-1.10505382685049 to -1.13838716068053
-1.36860690979345 to -1.40194024362349
-1.12599699167593 to -1.15933032550597
-2.03343622343206 to -2.0667695572621
-1.24867288736685 to -1.28200622119689
-1.37892149357183 to -1.41225482740187
-1.7091063406883 to -1.74243967451834
-1.2505934682308 to -1.28392680206084
-1.58228944210394 to -1.61562277593398
-1.10945956377371 to -1.14279289760375
Changing layer 4's weights from 
-1.20873336462362 to -1.24206669845366
-1.41756616501196 to -1.450899498842
-1.61804376510961 to -1.65137709893965
-1.33110903171881 to -1.36444236554885
-1.57036183742864 to -1.60369517125868
-1.33889661697729 to -1.37222995080733
-1.34325514702185 to -1.37658848085189
-1.80515266923291 to -1.83848600306295
-1.21145658163412 to -1.24478991546416
-1.6437585201679 to -1.67709185399794
Changing layer 5's weights from 
-1.05595354704244 to -1.08928688087248
-1.4980270710407 to -1.53136040487074
-1.07283870844229 to -1.10617204227232
-2.00835088981372 to -2.04168422364376
-1.47108207611425 to -1.50441540994429
-1.86955001918656 to -1.9028833530166
-1.56958471207006 to -1.6029180459001
-1.49713848499639 to -1.53047181882643
-1.10753647951467 to -1.14086981334471
-1.11404459146841 to -1.14737792529845
Trying to learn from memory 3, 0, -0.2
sum -2.38909523906269 distri -7.49253226168831
Using diff 5.70071083239129 and condRate 0.166666666666667
Changed category 0 weights from 
-0.859621507570257 to -1.04964520481487
-1.25965647165488 to -1.4496801688995
-1.54217716817569 to -1.73220086542031
-1.39067180220793 to -1.58069549945255
Changing layer 0's weights from 
-1.33446635522627 to -1.52449005247089
-1.70639281191611 to -1.89641650916073
-1.94074207761072 to -2.13076577485534
-1.90618282773279 to -2.09620652497741
-1.15831525840544 to -1.34833895565006
-1.84095721163534 to -2.03098090887996
-1.68712748803877 to -1.87715118528339
-1.7145818940618 to -1.90460559130642
-1.95207375504755 to -2.14209745229217
-1.99582381018184 to -2.18584750742646
Changing layer 1's weights from 
-1.9907268977978 to -2.18075059504242
-1.38955821313643 to -1.57958191038105
-1.51526095189833 to -1.70528464914295
-1.52516629018568 to -1.7151899874303
-1.71021907248282 to -1.90024276972744
-1.84845690645956 to -2.03848060370418
-1.57773818292403 to -1.76776188016865
-1.96354121186517 to -2.15356490910979
-1.55187823095106 to -1.74190192819568
-1.91452509381555 to -2.10454879106017
Changing layer 2's weights from 
-1.10533686198973 to -1.29536055923435
-1.65939034738326 to -1.84941404462788
-1.37480105676436 to -1.56482475400898
-1.20360096015715 to -1.39362465740177
-1.38085367002272 to -1.57087736726734
-1.58686329641127 to -1.77688699365589
-1.06805058040404 to -1.25807427764866
-1.14047260799193 to -1.33049630523655
-1.87247090258382 to -2.06249459982844
-1.82751338400625 to -2.01753708125087
Changing layer 3's weights from 
-1.13838716068053 to -1.32841085792515
-1.40194024362349 to -1.59196394086811
-1.15933032550597 to -1.34935402275058
-2.0667695572621 to -2.25679325450672
-1.28200622119689 to -1.4720299184415
-1.41225482740187 to -1.60227852464649
-1.74243967451834 to -1.93246337176296
-1.28392680206084 to -1.47395049930546
-1.61562277593398 to -1.8056464731786
-1.14279289760375 to -1.33281659484836
Changing layer 4's weights from 
-1.24206669845366 to -1.43209039569828
-1.450899498842 to -1.64092319608662
-1.65137709893965 to -1.84140079618427
-1.36444236554885 to -1.55446606279346
-1.60369517125868 to -1.7937188685033
-1.37222995080733 to -1.56225364805195
-1.37658848085189 to -1.5666121780965
-1.83848600306295 to -2.02850970030757
-1.24478991546416 to -1.43481361270878
-1.67709185399794 to -1.86711555124256
Changing layer 5's weights from 
-1.08928688087248 to -1.2793105781171
-1.53136040487074 to -1.72138410211536
-1.10617204227232 to -1.29619573951694
-2.04168422364376 to -2.23170792088838
-1.50441540994429 to -1.69443910718891
-1.9028833530166 to -2.09290705026122
-1.6029180459001 to -1.79294174314472
-1.53047181882643 to -1.72049551607105
-1.14086981334471 to -1.33089351058933
-1.14737792529845 to -1.33740162254307
Trying to learn from memory 4, 2, -0.2
sum -2.92240079881371 distri 3.53597471765373
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.357811462705989 to 0.324478128875951
0.636253428285978 to 0.60292009445594
0.549213063543698 to 0.51587972971366
-0.105358641022782 to -0.13869197485282
Changing layer 0's weights from 
-1.52449005247089 to -1.55782338630093
-1.89641650916073 to -1.92974984299077
-2.13076577485534 to -2.16409910868538
-2.09620652497741 to -2.12953985880745
-1.34833895565006 to -1.3816722894801
-2.03098090887996 to -2.06431424271
-1.87715118528339 to -1.91048451911343
-1.90460559130642 to -1.93793892513646
-2.14209745229217 to -2.17543078612221
-2.18584750742646 to -2.2191808412565
Changing layer 1's weights from 
-2.18075059504242 to -2.21408392887246
-1.57958191038105 to -1.61291524421109
-1.70528464914295 to -1.73861798297299
-1.7151899874303 to -1.74852332126034
-1.90024276972744 to -1.93357610355748
-2.03848060370418 to -2.07181393753422
-1.76776188016865 to -1.80109521399869
-2.15356490910979 to -2.18689824293983
-1.74190192819568 to -1.77523526202572
-2.10454879106017 to -2.13788212489021
Changing layer 2's weights from 
-1.29536055923435 to -1.32869389306439
-1.84941404462788 to -1.88274737845792
-1.56482475400898 to -1.59815808783902
-1.39362465740177 to -1.42695799123181
-1.57087736726734 to -1.60421070109738
-1.77688699365589 to -1.81022032748593
-1.25807427764866 to -1.29140761147869
-1.33049630523655 to -1.36382963906658
-2.06249459982844 to -2.09582793365848
-2.01753708125087 to -2.05087041508091
Changing layer 3's weights from 
-1.32841085792515 to -1.36174419175518
-1.59196394086811 to -1.62529727469815
-1.34935402275058 to -1.38268735658062
-2.25679325450672 to -2.29012658833676
-1.4720299184415 to -1.50536325227154
-1.60227852464649 to -1.63561185847653
-1.93246337176296 to -1.965796705593
-1.47395049930546 to -1.50728383313549
-1.8056464731786 to -1.83897980700864
-1.33281659484836 to -1.3661499286784
Changing layer 4's weights from 
-1.43209039569828 to -1.46542372952832
-1.64092319608662 to -1.67425652991666
-1.84140079618427 to -1.87473413001431
-1.55446606279346 to -1.5877993966235
-1.7937188685033 to -1.82705220233334
-1.56225364805195 to -1.59558698188199
-1.5666121780965 to -1.59994551192654
-2.02850970030757 to -2.06184303413761
-1.43481361270878 to -1.46814694653881
-1.86711555124256 to -1.9004488850726
Changing layer 5's weights from 
-1.2793105781171 to -1.31264391194714
-1.72138410211536 to -1.7547174359454
-1.29619573951694 to -1.32952907334698
-2.23170792088838 to -2.26504125471842
-1.69443910718891 to -1.72777244101895
-2.09290705026122 to -2.12624038409126
-1.79294174314472 to -1.82627507697476
-1.72049551607105 to -1.75382884990109
-1.33089351058933 to -1.36422684441937
-1.33740162254307 to -1.37073495637311
Trying to learn from memory 5, 0, -0.2
sum -3.74148961639305 distri -10.3666920215395
Using diff 7.56057480924468 and condRate 0.166666666666667
Changed category 0 weights from 
-1.04964520481487 to -1.30166436887841
-1.4496801688995 to -1.70169933296303
-1.73220086542031 to -1.98422002948384
-1.58069549945255 to -1.83271466351608
Changing layer 0's weights from 
-1.55782338630093 to -1.80984255036446
-1.92974984299077 to -2.1817690070543
-2.16409910868538 to -2.41611827274891
-2.12953985880745 to -2.38155902287098
-1.3816722894801 to -1.63369145354363
-2.06431424271 to -2.31633340677353
-1.91048451911343 to -2.16250368317696
-1.93793892513646 to -2.18995808919999
-2.17543078612221 to -2.42744995018574
-2.2191808412565 to -2.47120000532003
Changing layer 1's weights from 
-2.21408392887246 to -2.46610309293599
-1.61291524421109 to -1.86493440827462
-1.73861798297299 to -1.99063714703652
-1.74852332126034 to -2.00054248532387
-1.93357610355748 to -2.18559526762101
-2.07181393753422 to -2.32383310159775
-1.80109521399869 to -2.05311437806222
-2.18689824293983 to -2.43891740700336
-1.77523526202572 to -2.02725442608925
-2.13788212489021 to -2.38990128895374
Changing layer 2's weights from 
-1.32869389306439 to -1.58071305712792
-1.88274737845792 to -2.13476654252145
-1.59815808783902 to -1.85017725190255
-1.42695799123181 to -1.67897715529534
-1.60421070109738 to -1.85622986516091
-1.81022032748593 to -2.06223949154946
-1.29140761147869 to -1.54342677554223
-1.36382963906658 to -1.61584880313012
-2.09582793365848 to -2.34784709772201
-2.05087041508091 to -2.30288957914444
Changing layer 3's weights from 
-1.36174419175518 to -1.61376335581872
-1.62529727469815 to -1.87731643876168
-1.38268735658062 to -1.63470652064416
-2.29012658833676 to -2.54214575240029
-1.50536325227154 to -1.75738241633508
-1.63561185847653 to -1.88763102254006
-1.965796705593 to -2.21781586965653
-1.50728383313549 to -1.75930299719903
-1.83897980700864 to -2.09099897107217
-1.3661499286784 to -1.61816909274194
Changing layer 4's weights from 
-1.46542372952832 to -1.71744289359185
-1.67425652991666 to -1.92627569398019
-1.87473413001431 to -2.12675329407784
-1.5877993966235 to -1.83981856068704
-1.82705220233334 to -2.07907136639687
-1.59558698188199 to -1.84760614594552
-1.59994551192654 to -1.85196467599008
-2.06184303413761 to -2.31386219820114
-1.46814694653881 to -1.72016611060235
-1.9004488850726 to -2.15246804913613
Changing layer 5's weights from 
-1.31264391194714 to -1.56466307601067
-1.7547174359454 to -2.00673660000893
-1.32952907334698 to -1.58154823741051
-2.26504125471842 to -2.51706041878195
-1.72777244101895 to -1.97979160508248
-2.12624038409126 to -2.37825954815479
-1.82627507697476 to -2.07829424103829
-1.75382884990109 to -2.00584801396462
-1.36422684441937 to -1.6162460084829
-1.37073495637311 to -1.62275412043664
Trying to learn from memory 5, 0, -0.2
sum -3.74148961639305 distri -10.3666920215395
Using diff 7.56057480924468 and condRate 0.166666666666667
Changed category 0 weights from 
-1.30166436887841 to -1.55368353294194
-1.70169933296303 to -1.95371849702657
-1.98422002948384 to -2.23623919354738
-1.83271466351608 to -2.08473382757962
Changing layer 0's weights from 
-1.80984255036446 to -2.061861714428
-2.1817690070543 to -2.43378817111784
-2.41611827274891 to -2.66813743681245
-2.38155902287098 to -2.63357818693452
-1.63369145354363 to -1.88571061760717
-2.31633340677353 to -2.56835257083707
-2.16250368317696 to -2.4145228472405
-2.18995808919999 to -2.44197725326353
-2.42744995018574 to -2.67946911424928
-2.47120000532003 to -2.72321916938357
Changing layer 1's weights from 
-2.46610309293599 to -2.71812225699953
-1.86493440827462 to -2.11695357233816
-1.99063714703652 to -2.24265631110006
-2.00054248532387 to -2.25256164938741
-2.18559526762101 to -2.43761443168455
-2.32383310159775 to -2.57585226566129
-2.05311437806222 to -2.30513354212576
-2.43891740700336 to -2.6909365710669
-2.02725442608925 to -2.27927359015279
-2.38990128895374 to -2.64192045301728
Changing layer 2's weights from 
-1.58071305712792 to -1.83273222119146
-2.13476654252145 to -2.38678570658499
-1.85017725190255 to -2.10219641596608
-1.67897715529534 to -1.93099631935888
-1.85622986516091 to -2.10824902922445
-2.06223949154946 to -2.314258655613
-1.54342677554223 to -1.79544593960576
-1.61584880313012 to -1.86786796719365
-2.34784709772201 to -2.59986626178555
-2.30288957914444 to -2.55490874320798
Changing layer 3's weights from 
-1.61376335581872 to -1.86578251988225
-1.87731643876168 to -2.12933560282522
-1.63470652064416 to -1.88672568470769
-2.54214575240029 to -2.79416491646383
-1.75738241633508 to -2.00940158039861
-1.88763102254006 to -2.1396501866036
-2.21781586965653 to -2.46983503372007
-1.75930299719903 to -2.01132216126256
-2.09099897107217 to -2.34301813513571
-1.61816909274194 to -1.87018825680547
Changing layer 4's weights from 
-1.71744289359185 to -1.96946205765538
-1.92627569398019 to -2.17829485804373
-2.12675329407784 to -2.37877245814138
-1.83981856068704 to -2.09183772475057
-2.07907136639687 to -2.33109053046041
-1.84760614594552 to -2.09962531000905
-1.85196467599008 to -2.10398384005361
-2.31386219820114 to -2.56588136226468
-1.72016611060235 to -1.97218527466588
-2.15246804913613 to -2.40448721319967
Changing layer 5's weights from 
-1.56466307601067 to -1.81668224007421
-2.00673660000893 to -2.25875576407247
-1.58154823741051 to -1.83356740147405
-2.51706041878195 to -2.76907958284549
-1.97979160508248 to -2.23181076914602
-2.37825954815479 to -2.63027871221833
-2.07829424103829 to -2.33031340510183
-2.00584801396462 to -2.25786717802816
-1.6162460084829 to -1.86826517254644
-1.62275412043664 to -1.87477328450017
Trying to learn from memory 5, 2, -0.2
sum -3.74148961639305 distri 4.01004351703838
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.324478128875951 to 0.291144795045912
0.60292009445594 to 0.569586760625901
0.51587972971366 to 0.482546395883621
-0.13869197485282 to -0.172025308682859
Changing layer 0's weights from 
-2.061861714428 to -2.09519504825804
-2.43378817111784 to -2.46712150494787
-2.66813743681245 to -2.70147077064248
-2.63357818693452 to -2.66691152076455
-1.88571061760717 to -1.91904395143721
-2.56835257083707 to -2.6016859046671
-2.4145228472405 to -2.44785618107053
-2.44197725326353 to -2.47531058709356
-2.67946911424928 to -2.71280244807931
-2.72321916938357 to -2.7565525032136
Changing layer 1's weights from 
-2.71812225699953 to -2.75145559082956
-2.11695357233816 to -2.1502869061682
-2.24265631110006 to -2.2759896449301
-2.25256164938741 to -2.28589498321744
-2.43761443168455 to -2.47094776551458
-2.57585226566129 to -2.60918559949132
-2.30513354212576 to -2.33846687595579
-2.6909365710669 to -2.72426990489693
-2.27927359015279 to -2.31260692398283
-2.64192045301728 to -2.67525378684731
Changing layer 2's weights from 
-1.83273222119146 to -1.8660655550215
-2.38678570658499 to -2.42011904041502
-2.10219641596608 to -2.13552974979612
-1.93099631935888 to -1.96432965318891
-2.10824902922445 to -2.14158236305449
-2.314258655613 to -2.34759198944304
-1.79544593960576 to -1.8287792734358
-1.86786796719365 to -1.90120130102369
-2.59986626178555 to -2.63319959561558
-2.55490874320798 to -2.58824207703801
Changing layer 3's weights from 
-1.86578251988225 to -1.89911585371229
-2.12933560282522 to -2.16266893665526
-1.88672568470769 to -1.92005901853773
-2.79416491646383 to -2.82749825029386
-2.00940158039861 to -2.04273491422865
-2.1396501866036 to -2.17298352043364
-2.46983503372007 to -2.50316836755011
-2.01132216126256 to -2.0446554950926
-2.34301813513571 to -2.37635146896575
-1.87018825680547 to -1.90352159063551
Changing layer 4's weights from 
-1.96946205765538 to -2.00279539148542
-2.17829485804373 to -2.21162819187377
-2.37877245814138 to -2.41210579197142
-2.09183772475057 to -2.12517105858061
-2.33109053046041 to -2.36442386429044
-2.09962531000905 to -2.13295864383909
-2.10398384005361 to -2.13731717388365
-2.56588136226468 to -2.59921469609471
-1.97218527466588 to -2.00551860849592
-2.40448721319967 to -2.43782054702971
Changing layer 5's weights from 
-1.81668224007421 to -1.85001557390425
-2.25875576407247 to -2.29208909790251
-1.83356740147405 to -1.86690073530409
-2.76907958284549 to -2.80241291667552
-2.23181076914602 to -2.26514410297606
-2.63027871221833 to -2.66361204604836
-2.33031340510183 to -2.36364673893186
-2.25786717802816 to -2.29120051185819
-1.86826517254644 to -1.90159850637648
-1.87477328450017 to -1.90810661833021
Trying to learn from memory 6, 0, -0.2
sum -3.26077551954404 distri -8.87743824136772
Using diff 6.43185660170969 and condRate 0.166666666666667
Changed category 0 weights from 
-1.55368353294194 to -1.76807875619367
-1.95371849702657 to -2.1681137202783
-2.23623919354738 to -2.45063441679911
-2.08473382757962 to -2.29912905083135
Changing layer 0's weights from 
-2.09519504825804 to -2.30959027150976
-2.46712150494787 to -2.6815167281996
-2.70147077064248 to -2.91586599389421
-2.66691152076455 to -2.88130674401628
-1.91904395143721 to -2.13343917468893
-2.6016859046671 to -2.81608112791883
-2.44785618107053 to -2.66225140432226
-2.47531058709356 to -2.68970581034529
-2.71280244807931 to -2.92719767133104
-2.7565525032136 to -2.97094772646533
Changing layer 1's weights from 
-2.75145559082956 to -2.96585081408129
-2.1502869061682 to -2.36468212941992
-2.2759896449301 to -2.49038486818182
-2.28589498321744 to -2.50029020646917
-2.47094776551458 to -2.68534298876631
-2.60918559949132 to -2.82358082274305
-2.33846687595579 to -2.55286209920752
-2.72426990489693 to -2.93866512814866
-2.31260692398283 to -2.52700214723455
-2.67525378684731 to -2.88964901009904
Changing layer 2's weights from 
-1.8660655550215 to -2.08046077827322
-2.42011904041502 to -2.63451426366675
-2.13552974979612 to -2.34992497304785
-1.96432965318891 to -2.17872487644064
-2.14158236305449 to -2.35597758630621
-2.34759198944304 to -2.56198721269476
-1.8287792734358 to -2.04317449668753
-1.90120130102369 to -2.11559652427542
-2.63319959561558 to -2.84759481886731
-2.58824207703801 to -2.80263730028974
Changing layer 3's weights from 
-1.89911585371229 to -2.11351107696402
-2.16266893665526 to -2.37706415990698
-1.92005901853773 to -2.13445424178946
-2.82749825029386 to -3.04189347354559
-2.04273491422865 to -2.25713013748038
-2.17298352043364 to -2.38737874368536
-2.50316836755011 to -2.71756359080183
-2.0446554950926 to -2.25905071834433
-2.37635146896575 to -2.59074669221747
-1.90352159063551 to -2.11791681388724
Changing layer 4's weights from 
-2.00279539148542 to -2.21719061473715
-2.21162819187377 to -2.42602341512549
-2.41210579197142 to -2.62650101522314
-2.12517105858061 to -2.33956628183234
-2.36442386429044 to -2.57881908754217
-2.13295864383909 to -2.34735386709082
-2.13731717388365 to -2.35171239713538
-2.59921469609471 to -2.81360991934644
-2.00551860849592 to -2.21991383174765
-2.43782054702971 to -2.65221577028143
Changing layer 5's weights from 
-1.85001557390425 to -2.06441079715597
-2.29208909790251 to -2.50648432115423
-1.86690073530409 to -2.08129595855581
-2.80241291667552 to -3.01680813992725
-2.26514410297606 to -2.47953932622778
-2.66361204604836 to -2.87800726930009
-2.36364673893186 to -2.57804196218359
-2.29120051185819 to -2.50559573510992
-1.90159850637648 to -2.1159937296282
-1.90810661833021 to -2.12250184158194
Trying to learn from memory 7, 0, -0.2
sum -2.47936913873219 distri -7.50556294303881
Using diff 5.64603608898967 and condRate 0.166666666666667
Changed category 0 weights from 
-1.76807875619367 to -1.95627996196441
-2.1681137202783 to -2.35631492604903
-2.45063441679911 to -2.63883562256985
-2.29912905083135 to -2.48733025660209
Changing layer 0's weights from 
-2.30959027150976 to -2.4977914772805
-2.6815167281996 to -2.86971793397034
-2.91586599389421 to -3.10406719966495
-2.88130674401628 to -3.06950794978702
-2.13343917468893 to -2.32164038045967
-2.81608112791883 to -3.00428233368957
-2.66225140432226 to -2.850452610093
-2.68970581034529 to -2.87790701611603
-2.92719767133104 to -3.11539887710178
-2.97094772646533 to -3.15914893223607
Changing layer 1's weights from 
-2.96585081408129 to -3.15405201985203
-2.36468212941992 to -2.55288333519066
-2.49038486818182 to -2.67858607395256
-2.50029020646917 to -2.68849141223991
-2.68534298876631 to -2.87354419453705
-2.82358082274305 to -3.01178202851379
-2.55286209920752 to -2.74106330497826
-2.93866512814866 to -3.1268663339194
-2.52700214723455 to -2.71520335300529
-2.88964901009904 to -3.07785021586978
Changing layer 2's weights from 
-2.08046077827322 to -2.26866198404396
-2.63451426366675 to -2.82271546943749
-2.34992497304785 to -2.53812617881859
-2.17872487644064 to -2.36692608221138
-2.35597758630621 to -2.54417879207695
-2.56198721269476 to -2.7501884184655
-2.04317449668753 to -2.23137570245827
-2.11559652427542 to -2.30379773004616
-2.84759481886731 to -3.03579602463805
-2.80263730028974 to -2.99083850606048
Changing layer 3's weights from 
-2.11351107696402 to -2.30171228273476
-2.37706415990698 to -2.56526536567772
-2.13445424178946 to -2.3226554475602
-3.04189347354559 to -3.23009467931633
-2.25713013748038 to -2.44533134325111
-2.38737874368536 to -2.5755799494561
-2.71756359080183 to -2.90576479657257
-2.25905071834433 to -2.44725192411507
-2.59074669221747 to -2.77894789798821
-2.11791681388724 to -2.30611801965798
Changing layer 4's weights from 
-2.21719061473715 to -2.40539182050789
-2.42602341512549 to -2.61422462089623
-2.62650101522314 to -2.81470222099388
-2.33956628183234 to -2.52776748760307
-2.57881908754217 to -2.76702029331291
-2.34735386709082 to -2.53555507286156
-2.35171239713538 to -2.53991360290611
-2.81360991934644 to -3.00181112511718
-2.21991383174765 to -2.40811503751839
-2.65221577028143 to -2.84041697605217
Changing layer 5's weights from 
-2.06441079715597 to -2.25261200292671
-2.50648432115423 to -2.69468552692497
-2.08129595855581 to -2.26949716432655
-3.01680813992725 to -3.20500934569799
-2.47953932622778 to -2.66774053199852
-2.87800726930009 to -3.06620847507083
-2.57804196218359 to -2.76624316795433
-2.50559573510992 to -2.69379694088066
-2.1159937296282 to -2.30419493539894
-2.12250184158194 to -2.31070304735268
10/5/2016 11:13:23 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 7, 1, -0.2
sum -2.47936913873219 distri 2.00378615874804
Using diff 1 and condRate 0.166666666666667
Changed category 1 weights from 
0.289661421018457 to 0.256328087188418
0.522977008062219 to 0.48964367423218
0.0980517701681611 to 0.0647184363381224
0.102144582706308 to 0.0688112488762693
Changing layer 0's weights from 
-2.4977914772805 to -2.53112481111054
-2.86971793397034 to -2.90305126780038
-3.10406719966495 to -3.13740053349499
-3.06950794978702 to -3.10284128361706
-2.32164038045967 to -2.35497371428971
-3.00428233368957 to -3.03761566751961
-2.850452610093 to -2.88378594392304
-2.87790701611603 to -2.91124034994607
-3.11539887710178 to -3.14873221093182
-3.15914893223607 to -3.19248226606611
Changing layer 1's weights from 
-3.15405201985203 to -3.18738535368207
-2.55288333519066 to -2.5862166690207
-2.67858607395256 to -2.7119194077826
-2.68849141223991 to -2.72182474606995
-2.87354419453705 to -2.90687752836709
-3.01178202851379 to -3.04511536234383
-2.74106330497826 to -2.7743966388083
-3.1268663339194 to -3.16019966774944
-2.71520335300529 to -2.74853668683533
-3.07785021586978 to -3.11118354969982
Changing layer 2's weights from 
-2.26866198404396 to -2.301995317874
-2.82271546943749 to -2.85604880326753
-2.53812617881859 to -2.57145951264863
-2.36692608221138 to -2.40025941604142
-2.54417879207695 to -2.57751212590699
-2.7501884184655 to -2.78352175229554
-2.23137570245827 to -2.26470903628831
-2.30379773004616 to -2.3371310638762
-3.03579602463805 to -3.06912935846809
-2.99083850606048 to -3.02417183989052
Changing layer 3's weights from 
-2.30171228273476 to -2.3350456165648
-2.56526536567772 to -2.59859869950776
-2.3226554475602 to -2.35598878139024
-3.23009467931633 to -3.26342801314637
-2.44533134325111 to -2.47866467708115
-2.5755799494561 to -2.60891328328614
-2.90576479657257 to -2.93909813040261
-2.44725192411507 to -2.48058525794511
-2.77894789798821 to -2.81228123181825
-2.30611801965798 to -2.33945135348801
Changing layer 4's weights from 
-2.40539182050789 to -2.43872515433793
-2.61422462089623 to -2.64755795472627
-2.81470222099388 to -2.84803555482392
-2.52776748760307 to -2.56110082143311
-2.76702029331291 to -2.80035362714295
-2.53555507286156 to -2.5688884066916
-2.53991360290611 to -2.57324693673615
-3.00181112511718 to -3.03514445894722
-2.40811503751839 to -2.44144837134843
-2.84041697605217 to -2.87375030988221
Changing layer 5's weights from 
-2.25261200292671 to -2.28594533675675
-2.69468552692497 to -2.72801886075501
-2.26949716432655 to -2.30283049815659
-3.20500934569799 to -3.23834267952803
-2.66774053199852 to -2.70107386582856
-3.06620847507083 to -3.09954180890087
-2.76624316795433 to -2.79957650178437
-2.69379694088066 to -2.7271302747107
-2.30419493539894 to -2.33752826922898
-2.31070304735268 to -2.34403638118272
Trying to learn from memory 8, 0, -0.2
sum -2.25991770160383 distri -7.14996400485176
Using diff 5.45502572864888 and condRate 0.166666666666667
Changed category 0 weights from 
-1.95627996196441 to -2.13811415562891
-2.35631492604903 to -2.53814911971354
-2.63883562256985 to -2.82066981623435
-2.48733025660209 to -2.66916445026659
Changing layer 0's weights from 
-2.53112481111054 to -2.71295900477504
-2.90305126780038 to -3.08488546146488
-3.13740053349499 to -3.31923472715949
-3.10284128361706 to -3.28467547728156
-2.35497371428971 to -2.53680790795421
-3.03761566751961 to -3.21944986118411
-2.88378594392304 to -3.06562013758754
-2.91124034994607 to -3.09307454361057
-3.14873221093182 to -3.33056640459632
-3.19248226606611 to -3.37431645973061
Changing layer 1's weights from 
-3.18738535368207 to -3.36921954734657
-2.5862166690207 to -2.7680508626852
-2.7119194077826 to -2.8937536014471
-2.72182474606995 to -2.90365893973445
-2.90687752836709 to -3.08871172203159
-3.04511536234383 to -3.22694955600833
-2.7743966388083 to -2.9562308324728
-3.16019966774944 to -3.34203386141394
-2.74853668683533 to -2.93037088049983
-3.11118354969982 to -3.29301774336432
Changing layer 2's weights from 
-2.301995317874 to -2.4838295115385
-2.85604880326753 to -3.03788299693203
-2.57145951264863 to -2.75329370631313
-2.40025941604142 to -2.58209360970592
-2.57751212590699 to -2.75934631957149
-2.78352175229554 to -2.96535594596004
-2.26470903628831 to -2.44654322995281
-2.3371310638762 to -2.5189652575407
-3.06912935846809 to -3.25096355213259
-3.02417183989052 to -3.20600603355502
Changing layer 3's weights from 
-2.3350456165648 to -2.5168798102293
-2.59859869950776 to -2.78043289317226
-2.35598878139024 to -2.53782297505474
-3.26342801314637 to -3.44526220681087
-2.47866467708115 to -2.66049887074566
-2.60891328328614 to -2.79074747695064
-2.93909813040261 to -3.12093232406711
-2.48058525794511 to -2.66241945160961
-2.81228123181825 to -2.99411542548275
-2.33945135348801 to -2.52128554715252
Changing layer 4's weights from 
-2.43872515433793 to -2.62055934800243
-2.64755795472627 to -2.82939214839077
-2.84803555482392 to -3.02986974848842
-2.56110082143311 to -2.74293501509762
-2.80035362714295 to -2.98218782080745
-2.5688884066916 to -2.7507226003561
-2.57324693673615 to -2.75508113040066
-3.03514445894722 to -3.21697865261172
-2.44144837134843 to -2.62328256501293
-2.87375030988221 to -3.05558450354671
Changing layer 5's weights from 
-2.28594533675675 to -2.46777953042125
-2.72801886075501 to -2.90985305441951
-2.30283049815659 to -2.4846646918211
-3.23834267952803 to -3.42017687319253
-2.70107386582856 to -2.88290805949306
-3.09954180890087 to -3.28137600256537
-2.79957650178437 to -2.98141069544887
-2.7271302747107 to -2.9089644683752
-2.33752826922898 to -2.51936246289348
-2.34403638118272 to -2.52587057484722
Trying to learn from memory 9, 2, -0.2
sum -2.22200525250526 distri 2.92731590162201
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.291144795045912 to 0.257811461215873
0.569586760625901 to 0.536253426795862
0.482546395883621 to 0.449213062053582
-0.172025308682859 to -0.205358642512898
Changing layer 0's weights from 
-2.71295900477504 to -2.74629233860508
-3.08488546146488 to -3.11821879529492
-3.31923472715949 to -3.35256806098953
-3.28467547728156 to -3.3180088111116
-2.53680790795421 to -2.57014124178425
-3.21944986118411 to -3.25278319501415
-3.06562013758754 to -3.09895347141758
-3.09307454361057 to -3.12640787744061
-3.33056640459632 to -3.36389973842636
-3.37431645973061 to -3.40764979356065
Changing layer 1's weights from 
-3.36921954734657 to -3.40255288117661
-2.7680508626852 to -2.80138419651524
-2.8937536014471 to -2.92708693527714
-2.90365893973445 to -2.93699227356449
-3.08871172203159 to -3.12204505586163
-3.22694955600833 to -3.26028288983837
-2.9562308324728 to -2.98956416630284
-3.34203386141394 to -3.37536719524398
-2.93037088049983 to -2.96370421432987
-3.29301774336432 to -3.32635107719436
Changing layer 2's weights from 
-2.4838295115385 to -2.51716284536854
-3.03788299693203 to -3.07121633076207
-2.75329370631313 to -2.78662704014317
-2.58209360970592 to -2.61542694353596
-2.75934631957149 to -2.79267965340153
-2.96535594596004 to -2.99868927979008
-2.44654322995281 to -2.47987656378285
-2.5189652575407 to -2.55229859137074
-3.25096355213259 to -3.28429688596263
-3.20600603355502 to -3.23933936738506
Changing layer 3's weights from 
-2.5168798102293 to -2.55021314405934
-2.78043289317226 to -2.8137662270023
-2.53782297505474 to -2.57115630888478
-3.44526220681087 to -3.47859554064091
-2.66049887074566 to -2.6938322045757
-2.79074747695064 to -2.82408081078068
-3.12093232406711 to -3.15426565789715
-2.66241945160961 to -2.69575278543965
-2.99411542548275 to -3.02744875931279
-2.52128554715252 to -2.55461888098256
Changing layer 4's weights from 
-2.62055934800243 to -2.65389268183247
-2.82939214839077 to -2.86272548222081
-3.02986974848842 to -3.06320308231846
-2.74293501509762 to -2.77626834892765
-2.98218782080745 to -3.01552115463749
-2.7507226003561 to -2.78405593418614
-2.75508113040066 to -2.78841446423069
-3.21697865261172 to -3.25031198644176
-2.62328256501293 to -2.65661589884297
-3.05558450354671 to -3.08891783737675
Changing layer 5's weights from 
-2.46777953042125 to -2.50111286425129
-2.90985305441951 to -2.94318638824955
-2.4846646918211 to -2.51799802565113
-3.42017687319253 to -3.45351020702257
-2.88290805949306 to -2.9162413933231
-3.28137600256537 to -3.31470933639541
-2.98141069544887 to -3.01474402927891
-2.9089644683752 to -2.94229780220524
-2.51936246289348 to -2.55269579672352
-2.52587057484722 to -2.55920390867726
Trying to learn from memory 9, 1, -0.2
sum -2.22200525250526 distri 1.93931677222741
Using diff 1 and condRate 0.166666666666667
Changed category 1 weights from 
0.256328087188418 to 0.22299475335838
0.48964367423218 to 0.456310340402142
0.0647184363381224 to 0.0313851025080837
0.0688112488762693 to 0.0354779150462306
Changing layer 0's weights from 
-2.74629233860508 to -2.77962567243512
-3.11821879529492 to -3.15155212912496
-3.35256806098953 to -3.38590139481957
-3.3180088111116 to -3.35134214494164
-2.57014124178425 to -2.60347457561429
-3.25278319501415 to -3.28611652884419
-3.09895347141758 to -3.13228680524762
-3.12640787744061 to -3.15974121127065
-3.36389973842636 to -3.3972330722564
-3.40764979356065 to -3.44098312739069
Changing layer 1's weights from 
-3.40255288117661 to -3.43588621500665
-2.80138419651524 to -2.83471753034528
-2.92708693527714 to -2.96042026910718
-2.93699227356449 to -2.97032560739453
-3.12204505586163 to -3.15537838969167
-3.26028288983837 to -3.29361622366841
-2.98956416630284 to -3.02289750013288
-3.37536719524398 to -3.40870052907402
-2.96370421432987 to -2.99703754815991
-3.32635107719436 to -3.3596844110244
Changing layer 2's weights from 
-2.51716284536854 to -2.55049617919858
-3.07121633076207 to -3.10454966459211
-2.78662704014317 to -2.81996037397321
-2.61542694353596 to -2.648760277366
-2.79267965340153 to -2.82601298723157
-2.99868927979008 to -3.03202261362012
-2.47987656378285 to -2.51320989761289
-2.55229859137074 to -2.58563192520078
-3.28429688596263 to -3.31763021979267
-3.23933936738506 to -3.2726727012151
Changing layer 3's weights from 
-2.55021314405934 to -2.58354647788938
-2.8137662270023 to -2.84709956083234
-2.57115630888478 to -2.60448964271482
-3.47859554064091 to -3.51192887447095
-2.6938322045757 to -2.72716553840573
-2.82408081078068 to -2.85741414461072
-3.15426565789715 to -3.18759899172719
-2.69575278543965 to -2.72908611926969
-3.02744875931279 to -3.06078209314283
-2.55461888098256 to -2.58795221481259
Changing layer 4's weights from 
-2.65389268183247 to -2.68722601566251
-2.86272548222081 to -2.89605881605085
-3.06320308231846 to -3.0965364161485
-2.77626834892765 to -2.80960168275769
-3.01552115463749 to -3.04885448846753
-2.78405593418614 to -2.81738926801618
-2.78841446423069 to -2.82174779806073
-3.25031198644176 to -3.2836453202718
-2.65661589884297 to -2.68994923267301
-3.08891783737675 to -3.12225117120679
Changing layer 5's weights from 
-2.50111286425129 to -2.53444619808133
-2.94318638824955 to -2.97651972207959
-2.51799802565113 to -2.55133135948117
-3.45351020702257 to -3.48684354085261
-2.9162413933231 to -2.94957472715314
-3.31470933639541 to -3.34804267022545
-3.01474402927891 to -3.04807736310895
-2.94229780220524 to -2.97563113603528
-2.55269579672352 to -2.58602913055356
-2.55920390867726 to -2.5925372425073
Trying to learn from memory 9, 2, -0.2
sum -2.22200525250526 distri 2.92731590162201
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.257811461215873 to 0.224478127385834
0.536253426795862 to 0.502920092965824
0.449213062053582 to 0.415879728223544
-0.205358642512898 to -0.238691976342936
Changing layer 0's weights from 
-2.77962567243512 to -2.81295900626516
-3.15155212912496 to -3.184885462955
-3.38590139481957 to -3.41923472864961
-3.35134214494164 to -3.38467547877168
-2.60347457561429 to -2.63680790944433
-3.28611652884419 to -3.31944986267423
-3.13228680524762 to -3.16562013907766
-3.15974121127065 to -3.19307454510069
-3.3972330722564 to -3.43056640608644
-3.44098312739069 to -3.47431646122073
Changing layer 1's weights from 
-3.43588621500665 to -3.46921954883669
-2.83471753034528 to -2.86805086417532
-2.96042026910718 to -2.99375360293722
-2.97032560739453 to -3.00365894122457
-3.15537838969167 to -3.18871172352171
-3.29361622366841 to -3.32694955749845
-3.02289750013288 to -3.05623083396292
-3.40870052907402 to -3.44203386290406
-2.99703754815991 to -3.03037088198995
-3.3596844110244 to -3.39301774485444
Changing layer 2's weights from 
-2.55049617919858 to -2.58382951302862
-3.10454966459211 to -3.13788299842215
-2.81996037397321 to -2.85329370780325
-2.648760277366 to -2.68209361119604
-2.82601298723157 to -2.85934632106161
-3.03202261362012 to -3.06535594745016
-2.51320989761289 to -2.54654323144293
-2.58563192520078 to -2.61896525903082
-3.31763021979267 to -3.35096355362271
-3.2726727012151 to -3.30600603504514
Changing layer 3's weights from 
-2.58354647788938 to -2.61687981171942
-2.84709956083234 to -2.88043289466238
-2.60448964271482 to -2.63782297654485
-3.51192887447095 to -3.54526220830099
-2.72716553840573 to -2.76049887223577
-2.85741414461072 to -2.89074747844076
-3.18759899172719 to -3.22093232555723
-2.72908611926969 to -2.76241945309973
-3.06078209314283 to -3.09411542697287
-2.58795221481259 to -2.62128554864263
Changing layer 4's weights from 
-2.68722601566251 to -2.72055934949255
-2.89605881605085 to -2.92939214988089
-3.0965364161485 to -3.12986974997854
-2.80960168275769 to -2.84293501658773
-3.04885448846753 to -3.08218782229757
-2.81738926801618 to -2.85072260184622
-2.82174779806073 to -2.85508113189077
-3.2836453202718 to -3.31697865410184
-2.68994923267301 to -2.72328256650305
-3.12225117120679 to -3.15558450503683
Changing layer 5's weights from 
-2.53444619808133 to -2.56777953191137
-2.97651972207959 to -3.00985305590963
-2.55133135948117 to -2.58466469331121
-3.48684354085261 to -3.52017687468265
-2.94957472715314 to -2.98290806098318
-3.34804267022545 to -3.38137600405549
-3.04807736310895 to -3.08141069693899
-2.97563113603528 to -3.00896446986532
-2.58602913055356 to -2.6193624643836
-2.5925372425073 to -2.62587057633734
Trying to learn from memory 9, 2, -0.2
sum -2.22200525250526 distri 2.92731590162201
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.224478127385834 to 0.191144793555796
0.502920092965824 to 0.469586759135785
0.415879728223544 to 0.382546394393505
-0.238691976342936 to -0.272025310172975
Changing layer 0's weights from 
-2.81295900626516 to -2.8462923400952
-3.184885462955 to -3.21821879678504
-3.41923472864961 to -3.45256806247965
-3.38467547877168 to -3.41800881260172
-2.63680790944433 to -2.67014124327437
-3.31944986267423 to -3.35278319650427
-3.16562013907766 to -3.1989534729077
-3.19307454510069 to -3.22640787893073
-3.43056640608644 to -3.46389973991648
-3.47431646122073 to -3.50764979505077
Changing layer 1's weights from 
-3.46921954883669 to -3.50255288266673
-2.86805086417532 to -2.90138419800536
-2.99375360293722 to -3.02708693676726
-3.00365894122457 to -3.03699227505461
-3.18871172352171 to -3.22204505735175
-3.32694955749845 to -3.36028289132849
-3.05623083396292 to -3.08956416779296
-3.44203386290406 to -3.4753671967341
-3.03037088198995 to -3.06370421581999
-3.39301774485444 to -3.42635107868448
Changing layer 2's weights from 
-2.58382951302862 to -2.61716284685866
-3.13788299842215 to -3.17121633225219
-2.85329370780325 to -2.88662704163329
-2.68209361119604 to -2.71542694502608
-2.85934632106161 to -2.89267965489165
-3.06535594745016 to -3.0986892812802
-2.54654323144293 to -2.57987656527296
-2.61896525903082 to -2.65229859286085
-3.35096355362271 to -3.38429688745275
-3.30600603504514 to -3.33933936887518
Changing layer 3's weights from 
-2.61687981171942 to -2.65021314554945
-2.88043289466238 to -2.91376622849242
-2.63782297654485 to -2.67115631037489
-3.54526220830099 to -3.57859554213103
-2.76049887223577 to -2.79383220606581
-2.89074747844076 to -2.9240808122708
-3.22093232555723 to -3.25426565938727
-2.76241945309973 to -2.79575278692976
-3.09411542697287 to -3.12744876080291
-2.62128554864263 to -2.65461888247267
Changing layer 4's weights from 
-2.72055934949255 to -2.75389268332259
-2.92939214988089 to -2.96272548371093
-3.12986974997854 to -3.16320308380858
-2.84293501658773 to -2.87626835041777
-3.08218782229757 to -3.11552115612761
-2.85072260184622 to -2.88405593567625
-2.85508113189077 to -2.88841446572081
-3.31697865410184 to -3.35031198793188
-2.72328256650305 to -2.75661590033308
-3.15558450503683 to -3.18891783886687
Changing layer 5's weights from 
-2.56777953191137 to -2.60111286574141
-3.00985305590963 to -3.04318638973967
-2.58466469331121 to -2.61799802714125
-3.52017687468265 to -3.55351020851269
-2.98290806098318 to -3.01624139481322
-3.38137600405549 to -3.41470933788553
-3.08141069693899 to -3.11474403076903
-3.00896446986532 to -3.04229780369536
-2.6193624643836 to -2.65269579821364
-2.62587057633734 to -2.65920391016738
Trying to learn from memory 9, 1, -0.2
sum -2.22200525250526 distri 1.93931677222741
Using diff 1 and condRate 0.166666666666667
Changed category 1 weights from 
0.22299475335838 to 0.189661419528341
0.456310340402142 to 0.422977006572103
0.0313851025080837 to -0.001948231321955
0.0354779150462306 to 0.00214458121619189
Changing layer 0's weights from 
-2.8462923400952 to -2.87962567392524
-3.21821879678504 to -3.25155213061508
-3.45256806247965 to -3.48590139630969
-3.41800881260172 to -3.45134214643176
-2.67014124327437 to -2.70347457710441
-3.35278319650427 to -3.38611653033431
-3.1989534729077 to -3.23228680673774
-3.22640787893073 to -3.25974121276077
-3.46389973991648 to -3.49723307374652
-3.50764979505077 to -3.54098312888081
Changing layer 1's weights from 
-3.50255288266673 to -3.53588621649677
-2.90138419800536 to -2.9347175318354
-3.02708693676726 to -3.0604202705973
-3.03699227505461 to -3.07032560888465
-3.22204505735175 to -3.25537839118179
-3.36028289132849 to -3.39361622515853
-3.08956416779296 to -3.122897501623
-3.4753671967341 to -3.50870053056414
-3.06370421581999 to -3.09703754965003
-3.42635107868448 to -3.45968441251452
Changing layer 2's weights from 
-2.61716284685866 to -2.6504961806887
-3.17121633225219 to -3.20454966608223
-2.88662704163329 to -2.91996037546332
-2.71542694502608 to -2.74876027885612
-2.89267965489165 to -2.92601298872169
-3.0986892812802 to -3.13202261511024
-2.57987656527296 to -2.613209899103
-2.65229859286085 to -2.68563192669089
-3.38429688745275 to -3.41763022128279
-3.33933936887518 to -3.37267270270522
Changing layer 3's weights from 
-2.65021314554945 to -2.68354647937949
-2.91376622849242 to -2.94709956232246
-2.67115631037489 to -2.70448964420493
-3.57859554213103 to -3.61192887596107
-2.79383220606581 to -2.82716553989585
-2.9240808122708 to -2.95741414610084
-3.25426565938727 to -3.28759899321731
-2.79575278692976 to -2.8290861207598
-3.12744876080291 to -3.16078209463295
-2.65461888247267 to -2.68795221630271
Changing layer 4's weights from 
-2.75389268332259 to -2.78722601715262
-2.96272548371093 to -2.99605881754097
-3.16320308380858 to -3.19653641763862
-2.87626835041777 to -2.90960168424781
-3.11552115612761 to -3.14885448995765
-2.88405593567625 to -2.91738926950629
-2.88841446572081 to -2.92174779955085
-3.35031198793188 to -3.38364532176192
-2.75661590033308 to -2.78994923416312
-3.18891783886687 to -3.22225117269691
Changing layer 5's weights from 
-2.60111286574141 to -2.63444619957145
-3.04318638973967 to -3.07651972356971
-2.61799802714125 to -2.65133136097129
-3.55351020851269 to -3.58684354234273
-3.01624139481322 to -3.04957472864326
-3.41470933788553 to -3.44804267171557
-3.11474403076903 to -3.14807736459907
-3.04229780369536 to -3.0756311375254
-2.65269579821364 to -2.68602913204368
-2.65920391016738 to -2.69253724399741
Trying to learn from memory 9, 1, -0.2
sum -2.22200525250526 distri 1.93931677222741
Using diff 1 and condRate 0.166666666666667
Changed category 1 weights from 
0.189661419528341 to 0.156328085698302
0.422977006572103 to 0.389643672742064
-0.001948231321955 to -0.0352815651519937
0.00214458121619189 to -0.0311887526138468
Changing layer 0's weights from 
-2.87962567392524 to -2.91295900775528
-3.25155213061508 to -3.28488546444511
-3.48590139630969 to -3.51923473013972
-3.45134214643176 to -3.48467548026179
-2.70347457710441 to -2.73680791093445
-3.38611653033431 to -3.41944986416434
-3.23228680673774 to -3.26562014056777
-3.25974121276077 to -3.2930745465908
-3.49723307374652 to -3.53056640757655
-3.54098312888081 to -3.57431646271084
Changing layer 1's weights from 
-3.53588621649677 to -3.5692195503268
-2.9347175318354 to -2.96805086566543
-3.0604202705973 to -3.09375360442733
-3.07032560888465 to -3.10365894271468
-3.25537839118179 to -3.28871172501182
-3.39361622515853 to -3.42694955898856
-3.122897501623 to -3.15623083545303
-3.50870053056414 to -3.54203386439417
-3.09703754965003 to -3.13037088348006
-3.45968441251452 to -3.49301774634455
Changing layer 2's weights from 
-2.6504961806887 to -2.68382951451874
-3.20454966608223 to -3.23788299991226
-2.91996037546332 to -2.95329370929336
-2.74876027885612 to -2.78209361268615
-2.92601298872169 to -2.95934632255172
-3.13202261511024 to -3.16535594894027
-2.613209899103 to -2.64654323293304
-2.68563192669089 to -2.71896526052093
-3.41763022128279 to -3.45096355511282
-3.37267270270522 to -3.40600603653525
Changing layer 3's weights from 
-2.68354647937949 to -2.71687981320953
-2.94709956232246 to -2.98043289615249
-2.70448964420493 to -2.73782297803497
-3.61192887596107 to -3.6452622097911
-2.82716553989585 to -2.86049887372589
-2.95741414610084 to -2.99074747993087
-3.28759899321731 to -3.32093232704734
-2.8290861207598 to -2.86241945458984
-3.16078209463295 to -3.19411542846298
-2.68795221630271 to -2.72128555013275
Changing layer 4's weights from 
-2.78722601715262 to -2.82055935098266
-2.99605881754097 to -3.029392151371
-3.19653641763862 to -3.22986975146865
-2.90960168424781 to -2.94293501807785
-3.14885448995765 to -3.18218782378768
-2.91738926950629 to -2.95072260333633
-2.92174779955085 to -2.95508113338089
-3.38364532176192 to -3.41697865559195
-2.78994923416312 to -2.82328256799316
-3.22225117269691 to -3.25558450652694
Changing layer 5's weights from 
-2.63444619957145 to -2.66777953340149
-3.07651972356971 to -3.10985305739974
-2.65133136097129 to -2.68466469480133
-3.58684354234273 to -3.62017687617276
-3.04957472864326 to -3.08290806247329
-3.44804267171557 to -3.4813760055456
-3.14807736459907 to -3.1814106984291
-3.0756311375254 to -3.10896447135543
-2.68602913204368 to -2.71936246587372
-2.69253724399741 to -2.72587057782745
Trying to learn from memory 9, 0, -0.2
sum -2.22200525250526 distri -7.08863792635468
Using diff 5.42213398697573 and condRate 0.166666666666667
Changed category 0 weights from 
-2.13811415562891 to -2.31885195788797
-2.53814911971354 to -2.7188869219726
-2.82066981623435 to -3.00140761849341
-2.66916445026659 to -2.84990225252565
Changing layer 0's weights from 
-2.91295900775528 to -3.09369681001434
-3.28488546444511 to -3.46562326670418
-3.51923473013972 to -3.69997253239878
-3.48467548026179 to -3.66541328252085
-2.73680791093445 to -2.91754571319351
-3.41944986416434 to -3.6001876664234
-3.26562014056777 to -3.44635794282684
-3.2930745465908 to -3.47381234884987
-3.53056640757655 to -3.71130420983561
-3.57431646271084 to -3.7550542649699
Changing layer 1's weights from 
-3.5692195503268 to -3.74995735258586
-2.96805086566543 to -3.1487886679245
-3.09375360442733 to -3.2744914066864
-3.10365894271468 to -3.28439674497375
-3.28871172501182 to -3.46944952727089
-3.42694955898856 to -3.60768736124762
-3.15623083545303 to -3.3369686377121
-3.54203386439417 to -3.72277166665323
-3.13037088348006 to -3.31110868573913
-3.49301774634455 to -3.67375554860361
Changing layer 2's weights from 
-2.68382951451874 to -2.8645673167778
-3.23788299991226 to -3.41862080217133
-2.95329370929336 to -3.13403151155242
-2.78209361268615 to -2.96283141494521
-2.95934632255172 to -3.14008412481079
-3.16535594894027 to -3.34609375119934
-2.64654323293304 to -2.8272810351921
-2.71896526052093 to -2.89970306277999
-3.45096355511282 to -3.63170135737188
-3.40600603653525 to -3.58674383879431
Changing layer 3's weights from 
-2.71687981320953 to -2.89761761546859
-2.98043289615249 to -3.16117069841156
-2.73782297803497 to -2.91856078029403
-3.6452622097911 to -3.82600001205016
-2.86049887372589 to -3.04123667598495
-2.99074747993087 to -3.17148528218994
-3.32093232704734 to -3.50167012930641
-2.86241945458984 to -3.0431572568489
-3.19411542846298 to -3.37485323072205
-2.72128555013275 to -2.90202335239181
Changing layer 4's weights from 
-2.82055935098266 to -3.00129715324172
-3.029392151371 to -3.21012995363007
-3.22986975146865 to -3.41060755372772
-2.94293501807785 to -3.12367282033691
-3.18218782378768 to -3.36292562604675
-2.95072260333633 to -3.13146040559539
-2.95508113338089 to -3.13581893563995
-3.41697865559195 to -3.59771645785101
-2.82328256799316 to -3.00402037025222
-3.25558450652694 to -3.43632230878601
Changing layer 5's weights from 
-2.66777953340149 to -2.84851733566055
-3.10985305739974 to -3.29059085965881
-2.68466469480133 to -2.86540249706039
-3.62017687617276 to -3.80091467843182
-3.08290806247329 to -3.26364586473236
-3.4813760055456 to -3.66211380780466
-3.1814106984291 to -3.36214850068817
-3.10896447135543 to -3.2897022736145
-2.71936246587372 to -2.90010026813278
-2.72587057782745 to -2.90660838008651
Trying to learn from memory 9, 0, -0.2
sum -2.22200525250526 distri -7.08863792635468
Using diff 5.42213398697573 and condRate 0.166666666666667
Changed category 0 weights from 
-2.31885195788797 to -2.49958976014703
-2.7188869219726 to -2.89962472423166
-3.00140761849341 to -3.18214542075247
-2.84990225252565 to -3.03064005478471
Changing layer 0's weights from 
-3.09369681001434 to -3.2744346122734
-3.46562326670418 to -3.64636106896324
-3.69997253239878 to -3.88071033465785
-3.66541328252085 to -3.84615108477992
-2.91754571319351 to -3.09828351545257
-3.6001876664234 to -3.78092546868247
-3.44635794282684 to -3.6270957450859
-3.47381234884987 to -3.65455015110893
-3.71130420983561 to -3.89204201209468
-3.7550542649699 to -3.93579206722897
Changing layer 1's weights from 
-3.74995735258586 to -3.93069515484493
-3.1487886679245 to -3.32952647018356
-3.2744914066864 to -3.45522920894546
-3.28439674497375 to -3.46513454723281
-3.46944952727089 to -3.65018732952995
-3.60768736124762 to -3.78842516350669
-3.3369686377121 to -3.51770643997116
-3.72277166665323 to -3.9035094689123
-3.31110868573913 to -3.49184648799819
-3.67375554860361 to -3.85449335086268
Changing layer 2's weights from 
-2.8645673167778 to -3.04530511903686
-3.41862080217133 to -3.59935860443039
-3.13403151155242 to -3.31476931381148
-2.96283141494521 to -3.14356921720428
-3.14008412481079 to -3.32082192706985
-3.34609375119934 to -3.5268315534584
-2.8272810351921 to -3.00801883745116
-2.89970306277999 to -3.08044086503905
-3.63170135737188 to -3.81243915963095
-3.58674383879431 to -3.76748164105338
Changing layer 3's weights from 
-2.89761761546859 to -3.07835541772765
-3.16117069841156 to -3.34190850067062
-2.91856078029403 to -3.09929858255309
-3.82600001205016 to -4.00673781430923
-3.04123667598495 to -3.22197447824401
-3.17148528218994 to -3.352223084449
-3.50167012930641 to -3.68240793156547
-3.0431572568489 to -3.22389505910796
-3.37485323072205 to -3.55559103298111
-2.90202335239181 to -3.08276115465087
Changing layer 4's weights from 
-3.00129715324172 to -3.18203495550078
-3.21012995363007 to -3.39086775588913
-3.41060755372772 to -3.59134535598678
-3.12367282033691 to -3.30441062259597
-3.36292562604675 to -3.54366342830581
-3.13146040559539 to -3.31219820785445
-3.13581893563995 to -3.31655673789901
-3.59771645785101 to -3.77845426011008
-3.00402037025222 to -3.18475817251128
-3.43632230878601 to -3.61706011104507
Changing layer 5's weights from 
-2.84851733566055 to -3.02925513791961
-3.29059085965881 to -3.47132866191787
-2.86540249706039 to -3.04614029931945
-3.80091467843182 to -3.98165248069089
-3.26364586473236 to -3.44438366699142
-3.66211380780466 to -3.84285161006373
-3.36214850068817 to -3.54288630294723
-3.2897022736145 to -3.47044007587356
-2.90010026813278 to -3.08083807039184
-2.90660838008651 to -3.08734618234557
10/5/2016 11:13:23 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 9, 2, -0.2
sum -2.22200525250526 distri 2.92731590162201
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.191144793555796 to 0.157811459725757
0.469586759135785 to 0.436253425305746
0.382546394393505 to 0.349213060563466
-0.272025310172975 to -0.305358644003014
Changing layer 0's weights from 
-3.2744346122734 to -3.30776794610344
-3.64636106896324 to -3.67969440279327
-3.88071033465785 to -3.91404366848788
-3.84615108477992 to -3.87948441860995
-3.09828351545257 to -3.1316168492826
-3.78092546868247 to -3.8142588025125
-3.6270957450859 to -3.66042907891593
-3.65455015110893 to -3.68788348493896
-3.89204201209468 to -3.92537534592471
-3.93579206722897 to -3.969125401059
Changing layer 1's weights from 
-3.93069515484493 to -3.96402848867496
-3.32952647018356 to -3.36285980401359
-3.45522920894546 to -3.48856254277549
-3.46513454723281 to -3.49846788106284
-3.65018732952995 to -3.68352066335998
-3.78842516350669 to -3.82175849733672
-3.51770643997116 to -3.55103977380119
-3.9035094689123 to -3.93684280274233
-3.49184648799819 to -3.52517982182822
-3.85449335086268 to -3.88782668469271
Changing layer 2's weights from 
-3.04530511903686 to -3.07863845286689
-3.59935860443039 to -3.63269193826042
-3.31476931381148 to -3.34810264764152
-3.14356921720428 to -3.17690255103431
-3.32082192706985 to -3.35415526089988
-3.5268315534584 to -3.56016488728843
-3.00801883745116 to -3.0413521712812
-3.08044086503905 to -3.11377419886909
-3.81243915963095 to -3.84577249346098
-3.76748164105338 to -3.80081497488341
Changing layer 3's weights from 
-3.07835541772765 to -3.11168875155769
-3.34190850067062 to -3.37524183450065
-3.09929858255309 to -3.13263191638313
-4.00673781430923 to -4.04007114813926
-3.22197447824401 to -3.25530781207405
-3.352223084449 to -3.38555641827903
-3.68240793156547 to -3.7157412653955
-3.22389505910796 to -3.257228392938
-3.55559103298111 to -3.58892436681114
-3.08276115465087 to -3.11609448848091
Changing layer 4's weights from 
-3.18203495550078 to -3.21536828933082
-3.39086775588913 to -3.42420108971916
-3.59134535598678 to -3.62467868981681
-3.30441062259597 to -3.33774395642601
-3.54366342830581 to -3.57699676213584
-3.31219820785445 to -3.34553154168449
-3.31655673789901 to -3.34989007172905
-3.77845426011008 to -3.81178759394011
-3.18475817251128 to -3.21809150634132
-3.61706011104507 to -3.6503934448751
Changing layer 5's weights from 
-3.02925513791961 to -3.06258847174965
-3.47132866191787 to -3.5046619957479
-3.04614029931945 to -3.07947363314949
-3.98165248069089 to -4.01498581452092
-3.44438366699142 to -3.47771700082145
-3.84285161006373 to -3.87618494389376
-3.54288630294723 to -3.57621963677726
-3.47044007587356 to -3.50377340970359
-3.08083807039184 to -3.11417140422187
-3.08734618234557 to -3.12067951617561
Trying to learn from memory 9, 1, -0.2
sum -2.22200525250526 distri 1.93931677222741
Using diff 1 and condRate 0.166666666666667
Changed category 1 weights from 
0.156328085698302 to 0.122994751868263
0.389643672742064 to 0.356310338912025
-0.0352815651519937 to -0.0686148989820324
-0.0311887526138468 to -0.0645220864438855
Changing layer 0's weights from 
-3.30776794610344 to -3.34110127993347
-3.67969440279327 to -3.71302773662331
-3.91404366848788 to -3.94737700231792
-3.87948441860995 to -3.91281775243999
-3.1316168492826 to -3.16495018311264
-3.8142588025125 to -3.84759213634254
-3.66042907891593 to -3.69376241274597
-3.68788348493896 to -3.721216818769
-3.92537534592471 to -3.95870867975475
-3.969125401059 to -4.00245873488904
Changing layer 1's weights from 
-3.96402848867496 to -3.997361822505
-3.36285980401359 to -3.39619313784363
-3.48856254277549 to -3.52189587660553
-3.49846788106284 to -3.53180121489288
-3.68352066335998 to -3.71685399719002
-3.82175849733672 to -3.85509183116676
-3.55103977380119 to -3.58437310763123
-3.93684280274233 to -3.97017613657237
-3.52517982182822 to -3.55851315565826
-3.88782668469271 to -3.92116001852275
Changing layer 2's weights from 
-3.07863845286689 to -3.11197178669693
-3.63269193826042 to -3.66602527209046
-3.34810264764152 to -3.38143598147156
-3.17690255103431 to -3.21023588486435
-3.35415526089988 to -3.38748859472992
-3.56016488728843 to -3.59349822111847
-3.0413521712812 to -3.07468550511124
-3.11377419886909 to -3.14710753269913
-3.84577249346098 to -3.87910582729102
-3.80081497488341 to -3.83414830871345
Changing layer 3's weights from 
-3.11168875155769 to -3.14502208538773
-3.37524183450065 to -3.40857516833069
-3.13263191638313 to -3.16596525021317
-4.04007114813926 to -4.0734044819693
-3.25530781207405 to -3.28864114590409
-3.38555641827903 to -3.41888975210907
-3.7157412653955 to -3.74907459922554
-3.257228392938 to -3.29056172676804
-3.58892436681114 to -3.62225770064118
-3.11609448848091 to -3.14942782231095
Changing layer 4's weights from 
-3.21536828933082 to -3.24870162316086
-3.42420108971916 to -3.4575344235492
-3.62467868981681 to -3.65801202364685
-3.33774395642601 to -3.37107729025605
-3.57699676213584 to -3.61033009596588
-3.34553154168449 to -3.37886487551453
-3.34989007172905 to -3.38322340555909
-3.81178759394011 to -3.84512092777015
-3.21809150634132 to -3.25142484017136
-3.6503934448751 to -3.68372677870514
Changing layer 5's weights from 
-3.06258847174965 to -3.09592180557968
-3.5046619957479 to -3.53799532957794
-3.07947363314949 to -3.11280696697953
-4.01498581452092 to -4.04831914835096
-3.47771700082145 to -3.51105033465149
-3.87618494389376 to -3.9095182777238
-3.57621963677726 to -3.6095529706073
-3.50377340970359 to -3.53710674353363
-3.11417140422187 to -3.14750473805191
-3.12067951617561 to -3.15401285000565
Trying to learn from memory 9, 0, -0.2
sum -2.22200525250526 distri -7.08863792635468
Using diff 5.42213398697573 and condRate 0.166666666666667
Changed category 0 weights from 
-2.49958976014703 to -2.68032756240609
-2.89962472423166 to -3.08036252649072
-3.18214542075247 to -3.36288322301153
-3.03064005478471 to -3.21137785704377
Changing layer 0's weights from 
-3.34110127993347 to -3.52183908219253
-3.71302773662331 to -3.89376553888237
-3.94737700231792 to -4.12811480457698
-3.91281775243999 to -4.09355555469905
-3.16495018311264 to -3.3456879853717
-3.84759213634254 to -4.0283299386016
-3.69376241274597 to -3.87450021500503
-3.721216818769 to -3.90195462102806
-3.95870867975475 to -4.13944648201381
-4.00245873488904 to -4.1831965371481
Changing layer 1's weights from 
-3.997361822505 to -4.17809962476406
-3.39619313784363 to -3.57693094010269
-3.52189587660553 to -3.70263367886459
-3.53180121489288 to -3.71253901715194
-3.71685399719002 to -3.89759179944908
-3.85509183116676 to -4.03582963342582
-3.58437310763123 to -3.76511090989029
-3.97017613657237 to -4.15091393883143
-3.55851315565826 to -3.73925095791732
-3.92116001852275 to -4.10189782078181
Changing layer 2's weights from 
-3.11197178669693 to -3.29270958895599
-3.66602527209046 to -3.84676307434952
-3.38143598147156 to -3.56217378373062
-3.21023588486435 to -3.39097368712341
-3.38748859472992 to -3.56822639698898
-3.59349822111847 to -3.77423602337753
-3.07468550511124 to -3.2554233073703
-3.14710753269913 to -3.32784533495819
-3.87910582729102 to -4.05984362955008
-3.83414830871345 to -4.01488611097251
Changing layer 3's weights from 
-3.14502208538773 to -3.32575988764679
-3.40857516833069 to -3.58931297058975
-3.16596525021317 to -3.34670305247223
-4.0734044819693 to -4.25414228422836
-3.28864114590409 to -3.46937894816315
-3.41888975210907 to -3.59962755436813
-3.74907459922554 to -3.9298124014846
-3.29056172676804 to -3.4712995290271
-3.62225770064118 to -3.80299550290024
-3.14942782231095 to -3.33016562457001
Changing layer 4's weights from 
-3.24870162316086 to -3.42943942541992
-3.4575344235492 to -3.63827222580826
-3.65801202364685 to -3.83874982590591
-3.37107729025605 to -3.55181509251511
-3.61033009596588 to -3.79106789822494
-3.37886487551453 to -3.55960267777359
-3.38322340555909 to -3.56396120781815
-3.84512092777015 to -4.02585873002921
-3.25142484017136 to -3.43216264243042
-3.68372677870514 to -3.8644645809642
Changing layer 5's weights from 
-3.09592180557968 to -3.27665960783875
-3.53799532957794 to -3.718733131837
-3.11280696697953 to -3.29354476923859
-4.04831914835096 to -4.22905695061002
-3.51105033465149 to -3.69178813691055
-3.9095182777238 to -4.09025607998286
-3.6095529706073 to -3.79029077286636
-3.53710674353363 to -3.71784454579269
-3.14750473805191 to -3.32824254031097
-3.15401285000565 to -3.33475065226471
Trying to learn from memory 9, 2, -0.2
sum -2.22200525250526 distri 2.92731590162201
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.157811459725757 to 0.124478125895718
0.436253425305746 to 0.402920091475707
0.349213060563466 to 0.315879726733427
-0.305358644003014 to -0.338691977833053
Changing layer 0's weights from 
-3.52183908219253 to -3.55517241602257
-3.89376553888237 to -3.92709887271241
-4.12811480457698 to -4.16144813840702
-4.09355555469905 to -4.12688888852909
-3.3456879853717 to -3.37902131920174
-4.0283299386016 to -4.06166327243164
-3.87450021500503 to -3.90783354883507
-3.90195462102806 to -3.9352879548581
-4.13944648201381 to -4.17277981584385
-4.1831965371481 to -4.21652987097814
Changing layer 1's weights from 
-4.17809962476406 to -4.2114329585941
-3.57693094010269 to -3.61026427393273
-3.70263367886459 to -3.73596701269463
-3.71253901715194 to -3.74587235098198
-3.89759179944908 to -3.93092513327912
-4.03582963342582 to -4.06916296725586
-3.76511090989029 to -3.79844424372033
-4.15091393883143 to -4.18424727266147
-3.73925095791732 to -3.77258429174736
-4.10189782078181 to -4.13523115461185
Changing layer 2's weights from 
-3.29270958895599 to -3.32604292278603
-3.84676307434952 to -3.88009640817956
-3.56217378373062 to -3.59550711756066
-3.39097368712341 to -3.42430702095345
-3.56822639698898 to -3.60155973081902
-3.77423602337753 to -3.80756935720757
-3.2554233073703 to -3.28875664120034
-3.32784533495819 to -3.36117866878823
-4.05984362955008 to -4.09317696338012
-4.01488611097251 to -4.04821944480255
Changing layer 3's weights from 
-3.32575988764679 to -3.35909322147683
-3.58931297058975 to -3.62264630441979
-3.34670305247223 to -3.38003638630227
-4.25414228422836 to -4.2874756180584
-3.46937894816315 to -3.50271228199319
-3.59962755436813 to -3.63296088819817
-3.9298124014846 to -3.96314573531464
-3.4712995290271 to -3.50463286285714
-3.80299550290024 to -3.83632883673028
-3.33016562457001 to -3.36349895840005
Changing layer 4's weights from 
-3.42943942541992 to -3.46277275924996
-3.63827222580826 to -3.6716055596383
-3.83874982590591 to -3.87208315973595
-3.55181509251511 to -3.58514842634515
-3.79106789822494 to -3.82440123205498
-3.55960267777359 to -3.59293601160363
-3.56396120781815 to -3.59729454164819
-4.02585873002921 to -4.05919206385925
-3.43216264243042 to -3.46549597626046
-3.8644645809642 to -3.89779791479424
Changing layer 5's weights from 
-3.27665960783875 to -3.30999294166878
-3.718733131837 to -3.75206646566704
-3.29354476923859 to -3.32687810306862
-4.22905695061002 to -4.26239028444006
-3.69178813691055 to -3.72512147074059
-4.09025607998286 to -4.1235894138129
-3.79029077286636 to -3.8236241066964
-3.71784454579269 to -3.75117787962273
-3.32824254031097 to -3.36157587414101
-3.33475065226471 to -3.36808398609475
Trying to learn from memory 9, 0, -0.2
sum -2.22200525250526 distri -7.08863792635468
Using diff 5.42213398697573 and condRate 0.166666666666667
Changed category 0 weights from 
-2.68032756240609 to -2.86106536466515
-3.08036252649072 to -3.26110032874978
-3.36288322301153 to -3.54362102527059
-3.21137785704377 to -3.39211565930283
Changing layer 0's weights from 
-3.55517241602257 to -3.73591021828163
-3.92709887271241 to -4.10783667497147
-4.16144813840702 to -4.34218594066608
-4.12688888852909 to -4.30762669078815
-3.37902131920174 to -3.5597591214608
-4.06166327243164 to -4.2424010746907
-3.90783354883507 to -4.08857135109413
-3.9352879548581 to -4.11602575711716
-4.17277981584385 to -4.35351761810291
-4.21652987097814 to -4.3972676732372
Changing layer 1's weights from 
-4.2114329585941 to -4.39217076085316
-3.61026427393273 to -3.79100207619179
-3.73596701269463 to -3.91670481495369
-3.74587235098198 to -3.92661015324104
-3.93092513327912 to -4.11166293553818
-4.06916296725586 to -4.24990076951492
-3.79844424372033 to -3.97918204597939
-4.18424727266147 to -4.36498507492053
-3.77258429174736 to -3.95332209400642
-4.13523115461185 to -4.31596895687091
Changing layer 2's weights from 
-3.32604292278603 to -3.50678072504509
-3.88009640817956 to -4.06083421043862
-3.59550711756066 to -3.77624491981972
-3.42430702095345 to -3.60504482321251
-3.60155973081902 to -3.78229753307808
-3.80756935720757 to -3.98830715946663
-3.28875664120034 to -3.4694944434594
-3.36117866878823 to -3.54191647104729
-4.09317696338012 to -4.27391476563918
-4.04821944480255 to -4.22895724706161
Changing layer 3's weights from 
-3.35909322147683 to -3.53983102373589
-3.62264630441979 to -3.80338410667885
-3.38003638630227 to -3.56077418856133
-4.2874756180584 to -4.46821342031746
-3.50271228199319 to -3.68345008425225
-3.63296088819817 to -3.81369869045723
-3.96314573531464 to -4.1438835375737
-3.50463286285714 to -3.6853706651162
-3.83632883673028 to -4.01706663898934
-3.36349895840005 to -3.54423676065911
Changing layer 4's weights from 
-3.46277275924996 to -3.64351056150902
-3.6716055596383 to -3.85234336189736
-3.87208315973595 to -4.05282096199501
-3.58514842634515 to -3.76588622860421
-3.82440123205498 to -4.00513903431404
-3.59293601160363 to -3.77367381386269
-3.59729454164819 to -3.77803234390725
-4.05919206385925 to -4.23992986611831
-3.46549597626046 to -3.64623377851952
-3.89779791479424 to -4.0785357170533
Changing layer 5's weights from 
-3.30999294166878 to -3.49073074392784
-3.75206646566704 to -3.9328042679261
-3.32687810306862 to -3.50761590532769
-4.26239028444006 to -4.44312808669912
-3.72512147074059 to -3.90585927299965
-4.1235894138129 to -4.30432721607196
-3.8236241066964 to -4.00436190895546
-3.75117787962273 to -3.93191568188179
-3.36157587414101 to -3.54231367640007
-3.36808398609475 to -3.54882178835381
Trying to learn from memory 9, 1, -0.2
sum -2.22200525250526 distri 1.93931677222741
Using diff 1 and condRate 0.166666666666667
Changed category 1 weights from 
0.122994751868263 to 0.0896614180382248
0.356310338912025 to 0.322977005081987
-0.0686148989820324 to -0.101948232812071
-0.0645220864438855 to -0.0978554202739242
Changing layer 0's weights from 
-3.73591021828163 to -3.76924355211167
-4.10783667497147 to -4.14117000880151
-4.34218594066608 to -4.37551927449612
-4.30762669078815 to -4.34096002461819
-3.5597591214608 to -3.59309245529084
-4.2424010746907 to -4.27573440852074
-4.08857135109413 to -4.12190468492417
-4.11602575711716 to -4.1493590909472
-4.35351761810291 to -4.38685095193295
-4.3972676732372 to -4.43060100706724
Changing layer 1's weights from 
-4.39217076085316 to -4.4255040946832
-3.79100207619179 to -3.82433541002183
-3.91670481495369 to -3.95003814878373
-3.92661015324104 to -3.95994348707108
-4.11166293553818 to -4.14499626936822
-4.24990076951492 to -4.28323410334496
-3.97918204597939 to -4.01251537980943
-4.36498507492053 to -4.39831840875057
-3.95332209400642 to -3.98665542783646
-4.31596895687091 to -4.34930229070095
Changing layer 2's weights from 
-3.50678072504509 to -3.54011405887513
-4.06083421043862 to -4.09416754426866
-3.77624491981972 to -3.80957825364976
-3.60504482321251 to -3.63837815704255
-3.78229753307808 to -3.81563086690812
-3.98830715946663 to -4.02164049329667
-3.4694944434594 to -3.50282777728944
-3.54191647104729 to -3.57524980487733
-4.27391476563918 to -4.30724809946922
-4.22895724706161 to -4.26229058089165
Changing layer 3's weights from 
-3.53983102373589 to -3.57316435756593
-3.80338410667885 to -3.83671744050889
-3.56077418856133 to -3.59410752239137
-4.46821342031746 to -4.5015467541475
-3.68345008425225 to -3.71678341808228
-3.81369869045723 to -3.84703202428727
-4.1438835375737 to -4.17721687140374
-3.6853706651162 to -3.71870399894624
-4.01706663898934 to -4.05039997281938
-3.54423676065911 to -3.57757009448915
Changing layer 4's weights from 
-3.64351056150902 to -3.67684389533906
-3.85234336189736 to -3.8856766957274
-4.05282096199501 to -4.08615429582505
-3.76588622860421 to -3.79921956243424
-4.00513903431404 to -4.03847236814408
-3.77367381386269 to -3.80700714769273
-3.77803234390725 to -3.81136567773728
-4.23992986611831 to -4.27326319994835
-3.64623377851952 to -3.67956711234956
-4.0785357170533 to -4.11186905088334
Changing layer 5's weights from 
-3.49073074392784 to -3.52406407775788
-3.9328042679261 to -3.96613760175614
-3.50761590532769 to -3.54094923915772
-4.44312808669912 to -4.47646142052916
-3.90585927299965 to -3.93919260682969
-4.30432721607196 to -4.337660549902
-4.00436190895546 to -4.0376952427855
-3.93191568188179 to -3.96524901571183
-3.54231367640007 to -3.57564701023011
-3.54882178835381 to -3.58215512218385
Trying to learn from memory 9, 0, -0.2
sum -2.22200525250526 distri -7.08863792635468
Using diff 5.42213398697573 and condRate 0.166666666666667
Changed category 0 weights from 
-2.86106536466515 to -3.04180316692422
-3.26110032874978 to -3.44183813100884
-3.54362102527059 to -3.72435882752965
-3.39211565930283 to -3.57285346156189
Changing layer 0's weights from 
-3.76924355211167 to -3.94998135437073
-4.14117000880151 to -4.32190781106057
-4.37551927449612 to -4.55625707675518
-4.34096002461819 to -4.52169782687725
-3.59309245529084 to -3.7738302575499
-4.27573440852074 to -4.4564722107798
-4.12190468492417 to -4.30264248718323
-4.1493590909472 to -4.33009689320626
-4.38685095193295 to -4.56758875419201
-4.43060100706724 to -4.6113388093263
Changing layer 1's weights from 
-4.4255040946832 to -4.60624189694226
-3.82433541002183 to -4.00507321228089
-3.95003814878373 to -4.13077595104279
-3.95994348707108 to -4.14068128933014
-4.14499626936822 to -4.32573407162728
-4.28323410334496 to -4.46397190560402
-4.01251537980943 to -4.19325318206849
-4.39831840875057 to -4.57905621100963
-3.98665542783646 to -4.16739323009552
-4.34930229070095 to -4.53004009296001
Changing layer 2's weights from 
-3.54011405887513 to -3.72085186113419
-4.09416754426866 to -4.27490534652772
-3.80957825364976 to -3.99031605590882
-3.63837815704255 to -3.81911595930161
-3.81563086690812 to -3.99636866916718
-4.02164049329667 to -4.20237829555573
-3.50282777728944 to -3.6835655795485
-3.57524980487733 to -3.75598760713639
-4.30724809946922 to -4.48798590172828
-4.26229058089165 to -4.44302838315071
Changing layer 3's weights from 
-3.57316435756593 to -3.75390215982499
-3.83671744050889 to -4.01745524276795
-3.59410752239137 to -3.77484532465043
-4.5015467541475 to -4.68228455640656
-3.71678341808228 to -3.89752122034135
-3.84703202428727 to -4.02776982654633
-4.17721687140374 to -4.3579546736628
-3.71870399894624 to -3.8994418012053
-4.05039997281938 to -4.23113777507844
-3.57757009448915 to -3.75830789674821
Changing layer 4's weights from 
-3.67684389533906 to -3.85758169759812
-3.8856766957274 to -4.06641449798646
-4.08615429582505 to -4.26689209808411
-3.79921956243424 to -3.97995736469331
-4.03847236814408 to -4.21921017040314
-3.80700714769273 to -3.98774494995179
-3.81136567773728 to -3.99210347999635
-4.27326319994835 to -4.45400100220741
-3.67956711234956 to -3.86030491460862
-4.11186905088334 to -4.2926068531424
Changing layer 5's weights from 
-3.52406407775788 to -3.70480188001694
-3.96613760175614 to -4.1468754040152
-3.54094923915772 to -3.72168704141678
-4.47646142052916 to -4.65719922278822
-3.93919260682969 to -4.11993040908875
-4.337660549902 to -4.51839835216106
-4.0376952427855 to -4.21843304504456
-3.96524901571183 to -4.14598681797089
-3.57564701023011 to -3.75638481248917
-3.58215512218385 to -3.76289292444291
10/5/2016 11:13:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:13:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:15:11 PMStarting AI
Reading weights from 10/5/2016 11:15:11 PMStarting AI
Weights.txt
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
Layer 6's weights: -0.127011759170718 -0.527046723255344 -0.809567419776152 -0.658062053808399 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
Layer 8's weights: 0.391144796536028 0.669586762116017 0.582546397373737 -0.072025307192743 
Layer 0's weights: -3.94998135437073 -4.32190781106057 -4.55625707675518 -4.52169782687725 -3.7738302575499 -4.4564722107798 -4.30264248718323 -4.33009689320626 -4.56758875419201 -4.6113388093263 14 11 
Layer 1's weights: -4.60624189694226 -4.00507321228089 -4.13077595104279 -4.14068128933014 -4.32573407162728 -4.46397190560402 -4.19325318206849 -4.57905621100963 -4.16739323009552 -4.53004009296001 12 9 
Layer 2's weights: -3.72085186113419 -4.27490534652772 -3.99031605590882 -3.81911595930161 -3.99636866916718 -4.20237829555573 -3.6835655795485 -3.75598760713639 -4.48798590172828 -4.44302838315071 10 7 
Layer 3's weights: -3.75390215982499 -4.01745524276795 -3.77484532465043 -4.68228455640656 -3.89752122034135 -4.02776982654633 -4.3579546736628 -3.8994418012053 -4.23113777507844 -3.75830789674821 8 5 
Layer 4's weights: -3.85758169759812 -4.06641449798646 -4.26689209808411 -3.97995736469331 -4.21921017040314 -3.98774494995179 -3.99210347999635 -4.45400100220741 -3.86030491460862 -4.2926068531424 6 3 
Layer 5's weights: -3.70480188001694 -4.1468754040152 -3.72168704141678 -4.65719922278822 -4.11993040908875 -4.51839835216106 -4.21843304504456 -4.14598681797089 -3.75638481248917 -3.76289292444291 4 1 
Layer 6's weights: -3.04180316692422 -3.44183813100884 -3.72435882752965 -3.57285346156189 
Layer 7's weights: 0.0896614180382248 0.322977005081987 -0.101948232812071 -0.0978554202739242 
Layer 8's weights: 0.124478125895718 0.402920091475707 0.315879726733427 -0.338691977833053 
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
Layer 6's weights: -0.512188521173404 -0.912223485258029 -1.19474418177884 -1.04323881581108 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
10/5/2016 11:15:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:15:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:15:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:15:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:15:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:15:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:15:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:15:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:15:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:15:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:15:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:15:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:15:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:15:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:15:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:15:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:26 PMStarting AI
Reading weights from 10/5/2016 11:16:26 PMStarting AI
Weights.txt
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
Layer 6's weights: -0.127011759170718 -0.527046723255344 -0.809567419776152 -0.658062053808399 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
Layer 8's weights: 0.391144796536028 0.669586762116017 0.582546397373737 -0.072025307192743 
Layer 0's weights: -3.94998135437073 -4.32190781106057 -4.55625707675518 -4.52169782687725 -3.7738302575499 -4.4564722107798 -4.30264248718323 -4.33009689320626 -4.56758875419201 -4.6113388093263 14 11 
Layer 1's weights: -4.60624189694226 -4.00507321228089 -4.13077595104279 -4.14068128933014 -4.32573407162728 -4.46397190560402 -4.19325318206849 -4.57905621100963 -4.16739323009552 -4.53004009296001 12 9 
Layer 2's weights: -3.72085186113419 -4.27490534652772 -3.99031605590882 -3.81911595930161 -3.99636866916718 -4.20237829555573 -3.6835655795485 -3.75598760713639 -4.48798590172828 -4.44302838315071 10 7 
Layer 3's weights: -3.75390215982499 -4.01745524276795 -3.77484532465043 -4.68228455640656 -3.89752122034135 -4.02776982654633 -4.3579546736628 -3.8994418012053 -4.23113777507844 -3.75830789674821 8 5 
Layer 4's weights: -3.85758169759812 -4.06641449798646 -4.26689209808411 -3.97995736469331 -4.21921017040314 -3.98774494995179 -3.99210347999635 -4.45400100220741 -3.86030491460862 -4.2926068531424 6 3 
Layer 5's weights: -3.70480188001694 -4.1468754040152 -3.72168704141678 -4.65719922278822 -4.11993040908875 -4.51839835216106 -4.21843304504456 -4.14598681797089 -3.75638481248917 -3.76289292444291 4 1 
Layer 6's weights: -3.04180316692422 -3.44183813100884 -3.72435882752965 -3.57285346156189 
Layer 7's weights: 0.0896614180382248 0.322977005081987 -0.101948232812071 -0.0978554202739242 
Layer 8's weights: 0.124478125895718 0.402920091475707 0.315879726733427 -0.338691977833053 
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
Layer 6's weights: -0.512188521173404 -0.912223485258029 -1.19474418177884 -1.04323881581108 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
Layer 8's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
10/5/2016 11:16:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:16:59 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
Modified index 9's learning in memoryPool to -0.2
Modified index 10's learning in memoryPool to -0.2
Modified index 11's learning in memoryPool to -0.2
Modified index 12's learning in memoryPool to -0.2
Modified index 13's learning in memoryPool to -0.2
Modified index 14's learning in memoryPool to -0.2
Modified index 15's learning in memoryPool to -0.2
Modified index 16's learning in memoryPool to -0.2
Modified index 17's learning in memoryPool to -0.2
Modified index 18's learning in memoryPool to -0.2
Modified index 19's learning in memoryPool to -0.2
Modified index 20's learning in memoryPool to -0.2
Modified index 21's learning in memoryPool to -0.2
Modified index 22's learning in memoryPool to -0.2
Modified index 23's learning in memoryPool to -0.2
Modified index 24's learning in memoryPool to -0.2
Modified index 25's learning in memoryPool to -0.2
Modified index 26's learning in memoryPool to -0.2
10/5/2016 11:16:59 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 0, 2, -0.2
sum -0.274997076548412 distri 0.162952367700981
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.289661421018457 to 0.256328087188418
0.522977008062219 to 0.48964367423218
0.0980517701681611 to 0.0647184363381224
0.102144582706308 to 0.0688112488762693
Changing layer 0's weights from 
-0.568523272996696 to -0.601856606826735
-0.940449729686532 to -0.973783063516571
-1.17479899538115 to -1.20813232921119
-1.14023974550322 to -1.17357307933326
-0.392372176175865 to -0.425705510005904
-1.07501412940577 to -1.10834746323581
-0.921184405809198 to -0.954517739639237
-0.948638811832223 to -0.981972145662262
-1.18613067281798 to -1.21946400664802
-1.22988072795227 to -1.26321406178231
Changing layer 1's weights from 
-1.22478381556823 to -1.25811714939827
-0.623615130906854 to -0.656948464736893
-0.749317869668757 to -0.782651203498796
-0.75922320795611 to -0.792556541786149
-0.944275990253244 to -0.977609324083283
-1.08251382422999 to -1.11584715806003
-0.811795100694452 to -0.845128434524491
-1.1975981296356 to -1.23093146346564
-0.78593514872149 to -0.819268482551529
-1.14858201158598 to -1.18191534541602
Changing layer 2's weights from 
-0.339393779760155 to -0.372727113590194
-0.893447265153681 to -0.92678059898372
-0.608857974534783 to -0.642191308364822
-0.437657877927574 to -0.470991211757613
-0.614910587793145 to -0.648243921623184
-0.820920214181696 to -0.854253548011735
-0.302107498174461 to -0.3354408320045
-0.374529525762352 to -0.407862859592391
-1.10652782035425 to -1.13986115418429
-1.06157030177668 to -1.09490363560672
Changing layer 3's weights from 
-0.372444078450951 to -0.40577741228099
-0.635997161393914 to -0.669330495223953
-0.39338724327639 to -0.426720577106429
-1.30082647503253 to -1.33415980886257
-0.516063138967308 to -0.549396472797347
-0.646311745172296 to -0.679645079002335
-0.976496592288766 to -1.0098299261188
-0.517983719831261 to -0.5513170536613
-0.849679693704401 to -0.88301302753444
-0.376849815374169 to -0.410183149204208
Changing layer 4's weights from 
-0.476123616224083 to -0.509456950054122
-0.684956416612421 to -0.71828975044246
-0.885434016710077 to -0.918767350540116
-0.598499283319268 to -0.631832617149307
-0.837752089029108 to -0.871085422859147
-0.606286868577752 to -0.639620202407791
-0.610645398622308 to -0.643978732452347
-1.07254292083338 to -1.10587625466342
-0.478846833234581 to -0.51218016706462
-0.911148771768366 to -0.944482105598405
Changing layer 5's weights from 
-0.323343798642906 to -0.356677132472945
-0.765417322641168 to -0.798750656471207
-0.340228960042747 to -0.373562293872786
-1.27574114141419 to -1.30907447524423
-0.738472327714716 to -0.771805661544755
-1.13694027078703 to -1.17027360461707
-0.836974963670526 to -0.870308297500565
-0.764528736596857 to -0.797862070426896
-0.374926731115135 to -0.408260064945174
-0.381434843068872 to -0.414768176898911
Trying to learn from memory 1, 0, -0.2
sum -0.27528943347118 distri -0.601423255915102
Using diff 0.394956180811717 and condRate 0.166666666666667
Changed category 0 weights from 
-0.512188521173404 to -0.525353727396638
-0.912223485258029 to -0.925388691481263
-1.19474418177884 to -1.20790938800207
-1.04323881581108 to -1.05640402203431
Changing layer 0's weights from 
-0.601856606826735 to -0.615021813049969
-0.973783063516571 to -0.986948269739805
-1.20813232921119 to -1.22129753543442
-1.17357307933326 to -1.18673828555649
-0.425705510005904 to -0.438870716229138
-1.10834746323581 to -1.12151266945904
-0.954517739639237 to -0.967682945862471
-0.981972145662262 to -0.995137351885496
-1.21946400664802 to -1.23262921287125
-1.26321406178231 to -1.27637926800554
Changing layer 1's weights from 
-1.25811714939827 to -1.2712823556215
-0.656948464736893 to -0.670113670960127
-0.782651203498796 to -0.79581640972203
-0.792556541786149 to -0.805721748009383
-0.977609324083283 to -0.990774530306517
-1.11584715806003 to -1.12901236428326
-0.845128434524491 to -0.858293640747725
-1.23093146346564 to -1.24409666968887
-0.819268482551529 to -0.832433688774763
-1.18191534541602 to -1.19508055163925
Changing layer 2's weights from 
-0.372727113590194 to -0.385892319813428
-0.92678059898372 to -0.939945805206954
-0.642191308364822 to -0.655356514588056
-0.470991211757613 to -0.484156417980847
-0.648243921623184 to -0.661409127846418
-0.854253548011735 to -0.867418754234969
-0.3354408320045 to -0.348606038227734
-0.407862859592391 to -0.421028065815625
-1.13986115418429 to -1.15302636040752
-1.09490363560672 to -1.10806884182995
Changing layer 3's weights from 
-0.40577741228099 to -0.418942618504224
-0.669330495223953 to -0.682495701447187
-0.426720577106429 to -0.439885783329663
-1.33415980886257 to -1.3473250150858
-0.549396472797347 to -0.562561679020581
-0.679645079002335 to -0.692810285225569
-1.0098299261188 to -1.02299513234204
-0.5513170536613 to -0.564482259884534
-0.88301302753444 to -0.896178233757674
-0.410183149204208 to -0.423348355427442
Changing layer 4's weights from 
-0.509456950054122 to -0.522622156277356
-0.71828975044246 to -0.731454956665694
-0.918767350540116 to -0.93193255676335
-0.631832617149307 to -0.644997823372541
-0.871085422859147 to -0.884250629082381
-0.639620202407791 to -0.652785408631025
-0.643978732452347 to -0.657143938675581
-1.10587625466342 to -1.11904146088665
-0.51218016706462 to -0.525345373287854
-0.944482105598405 to -0.957647311821639
Changing layer 5's weights from 
-0.356677132472945 to -0.369842338696179
-0.798750656471207 to -0.811915862694441
-0.373562293872786 to -0.38672750009602
-1.30907447524423 to -1.32223968146746
-0.771805661544755 to -0.784970867767989
-1.17027360461707 to -1.1834388108403
-0.870308297500565 to -0.883473503723799
-0.797862070426896 to -0.81102727665013
-0.408260064945174 to -0.421425271168408
-0.414768176898911 to -0.427933383122145
Trying to learn from memory 2, 0, -0.2
sum -0.276118429974078 distri -0.602528179801355
Using diff 0.395439357320796 and condRate 0.166666666666667
Changed category 0 weights from 
-0.525353727396638 to -0.538535039503748
-0.925388691481263 to -0.938570003588373
-1.20790938800207 to -1.22109070010918
-1.05640402203431 to -1.06958533414142
Changing layer 0's weights from 
-0.615021813049969 to -0.628203125157079
-0.986948269739805 to -1.00012958184691
-1.22129753543442 to -1.23447884754153
-1.18673828555649 to -1.1999195976636
-0.438870716229138 to -0.452052028336248
-1.12151266945904 to -1.13469398156615
-0.967682945862471 to -0.980864257969581
-0.995137351885496 to -1.00831866399261
-1.23262921287125 to -1.24581052497836
-1.27637926800554 to -1.28956058011265
Changing layer 1's weights from 
-1.2712823556215 to -1.28446366772861
-0.670113670960127 to -0.683294983067237
-0.79581640972203 to -0.80899772182914
-0.805721748009383 to -0.818903060116493
-0.990774530306517 to -1.00395584241363
-1.12901236428326 to -1.14219367639037
-0.858293640747725 to -0.871474952854835
-1.24409666968887 to -1.25727798179598
-0.832433688774763 to -0.845615000881873
-1.19508055163925 to -1.20826186374636
Changing layer 2's weights from 
-0.385892319813428 to -0.399073631920538
-0.939945805206954 to -0.953127117314064
-0.655356514588056 to -0.668537826695166
-0.484156417980847 to -0.497337730087957
-0.661409127846418 to -0.674590439953528
-0.867418754234969 to -0.880600066342079
-0.348606038227734 to -0.361787350334844
-0.421028065815625 to -0.434209377922735
-1.15302636040752 to -1.16620767251463
-1.10806884182995 to -1.12125015393706
Changing layer 3's weights from 
-0.418942618504224 to -0.432123930611334
-0.682495701447187 to -0.695677013554297
-0.439885783329663 to -0.453067095436773
-1.3473250150858 to -1.36050632719291
-0.562561679020581 to -0.575742991127691
-0.692810285225569 to -0.705991597332679
-1.02299513234204 to -1.03617644444915
-0.564482259884534 to -0.577663571991644
-0.896178233757674 to -0.909359545864784
-0.423348355427442 to -0.436529667534552
Changing layer 4's weights from 
-0.522622156277356 to -0.535803468384466
-0.731454956665694 to -0.744636268772804
-0.93193255676335 to -0.94511386887046
-0.644997823372541 to -0.658179135479651
-0.884250629082381 to -0.897431941189491
-0.652785408631025 to -0.665966720738135
-0.657143938675581 to -0.670325250782691
-1.11904146088665 to -1.13222277299376
-0.525345373287854 to -0.538526685394964
-0.957647311821639 to -0.970828623928749
Changing layer 5's weights from 
-0.369842338696179 to -0.383023650803289
-0.811915862694441 to -0.825097174801551
-0.38672750009602 to -0.39990881220313
-1.32223968146746 to -1.33542099357457
-0.784970867767989 to -0.798152179875099
-1.1834388108403 to -1.19662012294741
-0.883473503723799 to -0.896654815830909
-0.81102727665013 to -0.82420858875724
-0.421425271168408 to -0.434606583275518
-0.427933383122145 to -0.441114695229255
Trying to learn from memory 2, 0, -0.2
sum -0.276118429974078 distri -0.602528179801355
Using diff 0.395439357320796 and condRate 0.166666666666667
Changed category 0 weights from 
-0.538535039503748 to -0.551716351610858
-0.938570003588373 to -0.951751315695483
-1.22109070010918 to -1.23427201221629
-1.06958533414142 to -1.08276664624853
Changing layer 0's weights from 
-0.628203125157079 to -0.641384437264189
-1.00012958184691 to -1.01331089395403
-1.23447884754153 to -1.24766015964864
-1.1999195976636 to -1.21310090977071
-0.452052028336248 to -0.465233340443358
-1.13469398156615 to -1.14787529367326
-0.980864257969581 to -0.994045570076691
-1.00831866399261 to -1.02149997609972
-1.24581052497836 to -1.25899183708547
-1.28956058011265 to -1.30274189221976
Changing layer 1's weights from 
-1.28446366772861 to -1.29764497983572
-0.683294983067237 to -0.696476295174347
-0.80899772182914 to -0.82217903393625
-0.818903060116493 to -0.832084372223603
-1.00395584241363 to -1.01713715452074
-1.14219367639037 to -1.15537498849748
-0.871474952854835 to -0.884656264961945
-1.25727798179598 to -1.27045929390309
-0.845615000881873 to -0.858796312988983
-1.20826186374636 to -1.22144317585347
Changing layer 2's weights from 
-0.399073631920538 to -0.412254944027648
-0.953127117314064 to -0.966308429421174
-0.668537826695166 to -0.681719138802276
-0.497337730087957 to -0.510519042195067
-0.674590439953528 to -0.687771752060638
-0.880600066342079 to -0.893781378449189
-0.361787350334844 to -0.374968662441954
-0.434209377922735 to -0.447390690029845
-1.16620767251463 to -1.17938898462174
-1.12125015393706 to -1.13443146604417
Changing layer 3's weights from 
-0.432123930611334 to -0.445305242718444
-0.695677013554297 to -0.708858325661407
-0.453067095436773 to -0.466248407543883
-1.36050632719291 to -1.37368763930002
-0.575742991127691 to -0.588924303234801
-0.705991597332679 to -0.719172909439789
-1.03617644444915 to -1.04935775655626
-0.577663571991644 to -0.590844884098754
-0.909359545864784 to -0.922540857971894
-0.436529667534552 to -0.449710979641662
Changing layer 4's weights from 
-0.535803468384466 to -0.548984780491576
-0.744636268772804 to -0.757817580879914
-0.94511386887046 to -0.95829518097757
-0.658179135479651 to -0.671360447586761
-0.897431941189491 to -0.910613253296601
-0.665966720738135 to -0.679148032845245
-0.670325250782691 to -0.683506562889801
-1.13222277299376 to -1.14540408510087
-0.538526685394964 to -0.551707997502074
-0.970828623928749 to -0.984009936035859
Changing layer 5's weights from 
-0.383023650803289 to -0.396204962910399
-0.825097174801551 to -0.838278486908661
-0.39990881220313 to -0.41309012431024
-1.33542099357457 to -1.34860230568168
-0.798152179875099 to -0.811333491982209
-1.19662012294741 to -1.20980143505452
-0.896654815830909 to -0.909836127938019
-0.82420858875724 to -0.83738990086435
-0.434606583275518 to -0.447787895382628
-0.441114695229255 to -0.454296007336365
Trying to learn from memory 2, 0, -0.2
sum -0.276118429974078 distri -0.602528179801355
Using diff 0.395439357320796 and condRate 0.166666666666667
Changed category 0 weights from 
-0.551716351610858 to -0.564897663717968
-0.951751315695483 to -0.964932627802593
-1.23427201221629 to -1.2474533243234
-1.08276664624853 to -1.09594795835564
Changing layer 0's weights from 
-0.641384437264189 to -0.654565749371299
-1.01331089395403 to -1.02649220606114
-1.24766015964864 to -1.26084147175575
-1.21310090977071 to -1.22628222187782
-0.465233340443358 to -0.478414652550468
-1.14787529367326 to -1.16105660578037
-0.994045570076691 to -1.0072268821838
-1.02149997609972 to -1.03468128820683
-1.25899183708547 to -1.27217314919258
-1.30274189221976 to -1.31592320432687
Changing layer 1's weights from 
-1.29764497983572 to -1.31082629194283
-0.696476295174347 to -0.709657607281457
-0.82217903393625 to -0.83536034604336
-0.832084372223603 to -0.845265684330713
-1.01713715452074 to -1.03031846662785
-1.15537498849748 to -1.16855630060459
-0.884656264961945 to -0.897837577069055
-1.27045929390309 to -1.2836406060102
-0.858796312988983 to -0.871977625096093
-1.22144317585347 to -1.23462448796058
Changing layer 2's weights from 
-0.412254944027648 to -0.425436256134758
-0.966308429421174 to -0.979489741528284
-0.681719138802276 to -0.694900450909386
-0.510519042195067 to -0.523700354302177
-0.687771752060638 to -0.700953064167748
-0.893781378449189 to -0.906962690556299
-0.374968662441954 to -0.388149974549064
-0.447390690029845 to -0.460572002136955
-1.17938898462174 to -1.19257029672885
-1.13443146604417 to -1.14761277815128
Changing layer 3's weights from 
-0.445305242718444 to -0.458486554825554
-0.708858325661407 to -0.722039637768517
-0.466248407543883 to -0.479429719650993
-1.37368763930002 to -1.38686895140713
-0.588924303234801 to -0.602105615341911
-0.719172909439789 to -0.732354221546899
-1.04935775655626 to -1.06253906866337
-0.590844884098754 to -0.604026196205864
-0.922540857971894 to -0.935722170079004
-0.449710979641662 to -0.462892291748772
Changing layer 4's weights from 
-0.548984780491576 to -0.562166092598686
-0.757817580879914 to -0.770998892987024
-0.95829518097757 to -0.97147649308468
-0.671360447586761 to -0.684541759693871
-0.910613253296601 to -0.923794565403711
-0.679148032845245 to -0.692329344952355
-0.683506562889801 to -0.696687874996911
-1.14540408510087 to -1.15858539720798
-0.551707997502074 to -0.564889309609184
-0.984009936035859 to -0.997191248142969
Changing layer 5's weights from 
-0.396204962910399 to -0.409386275017509
-0.838278486908661 to -0.851459799015771
-0.41309012431024 to -0.42627143641735
-1.34860230568168 to -1.36178361778879
-0.811333491982209 to -0.824514804089319
-1.20980143505452 to -1.22298274716163
-0.909836127938019 to -0.923017440045129
-0.83738990086435 to -0.85057121297146
-0.447787895382628 to -0.460969207489738
-0.454296007336365 to -0.467477319443475
Trying to learn from memory 2, 0, -0.2
sum -0.276118429974078 distri -0.602528179801355
Using diff 0.395439357320796 and condRate 0.166666666666667
Changed category 0 weights from 
-0.564897663717968 to -0.578078975825078
-0.964932627802593 to -0.978113939909703
-1.2474533243234 to -1.26063463643051
-1.09594795835564 to -1.10912927046275
Changing layer 0's weights from 
-0.654565749371299 to -0.667747061478409
-1.02649220606114 to -1.03967351816825
-1.26084147175575 to -1.27402278386286
-1.22628222187782 to -1.23946353398493
-0.478414652550468 to -0.491595964657578
-1.16105660578037 to -1.17423791788748
-1.0072268821838 to -1.02040819429091
-1.03468128820683 to -1.04786260031394
-1.27217314919258 to -1.28535446129969
-1.31592320432687 to -1.32910451643398
Changing layer 1's weights from 
-1.31082629194283 to -1.32400760404994
-0.709657607281457 to -0.722838919388567
-0.83536034604336 to -0.84854165815047
-0.845265684330713 to -0.858446996437823
-1.03031846662785 to -1.04349977873496
-1.16855630060459 to -1.1817376127117
-0.897837577069055 to -0.911018889176165
-1.2836406060102 to -1.29682191811731
-0.871977625096093 to -0.885158937203203
-1.23462448796058 to -1.24780580006769
Changing layer 2's weights from 
-0.425436256134758 to -0.438617568241868
-0.979489741528284 to -0.992671053635394
-0.694900450909386 to -0.708081763016496
-0.523700354302177 to -0.536881666409287
-0.700953064167748 to -0.714134376274858
-0.906962690556299 to -0.920144002663409
-0.388149974549064 to -0.401331286656174
-0.460572002136955 to -0.473753314244065
-1.19257029672885 to -1.20575160883596
-1.14761277815128 to -1.16079409025839
Changing layer 3's weights from 
-0.458486554825554 to -0.471667866932664
-0.722039637768517 to -0.735220949875627
-0.479429719650993 to -0.492611031758103
-1.38686895140713 to -1.40005026351424
-0.602105615341911 to -0.615286927449021
-0.732354221546899 to -0.745535533654009
-1.06253906866337 to -1.07572038077048
-0.604026196205864 to -0.617207508312974
-0.935722170079004 to -0.948903482186114
-0.462892291748772 to -0.476073603855882
Changing layer 4's weights from 
-0.562166092598686 to -0.575347404705796
-0.770998892987024 to -0.784180205094134
-0.97147649308468 to -0.98465780519179
-0.684541759693871 to -0.697723071800981
-0.923794565403711 to -0.936975877510821
-0.692329344952355 to -0.705510657059465
-0.696687874996911 to -0.709869187104021
-1.15858539720798 to -1.17176670931509
-0.564889309609184 to -0.578070621716294
-0.997191248142969 to -1.01037256025008
Changing layer 5's weights from 
-0.409386275017509 to -0.422567587124619
-0.851459799015771 to -0.864641111122881
-0.42627143641735 to -0.43945274852446
-1.36178361778879 to -1.3749649298959
-0.824514804089319 to -0.837696116196429
-1.22298274716163 to -1.23616405926874
-0.923017440045129 to -0.936198752152239
-0.85057121297146 to -0.86375252507857
-0.460969207489738 to -0.474150519596848
-0.467477319443475 to -0.480658631550585
Trying to learn from memory 2, 0, -0.2
sum -0.276118429974078 distri -0.602528179801355
Using diff 0.395439357320796 and condRate 0.166666666666667
Changed category 0 weights from 
-0.578078975825078 to -0.591260287932188
-0.978113939909703 to -0.991295252016813
-1.26063463643051 to -1.27381594853762
-1.10912927046275 to -1.12231058256986
Changing layer 0's weights from 
-0.667747061478409 to -0.680928373585519
-1.03967351816825 to -1.05285483027536
-1.27402278386286 to -1.28720409596997
-1.23946353398493 to -1.25264484609204
-0.491595964657578 to -0.504777276764688
-1.17423791788748 to -1.18741922999459
-1.02040819429091 to -1.03358950639802
-1.04786260031394 to -1.06104391242105
-1.28535446129969 to -1.2985357734068
-1.32910451643398 to -1.34228582854109
Changing layer 1's weights from 
-1.32400760404994 to -1.33718891615705
-0.722838919388567 to -0.736020231495677
-0.84854165815047 to -0.86172297025758
-0.858446996437823 to -0.871628308544933
-1.04349977873496 to -1.05668109084207
-1.1817376127117 to -1.19491892481881
-0.911018889176165 to -0.924200201283275
-1.29682191811731 to -1.31000323022442
-0.885158937203203 to -0.898340249310313
-1.24780580006769 to -1.2609871121748
Changing layer 2's weights from 
-0.438617568241868 to -0.451798880348978
-0.992671053635394 to -1.0058523657425
-0.708081763016496 to -0.721263075123606
-0.536881666409287 to -0.550062978516397
-0.714134376274858 to -0.727315688381968
-0.920144002663409 to -0.933325314770519
-0.401331286656174 to -0.414512598763284
-0.473753314244065 to -0.486934626351175
-1.20575160883596 to -1.21893292094307
-1.16079409025839 to -1.1739754023655
Changing layer 3's weights from 
-0.471667866932664 to -0.484849179039774
-0.735220949875627 to -0.748402261982737
-0.492611031758103 to -0.505792343865213
-1.40005026351424 to -1.41323157562135
-0.615286927449021 to -0.628468239556131
-0.745535533654009 to -0.758716845761119
-1.07572038077048 to -1.08890169287759
-0.617207508312974 to -0.630388820420084
-0.948903482186114 to -0.962084794293224
-0.476073603855882 to -0.489254915962992
Changing layer 4's weights from 
-0.575347404705796 to -0.588528716812906
-0.784180205094134 to -0.797361517201244
-0.98465780519179 to -0.9978391172989
-0.697723071800981 to -0.710904383908091
-0.936975877510821 to -0.950157189617931
-0.705510657059465 to -0.718691969166575
-0.709869187104021 to -0.723050499211131
-1.17176670931509 to -1.1849480214222
-0.578070621716294 to -0.591251933823404
-1.01037256025008 to -1.02355387235719
Changing layer 5's weights from 
-0.422567587124619 to -0.435748899231729
-0.864641111122881 to -0.877822423229991
-0.43945274852446 to -0.45263406063157
-1.3749649298959 to -1.38814624200301
-0.837696116196429 to -0.850877428303539
-1.23616405926874 to -1.24934537137585
-0.936198752152239 to -0.949380064259349
-0.86375252507857 to -0.87693383718568
-0.474150519596848 to -0.487331831703958
-0.480658631550585 to -0.493839943657695
Trying to learn from memory 2, 0, -0.2
sum -0.276118429974078 distri -0.602528179801355
Using diff 0.395439357320796 and condRate 0.166666666666667
Changed category 0 weights from 
-0.591260287932188 to -0.604441600039298
-0.991295252016813 to -1.00447656412392
-1.27381594853762 to -1.28699726064473
-1.12231058256986 to -1.13549189467697
Changing layer 0's weights from 
-0.680928373585519 to -0.694109685692629
-1.05285483027536 to -1.06603614238247
-1.28720409596997 to -1.30038540807708
-1.25264484609204 to -1.26582615819915
-0.504777276764688 to -0.517958588871798
-1.18741922999459 to -1.2006005421017
-1.03358950639802 to -1.04677081850513
-1.06104391242105 to -1.07422522452816
-1.2985357734068 to -1.31171708551391
-1.34228582854109 to -1.3554671406482
Changing layer 1's weights from 
-1.33718891615705 to -1.35037022826416
-0.736020231495677 to -0.749201543602787
-0.86172297025758 to -0.87490428236469
-0.871628308544933 to -0.884809620652043
-1.05668109084207 to -1.06986240294918
-1.19491892481881 to -1.20810023692592
-0.924200201283275 to -0.937381513390385
-1.31000323022442 to -1.32318454233153
-0.898340249310313 to -0.911521561417423
-1.2609871121748 to -1.27416842428191
Changing layer 2's weights from 
-0.451798880348978 to -0.464980192456088
-1.0058523657425 to -1.01903367784961
-0.721263075123606 to -0.734444387230716
-0.550062978516397 to -0.563244290623507
-0.727315688381968 to -0.740497000489078
-0.933325314770519 to -0.946506626877629
-0.414512598763284 to -0.427693910870394
-0.486934626351175 to -0.500115938458285
-1.21893292094307 to -1.23211423305018
-1.1739754023655 to -1.18715671447261
Changing layer 3's weights from 
-0.484849179039774 to -0.498030491146884
-0.748402261982737 to -0.761583574089847
-0.505792343865213 to -0.518973655972323
-1.41323157562135 to -1.42641288772846
-0.628468239556131 to -0.641649551663241
-0.758716845761119 to -0.771898157868229
-1.08890169287759 to -1.1020830049847
-0.630388820420084 to -0.643570132527194
-0.962084794293224 to -0.975266106400334
-0.489254915962992 to -0.502436228070102
Changing layer 4's weights from 
-0.588528716812906 to -0.601710028920016
-0.797361517201244 to -0.810542829308354
-0.9978391172989 to -1.01102042940601
-0.710904383908091 to -0.724085696015201
-0.950157189617931 to -0.963338501725041
-0.718691969166575 to -0.731873281273685
-0.723050499211131 to -0.736231811318241
-1.1849480214222 to -1.19812933352931
-0.591251933823404 to -0.604433245930514
-1.02355387235719 to -1.0367351844643
Changing layer 5's weights from 
-0.435748899231729 to -0.448930211338839
-0.877822423229991 to -0.891003735337101
-0.45263406063157 to -0.46581537273868
-1.38814624200301 to -1.40132755411012
-0.850877428303539 to -0.864058740410649
-1.24934537137585 to -1.26252668348296
-0.949380064259349 to -0.962561376366459
-0.87693383718568 to -0.89011514929279
-0.487331831703958 to -0.500513143811068
-0.493839943657695 to -0.507021255764805
Trying to learn from memory 2, 1, -0.2
sum -0.276118429974078 distri 0.163204874913638
Using diff 1 and condRate 0.166666666666667
Changed category 1 weights from 
0.289661421018457 to 0.256328087188418
0.522977008062219 to 0.48964367423218
0.0980517701681611 to 0.0647184363381224
0.102144582706308 to 0.0688112488762693
Changing layer 0's weights from 
-0.694109685692629 to -0.727443019522668
-1.06603614238247 to -1.0993694762125
-1.30038540807708 to -1.33371874190712
-1.26582615819915 to -1.29915949202919
-0.517958588871798 to -0.551291922701837
-1.2006005421017 to -1.23393387593174
-1.04677081850513 to -1.08010415233517
-1.07422522452816 to -1.10755855835819
-1.31171708551391 to -1.34505041934395
-1.3554671406482 to -1.38880047447824
Changing layer 1's weights from 
-1.35037022826416 to -1.3837035620942
-0.749201543602787 to -0.782534877432826
-0.87490428236469 to -0.908237616194729
-0.884809620652043 to -0.918142954482082
-1.06986240294918 to -1.10319573677922
-1.20810023692592 to -1.24143357075596
-0.937381513390385 to -0.970714847220424
-1.32318454233153 to -1.35651787616157
-0.911521561417423 to -0.944854895247462
-1.27416842428191 to -1.30750175811195
Changing layer 2's weights from 
-0.464980192456088 to -0.498313526286127
-1.01903367784961 to -1.05236701167965
-0.734444387230716 to -0.767777721060755
-0.563244290623507 to -0.596577624453546
-0.740497000489078 to -0.773830334319117
-0.946506626877629 to -0.979839960707668
-0.427693910870394 to -0.461027244700433
-0.500115938458285 to -0.533449272288324
-1.23211423305018 to -1.26544756688022
-1.18715671447261 to -1.22049004830265
Changing layer 3's weights from 
-0.498030491146884 to -0.531363824976923
-0.761583574089847 to -0.794916907919886
-0.518973655972323 to -0.552306989802362
-1.42641288772846 to -1.4597462215585
-0.641649551663241 to -0.67498288549328
-0.771898157868229 to -0.805231491698268
-1.1020830049847 to -1.13541633881474
-0.643570132527194 to -0.676903466357233
-0.975266106400334 to -1.00859944023037
-0.502436228070102 to -0.535769561900141
Changing layer 4's weights from 
-0.601710028920016 to -0.635043362750055
-0.810542829308354 to -0.843876163138393
-1.01102042940601 to -1.04435376323605
-0.724085696015201 to -0.75741902984524
-0.963338501725041 to -0.99667183555508
-0.731873281273685 to -0.765206615103724
-0.736231811318241 to -0.76956514514828
-1.19812933352931 to -1.23146266735935
-0.604433245930514 to -0.637766579760553
-1.0367351844643 to -1.07006851829434
Changing layer 5's weights from 
-0.448930211338839 to -0.482263545168878
-0.891003735337101 to -0.92433706916714
-0.46581537273868 to -0.499148706568719
-1.40132755411012 to -1.43466088794016
-0.864058740410649 to -0.897392074240688
-1.26252668348296 to -1.295860017313
-0.962561376366459 to -0.995894710196498
-0.89011514929279 to -0.923448483122829
-0.500513143811068 to -0.533846477641107
-0.507021255764805 to -0.540354589594844
Trying to learn from memory 3, 0, -0.2
sum -0.276028568123099 distri -0.603197543072779
Using diff 0.396176116980455 and condRate 0.166666666666667
Changed category 0 weights from 
-0.604441600039298 to -0.617647470802096
-1.00447656412392 to -1.01768243488672
-1.28699726064473 to -1.30020313140753
-1.13549189467697 to -1.14869776543977
Changing layer 0's weights from 
-0.727443019522668 to -0.740648890285466
-1.0993694762125 to -1.1125753469753
-1.33371874190712 to -1.34692461266992
-1.29915949202919 to -1.31236536279199
-0.551291922701837 to -0.564497793464635
-1.23393387593174 to -1.24713974669454
-1.08010415233517 to -1.09331002309797
-1.10755855835819 to -1.12076442912099
-1.34505041934395 to -1.35825629010675
-1.38880047447824 to -1.40200634524104
Changing layer 1's weights from 
-1.3837035620942 to -1.396909432857
-0.782534877432826 to -0.795740748195624
-0.908237616194729 to -0.921443486957527
-0.918142954482082 to -0.93134882524488
-1.10319573677922 to -1.11640160754201
-1.24143357075596 to -1.25463944151876
-0.970714847220424 to -0.983920717983222
-1.35651787616157 to -1.36972374692437
-0.944854895247462 to -0.95806076601026
-1.30750175811195 to -1.32070762887475
Changing layer 2's weights from 
-0.498313526286127 to -0.511519397048925
-1.05236701167965 to -1.06557288244245
-0.767777721060755 to -0.780983591823553
-0.596577624453546 to -0.609783495216344
-0.773830334319117 to -0.787036205081915
-0.979839960707668 to -0.993045831470466
-0.461027244700433 to -0.474233115463231
-0.533449272288324 to -0.546655143051122
-1.26544756688022 to -1.27865343764302
-1.22049004830265 to -1.23369591906545
Changing layer 3's weights from 
-0.531363824976923 to -0.544569695739721
-0.794916907919886 to -0.808122778682684
-0.552306989802362 to -0.56551286056516
-1.4597462215585 to -1.4729520923213
-0.67498288549328 to -0.688188756256078
-0.805231491698268 to -0.818437362461066
-1.13541633881474 to -1.14862220957754
-0.676903466357233 to -0.690109337120031
-1.00859944023037 to -1.02180531099317
-0.535769561900141 to -0.548975432662939
Changing layer 4's weights from 
-0.635043362750055 to -0.648249233512853
-0.843876163138393 to -0.857082033901191
-1.04435376323605 to -1.05755963399885
-0.75741902984524 to -0.770624900608038
-0.99667183555508 to -1.00987770631788
-0.765206615103724 to -0.778412485866522
-0.76956514514828 to -0.782771015911078
-1.23146266735935 to -1.24466853812215
-0.637766579760553 to -0.650972450523351
-1.07006851829434 to -1.08327438905714
Changing layer 5's weights from 
-0.482263545168878 to -0.495469415931676
-0.92433706916714 to -0.937542939929938
-0.499148706568719 to -0.512354577331517
-1.43466088794016 to -1.44786675870296
-0.897392074240688 to -0.910597945003486
-1.295860017313 to -1.3090658880758
-0.995894710196498 to -1.0091005809593
-0.923448483122829 to -0.936654353885627
-0.533846477641107 to -0.547052348403905
-0.540354589594844 to -0.553560460357642
10/5/2016 11:17:00 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 4, 0, -0.2
sum -0.275887167259984 distri -0.604249659059764
Using diff 0.397334283614776 and condRate 0.166666666666667
Changed category 0 weights from 
-0.617647470802096 to -0.630891947119947
-1.01768243488672 to -1.03092691120457
-1.30020313140753 to -1.31344760772538
-1.14869776543977 to -1.16194224175762
Changing layer 0's weights from 
-0.740648890285466 to -0.753893366603316
-1.1125753469753 to -1.12581982329315
-1.34692461266992 to -1.36016908898777
-1.31236536279199 to -1.32560983910984
-0.564497793464635 to -0.577742269782485
-1.24713974669454 to -1.26038422301239
-1.09331002309797 to -1.10655449941582
-1.12076442912099 to -1.13400890543884
-1.35825629010675 to -1.3715007664246
-1.40200634524104 to -1.41525082155889
Changing layer 1's weights from 
-1.396909432857 to -1.41015390917485
-0.795740748195624 to -0.808985224513474
-0.921443486957527 to -0.934687963275377
-0.93134882524488 to -0.94459330156273
-1.11640160754201 to -1.12964608385986
-1.25463944151876 to -1.26788391783661
-0.983920717983222 to -0.997165194301072
-1.36972374692437 to -1.38296822324222
-0.95806076601026 to -0.97130524232811
-1.32070762887475 to -1.3339521051926
Changing layer 2's weights from 
-0.511519397048925 to -0.524763873366775
-1.06557288244245 to -1.0788173587603
-0.780983591823553 to -0.794228068141403
-0.609783495216344 to -0.623027971534194
-0.787036205081915 to -0.800280681399765
-0.993045831470466 to -1.00629030778832
-0.474233115463231 to -0.487477591781081
-0.546655143051122 to -0.559899619368972
-1.27865343764302 to -1.29189791396087
-1.23369591906545 to -1.2469403953833
Changing layer 3's weights from 
-0.544569695739721 to -0.557814172057571
-0.808122778682684 to -0.821367255000534
-0.56551286056516 to -0.57875733688301
-1.4729520923213 to -1.48619656863915
-0.688188756256078 to -0.701433232573928
-0.818437362461066 to -0.831681838778916
-1.14862220957754 to -1.16186668589539
-0.690109337120031 to -0.703353813437881
-1.02180531099317 to -1.03504978731102
-0.548975432662939 to -0.562219908980789
Changing layer 4's weights from 
-0.648249233512853 to -0.661493709830703
-0.857082033901191 to -0.870326510219041
-1.05755963399885 to -1.0708041103167
-0.770624900608038 to -0.783869376925888
-1.00987770631788 to -1.02312218263573
-0.778412485866522 to -0.791656962184372
-0.782771015911078 to -0.796015492228928
-1.24466853812215 to -1.25791301444
-0.650972450523351 to -0.664216926841201
-1.08327438905714 to -1.09651886537499
Changing layer 5's weights from 
-0.495469415931676 to -0.508713892249526
-0.937542939929938 to -0.950787416247788
-0.512354577331517 to -0.525599053649367
-1.44786675870296 to -1.46111123502081
-0.910597945003486 to -0.923842421321336
-1.3090658880758 to -1.32231036439365
-1.0091005809593 to -1.02234505727715
-0.936654353885627 to -0.949898830203477
-0.547052348403905 to -0.560296824721755
-0.553560460357642 to -0.566804936675492
Trying to learn from memory 5, 0, -0.2
sum -0.275893649495846 distri -0.604200030584102
Using diff 0.397279793462217 and condRate 0.166666666666667
Changed category 0 weights from 
-0.630891947119947 to -0.644134607099352
-1.03092691120457 to -1.04416957118398
-1.31344760772538 to -1.32669026770479
-1.16194224175762 to -1.17518490173703
Changing layer 0's weights from 
-0.753893366603316 to -0.767136026582721
-1.12581982329315 to -1.13906248327256
-1.36016908898777 to -1.37341174896718
-1.32560983910984 to -1.33885249908925
-0.577742269782485 to -0.59098492976189
-1.26038422301239 to -1.2736268829918
-1.10655449941582 to -1.11979715939522
-1.13400890543884 to -1.14725156541825
-1.3715007664246 to -1.38474342640401
-1.41525082155889 to -1.4284934815383
Changing layer 1's weights from 
-1.41015390917485 to -1.42339656915426
-0.808985224513474 to -0.822227884492879
-0.934687963275377 to -0.947930623254782
-0.94459330156273 to -0.957835961542135
-1.12964608385986 to -1.14288874383927
-1.26788391783661 to -1.28112657781602
-0.997165194301072 to -1.01040785428048
-1.38296822324222 to -1.39621088322163
-0.97130524232811 to -0.984547902307515
-1.3339521051926 to -1.34719476517201
Changing layer 2's weights from 
-0.524763873366775 to -0.53800653334618
-1.0788173587603 to -1.09206001873971
-0.794228068141403 to -0.807470728120808
-0.623027971534194 to -0.636270631513599
-0.800280681399765 to -0.81352334137917
-1.00629030778832 to -1.01953296776772
-0.487477591781081 to -0.500720251760486
-0.559899619368972 to -0.573142279348377
-1.29189791396087 to -1.30514057394028
-1.2469403953833 to -1.26018305536271
Changing layer 3's weights from 
-0.557814172057571 to -0.571056832036976
-0.821367255000534 to -0.834609914979939
-0.57875733688301 to -0.591999996862415
-1.48619656863915 to -1.49943922861856
-0.701433232573928 to -0.714675892553333
-0.831681838778916 to -0.844924498758321
-1.16186668589539 to -1.17510934587479
-0.703353813437881 to -0.716596473417286
-1.03504978731102 to -1.04829244729043
-0.562219908980789 to -0.575462568960194
Changing layer 4's weights from 
-0.661493709830703 to -0.674736369810108
-0.870326510219041 to -0.883569170198446
-1.0708041103167 to -1.0840467702961
-0.783869376925888 to -0.797112036905293
-1.02312218263573 to -1.03636484261513
-0.791656962184372 to -0.804899622163777
-0.796015492228928 to -0.809258152208333
-1.25791301444 to -1.27115567441941
-0.664216926841201 to -0.677459586820606
-1.09651886537499 to -1.10976152535439
Changing layer 5's weights from 
-0.508713892249526 to -0.521956552228931
-0.950787416247788 to -0.964030076227193
-0.525599053649367 to -0.538841713628772
-1.46111123502081 to -1.47435389500022
-0.923842421321336 to -0.937085081300741
-1.32231036439365 to -1.33555302437306
-1.02234505727715 to -1.03558771725655
-0.949898830203477 to -0.963141490182882
-0.560296824721755 to -0.57353948470116
-0.566804936675492 to -0.580047596654897
Trying to learn from memory 6, 2, -0.2
sum -0.276088421996977 distri 0.163331170974541
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.256328087188418 to 0.22299475335838
0.48964367423218 to 0.456310340402142
0.0647184363381224 to 0.0313851025080837
0.0688112488762693 to 0.0354779150462306
Changing layer 0's weights from 
-0.767136026582721 to -0.80046936041276
-1.13906248327256 to -1.1723958171026
-1.37341174896718 to -1.40674508279721
-1.33885249908925 to -1.37218583291928
-0.59098492976189 to -0.624318263591929
-1.2736268829918 to -1.30696021682183
-1.11979715939522 to -1.15313049322526
-1.14725156541825 to -1.18058489924829
-1.38474342640401 to -1.41807676023404
-1.4284934815383 to -1.46182681536833
Changing layer 1's weights from 
-1.42339656915426 to -1.45672990298429
-0.822227884492879 to -0.855561218322918
-0.947930623254782 to -0.981263957084821
-0.957835961542135 to -0.991169295372174
-1.14288874383927 to -1.17622207766931
-1.28112657781602 to -1.31445991164605
-1.01040785428048 to -1.04374118811052
-1.39621088322163 to -1.42954421705166
-0.984547902307515 to -1.01788123613755
-1.34719476517201 to -1.38052809900204
Changing layer 2's weights from 
-0.53800653334618 to -0.571339867176219
-1.09206001873971 to -1.12539335256975
-0.807470728120808 to -0.840804061950847
-0.636270631513599 to -0.669603965343638
-0.81352334137917 to -0.846856675209209
-1.01953296776772 to -1.05286630159776
-0.500720251760486 to -0.534053585590525
-0.573142279348377 to -0.606475613178416
-1.30514057394028 to -1.33847390777031
-1.26018305536271 to -1.29351638919274
Changing layer 3's weights from 
-0.571056832036976 to -0.604390165867015
-0.834609914979939 to -0.867943248809978
-0.591999996862415 to -0.625333330692454
-1.49943922861856 to -1.53277256244859
-0.714675892553333 to -0.748009226383372
-0.844924498758321 to -0.87825783258836
-1.17510934587479 to -1.20844267970483
-0.716596473417286 to -0.749929807247325
-1.04829244729043 to -1.08162578112047
-0.575462568960194 to -0.608795902790233
Changing layer 4's weights from 
-0.674736369810108 to -0.708069703640147
-0.883569170198446 to -0.916902504028485
-1.0840467702961 to -1.11738010412614
-0.797112036905293 to -0.830445370735332
-1.03636484261513 to -1.06969817644517
-0.804899622163777 to -0.838232955993816
-0.809258152208333 to -0.842591486038372
-1.27115567441941 to -1.30448900824944
-0.677459586820606 to -0.710792920650645
-1.10976152535439 to -1.14309485918443
Changing layer 5's weights from 
-0.521956552228931 to -0.55528988605897
-0.964030076227193 to -0.997363410057232
-0.538841713628772 to -0.572175047458811
-1.47435389500022 to -1.50768722883025
-0.937085081300741 to -0.97041841513078
-1.33555302437306 to -1.36888635820309
-1.03558771725655 to -1.06892105108659
-0.963141490182882 to -0.996474824012921
-0.57353948470116 to -0.606872818531199
-0.580047596654897 to -0.613380930484936
Trying to learn from memory 7, 0, -0.2
sum -0.276131566657672 distri -0.602429754134981
Using diff 0.395331079141727 and condRate 0.166666666666667
Changed category 0 weights from 
-0.644134607099352 to -0.657312309933773
-1.04416957118398 to -1.0573472740184
-1.32669026770479 to -1.33986797053921
-1.17518490173703 to -1.18836260457145
Changing layer 0's weights from 
-0.80046936041276 to -0.813647063247181
-1.1723958171026 to -1.18557351993702
-1.40674508279721 to -1.41992278563163
-1.37218583291928 to -1.3853635357537
-0.624318263591929 to -0.63749596642635
-1.30696021682183 to -1.32013791965625
-1.15313049322526 to -1.16630819605968
-1.18058489924829 to -1.19376260208271
-1.41807676023404 to -1.43125446306846
-1.46182681536833 to -1.47500451820275
Changing layer 1's weights from 
-1.45672990298429 to -1.46990760581871
-0.855561218322918 to -0.868738921157339
-0.981263957084821 to -0.994441659919242
-0.991169295372174 to -1.00434699820659
-1.17622207766931 to -1.18939978050373
-1.31445991164605 to -1.32763761448047
-1.04374118811052 to -1.05691889094494
-1.42954421705166 to -1.44272191988608
-1.01788123613755 to -1.03105893897197
-1.38052809900204 to -1.39370580183646
Changing layer 2's weights from 
-0.571339867176219 to -0.58451757001064
-1.12539335256975 to -1.13857105540417
-0.840804061950847 to -0.853981764785268
-0.669603965343638 to -0.682781668178059
-0.846856675209209 to -0.86003437804363
-1.05286630159776 to -1.06604400443218
-0.534053585590525 to -0.547231288424946
-0.606475613178416 to -0.619653316012837
-1.33847390777031 to -1.35165161060473
-1.29351638919274 to -1.30669409202716
Changing layer 3's weights from 
-0.604390165867015 to -0.617567868701436
-0.867943248809978 to -0.881120951644399
-0.625333330692454 to -0.638511033526875
-1.53277256244859 to -1.54595026528301
-0.748009226383372 to -0.761186929217793
-0.87825783258836 to -0.891435535422781
-1.20844267970483 to -1.22162038253925
-0.749929807247325 to -0.763107510081746
-1.08162578112047 to -1.09480348395489
-0.608795902790233 to -0.621973605624654
Changing layer 4's weights from 
-0.708069703640147 to -0.721247406474568
-0.916902504028485 to -0.930080206862906
-1.11738010412614 to -1.13055780696056
-0.830445370735332 to -0.843623073569753
-1.06969817644517 to -1.08287587927959
-0.838232955993816 to -0.851410658828237
-0.842591486038372 to -0.855769188872793
-1.30448900824944 to -1.31766671108386
-0.710792920650645 to -0.723970623485066
-1.14309485918443 to -1.15627256201885
Changing layer 5's weights from 
-0.55528988605897 to -0.568467588893391
-0.997363410057232 to -1.01054111289165
-0.572175047458811 to -0.585352750293232
-1.50768722883025 to -1.52086493166467
-0.97041841513078 to -0.983596117965201
-1.36888635820309 to -1.38206406103751
-1.06892105108659 to -1.08209875392101
-0.996474824012921 to -1.00965252684734
-0.606872818531199 to -0.62005052136562
-0.613380930484936 to -0.626558633319357
Trying to learn from memory 8, 0, -0.2
sum -0.276135473277549 distri -0.602400670628722
Using diff 0.395299065670561 and condRate 0.166666666666667
Changed category 0 weights from 
-0.657312309933773 to -0.670488945652472
-1.0573472740184 to -1.0705239097371
-1.33986797053921 to -1.35304460625791
-1.18836260457145 to -1.20153924029015
Changing layer 0's weights from 
-0.813647063247181 to -0.82682369896588
-1.18557351993702 to -1.19875015565572
-1.41992278563163 to -1.43309942135033
-1.3853635357537 to -1.3985401714724
-0.63749596642635 to -0.650672602145049
-1.32013791965625 to -1.33331455537495
-1.16630819605968 to -1.17948483177838
-1.19376260208271 to -1.20693923780141
-1.43125446306846 to -1.44443109878716
-1.47500451820275 to -1.48818115392145
Changing layer 1's weights from 
-1.46990760581871 to -1.48308424153741
-0.868738921157339 to -0.881915556876038
-0.994441659919242 to -1.00761829563794
-1.00434699820659 to -1.01752363392529
-1.18939978050373 to -1.20257641622243
-1.32763761448047 to -1.34081425019917
-1.05691889094494 to -1.07009552666364
-1.44272191988608 to -1.45589855560478
-1.03105893897197 to -1.04423557469067
-1.39370580183646 to -1.40688243755516
Changing layer 2's weights from 
-0.58451757001064 to -0.597694205729339
-1.13857105540417 to -1.15174769112286
-0.853981764785268 to -0.867158400503967
-0.682781668178059 to -0.695958303896758
-0.86003437804363 to -0.873211013762329
-1.06604400443218 to -1.07922064015088
-0.547231288424946 to -0.560407924143645
-0.619653316012837 to -0.632829951731536
-1.35165161060473 to -1.36482824632343
-1.30669409202716 to -1.31987072774586
Changing layer 3's weights from 
-0.617567868701436 to -0.630744504420135
-0.881120951644399 to -0.894297587363098
-0.638511033526875 to -0.651687669245574
-1.54595026528301 to -1.55912690100171
-0.761186929217793 to -0.774363564936492
-0.891435535422781 to -0.90461217114148
-1.22162038253925 to -1.23479701825795
-0.763107510081746 to -0.776284145800445
-1.09480348395489 to -1.10798011967358
-0.621973605624654 to -0.635150241343353
Changing layer 4's weights from 
-0.721247406474568 to -0.734424042193267
-0.930080206862906 to -0.943256842581605
-1.13055780696056 to -1.14373444267926
-0.843623073569753 to -0.856799709288452
-1.08287587927959 to -1.09605251499829
-0.851410658828237 to -0.864587294546936
-0.855769188872793 to -0.868945824591492
-1.31766671108386 to -1.33084334680256
-0.723970623485066 to -0.737147259203765
-1.15627256201885 to -1.16944919773755
Changing layer 5's weights from 
-0.568467588893391 to -0.58164422461209
-1.01054111289165 to -1.02371774861035
-0.585352750293232 to -0.598529386011931
-1.52086493166467 to -1.53404156738337
-0.983596117965201 to -0.9967727536839
-1.38206406103751 to -1.39524069675621
-1.08209875392101 to -1.09527538963971
-1.00965252684734 to -1.02282916256604
-0.62005052136562 to -0.633227157084319
-0.626558633319357 to -0.639735269038056
Trying to learn from memory 9, 0, -0.2
sum -0.276135473277549 distri -0.602400670628722
Using diff 0.395299065670561 and condRate 0.166666666666667
Changed category 0 weights from 
-0.670488945652472 to -0.683665581371171
-1.0705239097371 to -1.0837005454558
-1.35304460625791 to -1.36622124197661
-1.20153924029015 to -1.21471587600885
Changing layer 0's weights from 
-0.82682369896588 to -0.840000334684579
-1.19875015565572 to -1.21192679137441
-1.43309942135033 to -1.44627605706903
-1.3985401714724 to -1.4117168071911
-0.650672602145049 to -0.663849237863748
-1.33331455537495 to -1.34649119109365
-1.17948483177838 to -1.19266146749708
-1.20693923780141 to -1.22011587352011
-1.44443109878716 to -1.45760773450586
-1.48818115392145 to -1.50135778964015
Changing layer 1's weights from 
-1.48308424153741 to -1.49626087725611
-0.881915556876038 to -0.895092192594737
-1.00761829563794 to -1.02079493135664
-1.01752363392529 to -1.03070026964399
-1.20257641622243 to -1.21575305194113
-1.34081425019917 to -1.35399088591787
-1.07009552666364 to -1.08327216238233
-1.45589855560478 to -1.46907519132348
-1.04423557469067 to -1.05741221040937
-1.40688243755516 to -1.42005907327386
Changing layer 2's weights from 
-0.597694205729339 to -0.610870841448038
-1.15174769112286 to -1.16492432684156
-0.867158400503967 to -0.880335036222666
-0.695958303896758 to -0.709134939615457
-0.873211013762329 to -0.886387649481028
-1.07922064015088 to -1.09239727586958
-0.560407924143645 to -0.573584559862344
-0.632829951731536 to -0.646006587450235
-1.36482824632343 to -1.37800488204213
-1.31987072774586 to -1.33304736346456
Changing layer 3's weights from 
-0.630744504420135 to -0.643921140138834
-0.894297587363098 to -0.907474223081797
-0.651687669245574 to -0.664864304964273
-1.55912690100171 to -1.57230353672041
-0.774363564936492 to -0.787540200655191
-0.90461217114148 to -0.917788806860179
-1.23479701825795 to -1.24797365397665
-0.776284145800445 to -0.789460781519144
-1.10798011967358 to -1.12115675539228
-0.635150241343353 to -0.648326877062052
Changing layer 4's weights from 
-0.734424042193267 to -0.747600677911966
-0.943256842581605 to -0.956433478300304
-1.14373444267926 to -1.15691107839796
-0.856799709288452 to -0.869976345007151
-1.09605251499829 to -1.10922915071699
-0.864587294546936 to -0.877763930265635
-0.868945824591492 to -0.882122460310191
-1.33084334680256 to -1.34401998252126
-0.737147259203765 to -0.750323894922464
-1.16944919773755 to -1.18262583345625
Changing layer 5's weights from 
-0.58164422461209 to -0.594820860330789
-1.02371774861035 to -1.03689438432905
-0.598529386011931 to -0.61170602173063
-1.53404156738337 to -1.54721820310207
-0.9967727536839 to -1.0099493894026
-1.39524069675621 to -1.40841733247491
-1.09527538963971 to -1.10845202535841
-1.02282916256604 to -1.03600579828474
-0.633227157084319 to -0.646403792803018
-0.639735269038056 to -0.652911904756755
Trying to learn from memory 10, 0, -0.2
sum -0.276135473277549 distri -0.602400670628722
Using diff 0.395299065670561 and condRate 0.166666666666667
Changed category 0 weights from 
-0.683665581371171 to -0.69684221708987
-1.0837005454558 to -1.09687718117449
-1.36622124197661 to -1.37939787769531
-1.21471587600885 to -1.22789251172755
Changing layer 0's weights from 
-0.840000334684579 to -0.853176970403278
-1.21192679137441 to -1.22510342709311
-1.44627605706903 to -1.45945269278773
-1.4117168071911 to -1.4248934429098
-0.663849237863748 to -0.677025873582447
-1.34649119109365 to -1.35966782681235
-1.19266146749708 to -1.20583810321578
-1.22011587352011 to -1.23329250923881
-1.45760773450586 to -1.47078437022456
-1.50135778964015 to -1.51453442535885
Changing layer 1's weights from 
-1.49626087725611 to -1.50943751297481
-0.895092192594737 to -0.908268828313436
-1.02079493135664 to -1.03397156707534
-1.03070026964399 to -1.04387690536269
-1.21575305194113 to -1.22892968765983
-1.35399088591787 to -1.36716752163657
-1.08327216238233 to -1.09644879810103
-1.46907519132348 to -1.48225182704218
-1.05741221040937 to -1.07058884612807
-1.42005907327386 to -1.43323570899256
Changing layer 2's weights from 
-0.610870841448038 to -0.624047477166737
-1.16492432684156 to -1.17810096256026
-0.880335036222666 to -0.893511671941365
-0.709134939615457 to -0.722311575334156
-0.886387649481028 to -0.899564285199727
-1.09239727586958 to -1.10557391158828
-0.573584559862344 to -0.586761195581043
-0.646006587450235 to -0.659183223168934
-1.37800488204213 to -1.39118151776083
-1.33304736346456 to -1.34622399918326
Changing layer 3's weights from 
-0.643921140138834 to -0.657097775857533
-0.907474223081797 to -0.920650858800496
-0.664864304964273 to -0.678040940682972
-1.57230353672041 to -1.58548017243911
-0.787540200655191 to -0.80071683637389
-0.917788806860179 to -0.930965442578878
-1.24797365397665 to -1.26115028969535
-0.789460781519144 to -0.802637417237843
-1.12115675539228 to -1.13433339111098
-0.648326877062052 to -0.661503512780751
Changing layer 4's weights from 
-0.747600677911966 to -0.760777313630665
-0.956433478300304 to -0.969610114019003
-1.15691107839796 to -1.17008771411666
-0.869976345007151 to -0.88315298072585
-1.10922915071699 to -1.12240578643569
-0.877763930265635 to -0.890940565984334
-0.882122460310191 to -0.89529909602889
-1.34401998252126 to -1.35719661823996
-0.750323894922464 to -0.763500530641163
-1.18262583345625 to -1.19580246917495
Changing layer 5's weights from 
-0.594820860330789 to -0.607997496049488
-1.03689438432905 to -1.05007102004775
-0.61170602173063 to -0.624882657449329
-1.54721820310207 to -1.56039483882077
-1.0099493894026 to -1.0231260251213
-1.40841733247491 to -1.42159396819361
-1.10845202535841 to -1.12162866107711
-1.03600579828474 to -1.04918243400344
-0.646403792803018 to -0.659580428521717
-0.652911904756755 to -0.666088540475454
Trying to learn from memory 11, 0, -0.2
sum -0.276135473277549 distri -0.602400670628722
Using diff 0.395299065670561 and condRate 0.166666666666667
Changed category 0 weights from 
-0.69684221708987 to -0.710018852808569
-1.09687718117449 to -1.11005381689319
-1.37939787769531 to -1.392574513414
-1.22789251172755 to -1.24106914744624
Changing layer 0's weights from 
-0.853176970403278 to -0.866353606121977
-1.22510342709311 to -1.23828006281181
-1.45945269278773 to -1.47262932850643
-1.4248934429098 to -1.4380700786285
-0.677025873582447 to -0.690202509301146
-1.35966782681235 to -1.37284446253105
-1.20583810321578 to -1.21901473893448
-1.23329250923881 to -1.2464691449575
-1.47078437022456 to -1.48396100594326
-1.51453442535885 to -1.52771106107755
Changing layer 1's weights from 
-1.50943751297481 to -1.52261414869351
-0.908268828313436 to -0.921445464032135
-1.03397156707534 to -1.04714820279404
-1.04387690536269 to -1.05705354108139
-1.22892968765983 to -1.24210632337853
-1.36716752163657 to -1.38034415735527
-1.09644879810103 to -1.10962543381973
-1.48225182704218 to -1.49542846276088
-1.07058884612807 to -1.08376548184677
-1.43323570899256 to -1.44641234471126
Changing layer 2's weights from 
-0.624047477166737 to -0.637224112885436
-1.17810096256026 to -1.19127759827896
-0.893511671941365 to -0.906688307660064
-0.722311575334156 to -0.735488211052855
-0.899564285199727 to -0.912740920918426
-1.10557391158828 to -1.11875054730698
-0.586761195581043 to -0.599937831299742
-0.659183223168934 to -0.672359858887633
-1.39118151776083 to -1.40435815347953
-1.34622399918326 to -1.35940063490196
Changing layer 3's weights from 
-0.657097775857533 to -0.670274411576232
-0.920650858800496 to -0.933827494519195
-0.678040940682972 to -0.691217576401671
-1.58548017243911 to -1.59865680815781
-0.80071683637389 to -0.813893472092589
-0.930965442578878 to -0.944142078297577
-1.26115028969535 to -1.27432692541405
-0.802637417237843 to -0.815814052956542
-1.13433339111098 to -1.14751002682968
-0.661503512780751 to -0.67468014849945
Changing layer 4's weights from 
-0.760777313630665 to -0.773953949349364
-0.969610114019003 to -0.982786749737702
-1.17008771411666 to -1.18326434983536
-0.88315298072585 to -0.896329616444549
-1.12240578643569 to -1.13558242215439
-0.890940565984334 to -0.904117201703033
-0.89529909602889 to -0.908475731747589
-1.35719661823996 to -1.37037325395866
-0.763500530641163 to -0.776677166359862
-1.19580246917495 to -1.20897910489365
Changing layer 5's weights from 
-0.607997496049488 to -0.621174131768187
-1.05007102004775 to -1.06324765576645
-0.624882657449329 to -0.638059293168028
-1.56039483882077 to -1.57357147453947
-1.0231260251213 to -1.03630266084
-1.42159396819361 to -1.43477060391231
-1.12162866107711 to -1.13480529679581
-1.04918243400344 to -1.06235906972214
-0.659580428521717 to -0.672757064240416
-0.666088540475454 to -0.679265176194153
Trying to learn from memory 12, 0, -0.2
sum -0.276135473277549 distri -0.602400670628722
Using diff 0.395299065670561 and condRate 0.166666666666667
Changed category 0 weights from 
-0.710018852808569 to -0.723195488527269
-1.11005381689319 to -1.12323045261189
-1.392574513414 to -1.4057511491327
-1.24106914744624 to -1.25424578316494
Changing layer 0's weights from 
-0.866353606121977 to -0.879530241840677
-1.23828006281181 to -1.25145669853051
-1.47262932850643 to -1.48580596422513
-1.4380700786285 to -1.4512467143472
-0.690202509301146 to -0.703379145019846
-1.37284446253105 to -1.38602109824975
-1.21901473893448 to -1.23219137465318
-1.2464691449575 to -1.2596457806762
-1.48396100594326 to -1.49713764166196
-1.52771106107755 to -1.54088769679625
Changing layer 1's weights from 
-1.52261414869351 to -1.53579078441221
-0.921445464032135 to -0.934622099750835
-1.04714820279404 to -1.06032483851274
-1.05705354108139 to -1.07023017680009
-1.24210632337853 to -1.25528295909722
-1.38034415735527 to -1.39352079307397
-1.10962543381973 to -1.12280206953843
-1.49542846276088 to -1.50860509847958
-1.08376548184677 to -1.09694211756547
-1.44641234471126 to -1.45958898042996
Changing layer 2's weights from 
-0.637224112885436 to -0.650400748604136
-1.19127759827896 to -1.20445423399766
-0.906688307660064 to -0.919864943378764
-0.735488211052855 to -0.748664846771555
-0.912740920918426 to -0.925917556637126
-1.11875054730698 to -1.13192718302568
-0.599937831299742 to -0.613114467018442
-0.672359858887633 to -0.685536494606333
-1.40435815347953 to -1.41753478919823
-1.35940063490196 to -1.37257727062066
Changing layer 3's weights from 
-0.670274411576232 to -0.683451047294932
-0.933827494519195 to -0.947004130237895
-0.691217576401671 to -0.704394212120371
-1.59865680815781 to -1.61183344387651
-0.813893472092589 to -0.827070107811289
-0.944142078297577 to -0.957318714016277
-1.27432692541405 to -1.28750356113275
-0.815814052956542 to -0.828990688675242
-1.14751002682968 to -1.16068666254838
-0.67468014849945 to -0.68785678421815
Changing layer 4's weights from 
-0.773953949349364 to -0.787130585068064
-0.982786749737702 to -0.995963385456402
-1.18326434983536 to -1.19644098555406
-0.896329616444549 to -0.909506252163249
-1.13558242215439 to -1.14875905787309
-0.904117201703033 to -0.917293837421733
-0.908475731747589 to -0.921652367466289
-1.37037325395866 to -1.38354988967736
-0.776677166359862 to -0.789853802078562
-1.20897910489365 to -1.22215574061235
Changing layer 5's weights from 
-0.621174131768187 to -0.634350767486887
-1.06324765576645 to -1.07642429148515
-0.638059293168028 to -0.651235928886728
-1.57357147453947 to -1.58674811025817
-1.03630266084 to -1.0494792965587
-1.43477060391231 to -1.44794723963101
-1.13480529679581 to -1.14798193251451
-1.06235906972214 to -1.07553570544084
-0.672757064240416 to -0.685933699959116
-0.679265176194153 to -0.692441811912853
Trying to learn from memory 12, 0, -0.2
sum -0.276135473277549 distri -0.602400670628722
Using diff 0.395299065670561 and condRate 0.166666666666667
Changed category 0 weights from 
-0.723195488527269 to -0.736372124245968
-1.12323045261189 to -1.13640708833059
-1.4057511491327 to -1.4189277848514
-1.25424578316494 to -1.26742241888364
Changing layer 0's weights from 
-0.879530241840677 to -0.892706877559376
-1.25145669853051 to -1.26463333424921
-1.48580596422513 to -1.49898259994383
-1.4512467143472 to -1.4644233500659
-0.703379145019846 to -0.716555780738545
-1.38602109824975 to -1.39919773396845
-1.23219137465318 to -1.24536801037188
-1.2596457806762 to -1.2728224163949
-1.49713764166196 to -1.51031427738066
-1.54088769679625 to -1.55406433251495
Changing layer 1's weights from 
-1.53579078441221 to -1.54896742013091
-0.934622099750835 to -0.947798735469534
-1.06032483851274 to -1.07350147423144
-1.07023017680009 to -1.08340681251879
-1.25528295909722 to -1.26845959481592
-1.39352079307397 to -1.40669742879267
-1.12280206953843 to -1.13597870525713
-1.50860509847958 to -1.52178173419828
-1.09694211756547 to -1.11011875328417
-1.45958898042996 to -1.47276561614866
Changing layer 2's weights from 
-0.650400748604136 to -0.663577384322835
-1.20445423399766 to -1.21763086971636
-0.919864943378764 to -0.933041579097463
-0.748664846771555 to -0.761841482490254
-0.925917556637126 to -0.939094192355825
-1.13192718302568 to -1.14510381874438
-0.613114467018442 to -0.626291102737141
-0.685536494606333 to -0.698713130325032
-1.41753478919823 to -1.43071142491693
-1.37257727062066 to -1.38575390633936
Changing layer 3's weights from 
-0.683451047294932 to -0.696627683013631
-0.947004130237895 to -0.960180765956594
-0.704394212120371 to -0.71757084783907
-1.61183344387651 to -1.62501007959521
-0.827070107811289 to -0.840246743529988
-0.957318714016277 to -0.970495349734976
-1.28750356113275 to -1.30068019685145
-0.828990688675242 to -0.842167324393941
-1.16068666254838 to -1.17386329826708
-0.68785678421815 to -0.701033419936849
Changing layer 4's weights from 
-0.787130585068064 to -0.800307220786763
-0.995963385456402 to -1.0091400211751
-1.19644098555406 to -1.20961762127276
-0.909506252163249 to -0.922682887881948
-1.14875905787309 to -1.16193569359179
-0.917293837421733 to -0.930470473140432
-0.921652367466289 to -0.934829003184988
-1.38354988967736 to -1.39672652539606
-0.789853802078562 to -0.803030437797261
-1.22215574061235 to -1.23533237633105
Changing layer 5's weights from 
-0.634350767486887 to -0.647527403205586
-1.07642429148515 to -1.08960092720385
-0.651235928886728 to -0.664412564605427
-1.58674811025817 to -1.59992474597687
-1.0494792965587 to -1.0626559322774
-1.44794723963101 to -1.46112387534971
-1.14798193251451 to -1.16115856823321
-1.07553570544084 to -1.08871234115954
-0.685933699959116 to -0.699110335677815
-0.692441811912853 to -0.705618447631552
10/5/2016 11:17:00 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 12, 1, -0.2
sum -0.276135473277549 distri 0.163132598675587
Using diff 1 and condRate 0.166666666666667
Changed category 1 weights from 
0.256328087188418 to 0.22299475335838
0.48964367423218 to 0.456310340402142
0.0647184363381224 to 0.0313851025080837
0.0688112488762693 to 0.0354779150462306
Changing layer 0's weights from 
-0.892706877559376 to -0.926040211389415
-1.26463333424921 to -1.29796666807925
-1.49898259994383 to -1.53231593377387
-1.4644233500659 to -1.49775668389594
-0.716555780738545 to -0.749889114568584
-1.39919773396845 to -1.43253106779849
-1.24536801037188 to -1.27870134420192
-1.2728224163949 to -1.30615575022494
-1.51031427738066 to -1.5436476112107
-1.55406433251495 to -1.58739766634499
Changing layer 1's weights from 
-1.54896742013091 to -1.58230075396095
-0.947798735469534 to -0.981132069299573
-1.07350147423144 to -1.10683480806147
-1.08340681251879 to -1.11674014634883
-1.26845959481592 to -1.30179292864596
-1.40669742879267 to -1.44003076262271
-1.13597870525713 to -1.16931203908717
-1.52178173419828 to -1.55511506802832
-1.11011875328417 to -1.14345208711421
-1.47276561614866 to -1.5060989499787
Changing layer 2's weights from 
-0.663577384322835 to -0.696910718152874
-1.21763086971636 to -1.2509642035464
-0.933041579097463 to -0.966374912927501
-0.761841482490254 to -0.795174816320293
-0.939094192355825 to -0.972427526185863
-1.14510381874438 to -1.17843715257441
-0.626291102737141 to -0.659624436567179
-0.698713130325032 to -0.73204646415507
-1.43071142491693 to -1.46404475874697
-1.38575390633936 to -1.4190872401694
Changing layer 3's weights from 
-0.696627683013631 to -0.72996101684367
-0.960180765956594 to -0.993514099786633
-0.71757084783907 to -0.750904181669109
-1.62501007959521 to -1.65834341342525
-0.840246743529988 to -0.873580077360027
-0.970495349734976 to -1.00382868356501
-1.30068019685145 to -1.33401353068148
-0.842167324393941 to -0.875500658223979
-1.17386329826708 to -1.20719663209712
-0.701033419936849 to -0.734366753766888
Changing layer 4's weights from 
-0.800307220786763 to -0.833640554616802
-1.0091400211751 to -1.04247335500514
-1.20961762127276 to -1.2429509551028
-0.922682887881948 to -0.956016221711987
-1.16193569359179 to -1.19526902742183
-0.930470473140432 to -0.963803806970471
-0.934829003184988 to -0.968162337015027
-1.39672652539606 to -1.4300598592261
-0.803030437797261 to -0.836363771627299
-1.23533237633105 to -1.26866571016108
Changing layer 5's weights from 
-0.647527403205586 to -0.680860737035624
-1.08960092720385 to -1.12293426103389
-0.664412564605427 to -0.697745898435466
-1.59992474597687 to -1.63325807980691
-1.0626559322774 to -1.09598926610743
-1.46112387534971 to -1.49445720917975
-1.16115856823321 to -1.19449190206324
-1.08871234115954 to -1.12204567498957
-0.699110335677815 to -0.732443669507854
-0.705618447631552 to -0.73895178146159
Trying to learn from memory 12, 0, -0.2
sum -0.276135473277549 distri -0.602400670628722
Using diff 0.395299065670561 and condRate 0.166666666666667
Changed category 0 weights from 
-0.736372124245968 to -0.749548759964667
-1.13640708833059 to -1.14958372404929
-1.4189277848514 to -1.4321044205701
-1.26742241888364 to -1.28059905460234
Changing layer 0's weights from 
-0.926040211389415 to -0.939216847108114
-1.29796666807925 to -1.31114330379795
-1.53231593377387 to -1.54549256949257
-1.49775668389594 to -1.51093331961464
-0.749889114568584 to -0.763065750287283
-1.43253106779849 to -1.44570770351719
-1.27870134420192 to -1.29187797992062
-1.30615575022494 to -1.31933238594364
-1.5436476112107 to -1.5568242469294
-1.58739766634499 to -1.60057430206369
Changing layer 1's weights from 
-1.58230075396095 to -1.59547738967965
-0.981132069299573 to -0.994308705018272
-1.10683480806147 to -1.12001144378017
-1.11674014634883 to -1.12991678206753
-1.30179292864596 to -1.31496956436466
-1.44003076262271 to -1.45320739834141
-1.16931203908717 to -1.18248867480587
-1.55511506802832 to -1.56829170374702
-1.14345208711421 to -1.15662872283291
-1.5060989499787 to -1.5192755856974
Changing layer 2's weights from 
-0.696910718152874 to -0.710087353871573
-1.2509642035464 to -1.2641408392651
-0.966374912927501 to -0.979551548646201
-0.795174816320293 to -0.808351452038992
-0.972427526185863 to -0.985604161904563
-1.17843715257441 to -1.19161378829311
-0.659624436567179 to -0.672801072285879
-0.73204646415507 to -0.74522309987377
-1.46404475874697 to -1.47722139446567
-1.4190872401694 to -1.4322638758881
Changing layer 3's weights from 
-0.72996101684367 to -0.743137652562369
-0.993514099786633 to -1.00669073550533
-0.750904181669109 to -0.764080817387808
-1.65834341342525 to -1.67152004914395
-0.873580077360027 to -0.886756713078726
-1.00382868356501 to -1.01700531928371
-1.33401353068148 to -1.34719016640018
-0.875500658223979 to -0.888677293942679
-1.20719663209712 to -1.22037326781582
-0.734366753766888 to -0.747543389485587
Changing layer 4's weights from 
-0.833640554616802 to -0.846817190335501
-1.04247335500514 to -1.05564999072384
-1.2429509551028 to -1.25612759082149
-0.956016221711987 to -0.969192857430686
-1.19526902742183 to -1.20844566314052
-0.963803806970471 to -0.97698044268917
-0.968162337015027 to -0.981338972733726
-1.4300598592261 to -1.4432364949448
-0.836363771627299 to -0.849540407345999
-1.26866571016108 to -1.28184234587978
Changing layer 5's weights from 
-0.680860737035624 to -0.694037372754324
-1.12293426103389 to -1.13611089675259
-0.697745898435466 to -0.710922534154165
-1.63325807980691 to -1.64643471552561
-1.09598926610743 to -1.10916590182613
-1.49445720917975 to -1.50763384489845
-1.19449190206324 to -1.20766853778194
-1.12204567498957 to -1.13522231070827
-0.732443669507854 to -0.745620305226553
-0.73895178146159 to -0.75212841718029
Trying to learn from memory 12, 0, -0.2
sum -0.276135473277549 distri -0.602400670628722
Using diff 0.395299065670561 and condRate 0.166666666666667
Changed category 0 weights from 
-0.749548759964667 to -0.762725395683366
-1.14958372404929 to -1.16276035976799
-1.4321044205701 to -1.4452810562888
-1.28059905460234 to -1.29377569032104
Changing layer 0's weights from 
-0.939216847108114 to -0.952393482826813
-1.31114330379795 to -1.32431993951665
-1.54549256949257 to -1.55866920521127
-1.51093331961464 to -1.52410995533334
-0.763065750287283 to -0.776242386005982
-1.44570770351719 to -1.45888433923589
-1.29187797992062 to -1.30505461563931
-1.31933238594364 to -1.33250902166234
-1.5568242469294 to -1.5700008826481
-1.60057430206369 to -1.61375093778239
Changing layer 1's weights from 
-1.59547738967965 to -1.60865402539835
-0.994308705018272 to -1.00748534073697
-1.12001144378017 to -1.13318807949887
-1.12991678206753 to -1.14309341778623
-1.31496956436466 to -1.32814620008336
-1.45320739834141 to -1.46638403406011
-1.18248867480587 to -1.19566531052457
-1.56829170374702 to -1.58146833946572
-1.15662872283291 to -1.16980535855161
-1.5192755856974 to -1.5324522214161
Changing layer 2's weights from 
-0.710087353871573 to -0.723263989590272
-1.2641408392651 to -1.2773174749838
-0.979551548646201 to -0.9927281843649
-0.808351452038992 to -0.821528087757691
-0.985604161904563 to -0.998780797623262
-1.19161378829311 to -1.20479042401181
-0.672801072285879 to -0.685977708004578
-0.74522309987377 to -0.758399735592469
-1.47722139446567 to -1.49039803018437
-1.4322638758881 to -1.4454405116068
Changing layer 3's weights from 
-0.743137652562369 to -0.756314288281068
-1.00669073550533 to -1.01986737122403
-0.764080817387808 to -0.777257453106507
-1.67152004914395 to -1.68469668486265
-0.886756713078726 to -0.899933348797425
-1.01700531928371 to -1.03018195500241
-1.34719016640018 to -1.36036680211888
-0.888677293942679 to -0.901853929661378
-1.22037326781582 to -1.23354990353452
-0.747543389485587 to -0.760720025204286
Changing layer 4's weights from 
-0.846817190335501 to -0.8599938260542
-1.05564999072384 to -1.06882662644254
-1.25612759082149 to -1.26930422654019
-0.969192857430686 to -0.982369493149385
-1.20844566314052 to -1.22162229885922
-0.97698044268917 to -0.990157078407869
-0.981338972733726 to -0.994515608452425
-1.4432364949448 to -1.4564131306635
-0.849540407345999 to -0.862717043064698
-1.28184234587978 to -1.29501898159848
Changing layer 5's weights from 
-0.694037372754324 to -0.707214008473023
-1.13611089675259 to -1.14928753247128
-0.710922534154165 to -0.724099169872864
-1.64643471552561 to -1.65961135124431
-1.10916590182613 to -1.12234253754483
-1.50763384489845 to -1.52081048061715
-1.20766853778194 to -1.22084517350064
-1.13522231070827 to -1.14839894642697
-0.745620305226553 to -0.758796940945252
-0.75212841718029 to -0.765305052898989
Trying to learn from memory 12, 2, -0.2
sum -0.276135473277549 distri 0.163132598675587
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.22299475335838 to 0.189661419528341
0.456310340402142 to 0.422977006572103
0.0313851025080837 to -0.001948231321955
0.0354779150462306 to 0.00214458121619189
Changing layer 0's weights from 
-0.952393482826813 to -0.985726816656852
-1.32431993951665 to -1.35765327334669
-1.55866920521127 to -1.59200253904131
-1.52410995533334 to -1.55744328916338
-0.776242386005982 to -0.809575719836021
-1.45888433923589 to -1.49221767306593
-1.30505461563931 to -1.33838794946935
-1.33250902166234 to -1.36584235549238
-1.5700008826481 to -1.60333421647814
-1.61375093778239 to -1.64708427161243
Changing layer 1's weights from 
-1.60865402539835 to -1.64198735922839
-1.00748534073697 to -1.04081867456701
-1.13318807949887 to -1.16652141332891
-1.14309341778623 to -1.17642675161626
-1.32814620008336 to -1.3614795339134
-1.46638403406011 to -1.49971736789015
-1.19566531052457 to -1.22899864435461
-1.58146833946572 to -1.61480167329576
-1.16980535855161 to -1.20313869238164
-1.5324522214161 to -1.56578555524614
Changing layer 2's weights from 
-0.723263989590272 to -0.756597323420311
-1.2773174749838 to -1.31065080881384
-0.9927281843649 to -1.02606151819494
-0.821528087757691 to -0.85486142158773
-0.998780797623262 to -1.0321141314533
-1.20479042401181 to -1.23812375784185
-0.685977708004578 to -0.719311041834616
-0.758399735592469 to -0.791733069422508
-1.49039803018437 to -1.52373136401441
-1.4454405116068 to -1.47877384543684
Changing layer 3's weights from 
-0.756314288281068 to -0.789647622111107
-1.01986737122403 to -1.05320070505407
-0.777257453106507 to -0.810590786936546
-1.68469668486265 to -1.71803001869269
-0.899933348797425 to -0.933266682627464
-1.03018195500241 to -1.06351528883245
-1.36036680211888 to -1.39370013594892
-0.901853929661378 to -0.935187263491417
-1.23354990353452 to -1.26688323736456
-0.760720025204286 to -0.794053359034325
Changing layer 4's weights from 
-0.8599938260542 to -0.893327159884239
-1.06882662644254 to -1.10215996027258
-1.26930422654019 to -1.30263756037023
-0.982369493149385 to -1.01570282697942
-1.22162229885922 to -1.25495563268926
-0.990157078407869 to -1.02349041223791
-0.994515608452425 to -1.02784894228246
-1.4564131306635 to -1.48974646449354
-0.862717043064698 to -0.896050376894737
-1.29501898159848 to -1.32835231542852
Changing layer 5's weights from 
-0.707214008473023 to -0.740547342303062
-1.14928753247128 to -1.18262086630132
-0.724099169872864 to -0.757432503702903
-1.65961135124431 to -1.69294468507435
-1.12234253754483 to -1.15567587137487
-1.52081048061715 to -1.55414381444719
-1.22084517350064 to -1.25417850733068
-1.14839894642697 to -1.18173228025701
-0.758796940945252 to -0.792130274775291
-0.765305052898989 to -0.798638386729028
Trying to learn from memory 12, 2, -0.2
sum -0.276135473277549 distri 0.163132598675587
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.189661419528341 to 0.156328085698302
0.422977006572103 to 0.389643672742064
-0.001948231321955 to -0.0352815651519937
0.00214458121619189 to -0.0311887526138468
Changing layer 0's weights from 
-0.985726816656852 to -1.01906015048689
-1.35765327334669 to -1.39098660717673
-1.59200253904131 to -1.62533587287134
-1.55744328916338 to -1.59077662299341
-0.809575719836021 to -0.842909053666059
-1.49221767306593 to -1.52555100689596
-1.33838794946935 to -1.37172128329939
-1.36584235549238 to -1.39917568932242
-1.60333421647814 to -1.63666755030817
-1.64708427161243 to -1.68041760544246
Changing layer 1's weights from 
-1.64198735922839 to -1.67532069305842
-1.04081867456701 to -1.07415200839705
-1.16652141332891 to -1.19985474715895
-1.17642675161626 to -1.2097600854463
-1.3614795339134 to -1.39481286774344
-1.49971736789015 to -1.53305070172018
-1.22899864435461 to -1.26233197818465
-1.61480167329576 to -1.64813500712579
-1.20313869238164 to -1.23647202621168
-1.56578555524614 to -1.59911888907617
Changing layer 2's weights from 
-0.756597323420311 to -0.789930657250349
-1.31065080881384 to -1.34398414264387
-1.02606151819494 to -1.05939485202498
-0.85486142158773 to -0.888194755417768
-1.0321141314533 to -1.06544746528334
-1.23812375784185 to -1.27145709167189
-0.719311041834616 to -0.752644375664655
-0.791733069422508 to -0.825066403252546
-1.52373136401441 to -1.55706469784444
-1.47877384543684 to -1.51210717926687
Changing layer 3's weights from 
-0.789647622111107 to -0.822980955941145
-1.05320070505407 to -1.08653403888411
-0.810590786936546 to -0.843924120766584
-1.71803001869269 to -1.75136335252272
-0.933266682627464 to -0.966600016457502
-1.06351528883245 to -1.09684862266249
-1.39370013594892 to -1.42703346977896
-0.935187263491417 to -0.968520597321455
-1.26688323736456 to -1.30021657119459
-0.794053359034325 to -0.827386692864363
Changing layer 4's weights from 
-0.893327159884239 to -0.926660493714277
-1.10215996027258 to -1.13549329410262
-1.30263756037023 to -1.33597089420027
-1.01570282697942 to -1.04903616080946
-1.25495563268926 to -1.2882889665193
-1.02349041223791 to -1.05682374606795
-1.02784894228246 to -1.0611822761125
-1.48974646449354 to -1.52307979832357
-0.896050376894737 to -0.929383710724775
-1.32835231542852 to -1.36168564925856
Changing layer 5's weights from 
-0.740547342303062 to -0.7738806761331
-1.18262086630132 to -1.21595420013136
-0.757432503702903 to -0.790765837532941
-1.69294468507435 to -1.72627801890438
-1.15567587137487 to -1.18900920520491
-1.55414381444719 to -1.58747714827722
-1.25417850733068 to -1.28751184116072
-1.18173228025701 to -1.21506561408705
-0.792130274775291 to -0.825463608605329
-0.798638386729028 to -0.831971720559066
Trying to learn from memory 13, 0, -0.2
sum -0.276135473277549 distri -0.602400670628722
Using diff 0.395299065670561 and condRate 0.166666666666667
Changed category 0 weights from 
-0.762725395683366 to -0.775902031402065
-1.16276035976799 to -1.17593699548669
-1.4452810562888 to -1.4584576920075
-1.29377569032104 to -1.30695232603974
Changing layer 0's weights from 
-1.01906015048689 to -1.03223678620559
-1.39098660717673 to -1.40416324289542
-1.62533587287134 to -1.63851250859004
-1.59077662299341 to -1.60395325871211
-0.842909053666059 to -0.856085689384758
-1.52555100689596 to -1.53872764261466
-1.37172128329939 to -1.38489791901809
-1.39917568932242 to -1.41235232504112
-1.63666755030817 to -1.64984418602687
-1.68041760544246 to -1.69359424116116
Changing layer 1's weights from 
-1.67532069305842 to -1.68849732877712
-1.07415200839705 to -1.08732864411575
-1.19985474715895 to -1.21303138287765
-1.2097600854463 to -1.222936721165
-1.39481286774344 to -1.40798950346214
-1.53305070172018 to -1.54622733743888
-1.26233197818465 to -1.27550861390334
-1.64813500712579 to -1.66131164284449
-1.23647202621168 to -1.24964866193038
-1.59911888907617 to -1.61229552479487
Changing layer 2's weights from 
-0.789930657250349 to -0.803107292969048
-1.34398414264387 to -1.35716077836257
-1.05939485202498 to -1.07257148774368
-0.888194755417768 to -0.901371391136467
-1.06544746528334 to -1.07862410100204
-1.27145709167189 to -1.28463372739059
-0.752644375664655 to -0.765821011383354
-0.825066403252546 to -0.838243038971245
-1.55706469784444 to -1.57024133356314
-1.51210717926687 to -1.52528381498557
Changing layer 3's weights from 
-0.822980955941145 to -0.836157591659844
-1.08653403888411 to -1.09971067460281
-0.843924120766584 to -0.857100756485284
-1.75136335252272 to -1.76453998824142
-0.966600016457502 to -0.979776652176201
-1.09684862266249 to -1.11002525838119
-1.42703346977896 to -1.44021010549766
-0.968520597321455 to -0.981697233040154
-1.30021657119459 to -1.31339320691329
-0.827386692864363 to -0.840563328583062
Changing layer 4's weights from 
-0.926660493714277 to -0.939837129432976
-1.13549329410262 to -1.14866992982131
-1.33597089420027 to -1.34914752991897
-1.04903616080946 to -1.06221279652816
-1.2882889665193 to -1.301465602238
-1.05682374606795 to -1.07000038178665
-1.0611822761125 to -1.0743589118312
-1.52307979832357 to -1.53625643404227
-0.929383710724775 to -0.942560346443474
-1.36168564925856 to -1.37486228497726
Changing layer 5's weights from 
-0.7738806761331 to -0.787057311851799
-1.21595420013136 to -1.22913083585006
-0.790765837532941 to -0.80394247325164
-1.72627801890438 to -1.73945465462308
-1.18900920520491 to -1.20218584092361
-1.58747714827722 to -1.60065378399592
-1.28751184116072 to -1.30068847687942
-1.21506561408705 to -1.22824224980575
-0.825463608605329 to -0.838640244324029
-0.831971720559066 to -0.845148356277765
Trying to learn from memory 14, 2, -0.2
sum -0.276135473277549 distri 0.163132598675587
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.156328085698302 to 0.122994751868263
0.389643672742064 to 0.356310338912025
-0.0352815651519937 to -0.0686148989820324
-0.0311887526138468 to -0.0645220864438855
Changing layer 0's weights from 
-1.03223678620559 to -1.06557012003563
-1.40416324289542 to -1.43749657672546
-1.63851250859004 to -1.67184584242008
-1.60395325871211 to -1.63728659254215
-0.856085689384758 to -0.889419023214797
-1.53872764261466 to -1.5720609764447
-1.38489791901809 to -1.41823125284813
-1.41235232504112 to -1.44568565887115
-1.64984418602687 to -1.68317751985691
-1.69359424116116 to -1.7269275749912
Changing layer 1's weights from 
-1.68849732877712 to -1.72183066260716
-1.08732864411575 to -1.12066197794579
-1.21303138287765 to -1.24636471670769
-1.222936721165 to -1.25627005499504
-1.40798950346214 to -1.44132283729218
-1.54622733743888 to -1.57956067126892
-1.27550861390334 to -1.30884194773338
-1.66131164284449 to -1.69464497667453
-1.24964866193038 to -1.28298199576042
-1.61229552479487 to -1.64562885862491
Changing layer 2's weights from 
-0.803107292969048 to -0.836440626799087
-1.35716077836257 to -1.39049411219261
-1.07257148774368 to -1.10590482157372
-0.901371391136467 to -0.934704724966506
-1.07862410100204 to -1.11195743483208
-1.28463372739059 to -1.31796706122063
-0.765821011383354 to -0.799154345213393
-0.838243038971245 to -0.871576372801284
-1.57024133356314 to -1.60357466739318
-1.52528381498557 to -1.55861714881561
Changing layer 3's weights from 
-0.836157591659844 to -0.869490925489883
-1.09971067460281 to -1.13304400843285
-0.857100756485284 to -0.890434090315322
-1.76453998824142 to -1.79787332207146
-0.979776652176201 to -1.01310998600624
-1.11002525838119 to -1.14335859221123
-1.44021010549766 to -1.4735434393277
-0.981697233040154 to -1.01503056687019
-1.31339320691329 to -1.34672654074333
-0.840563328583062 to -0.873896662413101
Changing layer 4's weights from 
-0.939837129432976 to -0.973170463263015
-1.14866992982131 to -1.18200326365135
-1.34914752991897 to -1.38248086374901
-1.06221279652816 to -1.0955461303582
-1.301465602238 to -1.33479893606804
-1.07000038178665 to -1.10333371561668
-1.0743589118312 to -1.10769224566124
-1.53625643404227 to -1.56958976787231
-0.942560346443474 to -0.975893680273513
-1.37486228497726 to -1.4081956188073
Changing layer 5's weights from 
-0.787057311851799 to -0.820390645681838
-1.22913083585006 to -1.2624641696801
-0.80394247325164 to -0.837275807081679
-1.73945465462308 to -1.77278798845312
-1.20218584092361 to -1.23551917475365
-1.60065378399592 to -1.63398711782596
-1.30068847687942 to -1.33402181070946
-1.22824224980575 to -1.26157558363579
-0.838640244324029 to -0.871973578154067
-0.845148356277765 to -0.878481690107804
10/5/2016 11:17:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:32 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
Modified index 9's learning in memoryPool to -0.2
Modified index 10's learning in memoryPool to -0.2
Modified index 11's learning in memoryPool to -0.2
Modified index 12's learning in memoryPool to -0.2
Modified index 13's learning in memoryPool to -0.2
Modified index 14's learning in memoryPool to -0.2
Modified index 15's learning in memoryPool to -0.2
Modified index 16's learning in memoryPool to -0.2
Modified index 17's learning in memoryPool to -0.2
Modified index 18's learning in memoryPool to -0.2
Modified index 19's learning in memoryPool to -0.2
Modified index 20's learning in memoryPool to -0.2
Modified index 21's learning in memoryPool to -0.2
Modified index 22's learning in memoryPool to -0.2
Modified index 23's learning in memoryPool to -0.2
Modified index 24's learning in memoryPool to -0.2
Modified index 25's learning in memoryPool to -0.2
10/5/2016 11:17:32 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 15, 0, -0.2
sum -3.70921781624855 distri -5.69045327058484
Using diff 2.90853990839843 and condRate 0.166666666666667
Changed category 0 weights from 
-0.775902031402065 to -0.8728533631267
-1.17593699548669 to -1.27288832721132
-1.4584576920075 to -1.55540902373214
-1.30695232603974 to -1.40390365776438
Changing layer 0's weights from 
-1.06557012003563 to -1.16252145176026
-1.43749657672546 to -1.5344479084501
-1.67184584242008 to -1.76879717414472
-1.63728659254215 to -1.73423792426679
-0.889419023214797 to -0.986370354939432
-1.5720609764447 to -1.66901230816934
-1.41823125284813 to -1.51518258457276
-1.44568565887115 to -1.54263699059579
-1.68317751985691 to -1.78012885158155
-1.7269275749912 to -1.82387890671584
Changing layer 1's weights from 
-1.72183066260716 to -1.8187819943318
-1.12066197794579 to -1.21761330967042
-1.24636471670769 to -1.34331604843232
-1.25627005499504 to -1.35322138671968
-1.44132283729218 to -1.53827416901681
-1.57956067126892 to -1.67651200299356
-1.30884194773338 to -1.40579327945802
-1.69464497667453 to -1.79159630839917
-1.28298199576042 to -1.37993332748506
-1.64562885862491 to -1.74258019034955
Changing layer 2's weights from 
-0.836440626799087 to -0.933391958523722
-1.39049411219261 to -1.48744544391725
-1.10590482157372 to -1.20285615329835
-0.934704724966506 to -1.03165605669114
-1.11195743483208 to -1.20890876655671
-1.31796706122063 to -1.41491839294526
-0.799154345213393 to -0.896105676938028
-0.871576372801284 to -0.968527704525919
-1.60357466739318 to -1.70052599911782
-1.55861714881561 to -1.65556848054025
Changing layer 3's weights from 
-0.869490925489883 to -0.966442257214518
-1.13304400843285 to -1.22999534015748
-0.890434090315322 to -0.987385422039957
-1.79787332207146 to -1.8948246537961
-1.01310998600624 to -1.11006131773088
-1.14335859221123 to -1.24030992393586
-1.4735434393277 to -1.57049477105233
-1.01503056687019 to -1.11198189859483
-1.34672654074333 to -1.44367787246797
-0.873896662413101 to -0.970847994137736
Changing layer 4's weights from 
-0.973170463263015 to -1.07012179498765
-1.18200326365135 to -1.27895459537599
-1.38248086374901 to -1.47943219547364
-1.0955461303582 to -1.19249746208284
-1.33479893606804 to -1.43175026779267
-1.10333371561668 to -1.20028504734132
-1.10769224566124 to -1.20464357738588
-1.56958976787231 to -1.66654109959695
-0.975893680273513 to -1.07284501199815
-1.4081956188073 to -1.50514695053193
Changing layer 5's weights from 
-0.820390645681838 to -0.917341977406473
-1.2624641696801 to -1.35941550140473
-0.837275807081679 to -0.934227138806314
-1.77278798845312 to -1.86973932017776
-1.23551917475365 to -1.33247050647828
-1.63398711782596 to -1.7309384495506
-1.33402181070946 to -1.43097314243409
-1.26157558363579 to -1.35852691536042
-0.871973578154067 to -0.968924909878702
-0.878481690107804 to -0.975433021832439
Trying to learn from memory 16, 0, -0.2
sum -10.9570666975903 distri -14.1067794544545
Using diff 5.8889794312618 and condRate 0.166666666666667
Changed category 0 weights from 
-0.8728533631267 to -1.06915268042718
-1.27288832721132 to -1.46918764451181
-1.55540902373214 to -1.75170834103262
-1.40390365776438 to -1.60020297506486
Changing layer 0's weights from 
-1.16252145176026 to -1.35882076906074
-1.5344479084501 to -1.73074722575058
-1.76879717414472 to -1.9650964914452
-1.73423792426679 to -1.93053724156727
-0.986370354939432 to -1.18266967223991
-1.66901230816934 to -1.86531162546982
-1.51518258457276 to -1.71148190187325
-1.54263699059579 to -1.73893630789627
-1.78012885158155 to -1.97642816888203
-1.82387890671584 to -2.02017822401632
Changing layer 1's weights from 
-1.8187819943318 to -2.01508131163228
-1.21761330967042 to -1.4139126269709
-1.34331604843232 to -1.5396153657328
-1.35322138671968 to -1.54952070402016
-1.53827416901681 to -1.73457348631729
-1.67651200299356 to -1.87281132029404
-1.40579327945802 to -1.6020925967585
-1.79159630839917 to -1.98789562569965
-1.37993332748506 to -1.57623264478554
-1.74258019034955 to -1.93887950765003
Changing layer 2's weights from 
-0.933391958523722 to -1.1296912758242
-1.48744544391725 to -1.68374476121773
-1.20285615329835 to -1.39915547059883
-1.03165605669114 to -1.22795537399162
-1.20890876655671 to -1.40520808385719
-1.41491839294526 to -1.61121771024574
-0.896105676938028 to -1.09240499423851
-0.968527704525919 to -1.1648270218264
-1.70052599911782 to -1.8968253164183
-1.65556848054025 to -1.85186779784073
Changing layer 3's weights from 
-0.966442257214518 to -1.162741574515
-1.22999534015748 to -1.42629465745796
-0.987385422039957 to -1.18368473934044
-1.8948246537961 to -2.09112397109658
-1.11006131773088 to -1.30636063503136
-1.24030992393586 to -1.43660924123634
-1.57049477105233 to -1.76679408835281
-1.11198189859483 to -1.30828121589531
-1.44367787246797 to -1.63997718976845
-0.970847994137736 to -1.16714731143822
Changing layer 4's weights from 
-1.07012179498765 to -1.26642111228813
-1.27895459537599 to -1.47525391267647
-1.47943219547364 to -1.67573151277412
-1.19249746208284 to -1.38879677938332
-1.43175026779267 to -1.62804958509316
-1.20028504734132 to -1.3965843646418
-1.20464357738588 to -1.40094289468636
-1.66654109959695 to -1.86284041689743
-1.07284501199815 to -1.26914432929863
-1.50514695053193 to -1.70144626783241
Changing layer 5's weights from 
-0.917341977406473 to -1.11364129470695
-1.35941550140473 to -1.55571481870522
-0.934227138806314 to -1.13052645610679
-1.86973932017776 to -2.06603863747824
-1.33247050647828 to -1.52876982377876
-1.7309384495506 to -1.92723776685108
-1.43097314243409 to -1.62727245973457
-1.35852691536042 to -1.5548262326609
-0.968924909878702 to -1.16522422717918
-0.975433021832439 to -1.17173233913292
Trying to learn from memory 17, 0, -0.2
sum -10.974329389738 distri -14.127553062638
Using diff 5.89680602033448 and condRate 0.166666666666667
Changed category 0 weights from 
-1.06915268042718 to -1.26571288403397
-1.46918764451181 to -1.6657478481186
-1.75170834103262 to -1.94826854463941
-1.60020297506486 to -1.79676317867165
Changing layer 0's weights from 
-1.35882076906074 to -1.55538097266754
-1.73074722575058 to -1.92730742935737
-1.9650964914452 to -2.16165669505199
-1.93053724156727 to -2.12709744517406
-1.18266967223991 to -1.3792298758467
-1.86531162546982 to -2.06187182907661
-1.71148190187325 to -1.90804210548004
-1.73893630789627 to -1.93549651150306
-1.97642816888203 to -2.17298837248882
-2.02017822401632 to -2.21673842762311
Changing layer 1's weights from 
-2.01508131163228 to -2.21164151523907
-1.4139126269709 to -1.61047283057769
-1.5396153657328 to -1.7361755693396
-1.54952070402016 to -1.74608090762695
-1.73457348631729 to -1.93113368992408
-1.87281132029404 to -2.06937152390083
-1.6020925967585 to -1.79865280036529
-1.98789562569965 to -2.18445582930644
-1.57623264478554 to -1.77279284839233
-1.93887950765003 to -2.13543971125682
Changing layer 2's weights from 
-1.1296912758242 to -1.32625147943099
-1.68374476121773 to -1.88030496482452
-1.39915547059883 to -1.59571567420562
-1.22795537399162 to -1.42451557759841
-1.40520808385719 to -1.60176828746398
-1.61121771024574 to -1.80777791385253
-1.09240499423851 to -1.2889651978453
-1.1648270218264 to -1.36138722543319
-1.8968253164183 to -2.09338552002509
-1.85186779784073 to -2.04842800144752
Changing layer 3's weights from 
-1.162741574515 to -1.35930177812179
-1.42629465745796 to -1.62285486106475
-1.18368473934044 to -1.38024494294723
-2.09112397109658 to -2.28768417470337
-1.30636063503136 to -1.50292083863815
-1.43660924123634 to -1.63316944484314
-1.76679408835281 to -1.9633542919596
-1.30828121589531 to -1.5048414195021
-1.63997718976845 to -1.83653739337524
-1.16714731143822 to -1.36370751504501
Changing layer 4's weights from 
-1.26642111228813 to -1.46298131589492
-1.47525391267647 to -1.67181411628326
-1.67573151277412 to -1.87229171638092
-1.38879677938332 to -1.58535698299011
-1.62804958509316 to -1.82460978869995
-1.3965843646418 to -1.59314456824859
-1.40094289468636 to -1.59750309829315
-1.86284041689743 to -2.05940062050422
-1.26914432929863 to -1.46570453290542
-1.70144626783241 to -1.8980064714392
Changing layer 5's weights from 
-1.11364129470695 to -1.31020149831374
-1.55571481870522 to -1.75227502231201
-1.13052645610679 to -1.32708665971359
-2.06603863747824 to -2.26259884108503
-1.52876982377876 to -1.72533002738555
-1.92723776685108 to -2.12379797045787
-1.62727245973457 to -1.82383266334136
-1.5548262326609 to -1.7513864362677
-1.16522422717918 to -1.36178443078597
-1.17173233913292 to -1.36829254273971
Trying to learn from memory 18, 0, -0.2
sum -11.2740385550228 distri -14.4362947707525
Using diff 5.98076585448541 and condRate 0.166666666666667
Changed category 0 weights from 
-1.26571288403397 to -1.46507174882083
-1.6657478481186 to -1.86510671290546
-1.94826854463941 to -2.14762740942627
-1.79676317867165 to -1.99612204345851
Changing layer 0's weights from 
-1.55538097266754 to -1.75473983745439
-1.92730742935737 to -2.12666629414423
-2.16165669505199 to -2.36101555983885
-2.12709744517406 to -2.32645630996092
-1.3792298758467 to -1.57858874063356
-2.06187182907661 to -2.26123069386347
-1.90804210548004 to -2.1074009702669
-1.93549651150306 to -2.13485537628992
-2.17298837248882 to -2.37234723727568
-2.21673842762311 to -2.41609729240997
Changing layer 1's weights from 
-2.21164151523907 to -2.41100038002593
-1.61047283057769 to -1.80983169536455
-1.7361755693396 to -1.93553443412645
-1.74608090762695 to -1.94543977241381
-1.93113368992408 to -2.13049255471094
-2.06937152390083 to -2.26873038868769
-1.79865280036529 to -1.99801166515215
-2.18445582930644 to -2.3838146940933
-1.77279284839233 to -1.97215171317919
-2.13543971125682 to -2.33479857604368
Changing layer 2's weights from 
-1.32625147943099 to -1.52561034421785
-1.88030496482452 to -2.07966382961138
-1.59571567420562 to -1.79507453899248
-1.42451557759841 to -1.62387444238527
-1.60176828746398 to -1.80112715225084
-1.80777791385253 to -2.00713677863939
-1.2889651978453 to -1.48832406263216
-1.36138722543319 to -1.56074609022005
-2.09338552002509 to -2.29274438481195
-2.04842800144752 to -2.24778686623438
Changing layer 3's weights from 
-1.35930177812179 to -1.55866064290865
-1.62285486106475 to -1.82221372585161
-1.38024494294723 to -1.57960380773409
-2.28768417470337 to -2.48704303949023
-1.50292083863815 to -1.70227970342501
-1.63316944484314 to -1.83252830962999
-1.9633542919596 to -2.16271315674646
-1.5048414195021 to -1.70420028428896
-1.83653739337524 to -2.0358962581621
-1.36370751504501 to -1.56306637983187
Changing layer 4's weights from 
-1.46298131589492 to -1.66234018068178
-1.67181411628326 to -1.87117298107012
-1.87229171638092 to -2.07165058116777
-1.58535698299011 to -1.78471584777697
-1.82460978869995 to -2.02396865348681
-1.59314456824859 to -1.79250343303545
-1.59750309829315 to -1.79686196308001
-2.05940062050422 to -2.25875948529108
-1.46570453290542 to -1.66506339769228
-1.8980064714392 to -2.09736533622606
Changing layer 5's weights from 
-1.31020149831374 to -1.5095603631006
-1.75227502231201 to -1.95163388709887
-1.32708665971359 to -1.52644552450044
-2.26259884108503 to -2.46195770587189
-1.72533002738555 to -1.92468889217241
-2.12379797045787 to -2.32315683524473
-1.82383266334136 to -2.02319152812822
-1.7513864362677 to -1.95074530105455
-1.36178443078597 to -1.56114329557283
-1.36829254273971 to -1.56765140752657
Trying to learn from memory 19, 0, -0.2
sum -11.2701939096653 distri -14.4293559914398
Using diff 5.97671055919082 and condRate 0.166666666666667
Changed category 0 weights from 
-1.46507174882083 to -1.66429543709586
-1.86510671290546 to -2.06433040118048
-2.14762740942627 to -2.34685109770129
-1.99612204345851 to -2.19534573173353
Changing layer 0's weights from 
-1.75473983745439 to -1.95396352572942
-2.12666629414423 to -2.32588998241925
-2.36101555983885 to -2.56023924811387
-2.32645630996092 to -2.52567999823594
-1.57858874063356 to -1.77781242890859
-2.26123069386347 to -2.46045438213849
-2.1074009702669 to -2.30662465854192
-2.13485537628992 to -2.33407906456495
-2.37234723727568 to -2.5715709255507
-2.41609729240997 to -2.61532098068499
Changing layer 1's weights from 
-2.41100038002593 to -2.61022406830095
-1.80983169536455 to -2.00905538363958
-1.93553443412645 to -2.13475812240148
-1.94543977241381 to -2.14466346068883
-2.13049255471094 to -2.32971624298597
-2.26873038868769 to -2.46795407696271
-1.99801166515215 to -2.19723535342717
-2.3838146940933 to -2.58303838236832
-1.97215171317919 to -2.17137540145421
-2.33479857604368 to -2.5340222643187
Changing layer 2's weights from 
-1.52561034421785 to -1.72483403249288
-2.07966382961138 to -2.2788875178864
-1.79507453899248 to -1.99429822726751
-1.62387444238527 to -1.8230981306603
-1.80112715225084 to -2.00035084052587
-2.00713677863939 to -2.20636046691442
-1.48832406263216 to -1.68754775090718
-1.56074609022005 to -1.75996977849507
-2.29274438481195 to -2.49196807308697
-2.24778686623438 to -2.4470105545094
Changing layer 3's weights from 
-1.55866064290865 to -1.75788433118367
-1.82221372585161 to -2.02143741412664
-1.57960380773409 to -1.77882749600911
-2.48704303949023 to -2.68626672776525
-1.70227970342501 to -1.90150339170003
-1.83252830962999 to -2.03175199790502
-2.16271315674646 to -2.36193684502149
-1.70420028428896 to -1.90342397256398
-2.0358962581621 to -2.23511994643712
-1.56306637983187 to -1.76229006810689
Changing layer 4's weights from 
-1.66234018068178 to -1.86156386895681
-1.87117298107012 to -2.07039666934514
-2.07165058116777 to -2.2708742694428
-1.78471584777697 to -1.98393953605199
-2.02396865348681 to -2.22319234176183
-1.79250343303545 to -1.99172712131048
-1.79686196308001 to -1.99608565135503
-2.25875948529108 to -2.4579831735661
-1.66506339769228 to -1.8642870859673
-2.09736533622606 to -2.29658902450109
Changing layer 5's weights from 
-1.5095603631006 to -1.70878405137563
-1.95163388709887 to -2.15085757537389
-1.52644552450044 to -1.72566921277547
-2.46195770587189 to -2.66118139414691
-1.92468889217241 to -2.12391258044744
-2.32315683524473 to -2.52238052351975
-2.02319152812822 to -2.22241521640325
-1.95074530105455 to -2.14996898932958
-1.56114329557283 to -1.76036698384786
-1.56765140752657 to -1.76687509580159
Trying to learn from memory 19, 0, -0.2
sum -11.2701939096653 distri -14.4293559914398
Using diff 5.97671055919082 and condRate 0.166666666666667
Changed category 0 weights from 
-1.66429543709586 to -1.86351912537088
-2.06433040118048 to -2.2635540894555
-2.34685109770129 to -2.54607478597632
-2.19534573173353 to -2.39456942000856
Changing layer 0's weights from 
-1.95396352572942 to -2.15318721400444
-2.32588998241925 to -2.52511367069428
-2.56023924811387 to -2.7594629363889
-2.52567999823594 to -2.72490368651097
-1.77781242890859 to -1.97703611718361
-2.46045438213849 to -2.65967807041352
-2.30662465854192 to -2.50584834681695
-2.33407906456495 to -2.53330275283997
-2.5715709255507 to -2.77079461382573
-2.61532098068499 to -2.81454466896002
Changing layer 1's weights from 
-2.61022406830095 to -2.80944775657598
-2.00905538363958 to -2.2082790719146
-2.13475812240148 to -2.3339818106765
-2.14466346068883 to -2.34388714896386
-2.32971624298597 to -2.52893993126099
-2.46795407696271 to -2.66717776523774
-2.19723535342717 to -2.3964590417022
-2.58303838236832 to -2.78226207064335
-2.17137540145421 to -2.37059908972924
-2.5340222643187 to -2.73324595259373
Changing layer 2's weights from 
-1.72483403249288 to -1.9240577207679
-2.2788875178864 to -2.47811120616143
-1.99429822726751 to -2.19352191554253
-1.8230981306603 to -2.02232181893532
-2.00035084052587 to -2.19957452880089
-2.20636046691442 to -2.40558415518944
-1.68754775090718 to -1.88677143918221
-1.75996977849507 to -1.9591934667701
-2.49196807308697 to -2.691191761362
-2.4470105545094 to -2.64623424278443
Changing layer 3's weights from 
-1.75788433118367 to -1.9571080194587
-2.02143741412664 to -2.22066110240166
-1.77882749600911 to -1.97805118428414
-2.68626672776525 to -2.88549041604028
-1.90150339170003 to -2.10072707997506
-2.03175199790502 to -2.23097568618004
-2.36193684502149 to -2.56116053329651
-1.90342397256398 to -2.10264766083901
-2.23511994643712 to -2.43434363471215
-1.76229006810689 to -1.96151375638192
Changing layer 4's weights from 
-1.86156386895681 to -2.06078755723183
-2.07039666934514 to -2.26962035762017
-2.2708742694428 to -2.47009795771782
-1.98393953605199 to -2.18316322432702
-2.22319234176183 to -2.42241603003685
-1.99172712131048 to -2.1909508095855
-1.99608565135503 to -2.19530933963006
-2.4579831735661 to -2.65720686184113
-1.8642870859673 to -2.06351077424233
-2.29658902450109 to -2.49581271277611
Changing layer 5's weights from 
-1.70878405137563 to -1.90800773965065
-2.15085757537389 to -2.35008126364892
-1.72566921277547 to -1.92489290105049
-2.66118139414691 to -2.86040508242194
-2.12391258044744 to -2.32313626872246
-2.52238052351975 to -2.72160421179478
-2.22241521640325 to -2.42163890467827
-2.14996898932958 to -2.3491926776046
-1.76036698384786 to -1.95959067212288
-1.76687509580159 to -1.96609878407662
Trying to learn from memory 19, 0, -0.2
sum -11.2701939096653 distri -14.4293559914398
Using diff 5.97671055919082 and condRate 0.166666666666667
Changed category 0 weights from 
-1.86351912537088 to -2.06274281364591
-2.2635540894555 to -2.46277777773053
-2.54607478597632 to -2.74529847425134
-2.39456942000856 to -2.59379310828358
Changing layer 0's weights from 
-2.15318721400444 to -2.35241090227947
-2.52511367069428 to -2.7243373589693
-2.7594629363889 to -2.95868662466392
-2.72490368651097 to -2.92412737478599
-1.97703611718361 to -2.17625980545864
-2.65967807041352 to -2.85890175868854
-2.50584834681695 to -2.70507203509197
-2.53330275283997 to -2.732526441115
-2.77079461382573 to -2.97001830210075
-2.81454466896002 to -3.01376835723504
Changing layer 1's weights from 
-2.80944775657598 to -3.008671444851
-2.2082790719146 to -2.40750276018963
-2.3339818106765 to -2.53320549895153
-2.34388714896386 to -2.54311083723888
-2.52893993126099 to -2.72816361953602
-2.66717776523774 to -2.86640145351276
-2.3964590417022 to -2.59568272997722
-2.78226207064335 to -2.98148575891837
-2.37059908972924 to -2.56982277800426
-2.73324595259373 to -2.93246964086875
Changing layer 2's weights from 
-1.9240577207679 to -2.12328140904293
-2.47811120616143 to -2.67733489443645
-2.19352191554253 to -2.39274560381756
-2.02232181893532 to -2.22154550721035
-2.19957452880089 to -2.39879821707592
-2.40558415518944 to -2.60480784346447
-1.88677143918221 to -2.08599512745723
-1.9591934667701 to -2.15841715504512
-2.691191761362 to -2.89041544963702
-2.64623424278443 to -2.84545793105945
Changing layer 3's weights from 
-1.9571080194587 to -2.15633170773372
-2.22066110240166 to -2.41988479067669
-1.97805118428414 to -2.17727487255916
-2.88549041604028 to -3.0847141043153
-2.10072707997506 to -2.29995076825008
-2.23097568618004 to -2.43019937445507
-2.56116053329651 to -2.76038422157154
-2.10264766083901 to -2.30187134911403
-2.43434363471215 to -2.63356732298717
-1.96151375638192 to -2.16073744465694
Changing layer 4's weights from 
-2.06078755723183 to -2.26001124550686
-2.26962035762017 to -2.46884404589519
-2.47009795771782 to -2.66932164599285
-2.18316322432702 to -2.38238691260204
-2.42241603003685 to -2.62163971831188
-2.1909508095855 to -2.39017449786052
-2.19530933963006 to -2.39453302790508
-2.65720686184113 to -2.85643055011615
-2.06351077424233 to -2.26273446251735
-2.49581271277611 to -2.69503640105114
Changing layer 5's weights from 
-1.90800773965065 to -2.10723142792568
-2.35008126364892 to -2.54930495192394
-1.92489290105049 to -2.12411658932552
-2.86040508242194 to -3.05962877069696
-2.32313626872246 to -2.52235995699749
-2.72160421179478 to -2.9208279000698
-2.42163890467827 to -2.6208625929533
-2.3491926776046 to -2.54841636587963
-1.95959067212288 to -2.15881436039791
-1.96609878407662 to -2.16532247235164
Trying to learn from memory 19, 0, -0.2
sum -11.2701939096653 distri -14.4293559914398
Using diff 5.97671055919082 and condRate 0.166666666666667
Changed category 0 weights from 
-2.06274281364591 to -2.26196650192093
-2.46277777773053 to -2.66200146600555
-2.74529847425134 to -2.94452216252637
-2.59379310828358 to -2.79301679655861
Changing layer 0's weights from 
-2.35241090227947 to -2.55163459055449
-2.7243373589693 to -2.92356104724433
-2.95868662466392 to -3.15791031293895
-2.92412737478599 to -3.12335106306102
-2.17625980545864 to -2.37548349373366
-2.85890175868854 to -3.05812544696357
-2.70507203509197 to -2.904295723367
-2.732526441115 to -2.93175012939002
-2.97001830210075 to -3.16924199037578
-3.01376835723504 to -3.21299204551007
Changing layer 1's weights from 
-3.008671444851 to -3.20789513312603
-2.40750276018963 to -2.60672644846465
-2.53320549895153 to -2.73242918722655
-2.54311083723888 to -2.74233452551391
-2.72816361953602 to -2.92738730781104
-2.86640145351276 to -3.06562514178779
-2.59568272997722 to -2.79490641825225
-2.98148575891837 to -3.1807094471934
-2.56982277800426 to -2.76904646627929
-2.93246964086875 to -3.13169332914378
Changing layer 2's weights from 
-2.12328140904293 to -2.32250509731795
-2.67733489443645 to -2.87655858271148
-2.39274560381756 to -2.59196929209258
-2.22154550721035 to -2.42076919548537
-2.39879821707592 to -2.59802190535094
-2.60480784346447 to -2.80403153173949
-2.08599512745723 to -2.28521881573226
-2.15841715504512 to -2.35764084332015
-2.89041544963702 to -3.08963913791205
-2.84545793105945 to -3.04468161933448
Changing layer 3's weights from 
-2.15633170773372 to -2.35555539600875
-2.41988479067669 to -2.61910847895171
-2.17727487255916 to -2.37649856083419
-3.0847141043153 to -3.28393779259033
-2.29995076825008 to -2.49917445652511
-2.43019937445507 to -2.62942306273009
-2.76038422157154 to -2.95960790984656
-2.30187134911403 to -2.50109503738906
-2.63356732298717 to -2.8327910112622
-2.16073744465694 to -2.35996113293197
Changing layer 4's weights from 
-2.26001124550686 to -2.45923493378188
-2.46884404589519 to -2.66806773417022
-2.66932164599285 to -2.86854533426787
-2.38238691260204 to -2.58161060087707
-2.62163971831188 to -2.8208634065869
-2.39017449786052 to -2.58939818613555
-2.39453302790508 to -2.59375671618011
-2.85643055011615 to -3.05565423839118
-2.26273446251735 to -2.46195815079238
-2.69503640105114 to -2.89426008932616
Changing layer 5's weights from 
-2.10723142792568 to -2.3064551162007
-2.54930495192394 to -2.74852864019896
-2.12411658932552 to -2.32334027760054
-3.05962877069696 to -3.25885245897199
-2.52235995699749 to -2.72158364527251
-2.9208279000698 to -3.12005158834483
-2.6208625929533 to -2.82008628122832
-2.54841636587963 to -2.74764005415465
-2.15881436039791 to -2.35803804867293
-2.16532247235164 to -2.36454616062667
Trying to learn from memory 19, 0, -0.2
sum -11.2701939096653 distri -14.4293559914398
Using diff 5.97671055919082 and condRate 0.166666666666667
Changed category 0 weights from 
-2.26196650192093 to -2.46119019019596
-2.66200146600555 to -2.86122515428058
-2.94452216252637 to -3.14374585080139
-2.79301679655861 to -2.99224048483363
Changing layer 0's weights from 
-2.55163459055449 to -2.75085827882952
-2.92356104724433 to -3.12278473551935
-3.15791031293895 to -3.35713400121397
-3.12335106306102 to -3.32257475133604
-2.37548349373366 to -2.57470718200869
-3.05812544696357 to -3.25734913523859
-2.904295723367 to -3.10351941164202
-2.93175012939002 to -3.13097381766505
-3.16924199037578 to -3.3684656786508
-3.21299204551007 to -3.41221573378509
Changing layer 1's weights from 
-3.20789513312603 to -3.40711882140105
-2.60672644846465 to -2.80595013673968
-2.73242918722655 to -2.93165287550158
-2.74233452551391 to -2.94155821378893
-2.92738730781104 to -3.12661099608607
-3.06562514178779 to -3.26484883006281
-2.79490641825225 to -2.99413010652727
-3.1807094471934 to -3.37993313546842
-2.76904646627929 to -2.96827015455431
-3.13169332914378 to -3.3309170174188
Changing layer 2's weights from 
-2.32250509731795 to -2.52172878559298
-2.87655858271148 to -3.0757822709865
-2.59196929209258 to -2.79119298036761
-2.42076919548537 to -2.6199928837604
-2.59802190535094 to -2.79724559362597
-2.80403153173949 to -3.00325522001452
-2.28521881573226 to -2.48444250400728
-2.35764084332015 to -2.55686453159517
-3.08963913791205 to -3.28886282618707
-3.04468161933448 to -3.2439053076095
Changing layer 3's weights from 
-2.35555539600875 to -2.55477908428377
-2.61910847895171 to -2.81833216722674
-2.37649856083419 to -2.57572224910921
-3.28393779259033 to -3.48316148086535
-2.49917445652511 to -2.69839814480013
-2.62942306273009 to -2.82864675100512
-2.95960790984656 to -3.15883159812159
-2.50109503738906 to -2.70031872566408
-2.8327910112622 to -3.03201469953722
-2.35996113293197 to -2.55918482120699
Changing layer 4's weights from 
-2.45923493378188 to -2.65845862205691
-2.66806773417022 to -2.86729142244524
-2.86854533426787 to -3.0677690225429
-2.58161060087707 to -2.78083428915209
-2.8208634065869 to -3.02008709486193
-2.58939818613555 to -2.78862187441057
-2.59375671618011 to -2.79298040445513
-3.05565423839118 to -3.2548779266662
-2.46195815079238 to -2.6611818390674
-2.89426008932616 to -3.09348377760119
Changing layer 5's weights from 
-2.3064551162007 to -2.50567880447573
-2.74852864019896 to -2.94775232847399
-2.32334027760054 to -2.52256396587557
-3.25885245897199 to -3.45807614724701
-2.72158364527251 to -2.92080733354754
-3.12005158834483 to -3.31927527661985
-2.82008628122832 to -3.01930996950335
-2.74764005415465 to -2.94686374242968
-2.35803804867293 to -2.55726173694796
-2.36454616062667 to -2.56376984890169
Trying to learn from memory 19, 2, -0.2
sum -11.2701939096653 distri 0.964899967350802
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.122994751868263 to 0.0896614180382248
0.356310338912025 to 0.322977005081987
-0.0686148989820324 to -0.101948232812071
-0.0645220864438855 to -0.0978554202739242
Changing layer 0's weights from 
-2.75085827882952 to -2.78419161265956
-3.12278473551935 to -3.15611806934939
-3.35713400121397 to -3.39046733504401
-3.32257475133604 to -3.35590808516608
-2.57470718200869 to -2.60804051583873
-3.25734913523859 to -3.29068246906863
-3.10351941164202 to -3.13685274547206
-3.13097381766505 to -3.16430715149508
-3.3684656786508 to -3.40179901248084
-3.41221573378509 to -3.44554906761513
Changing layer 1's weights from 
-3.40711882140105 to -3.44045215523109
-2.80595013673968 to -2.83928347056972
-2.93165287550158 to -2.96498620933162
-2.94155821378893 to -2.97489154761897
-3.12661099608607 to -3.1599443299161
-3.26484883006281 to -3.29818216389285
-2.99413010652727 to -3.02746344035731
-3.37993313546842 to -3.41326646929846
-2.96827015455431 to -3.00160348838435
-3.3309170174188 to -3.36425035124884
Changing layer 2's weights from 
-2.52172878559298 to -2.55506211942302
-3.0757822709865 to -3.10911560481654
-2.79119298036761 to -2.82452631419764
-2.6199928837604 to -2.65332621759043
-2.79724559362597 to -2.83057892745601
-3.00325522001452 to -3.03658855384456
-2.48444250400728 to -2.51777583783732
-2.55686453159517 to -2.59019786542521
-3.28886282618707 to -3.32219616001711
-3.2439053076095 to -3.27723864143954
Changing layer 3's weights from 
-2.55477908428377 to -2.58811241811381
-2.81833216722674 to -2.85166550105677
-2.57572224910921 to -2.60905558293925
-3.48316148086535 to -3.51649481469539
-2.69839814480013 to -2.73173147863017
-2.82864675100512 to -2.86198008483516
-3.15883159812159 to -3.19216493195163
-2.70031872566408 to -2.73365205949412
-3.03201469953722 to -3.06534803336726
-2.55918482120699 to -2.59251815503703
Changing layer 4's weights from 
-2.65845862205691 to -2.69179195588694
-2.86729142244524 to -2.90062475627528
-3.0677690225429 to -3.10110235637294
-2.78083428915209 to -2.81416762298213
-3.02008709486193 to -3.05342042869197
-2.78862187441057 to -2.82195520824061
-2.79298040445513 to -2.82631373828517
-3.2548779266662 to -3.28821126049624
-2.6611818390674 to -2.69451517289744
-3.09348377760119 to -3.12681711143123
Changing layer 5's weights from 
-2.50567880447573 to -2.53901213830577
-2.94775232847399 to -2.98108566230403
-2.52256396587557 to -2.55589729970561
-3.45807614724701 to -3.49140948107705
-2.92080733354754 to -2.95414066737758
-3.31927527661985 to -3.35260861044989
-3.01930996950335 to -3.05264330333339
-2.94686374242968 to -2.98019707625972
-2.55726173694796 to -2.590595070778
-2.56376984890169 to -2.59710318273173
10/5/2016 11:17:32 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 20, 0, -0.2
sum -11.2683776588109 distri -14.4260780392013
Using diff 5.97479479509319 and condRate 0.166666666666667
Changed category 0 weights from 
-2.46119019019596 to -2.66035001966677
-2.86122515428058 to -3.0603849837514
-3.14374585080139 to -3.34290568027221
-2.99224048483363 to -3.19140031430445
Changing layer 0's weights from 
-2.78419161265956 to -2.98335144213038
-3.15611806934939 to -3.35527789882021
-3.39046733504401 to -3.58962716451483
-3.35590808516608 to -3.5550679146369
-2.60804051583873 to -2.80720034530954
-3.29068246906863 to -3.48984229853945
-3.13685274547206 to -3.33601257494288
-3.16430715149508 to -3.3634669809659
-3.40179901248084 to -3.60095884195166
-3.44554906761513 to -3.64470889708595
Changing layer 1's weights from 
-3.44045215523109 to -3.63961198470191
-2.83928347056972 to -3.03844330004053
-2.96498620933162 to -3.16414603880244
-2.97489154761897 to -3.17405137708979
-3.1599443299161 to -3.35910415938692
-3.29818216389285 to -3.49734199336367
-3.02746344035731 to -3.22662326982813
-3.41326646929846 to -3.61242629876928
-3.00160348838435 to -3.20076331785517
-3.36425035124884 to -3.56341018071966
Changing layer 2's weights from 
-2.55506211942302 to -2.75422194889383
-3.10911560481654 to -3.30827543428736
-2.82452631419764 to -3.02368614366846
-2.65332621759043 to -2.85248604706125
-2.83057892745601 to -3.02973875692683
-3.03658855384456 to -3.23574838331538
-2.51777583783732 to -2.71693566730814
-2.59019786542521 to -2.78935769489603
-3.32219616001711 to -3.52135598948793
-3.27723864143954 to -3.47639847091036
Changing layer 3's weights from 
-2.58811241811381 to -2.78727224758463
-2.85166550105677 to -3.05082533052759
-2.60905558293925 to -2.80821541241007
-3.51649481469539 to -3.71565464416621
-2.73173147863017 to -2.93089130810099
-2.86198008483516 to -3.06113991430598
-3.19216493195163 to -3.39132476142245
-2.73365205949412 to -2.93281188896494
-3.06534803336726 to -3.26450786283808
-2.59251815503703 to -2.79167798450785
Changing layer 4's weights from 
-2.69179195588694 to -2.89095178535776
-2.90062475627528 to -3.0997845857461
-3.10110235637294 to -3.30026218584376
-2.81416762298213 to -3.01332745245295
-3.05342042869197 to -3.25258025816279
-2.82195520824061 to -3.02111503771143
-2.82631373828517 to -3.02547356775599
-3.28821126049624 to -3.48737108996706
-2.69451517289744 to -2.89367500236826
-3.12681711143123 to -3.32597694090205
Changing layer 5's weights from 
-2.53901213830577 to -2.73817196777659
-2.98108566230403 to -3.18024549177485
-2.55589729970561 to -2.75505712917643
-3.49140948107705 to -3.69056931054787
-2.95414066737758 to -3.1533004968484
-3.35260861044989 to -3.55176843992071
-3.05264330333339 to -3.25180313280421
-2.98019707625972 to -3.17935690573054
-2.590595070778 to -2.78975490024881
-2.59710318273173 to -2.79626301220255
Trying to learn from memory 21, 0, -0.2
sum -11.2683016256543 distri -14.4259408152847
Using diff 5.97471459604401 and condRate 0.166666666666667
Changed category 0 weights from 
-2.66035001966677 to -2.85950717583591
-3.0603849837514 to -3.25954213992054
-3.34290568027221 to -3.54206283644135
-3.19140031430445 to -3.39055747047359
Changing layer 0's weights from 
-2.98335144213038 to -3.18250859829952
-3.35527789882021 to -3.55443505498935
-3.58962716451483 to -3.78878432068397
-3.5550679146369 to -3.75422507080604
-2.80720034530954 to -3.00635750147868
-3.48984229853945 to -3.68899945470859
-3.33601257494288 to -3.53516973111202
-3.3634669809659 to -3.56262413713504
-3.60095884195166 to -3.8001159981208
-3.64470889708595 to -3.84386605325509
Changing layer 1's weights from 
-3.63961198470191 to -3.83876914087105
-3.03844330004053 to -3.23760045620967
-3.16414603880244 to -3.36330319497158
-3.17405137708979 to -3.37320853325893
-3.35910415938692 to -3.55826131555606
-3.49734199336367 to -3.69649914953281
-3.22662326982813 to -3.42578042599727
-3.61242629876928 to -3.81158345493842
-3.20076331785517 to -3.39992047402431
-3.56341018071966 to -3.7625673368888
Changing layer 2's weights from 
-2.75422194889383 to -2.95337910506297
-3.30827543428736 to -3.5074325904565
-3.02368614366846 to -3.2228432998376
-2.85248604706125 to -3.05164320323039
-3.02973875692683 to -3.22889591309597
-3.23574838331538 to -3.43490553948452
-2.71693566730814 to -2.91609282347728
-2.78935769489603 to -2.98851485106517
-3.52135598948793 to -3.72051314565707
-3.47639847091036 to -3.6755556270795
Changing layer 3's weights from 
-2.78727224758463 to -2.98642940375377
-3.05082533052759 to -3.24998248669673
-2.80821541241007 to -3.00737256857921
-3.71565464416621 to -3.91481180033535
-2.93089130810099 to -3.13004846427013
-3.06113991430598 to -3.26029707047512
-3.39132476142245 to -3.59048191759159
-2.93281188896494 to -3.13196904513408
-3.26450786283808 to -3.46366501900722
-2.79167798450785 to -2.99083514067699
Changing layer 4's weights from 
-2.89095178535776 to -3.0901089415269
-3.0997845857461 to -3.29894174191524
-3.30026218584376 to -3.4994193420129
-3.01332745245295 to -3.21248460862209
-3.25258025816279 to -3.45173741433193
-3.02111503771143 to -3.22027219388057
-3.02547356775599 to -3.22463072392513
-3.48737108996706 to -3.6865282461362
-2.89367500236826 to -3.0928321585374
-3.32597694090205 to -3.52513409707119
Changing layer 5's weights from 
-2.73817196777659 to -2.93732912394573
-3.18024549177485 to -3.37940264794399
-2.75505712917643 to -2.95421428534557
-3.69056931054787 to -3.88972646671701
-3.1533004968484 to -3.35245765301754
-3.55176843992071 to -3.75092559608985
-3.25180313280421 to -3.45096028897335
-3.17935690573054 to -3.37851406189968
-2.78975490024881 to -2.98891205641795
-2.79626301220255 to -2.99542016837169
Trying to learn from memory 22, 1, -0.2
sum -11.2683016256543 distri 2.19328059055396
Using diff 1 and condRate 0.166666666666667
Changed category 1 weights from 
0.22299475335838 to 0.189661419528341
0.456310340402142 to 0.422977006572103
0.0313851025080837 to -0.001948231321955
0.0354779150462306 to 0.00214458121619189
Changing layer 0's weights from 
-3.18250859829952 to -3.21584193212955
-3.55443505498935 to -3.58776838881939
-3.78878432068397 to -3.82211765451401
-3.75422507080604 to -3.78755840463608
-3.00635750147868 to -3.03969083530872
-3.68899945470859 to -3.72233278853863
-3.53516973111202 to -3.56850306494206
-3.56262413713504 to -3.59595747096508
-3.8001159981208 to -3.83344933195084
-3.84386605325509 to -3.87719938708513
Changing layer 1's weights from 
-3.83876914087105 to -3.87210247470109
-3.23760045620967 to -3.27093379003971
-3.36330319497158 to -3.39663652880161
-3.37320853325893 to -3.40654186708897
-3.55826131555606 to -3.5915946493861
-3.69649914953281 to -3.72983248336285
-3.42578042599727 to -3.45911375982731
-3.81158345493842 to -3.84491678876846
-3.39992047402431 to -3.43325380785435
-3.7625673368888 to -3.79590067071884
Changing layer 2's weights from 
-2.95337910506297 to -2.98671243889301
-3.5074325904565 to -3.54076592428654
-3.2228432998376 to -3.25617663366764
-3.05164320323039 to -3.08497653706043
-3.22889591309597 to -3.262229246926
-3.43490553948452 to -3.46823887331455
-2.91609282347728 to -2.94942615730732
-2.98851485106517 to -3.02184818489521
-3.72051314565707 to -3.75384647948711
-3.6755556270795 to -3.70888896090954
Changing layer 3's weights from 
-2.98642940375377 to -3.01976273758381
-3.24998248669673 to -3.28331582052677
-3.00737256857921 to -3.04070590240925
-3.91481180033535 to -3.94814513416539
-3.13004846427013 to -3.16338179810017
-3.26029707047512 to -3.29363040430515
-3.59048191759159 to -3.62381525142162
-3.13196904513408 to -3.16530237896412
-3.46366501900722 to -3.49699835283726
-2.99083514067699 to -3.02416847450703
Changing layer 4's weights from 
-3.0901089415269 to -3.12344227535694
-3.29894174191524 to -3.33227507574528
-3.4994193420129 to -3.53275267584293
-3.21248460862209 to -3.24581794245213
-3.45173741433193 to -3.48507074816197
-3.22027219388057 to -3.25360552771061
-3.22463072392513 to -3.25796405775517
-3.6865282461362 to -3.71986157996624
-3.0928321585374 to -3.12616549236744
-3.52513409707119 to -3.55846743090122
Changing layer 5's weights from 
-2.93732912394573 to -2.97066245777576
-3.37940264794399 to -3.41273598177403
-2.95421428534557 to -2.98754761917561
-3.88972646671701 to -3.92305980054705
-3.35245765301754 to -3.38579098684757
-3.75092559608985 to -3.78425892991989
-3.45096028897335 to -3.48429362280338
-3.37851406189968 to -3.41184739572971
-2.98891205641795 to -3.02224539024799
-2.99542016837169 to -3.02875350220173
Trying to learn from memory 23, 0, -0.2
sum -11.2683016256543 distri -14.4259408152847
Using diff 5.97471459604401 and condRate 0.166666666666667
Changed category 0 weights from 
-2.85950717583591 to -3.05866433200505
-3.25954213992054 to -3.45869929608968
-3.54206283644135 to -3.74121999261049
-3.39055747047359 to -3.58971462664273
Changing layer 0's weights from 
-3.21584193212955 to -3.41499908829869
-3.58776838881939 to -3.78692554498853
-3.82211765451401 to -4.02127481068315
-3.78755840463608 to -3.98671556080522
-3.03969083530872 to -3.23884799147786
-3.72233278853863 to -3.92148994470777
-3.56850306494206 to -3.7676602211112
-3.59595747096508 to -3.79511462713422
-3.83344933195084 to -4.03260648811998
-3.87719938708513 to -4.07635654325427
Changing layer 1's weights from 
-3.87210247470109 to -4.07125963087023
-3.27093379003971 to -3.47009094620885
-3.39663652880161 to -3.59579368497075
-3.40654186708897 to -3.60569902325811
-3.5915946493861 to -3.79075180555524
-3.72983248336285 to -3.92898963953199
-3.45911375982731 to -3.65827091599645
-3.84491678876846 to -4.0440739449376
-3.43325380785435 to -3.63241096402349
-3.79590067071884 to -3.99505782688798
Changing layer 2's weights from 
-2.98671243889301 to -3.18586959506215
-3.54076592428654 to -3.73992308045568
-3.25617663366764 to -3.45533378983678
-3.08497653706043 to -3.28413369322957
-3.262229246926 to -3.46138640309514
-3.46823887331455 to -3.66739602948369
-2.94942615730732 to -3.14858331347646
-3.02184818489521 to -3.22100534106435
-3.75384647948711 to -3.95300363565625
-3.70888896090954 to -3.90804611707868
Changing layer 3's weights from 
-3.01976273758381 to -3.21891989375295
-3.28331582052677 to -3.48247297669591
-3.04070590240925 to -3.23986305857839
-3.94814513416539 to -4.14730229033453
-3.16338179810017 to -3.36253895426931
-3.29363040430515 to -3.49278756047429
-3.62381525142162 to -3.82297240759076
-3.16530237896412 to -3.36445953513326
-3.49699835283726 to -3.6961555090064
-3.02416847450703 to -3.22332563067617
Changing layer 4's weights from 
-3.12344227535694 to -3.32259943152608
-3.33227507574528 to -3.53143223191442
-3.53275267584293 to -3.73190983201207
-3.24581794245213 to -3.44497509862127
-3.48507074816197 to -3.68422790433111
-3.25360552771061 to -3.45276268387975
-3.25796405775517 to -3.45712121392431
-3.71986157996624 to -3.91901873613538
-3.12616549236744 to -3.32532264853658
-3.55846743090122 to -3.75762458707036
Changing layer 5's weights from 
-2.97066245777576 to -3.1698196139449
-3.41273598177403 to -3.61189313794317
-2.98754761917561 to -3.18670477534475
-3.92305980054705 to -4.12221695671619
-3.38579098684757 to -3.58494814301671
-3.78425892991989 to -3.98341608608903
-3.48429362280338 to -3.68345077897252
-3.41184739572971 to -3.61100455189885
-3.02224539024799 to -3.22140254641713
-3.02875350220173 to -3.22791065837087
Trying to learn from memory 23, 0, -0.2
sum -11.2683016256543 distri -14.4259408152847
Using diff 5.97471459604401 and condRate 0.166666666666667
Changed category 0 weights from 
-3.05866433200505 to -3.25782148817419
-3.45869929608968 to -3.65785645225882
-3.74121999261049 to -3.94037714877963
-3.58971462664273 to -3.78887178281187
Changing layer 0's weights from 
-3.41499908829869 to -3.61415624446783
-3.78692554498853 to -3.98608270115767
-4.02127481068315 to -4.22043196685229
-3.98671556080522 to -4.18587271697436
-3.23884799147786 to -3.438005147647
-3.92148994470777 to -4.12064710087691
-3.7676602211112 to -3.96681737728034
-3.79511462713422 to -3.99427178330336
-4.03260648811998 to -4.23176364428912
-4.07635654325427 to -4.27551369942341
Changing layer 1's weights from 
-4.07125963087023 to -4.27041678703937
-3.47009094620885 to -3.66924810237799
-3.59579368497075 to -3.79495084113989
-3.60569902325811 to -3.80485617942725
-3.79075180555524 to -3.98990896172438
-3.92898963953199 to -4.12814679570113
-3.65827091599645 to -3.85742807216559
-4.0440739449376 to -4.24323110110674
-3.63241096402349 to -3.83156812019263
-3.99505782688798 to -4.19421498305712
Changing layer 2's weights from 
-3.18586959506215 to -3.38502675123129
-3.73992308045568 to -3.93908023662482
-3.45533378983678 to -3.65449094600592
-3.28413369322957 to -3.48329084939871
-3.46138640309514 to -3.66054355926428
-3.66739602948369 to -3.86655318565283
-3.14858331347646 to -3.3477404696456
-3.22100534106435 to -3.42016249723349
-3.95300363565625 to -4.15216079182539
-3.90804611707868 to -4.10720327324782
Changing layer 3's weights from 
-3.21891989375295 to -3.41807704992209
-3.48247297669591 to -3.68163013286505
-3.23986305857839 to -3.43902021474753
-4.14730229033453 to -4.34645944650367
-3.36253895426931 to -3.56169611043845
-3.49278756047429 to -3.69194471664343
-3.82297240759076 to -4.0221295637599
-3.36445953513326 to -3.5636166913024
-3.6961555090064 to -3.89531266517554
-3.22332563067617 to -3.42248278684531
Changing layer 4's weights from 
-3.32259943152608 to -3.52175658769522
-3.53143223191442 to -3.73058938808356
-3.73190983201207 to -3.93106698818121
-3.44497509862127 to -3.64413225479041
-3.68422790433111 to -3.88338506050025
-3.45276268387975 to -3.65191984004889
-3.45712121392431 to -3.65627837009345
-3.91901873613538 to -4.11817589230452
-3.32532264853658 to -3.52447980470572
-3.75762458707036 to -3.9567817432395
Changing layer 5's weights from 
-3.1698196139449 to -3.36897677011404
-3.61189313794317 to -3.81105029411231
-3.18670477534475 to -3.38586193151388
-4.12221695671619 to -4.32137411288533
-3.58494814301671 to -3.78410529918585
-3.98341608608903 to -4.18257324225817
-3.68345077897252 to -3.88260793514166
-3.61100455189885 to -3.81016170806799
-3.22140254641713 to -3.42055970258627
-3.22791065837087 to -3.42706781454001
Trying to learn from memory 23, 1, -0.2
sum -11.2683016256543 distri 2.19328059055396
Using diff 1 and condRate 0.166666666666667
Changed category 1 weights from 
0.189661419528341 to 0.156328085698302
0.422977006572103 to 0.389643672742064
-0.001948231321955 to -0.0352815651519937
0.00214458121619189 to -0.0311887526138468
Changing layer 0's weights from 
-3.61415624446783 to -3.64748957829787
-3.98608270115767 to -4.01941603498771
-4.22043196685229 to -4.25376530068233
-4.18587271697436 to -4.2192060508044
-3.438005147647 to -3.47133848147704
-4.12064710087691 to -4.15398043470695
-3.96681737728034 to -4.00015071111037
-3.99427178330336 to -4.0276051171334
-4.23176364428912 to -4.26509697811916
-4.27551369942341 to -4.30884703325345
Changing layer 1's weights from 
-4.27041678703937 to -4.30375012086941
-3.66924810237799 to -3.70258143620803
-3.79495084113989 to -3.82828417496993
-3.80485617942725 to -3.83818951325729
-3.98990896172438 to -4.02324229555442
-4.12814679570113 to -4.16148012953117
-3.85742807216559 to -3.89076140599563
-4.24323110110674 to -4.27656443493678
-3.83156812019263 to -3.86490145402267
-4.19421498305712 to -4.22754831688716
Changing layer 2's weights from 
-3.38502675123129 to -3.41836008506133
-3.93908023662482 to -3.97241357045486
-3.65449094600592 to -3.68782427983596
-3.48329084939871 to -3.51662418322875
-3.66054355926428 to -3.69387689309432
-3.86655318565283 to -3.89988651948287
-3.3477404696456 to -3.38107380347564
-3.42016249723349 to -3.45349583106353
-4.15216079182539 to -4.18549412565543
-4.10720327324782 to -4.14053660707786
Changing layer 3's weights from 
-3.41807704992209 to -3.45141038375213
-3.68163013286505 to -3.71496346669509
-3.43902021474753 to -3.47235354857757
-4.34645944650367 to -4.37979278033371
-3.56169611043845 to -3.59502944426848
-3.69194471664343 to -3.72527805047347
-4.0221295637599 to -4.05546289758994
-3.5636166913024 to -3.59695002513244
-3.89531266517554 to -3.92864599900558
-3.42248278684531 to -3.45581612067535
Changing layer 4's weights from 
-3.52175658769522 to -3.55508992152526
-3.73058938808356 to -3.7639227219136
-3.93106698818121 to -3.96440032201125
-3.64413225479041 to -3.67746558862045
-3.88338506050025 to -3.91671839433028
-3.65191984004889 to -3.68525317387893
-3.65627837009345 to -3.68961170392349
-4.11817589230452 to -4.15150922613456
-3.52447980470572 to -3.55781313853576
-3.9567817432395 to -3.99011507706954
Changing layer 5's weights from 
-3.36897677011404 to -3.40231010394408
-3.81105029411231 to -3.84438362794234
-3.38586193151388 to -3.41919526534392
-4.32137411288533 to -4.35470744671537
-3.78410529918585 to -3.81743863301589
-4.18257324225817 to -4.21590657608821
-3.88260793514166 to -3.9159412689717
-3.81016170806799 to -3.84349504189803
-3.42055970258627 to -3.45389303641631
-3.42706781454001 to -3.46040114837005
Trying to learn from memory 24, 0, -0.2
sum -11.2683016256543 distri -14.4259408152847
Using diff 5.97471459604401 and condRate 0.166666666666667
Changed category 0 weights from 
-3.25782148817419 to -3.45697864434333
-3.65785645225882 to -3.85701360842796
-3.94037714877963 to -4.13953430494877
-3.78887178281187 to -3.98802893898101
Changing layer 0's weights from 
-3.64748957829787 to -3.84664673446701
-4.01941603498771 to -4.21857319115685
-4.25376530068233 to -4.45292245685147
-4.2192060508044 to -4.41836320697354
-3.47133848147704 to -3.67049563764618
-4.15398043470695 to -4.35313759087609
-4.00015071111037 to -4.19930786727951
-4.0276051171334 to -4.22676227330254
-4.26509697811916 to -4.4642541342883
-4.30884703325345 to -4.50800418942259
Changing layer 1's weights from 
-4.30375012086941 to -4.50290727703855
-3.70258143620803 to -3.90173859237717
-3.82828417496993 to -4.02744133113907
-3.83818951325729 to -4.03734666942643
-4.02324229555442 to -4.22239945172356
-4.16148012953117 to -4.36063728570031
-3.89076140599563 to -4.08991856216477
-4.27656443493678 to -4.47572159110592
-3.86490145402267 to -4.06405861019181
-4.22754831688716 to -4.4267054730563
Changing layer 2's weights from 
-3.41836008506133 to -3.61751724123047
-3.97241357045486 to -4.171570726624
-3.68782427983596 to -3.8869814360051
-3.51662418322875 to -3.71578133939789
-3.69387689309432 to -3.89303404926346
-3.89988651948287 to -4.09904367565201
-3.38107380347564 to -3.58023095964478
-3.45349583106353 to -3.65265298723267
-4.18549412565543 to -4.38465128182457
-4.14053660707786 to -4.339693763247
Changing layer 3's weights from 
-3.45141038375213 to -3.65056753992127
-3.71496346669509 to -3.91412062286423
-3.47235354857757 to -3.67151070474671
-4.37979278033371 to -4.57894993650285
-3.59502944426848 to -3.79418660043762
-3.72527805047347 to -3.92443520664261
-4.05546289758994 to -4.25462005375908
-3.59695002513244 to -3.79610718130158
-3.92864599900558 to -4.12780315517472
-3.45581612067535 to -3.65497327684449
Changing layer 4's weights from 
-3.55508992152526 to -3.7542470776944
-3.7639227219136 to -3.96307987808274
-3.96440032201125 to -4.16355747818039
-3.67746558862045 to -3.87662274478958
-3.91671839433028 to -4.11587555049942
-3.68525317387893 to -3.88441033004807
-3.68961170392349 to -3.88876886009262
-4.15150922613456 to -4.3506663823037
-3.55781313853576 to -3.7569702947049
-3.99011507706954 to -4.18927223323868
Changing layer 5's weights from 
-3.40231010394408 to -3.60146726011322
-3.84438362794234 to -4.04354078411148
-3.41919526534392 to -3.61835242151306
-4.35470744671537 to -4.55386460288451
-3.81743863301589 to -4.01659578918503
-4.21590657608821 to -4.41506373225735
-3.9159412689717 to -4.11509842514084
-3.84349504189803 to -4.04265219806717
-3.45389303641631 to -3.65305019258545
-3.46040114837005 to -3.65955830453919
Trying to learn from memory 25, 0, -0.2
sum -11.2683016256543 distri -14.4259408152847
Using diff 5.97471459604401 and condRate 0.166666666666667
Changed category 0 weights from 
-3.45697864434333 to -3.65613580051247
-3.85701360842796 to -4.0561707645971
-4.13953430494877 to -4.33869146111791
-3.98802893898101 to -4.18718609515015
Changing layer 0's weights from 
-3.84664673446701 to -4.04580389063615
-4.21857319115685 to -4.41773034732599
-4.45292245685147 to -4.6520796130206
-4.41836320697354 to -4.61752036314268
-3.67049563764618 to -3.86965279381532
-4.35313759087609 to -4.55229474704523
-4.19930786727951 to -4.39846502344865
-4.22676227330254 to -4.42591942947168
-4.4642541342883 to -4.66341129045744
-4.50800418942259 to -4.70716134559173
Changing layer 1's weights from 
-4.50290727703855 to -4.70206443320768
-3.90173859237717 to -4.10089574854631
-4.02744133113907 to -4.22659848730821
-4.03734666942643 to -4.23650382559557
-4.22239945172356 to -4.4215566078927
-4.36063728570031 to -4.55979444186945
-4.08991856216477 to -4.28907571833391
-4.47572159110592 to -4.67487874727505
-4.06405861019181 to -4.26321576636095
-4.4267054730563 to -4.62586262922544
Changing layer 2's weights from 
-3.61751724123047 to -3.81667439739961
-4.171570726624 to -4.37072788279314
-3.8869814360051 to -4.08613859217424
-3.71578133939789 to -3.91493849556703
-3.89303404926346 to -4.0921912054326
-4.09904367565201 to -4.29820083182115
-3.58023095964478 to -3.77938811581392
-3.65265298723267 to -3.85181014340181
-4.38465128182457 to -4.58380843799371
-4.339693763247 to -4.53885091941614
Changing layer 3's weights from 
-3.65056753992127 to -3.84972469609041
-3.91412062286423 to -4.11327777903337
-3.67151070474671 to -3.87066786091585
-4.57894993650285 to -4.77810709267199
-3.79418660043762 to -3.99334375660676
-3.92443520664261 to -4.12359236281175
-4.25462005375908 to -4.45377720992822
-3.79610718130158 to -3.99526433747072
-4.12780315517472 to -4.32696031134386
-3.65497327684449 to -3.85413043301363
Changing layer 4's weights from 
-3.7542470776944 to -3.95340423386354
-3.96307987808274 to -4.16223703425188
-4.16355747818039 to -4.36271463434953
-3.87662274478958 to -4.07577990095872
-4.11587555049942 to -4.31503270666856
-3.88441033004807 to -4.08356748621721
-3.88876886009262 to -4.08792601626176
-4.3506663823037 to -4.54982353847284
-3.7569702947049 to -3.95612745087404
-4.18927223323868 to -4.38842938940782
Changing layer 5's weights from 
-3.60146726011322 to -3.80062441628236
-4.04354078411148 to -4.24269794028062
-3.61835242151306 to -3.8175095776822
-4.55386460288451 to -4.75302175905365
-4.01659578918503 to -4.21575294535417
-4.41506373225735 to -4.61422088842649
-4.11509842514084 to -4.31425558130998
-4.04265219806717 to -4.24180935423631
-3.65305019258545 to -3.85220734875459
-3.65955830453919 to -3.85871546070833
Trying to learn from memory 26, 0, -0.2
sum -11.2683016256543 distri -14.4259408152847
Using diff 5.97471459604401 and condRate 0.166666666666667
Changed category 0 weights from 
-3.65613580051247 to -3.85529295668161
-4.0561707645971 to -4.25532792076624
-4.33869146111791 to -4.53784861728705
-4.18718609515015 to -4.38634325131929
Changing layer 0's weights from 
-4.04580389063615 to -4.24496104680529
-4.41773034732599 to -4.61688750349513
-4.6520796130206 to -4.85123676918974
-4.61752036314268 to -4.81667751931182
-3.86965279381532 to -4.06880994998446
-4.55229474704523 to -4.75145190321436
-4.39846502344865 to -4.59762217961779
-4.42591942947168 to -4.62507658564082
-4.66341129045744 to -4.86256844662657
-4.70716134559173 to -4.90631850176086
Changing layer 1's weights from 
-4.70206443320768 to -4.90122158937682
-4.10089574854631 to -4.30005290471545
-4.22659848730821 to -4.42575564347735
-4.23650382559557 to -4.4356609817647
-4.4215566078927 to -4.62071376406184
-4.55979444186945 to -4.75895159803859
-4.28907571833391 to -4.48823287450305
-4.67487874727505 to -4.87403590344419
-4.26321576636095 to -4.46237292253008
-4.62586262922544 to -4.82501978539458
Changing layer 2's weights from 
-3.81667439739961 to -4.01583155356875
-4.37072788279314 to -4.56988503896228
-4.08613859217424 to -4.28529574834338
-3.91493849556703 to -4.11409565173617
-4.0921912054326 to -4.29134836160174
-4.29820083182115 to -4.49735798799029
-3.77938811581392 to -3.97854527198306
-3.85181014340181 to -4.05096729957095
-4.58380843799371 to -4.78296559416285
-4.53885091941614 to -4.73800807558528
Changing layer 3's weights from 
-3.84972469609041 to -4.04888185225955
-4.11327777903337 to -4.31243493520251
-3.87066786091585 to -4.06982501708499
-4.77810709267199 to -4.97726424884112
-3.99334375660676 to -4.1925009127759
-4.12359236281175 to -4.32274951898089
-4.45377720992822 to -4.65293436609736
-3.99526433747072 to -4.19442149363986
-4.32696031134386 to -4.526117467513
-3.85413043301363 to -4.05328758918276
Changing layer 4's weights from 
-3.95340423386354 to -4.15256139003268
-4.16223703425188 to -4.36139419042102
-4.36271463434953 to -4.56187179051867
-4.07577990095872 to -4.27493705712786
-4.31503270666856 to -4.5141898628377
-4.08356748621721 to -4.28272464238635
-4.08792601626176 to -4.2870831724309
-4.54982353847284 to -4.74898069464198
-3.95612745087404 to -4.15528460704318
-4.38842938940782 to -4.58758654557696
Changing layer 5's weights from 
-3.80062441628236 to -3.9997815724515
-4.24269794028062 to -4.44185509644976
-3.8175095776822 to -4.01666673385134
-4.75302175905365 to -4.95217891522278
-4.21575294535417 to -4.41491010152331
-4.61422088842649 to -4.81337804459563
-4.31425558130998 to -4.51341273747912
-4.24180935423631 to -4.44096651040545
-3.85220734875459 to -4.05136450492373
-3.85871546070833 to -4.05787261687747
Trying to learn from memory 26, 0, -0.2
sum -11.2683016256543 distri -14.4259408152847
Using diff 5.97471459604401 and condRate 0.166666666666667
Changed category 0 weights from 
-3.85529295668161 to -4.05445011285075
-4.25532792076624 to -4.45448507693538
-4.53784861728705 to -4.73700577345619
-4.38634325131929 to -4.58550040748843
Changing layer 0's weights from 
-4.24496104680529 to -4.44411820297443
-4.61688750349513 to -4.81604465966427
-4.85123676918974 to -5.05039392535888
-4.81667751931182 to -5.01583467548095
-4.06880994998446 to -4.2679671061536
-4.75145190321436 to -4.9506090593835
-4.59762217961779 to -4.79677933578693
-4.62507658564082 to -4.82423374180996
-4.86256844662657 to -5.06172560279571
-4.90631850176086 to -5.10547565793
Changing layer 1's weights from 
-4.90122158937682 to -5.10037874554596
-4.30005290471545 to -4.49921006088459
-4.42575564347735 to -4.62491279964649
-4.4356609817647 to -4.63481813793384
-4.62071376406184 to -4.81987092023098
-4.75895159803859 to -4.95810875420772
-4.48823287450305 to -4.68739003067219
-4.87403590344419 to -5.07319305961333
-4.46237292253008 to -4.66153007869922
-4.82501978539458 to -5.02417694156371
Changing layer 2's weights from 
-4.01583155356875 to -4.21498870973789
-4.56988503896228 to -4.76904219513141
-4.28529574834338 to -4.48445290451252
-4.11409565173617 to -4.31325280790531
-4.29134836160174 to -4.49050551777088
-4.49735798799029 to -4.69651514415943
-3.97854527198306 to -4.1777024281522
-4.05096729957095 to -4.25012445574009
-4.78296559416285 to -4.98212275033198
-4.73800807558528 to -4.93716523175442
Changing layer 3's weights from 
-4.04888185225955 to -4.24803900842869
-4.31243493520251 to -4.51159209137165
-4.06982501708499 to -4.26898217325413
-4.97726424884112 to -5.17642140501026
-4.1925009127759 to -4.39165806894504
-4.32274951898089 to -4.52190667515003
-4.65293436609736 to -4.8520915222665
-4.19442149363986 to -4.393578649809
-4.526117467513 to -4.72527462368214
-4.05328758918276 to -4.2524447453519
Changing layer 4's weights from 
-4.15256139003268 to -4.35171854620182
-4.36139419042102 to -4.56055134659016
-4.56187179051867 to -4.76102894668781
-4.27493705712786 to -4.474094213297
-4.5141898628377 to -4.71334701900684
-4.28272464238635 to -4.48188179855549
-4.2870831724309 to -4.48624032860004
-4.74898069464198 to -4.94813785081111
-4.15528460704318 to -4.35444176321232
-4.58758654557696 to -4.7867437017461
Changing layer 5's weights from 
-3.9997815724515 to -4.19893872862064
-4.44185509644976 to -4.6410122526189
-4.01666673385134 to -4.21582389002048
-4.95217891522278 to -5.15133607139192
-4.41491010152331 to -4.61406725769245
-4.81337804459563 to -5.01253520076476
-4.51341273747912 to -4.71256989364826
-4.44096651040545 to -4.64012366657459
-4.05136450492373 to -4.25052166109287
-4.05787261687747 to -4.25702977304661
10/5/2016 11:17:33 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 26, 0, -0.2
sum -11.2683016256543 distri -14.4259408152847
Using diff 5.97471459604401 and condRate 0.166666666666667
Changed category 0 weights from 
-4.05445011285075 to -4.25360726901989
-4.45448507693538 to -4.65364223310452
-4.73700577345619 to -4.93616292962533
-4.58550040748843 to -4.78465756365757
Changing layer 0's weights from 
-4.44411820297443 to -4.64327535914357
-4.81604465966427 to -5.01520181583341
-5.05039392535888 to -5.24955108152802
-5.01583467548095 to -5.21499183165009
-4.2679671061536 to -4.46712426232274
-4.9506090593835 to -5.14976621555264
-4.79677933578693 to -4.99593649195607
-4.82423374180996 to -5.0233908979791
-5.06172560279571 to -5.26088275896485
-5.10547565793 to -5.30463281409914
Changing layer 1's weights from 
-5.10037874554596 to -5.2995359017151
-4.49921006088459 to -4.69836721705373
-4.62491279964649 to -4.82406995581563
-4.63481813793384 to -4.83397529410298
-4.81987092023098 to -5.01902807640012
-4.95810875420772 to -5.15726591037686
-4.68739003067219 to -4.88654718684133
-5.07319305961333 to -5.27235021578247
-4.66153007869922 to -4.86068723486836
-5.02417694156371 to -5.22333409773285
Changing layer 2's weights from 
-4.21498870973789 to -4.41414586590703
-4.76904219513141 to -4.96819935130055
-4.48445290451252 to -4.68361006068166
-4.31325280790531 to -4.51240996407445
-4.49050551777088 to -4.68966267394002
-4.69651514415943 to -4.89567230032857
-4.1777024281522 to -4.37685958432134
-4.25012445574009 to -4.44928161190923
-4.98212275033198 to -5.18127990650112
-4.93716523175442 to -5.13632238792355
Changing layer 3's weights from 
-4.24803900842869 to -4.44719616459783
-4.51159209137165 to -4.71074924754079
-4.26898217325413 to -4.46813932942326
-5.17642140501026 to -5.3755785611794
-4.39165806894504 to -4.59081522511418
-4.52190667515003 to -4.72106383131917
-4.8520915222665 to -5.05124867843564
-4.393578649809 to -4.59273580597814
-4.72527462368214 to -4.92443177985127
-4.2524447453519 to -4.45160190152104
Changing layer 4's weights from 
-4.35171854620182 to -4.55087570237096
-4.56055134659016 to -4.7597085027593
-4.76102894668781 to -4.96018610285695
-4.474094213297 to -4.67325136946614
-4.71334701900684 to -4.91250417517598
-4.48188179855549 to -4.68103895472463
-4.48624032860004 to -4.68539748476918
-4.94813785081111 to -5.14729500698025
-4.35444176321232 to -4.55359891938146
-4.7867437017461 to -4.98590085791524
Changing layer 5's weights from 
-4.19893872862064 to -4.39809588478978
-4.6410122526189 to -4.84016940878804
-4.21582389002048 to -4.41498104618962
-5.15133607139192 to -5.35049322756106
-4.61406725769245 to -4.81322441386159
-5.01253520076476 to -5.2116923569339
-4.71256989364826 to -4.9117270498174
-4.64012366657459 to -4.83928082274373
-4.25052166109287 to -4.44967881726201
-4.25702977304661 to -4.45618692921575
Trying to learn from memory 26, 0, -0.2
sum -11.2683016256543 distri -14.4259408152847
Using diff 5.97471459604401 and condRate 0.166666666666667
Changed category 0 weights from 
-4.25360726901989 to -4.45276442518903
-4.65364223310452 to -4.85279938927366
-4.93616292962533 to -5.13532008579447
-4.78465756365757 to -4.98381471982671
Changing layer 0's weights from 
-4.64327535914357 to -4.84243251531271
-5.01520181583341 to -5.21435897200254
-5.24955108152802 to -5.44870823769716
-5.21499183165009 to -5.41414898781923
-4.46712426232274 to -4.66628141849188
-5.14976621555264 to -5.34892337172178
-4.99593649195607 to -5.19509364812521
-5.0233908979791 to -5.22254805414824
-5.26088275896485 to -5.46003991513399
-5.30463281409914 to -5.50378997026828
Changing layer 1's weights from 
-5.2995359017151 to -5.49869305788424
-4.69836721705373 to -4.89752437322287
-4.82406995581563 to -5.02322711198477
-4.83397529410298 to -5.03313245027212
-5.01902807640012 to -5.21818523256926
-5.15726591037686 to -5.356423066546
-4.88654718684133 to -5.08570434301047
-5.27235021578247 to -5.47150737195161
-4.86068723486836 to -5.0598443910375
-5.22333409773285 to -5.42249125390199
Changing layer 2's weights from 
-4.41414586590703 to -4.61330302207617
-4.96819935130055 to -5.16735650746969
-4.68361006068166 to -4.8827672168508
-4.51240996407445 to -4.71156712024359
-4.68966267394002 to -4.88881983010916
-4.89567230032857 to -5.09482945649771
-4.37685958432134 to -4.57601674049048
-4.44928161190923 to -4.64843876807837
-5.18127990650112 to -5.38043706267026
-5.13632238792355 to -5.33547954409269
Changing layer 3's weights from 
-4.44719616459783 to -4.64635332076697
-4.71074924754079 to -4.90990640370993
-4.46813932942326 to -4.6672964855924
-5.3755785611794 to -5.57473571734854
-4.59081522511418 to -4.78997238128332
-4.72106383131917 to -4.92022098748831
-5.05124867843564 to -5.25040583460478
-4.59273580597814 to -4.79189296214728
-4.92443177985127 to -5.12358893602041
-4.45160190152104 to -4.65075905769018
Changing layer 4's weights from 
-4.55087570237096 to -4.7500328585401
-4.7597085027593 to -4.95886565892843
-4.96018610285695 to -5.15934325902609
-4.67325136946614 to -4.87240852563528
-4.91250417517598 to -5.11166133134512
-4.68103895472463 to -4.88019611089377
-4.68539748476918 to -4.88455464093832
-5.14729500698025 to -5.34645216314939
-4.55359891938146 to -4.7527560755506
-4.98590085791524 to -5.18505801408438
Changing layer 5's weights from 
-4.39809588478978 to -4.59725304095892
-4.84016940878804 to -5.03932656495718
-4.41498104618962 to -4.61413820235876
-5.35049322756106 to -5.5496503837302
-4.81322441386159 to -5.01238157003073
-5.2116923569339 to -5.41084951310304
-4.9117270498174 to -5.11088420598654
-4.83928082274373 to -5.03843797891287
-4.44967881726201 to -4.64883597343115
-4.45618692921575 to -4.65534408538489
Trying to learn from memory 26, 0, -0.2
sum -11.2683016256543 distri -14.4259408152847
Using diff 5.97471459604401 and condRate 0.166666666666667
Changed category 0 weights from 
-4.45276442518903 to -4.65192158135817
-4.85279938927366 to -5.05195654544279
-5.13532008579447 to -5.33447724196361
-4.98381471982671 to -5.18297187599585
Changing layer 0's weights from 
-4.84243251531271 to -5.04158967148185
-5.21435897200254 to -5.41351612817168
-5.44870823769716 to -5.6478653938663
-5.41414898781923 to -5.61330614398837
-4.66628141849188 to -4.86543857466102
-5.34892337172178 to -5.54808052789092
-5.19509364812521 to -5.39425080429435
-5.22254805414824 to -5.42170521031738
-5.46003991513399 to -5.65919707130313
-5.50378997026828 to -5.70294712643742
Changing layer 1's weights from 
-5.49869305788424 to -5.69785021405338
-4.89752437322287 to -5.09668152939201
-5.02322711198477 to -5.22238426815391
-5.03313245027212 to -5.23228960644126
-5.21818523256926 to -5.4173423887384
-5.356423066546 to -5.55558022271514
-5.08570434301047 to -5.2848614991796
-5.47150737195161 to -5.67066452812075
-5.0598443910375 to -5.25900154720664
-5.42249125390199 to -5.62164841007113
Changing layer 2's weights from 
-4.61330302207617 to -4.81246017824531
-5.16735650746969 to -5.36651366363883
-4.8827672168508 to -5.08192437301994
-4.71156712024359 to -4.91072427641273
-4.88881983010916 to -5.0879769862783
-5.09482945649771 to -5.29398661266685
-4.57601674049048 to -4.77517389665961
-4.64843876807837 to -4.84759592424751
-5.38043706267026 to -5.5795942188394
-5.33547954409269 to -5.53463670026183
Changing layer 3's weights from 
-4.64635332076697 to -4.8455104769361
-4.90990640370993 to -5.10906355987907
-4.6672964855924 to -4.86645364176154
-5.57473571734854 to -5.77389287351768
-4.78997238128332 to -4.98912953745246
-4.92022098748831 to -5.11937814365745
-5.25040583460478 to -5.44956299077392
-4.79189296214728 to -4.99105011831641
-5.12358893602041 to -5.32274609218955
-4.65075905769018 to -4.84991621385932
Changing layer 4's weights from 
-4.7500328585401 to -4.94919001470924
-4.95886565892843 to -5.15802281509757
-5.15934325902609 to -5.35850041519523
-4.87240852563528 to -5.07156568180442
-5.11166133134512 to -5.31081848751426
-4.88019611089377 to -5.07935326706291
-4.88455464093832 to -5.08371179710746
-5.34645216314939 to -5.54560931931853
-4.7527560755506 to -4.95191323171974
-5.18505801408438 to -5.38421517025352
Changing layer 5's weights from 
-4.59725304095892 to -4.79641019712806
-5.03932656495718 to -5.23848372112632
-4.61413820235876 to -4.8132953585279
-5.5496503837302 to -5.74880753989934
-5.01238157003073 to -5.21153872619987
-5.41084951310304 to -5.61000666927218
-5.11088420598654 to -5.31004136215568
-5.03843797891287 to -5.23759513508201
-4.64883597343115 to -4.84799312960029
-4.65534408538489 to -4.85450124155403
Trying to learn from memory 26, 1, -0.2
sum -11.2683016256543 distri 2.19328059055396
Using diff 1 and condRate 0.166666666666667
Changed category 1 weights from 
0.156328085698302 to 0.122994751868263
0.389643672742064 to 0.356310338912025
-0.0352815651519937 to -0.0686148989820324
-0.0311887526138468 to -0.0645220864438855
Changing layer 0's weights from 
-5.04158967148185 to -5.07492300531189
-5.41351612817168 to -5.44684946200172
-5.6478653938663 to -5.68119872769634
-5.61330614398837 to -5.64663947781841
-4.86543857466102 to -4.89877190849106
-5.54808052789092 to -5.58141386172096
-5.39425080429435 to -5.42758413812439
-5.42170521031738 to -5.45503854414741
-5.65919707130313 to -5.69253040513317
-5.70294712643742 to -5.73628046026746
Changing layer 1's weights from 
-5.69785021405338 to -5.73118354788342
-5.09668152939201 to -5.13001486322205
-5.22238426815391 to -5.25571760198395
-5.23228960644126 to -5.2656229402713
-5.4173423887384 to -5.45067572256844
-5.55558022271514 to -5.58891355654518
-5.2848614991796 to -5.31819483300964
-5.67066452812075 to -5.70399786195079
-5.25900154720664 to -5.29233488103668
-5.62164841007113 to -5.65498174390117
Changing layer 2's weights from 
-4.81246017824531 to -4.84579351207535
-5.36651366363883 to -5.39984699746887
-5.08192437301994 to -5.11525770684998
-4.91072427641273 to -4.94405761024277
-5.0879769862783 to -5.12131032010834
-5.29398661266685 to -5.32731994649689
-4.77517389665961 to -4.80850723048965
-4.84759592424751 to -4.88092925807754
-5.5795942188394 to -5.61292755266944
-5.53463670026183 to -5.56797003409187
Changing layer 3's weights from 
-4.8455104769361 to -4.87884381076614
-5.10906355987907 to -5.14239689370911
-4.86645364176154 to -4.89978697559158
-5.77389287351768 to -5.80722620734772
-4.98912953745246 to -5.0224628712825
-5.11937814365745 to -5.15271147748749
-5.44956299077392 to -5.48289632460396
-4.99105011831641 to -5.02438345214645
-5.32274609218955 to -5.35607942601959
-4.84991621385932 to -4.88324954768936
Changing layer 4's weights from 
-4.94919001470924 to -4.98252334853928
-5.15802281509757 to -5.19135614892761
-5.35850041519523 to -5.39183374902527
-5.07156568180442 to -5.10489901563446
-5.31081848751426 to -5.3441518213443
-5.07935326706291 to -5.11268660089294
-5.08371179710746 to -5.1170451309375
-5.54560931931853 to -5.57894265314857
-4.95191323171974 to -4.98524656554977
-5.38421517025352 to -5.41754850408356
Changing layer 5's weights from 
-4.79641019712806 to -4.8297435309581
-5.23848372112632 to -5.27181705495636
-4.8132953585279 to -4.84662869235794
-5.74880753989934 to -5.78214087372938
-5.21153872619987 to -5.24487206002991
-5.61000666927218 to -5.64334000310222
-5.31004136215568 to -5.34337469598572
-5.23759513508201 to -5.27092846891205
-4.84799312960029 to -4.88132646343033
-4.85450124155403 to -4.88783457538406
Trying to learn from memory 27, 0, -0.2
sum -11.2688445689157 distri -14.4270127200683
Using diff 5.97537929338149 and condRate 0.166666666666667
Changed category 0 weights from 
-4.65192158135817 to -4.85110089410556
-5.05195654544279 to -5.25113585819018
-5.33447724196361 to -5.53365655471099
-5.18297187599585 to -5.38215118874323
Changing layer 0's weights from 
-5.07492300531189 to -5.27410231805928
-5.44684946200172 to -5.64602877474911
-5.68119872769634 to -5.88037804044373
-5.64663947781841 to -5.8458187905658
-4.89877190849106 to -5.09795122123844
-5.58141386172096 to -5.78059317446835
-5.42758413812439 to -5.62676345087178
-5.45503854414741 to -5.6542178568948
-5.69253040513317 to -5.89170971788056
-5.73628046026746 to -5.93545977301485
Changing layer 1's weights from 
-5.73118354788342 to -5.93036286063081
-5.13001486322205 to -5.32919417596943
-5.25571760198395 to -5.45489691473133
-5.2656229402713 to -5.46480225301869
-5.45067572256844 to -5.64985503531582
-5.58891355654518 to -5.78809286929257
-5.31819483300964 to -5.51737414575703
-5.70399786195079 to -5.90317717469818
-5.29233488103668 to -5.49151419378407
-5.65498174390117 to -5.85416105664856
Changing layer 2's weights from 
-4.84579351207535 to -5.04497282482273
-5.39984699746887 to -5.59902631021626
-5.11525770684998 to -5.31443701959736
-4.94405761024277 to -5.14323692299015
-5.12131032010834 to -5.32048963285572
-5.32731994649689 to -5.52649925924427
-4.80850723048965 to -5.00768654323704
-4.88092925807754 to -5.08010857082493
-5.61292755266944 to -5.81210686541683
-5.56797003409187 to -5.76714934683926
Changing layer 3's weights from 
-4.87884381076614 to -5.07802312351353
-5.14239689370911 to -5.34157620645649
-4.89978697559158 to -5.09896628833897
-5.80722620734772 to -6.00640552009511
-5.0224628712825 to -5.22164218402989
-5.15271147748749 to -5.35189079023487
-5.48289632460396 to -5.68207563735134
-5.02438345214645 to -5.22356276489384
-5.35607942601959 to -5.55525873876698
-4.88324954768936 to -5.08242886043675
Changing layer 4's weights from 
-4.98252334853928 to -5.18170266128666
-5.19135614892761 to -5.390535461675
-5.39183374902527 to -5.59101306177265
-5.10489901563446 to -5.30407832838185
-5.3441518213443 to -5.54333113409169
-5.11268660089294 to -5.31186591364033
-5.1170451309375 to -5.31622444368489
-5.57894265314857 to -5.77812196589596
-4.98524656554977 to -5.18442587829716
-5.41754850408356 to -5.61672781683094
Changing layer 5's weights from 
-4.8297435309581 to -5.02892284370549
-5.27181705495636 to -5.47099636770375
-4.84662869235794 to -5.04580800510533
-5.78214087372938 to -5.98132018647677
-5.24487206002991 to -5.44405137277729
-5.64334000310222 to -5.84251931584961
-5.34337469598572 to -5.5425540087331
-5.27092846891205 to -5.47010778165943
-4.88132646343033 to -5.08050577617771
-4.88783457538406 to -5.08701388813145
Trying to learn from memory 28, 0, -0.2
sum -11.2815192164742 distri -14.4520856062315
Using diff 5.99094619387584 and condRate 0.166666666666667
Changed category 0 weights from 
-4.85110089410556 to -5.05079910354382
-5.25113585819018 to -5.45083406762844
-5.53365655471099 to -5.73335476414925
-5.38215118874323 to -5.58184939818149
Changing layer 0's weights from 
-5.27410231805928 to -5.47380052749754
-5.64602877474911 to -5.84572698418737
-5.88037804044373 to -6.08007624988199
-5.8458187905658 to -6.04551700000406
-5.09795122123844 to -5.29764943067671
-5.78059317446835 to -5.98029138390661
-5.62676345087178 to -5.82646166031004
-5.6542178568948 to -5.85391606633306
-5.89170971788056 to -6.09140792731882
-5.93545977301485 to -6.13515798245311
Changing layer 1's weights from 
-5.93036286063081 to -6.13006107006907
-5.32919417596943 to -5.5288923854077
-5.45489691473133 to -5.6545951241696
-5.46480225301869 to -5.66450046245695
-5.64985503531582 to -5.84955324475408
-5.78809286929257 to -5.98779107873083
-5.51737414575703 to -5.71707235519529
-5.90317717469818 to -6.10287538413644
-5.49151419378407 to -5.69121240322233
-5.85416105664856 to -6.05385926608682
Changing layer 2's weights from 
-5.04497282482273 to -5.244671034261
-5.59902631021626 to -5.79872451965452
-5.31443701959736 to -5.51413522903562
-5.14323692299015 to -5.34293513242842
-5.32048963285572 to -5.52018784229399
-5.52649925924427 to -5.72619746868254
-5.00768654323704 to -5.2073847526753
-5.08010857082493 to -5.27980678026319
-5.81210686541683 to -6.01180507485509
-5.76714934683926 to -5.96684755627752
Changing layer 3's weights from 
-5.07802312351353 to -5.27772133295179
-5.34157620645649 to -5.54127441589476
-5.09896628833897 to -5.29866449777723
-6.00640552009511 to -6.20610372953337
-5.22164218402989 to -5.42134039346815
-5.35189079023487 to -5.55158899967314
-5.68207563735134 to -5.88177384678961
-5.22356276489384 to -5.4232609743321
-5.55525873876698 to -5.75495694820524
-5.08242886043675 to -5.28212706987501
Changing layer 4's weights from 
-5.18170266128666 to -5.38140087072493
-5.390535461675 to -5.59023367111326
-5.59101306177265 to -5.79071127121092
-5.30407832838185 to -5.50377653782011
-5.54333113409169 to -5.74302934352995
-5.31186591364033 to -5.51156412307859
-5.31622444368489 to -5.51592265312315
-5.77812196589596 to -5.97782017533422
-5.18442587829716 to -5.38412408773542
-5.61672781683094 to -5.81642602626921
Changing layer 5's weights from 
-5.02892284370549 to -5.22862105314375
-5.47099636770375 to -5.67069457714201
-5.04580800510533 to -5.24550621454359
-5.98132018647677 to -6.18101839591503
-5.44405137277729 to -5.64374958221556
-5.84251931584961 to -6.04221752528787
-5.5425540087331 to -5.74225221817137
-5.47010778165943 to -5.6698059910977
-5.08050577617771 to -5.28020398561598
-5.08701388813145 to -5.28671209756971
10/5/2016 11:17:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:41 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:42 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:45 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:46 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:17:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:18:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:00 PMStarting AI
Reading weights from 10/5/2016 11:19:00 PMStarting AI
Weights.txt
Layer 0's weights: -0.568523272996696 -0.940449729686532 -1.17479899538115 -1.14023974550322 -0.392372176175865 -1.07501412940577 -0.921184405809198 -0.948638811832223 -1.18613067281798 -1.22988072795227 14 11 
Layer 1's weights: -1.22478381556823 -0.623615130906854 -0.749317869668757 -0.75922320795611 -0.944275990253244 -1.08251382422999 -0.811795100694452 -1.1975981296356 -0.78593514872149 -1.14858201158598 12 9 
Layer 2's weights: -0.339393779760155 -0.893447265153681 -0.608857974534783 -0.437657877927574 -0.614910587793145 -0.820920214181696 -0.302107498174461 -0.374529525762352 -1.10652782035425 -1.06157030177668 10 7 
Layer 3's weights: -0.372444078450951 -0.635997161393914 -0.39338724327639 -1.30082647503253 -0.516063138967308 -0.646311745172296 -0.976496592288766 -0.517983719831261 -0.849679693704401 -0.376849815374169 8 5 
Layer 4's weights: -0.476123616224083 -0.684956416612421 -0.885434016710077 -0.598499283319268 -0.837752089029108 -0.606286868577752 -0.610645398622308 -1.07254292083338 -0.478846833234581 -0.911148771768366 6 3 
Layer 5's weights: -0.323343798642906 -0.765417322641168 -0.340228960042747 -1.27574114141419 -0.738472327714716 -1.13694027078703 -0.836974963670526 -0.764528736596857 -0.374926731115135 -0.381434843068872 4 1 
Layer 6's weights: -0.127011759170718 -0.527046723255344 -0.809567419776152 -0.658062053808399 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
Layer 8's weights: 0.391144796536028 0.669586762116017 0.582546397373737 -0.072025307192743 
Layer 0's weights: -3.94998135437073 -4.32190781106057 -4.55625707675518 -4.52169782687725 -3.7738302575499 -4.4564722107798 -4.30264248718323 -4.33009689320626 -4.56758875419201 -4.6113388093263 14 11 
Layer 1's weights: -4.60624189694226 -4.00507321228089 -4.13077595104279 -4.14068128933014 -4.32573407162728 -4.46397190560402 -4.19325318206849 -4.57905621100963 -4.16739323009552 -4.53004009296001 12 9 
Layer 2's weights: -3.72085186113419 -4.27490534652772 -3.99031605590882 -3.81911595930161 -3.99636866916718 -4.20237829555573 -3.6835655795485 -3.75598760713639 -4.48798590172828 -4.44302838315071 10 7 
Layer 3's weights: -3.75390215982499 -4.01745524276795 -3.77484532465043 -4.68228455640656 -3.89752122034135 -4.02776982654633 -4.3579546736628 -3.8994418012053 -4.23113777507844 -3.75830789674821 8 5 
Layer 4's weights: -3.85758169759812 -4.06641449798646 -4.26689209808411 -3.97995736469331 -4.21921017040314 -3.98774494995179 -3.99210347999635 -4.45400100220741 -3.86030491460862 -4.2926068531424 6 3 
Layer 5's weights: -3.70480188001694 -4.1468754040152 -3.72168704141678 -4.65719922278822 -4.11993040908875 -4.51839835216106 -4.21843304504456 -4.14598681797089 -3.75638481248917 -3.76289292444291 4 1 
Layer 6's weights: -3.04180316692422 -3.44183813100884 -3.72435882752965 -3.57285346156189 
Layer 7's weights: 0.0896614180382248 0.322977005081987 -0.101948232812071 -0.0978554202739242 
Layer 8's weights: 0.124478125895718 0.402920091475707 0.315879726733427 -0.338691977833053 
Layer 0's weights: -5.47380052749754 -5.84572698418737 -6.08007624988199 -6.04551700000406 -5.29764943067671 -5.98029138390661 -5.82646166031004 -5.85391606633306 -6.09140792731882 -6.13515798245311 14 11 
Layer 1's weights: -6.13006107006907 -5.5288923854077 -5.6545951241696 -5.66450046245695 -5.84955324475408 -5.98779107873083 -5.71707235519529 -6.10287538413644 -5.69121240322233 -6.05385926608682 12 9 
Layer 2's weights: -5.244671034261 -5.79872451965452 -5.51413522903562 -5.34293513242842 -5.52018784229399 -5.72619746868254 -5.2073847526753 -5.27980678026319 -6.01180507485509 -5.96684755627752 10 7 
Layer 3's weights: -5.27772133295179 -5.54127441589476 -5.29866449777723 -6.20610372953337 -5.42134039346815 -5.55158899967314 -5.88177384678961 -5.4232609743321 -5.75495694820524 -5.28212706987501 8 5 
Layer 4's weights: -5.38140087072493 -5.59023367111326 -5.79071127121092 -5.50377653782011 -5.74302934352995 -5.51156412307859 -5.51592265312315 -5.97782017533422 -5.38412408773542 -5.81642602626921 6 3 
Layer 5's weights: -5.22862105314375 -5.67069457714201 -5.24550621454359 -6.18101839591503 -5.64374958221556 -6.04221752528787 -5.74225221817137 -5.6698059910977 -5.28020398561598 -5.28671209756971 4 1 
Layer 6's weights: -5.05079910354382 -5.45083406762844 -5.73335476414925 -5.58184939818149 
Layer 7's weights: 0.122994751868263 0.356310338912025 -0.0686148989820324 -0.0645220864438855 
Layer 8's weights: 0.0896614180382248 0.322977005081987 -0.101948232812071 -0.0978554202739242 
10/5/2016 11:19:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:04 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:05 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:19:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:19 PMStarting AI
Weights.txt is empty, so generating random weights now
Layer #0 weights: 0.955237507820129 0.696360051631927 0.960165500640869 0.985891580581665 0.272570043802261 0.715547382831573 0.75896942615509 0.562085688114166 0.54054981470108 0.775054693222046 
Layer #1 weights: 0.562728464603424 0.209144026041031 0.928257584571838 0.842069149017334 0.982627630233765 0.464809954166412 0.980252146720886 0.702007591724396 0.595309555530548 0.227690845727921 
Layer #2 weights: 0.400412142276764 0.0633996799588203 0.184868231415749 0.902037262916565 0.973703265190125 0.543037354946136 0.878168821334839 0.441375076770782 0.0449487008154392 0.136423602700233 
Layer #3 weights: 0.725820124149323 0.535841405391693 0.37372425198555 0.00552237080410123 0.0177018661051989 0.0207446832209826 0.690794885158539 0.596583068370819 0.326670438051224 0.26811209321022 
Layer #4 weights: 0.00445544766262174 0.228997975587845 0.828677654266357 0.906726956367493 0.995200276374817 0.509438335895538 0.000160098090418614 0.914682507514954 0.475222170352936 0.712110579013824 
Layer #5 weights: 0.601044833660126 0.883991837501526 0.834777235984802 0.425808131694794 0.421939074993134 0.522199332714081 0.120432630181313 0.00292944931425154 0.314216285943985 0.573857605457306 
Categorical Layer #0 weights: 0.0774668529629707 0.34473791718483 0.640887200832367 0.49875682592392 
Categorical Layer #1 weights: 0.946191072463989 0.499836146831512 0.311079174280167 0.555415332317352 
Categorical Layer #2 weights: 0.185939326882362 0.853940725326538 0.239859610795975 0.711585700511932 
Layer #0 weights: 0.745112240314484 0.85063624382019 0.2348293364048 0.0976715236902237 0.205172568559647 0.756842613220215 0.91040027141571 0.914320111274719 0.996559858322144 0.688524901866913 
Layer #1 weights: 0.0864991024136543 0.255401402711868 0.786065220832825 0.0842046812176704 0.323420077562332 0.914236068725586 0.783999085426331 0.0343617238104343 0.927437663078308 0.527728378772736 
Layer #2 weights: 0.087914951145649 0.11510743200779 0.0615832880139351 0.320659667253494 0.185269489884377 0.917577743530273 0.567713558673859 0.888832688331604 0.719386756420136 0.199152261018753 
Layer #3 weights: 0.920961141586304 0.721494972705841 0.431788504123688 0.655021846294403 0.403901636600494 0.210535198450089 0.941261172294617 0.635129868984222 0.438765823841095 0.0492484644055367 
Layer #4 weights: 0.894138813018799 0.283203274011612 0.0492999628186226 0.990876913070679 0.888932585716248 0.688647925853729 0.825439453125 0.310703545808792 0.178780093789101 0.283261090517044 
Layer #5 weights: 0.908520102500916 0.180282011628151 0.901205062866211 0.836043238639832 0.94907808303833 0.435037195682526 0.133789911866188 0.132568374276161 0.794338583946228 0.587445914745331 
Categorical Layer #0 weights: 0.103364363312721 0.914867758750916 0.270773202180862 0.255117684602737 
Categorical Layer #1 weights: 0.629384815692902 0.454867899417877 0.952324151992798 0.143329039216042 
Categorical Layer #2 weights: 0.851679921150208 0.661478102207184 0.894875407218933 0.986612796783447 
Layer #0 weights: 0.999697685241699 0.0415786541998386 0.201600819826126 0.294023424386978 0.232294350862503 0.733447134494781 0.392395436763763 0.865650296211243 0.934694886207581 0.318807989358902 
Layer #1 weights: 0.472819268703461 0.525825202465057 0.94994044303894 0.473495423793793 0.802372336387634 0.204989105463028 0.342359095811844 0.0770460441708565 0.177162423729897 0.177845016121864 
Layer #2 weights: 0.299264460802078 0.182133808732033 0.168343082070351 0.138867035508156 0.548329055309296 0.0484099462628365 0.624533236026764 0.985930323600769 0.832122325897217 0.0614519193768501 
Layer #3 weights: 0.533240854740143 0.44668585062027 0.704083263874054 0.483834326267242 0.812664389610291 0.738218605518341 0.245967894792557 0.0160604733973742 0.28778675198555 0.281512409448624 
Layer #4 weights: 0.48167496919632 0.358835488557816 0.0912650898098946 0.868014812469482 0.984627723693848 0.653753817081451 0.746049344539642 0.453526556491852 0.598036825656891 0.462529599666595 
Layer #5 weights: 0.626199901103973 0.455077946186066 0.642982184886932 0.806226849555969 0.460796654224396 0.261086016893387 0.506959021091461 0.8332599401474 0.0948360115289688 0.895085453987122 
Categorical Layer #0 weights: 0.509997665882111 0.424197256565094 0.5810427069664 0.421492636203766 
Categorical Layer #1 weights: 0.199459463357925 0.948052048683167 0.921223640441895 0.00692010018974543 
Categorical Layer #2 weights: 0.623229801654816 0.261967927217484 0.0576795414090157 0.786092281341553 
10/5/2016 11:21:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:34 PMStarting learning phase with deltaScore: 0.6
Modified index 0's learning in memoryPool to 0.12
Modified index 1's learning in memoryPool to 0.12
Modified index 2's learning in memoryPool to 0.12
Modified index 3's learning in memoryPool to 0.12
Modified index 4's learning in memoryPool to 0.12
Modified index 5's learning in memoryPool to 0.12
Modified index 6's learning in memoryPool to 0.12
Modified index 7's learning in memoryPool to 0.12
Modified index 8's learning in memoryPool to 0.12
Modified index 9's learning in memoryPool to 0.12
Modified index 10's learning in memoryPool to 0.12
Modified index 11's learning in memoryPool to 0.12
Modified index 12's learning in memoryPool to 0.12
10/5/2016 11:21:34 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 0, 2, 0.12
sum 0.747673883922957 distri 0.225946219209628
Using diff 0.33480919373259 and condRate 0.166666666666667
Changed category 2 weights from 
0.623229801654816 to 0.62992598579555
0.261967927217484 to 0.268664111358218
0.0576795414090157 to 0.0643757255497499
0.786092281341553 to 0.792788465482287
Changing layer 0's weights from 
0.999697685241699 to 1.00639386938243
0.0415786541998386 to 0.0482748383405729
0.201600819826126 to 0.20829700396686
0.294023424386978 to 0.300719608527712
0.232294350862503 to 0.238990535003237
0.733447134494781 to 0.740143318635516
0.392395436763763 to 0.399091620904498
0.865650296211243 to 0.872346480351977
0.934694886207581 to 0.941391070348315
0.318807989358902 to 0.325504173499636
Changing layer 1's weights from 
0.472819268703461 to 0.479515452844195
0.525825202465057 to 0.532521386605792
0.94994044303894 to 0.956636627179675
0.473495423793793 to 0.480191607934527
0.802372336387634 to 0.809068520528369
0.204989105463028 to 0.211685289603762
0.342359095811844 to 0.349055279952578
0.0770460441708565 to 0.0837422283115907
0.177162423729897 to 0.183858607870631
0.177845016121864 to 0.184541200262599
Changing layer 2's weights from 
0.299264460802078 to 0.305960644942813
0.182133808732033 to 0.188829992872767
0.168343082070351 to 0.175039266211085
0.138867035508156 to 0.14556321964889
0.548329055309296 to 0.55502523945003
0.0484099462628365 to 0.0551061304035707
0.624533236026764 to 0.631229420167498
0.985930323600769 to 0.992626507741503
0.832122325897217 to 0.838818510037951
0.0614519193768501 to 0.0681481035175844
Changing layer 3's weights from 
0.533240854740143 to 0.539937038880877
0.44668585062027 to 0.453382034761004
0.704083263874054 to 0.710779448014788
0.483834326267242 to 0.490530510407977
0.812664389610291 to 0.819360573751025
0.738218605518341 to 0.744914789659075
0.245967894792557 to 0.252664078933291
0.0160604733973742 to 0.0227566575381084
0.28778675198555 to 0.294482936126284
0.281512409448624 to 0.288208593589358
Changing layer 4's weights from 
0.48167496919632 to 0.488371153337054
0.358835488557816 to 0.36553167269855
0.0912650898098946 to 0.0979612739506288
0.868014812469482 to 0.874710996610217
0.984627723693848 to 0.991323907834582
0.653753817081451 to 0.660450001222186
0.746049344539642 to 0.752745528680377
0.453526556491852 to 0.460222740632586
0.598036825656891 to 0.604733009797625
0.462529599666595 to 0.46922578380733
Changing layer 5's weights from 
0.626199901103973 to 0.632896085244708
0.455077946186066 to 0.4617741303268
0.642982184886932 to 0.649678369027667
0.806226849555969 to 0.812923033696703
0.460796654224396 to 0.46749283836513
0.261086016893387 to 0.267782201034121
0.506959021091461 to 0.513655205232195
0.8332599401474 to 0.839956124288134
0.0948360115289688 to 0.101532195669703
0.895085453987122 to 0.901781638127856
Trying to learn from memory 1, 1, 0.12
sum 0.747695500654191 distri 0.26942380486618
Using diff 0.291347820624464 and condRate 0.166666666666667
Changed category 1 weights from 
0.199459463357925 to 0.205286420001957
0.948052048683167 to 0.953879005327198
0.921223640441895 to 0.927050597085926
0.00692010018974543 to 0.0127470568337771
Changing layer 0's weights from 
1.00639386938243 to 1.01222082602647
0.0482748383405729 to 0.0541017949846046
0.20829700396686 to 0.214123960610892
0.300719608527712 to 0.306546565171744
0.238990535003237 to 0.244817491647269
0.740143318635516 to 0.745970275279548
0.399091620904498 to 0.404918577548529
0.872346480351977 to 0.878173436996009
0.941391070348315 to 0.947218026992347
0.325504173499636 to 0.331331130143668
Changing layer 1's weights from 
0.479515452844195 to 0.485342409488227
0.532521386605792 to 0.538348343249823
0.956636627179675 to 0.962463583823706
0.480191607934527 to 0.486018564578559
0.809068520528369 to 0.8148954771724
0.211685289603762 to 0.217512246247794
0.349055279952578 to 0.35488223659661
0.0837422283115907 to 0.0895691849556224
0.183858607870631 to 0.189685564514662
0.184541200262599 to 0.19036815690663
Changing layer 2's weights from 
0.305960644942813 to 0.311787601586844
0.188829992872767 to 0.194656949516799
0.175039266211085 to 0.180866222855117
0.14556321964889 to 0.151390176292922
0.55502523945003 to 0.560852196094062
0.0551061304035707 to 0.0609330870476024
0.631229420167498 to 0.63705637681153
0.992626507741503 to 0.998453464385535
0.838818510037951 to 0.844645466681983
0.0681481035175844 to 0.0739750601616161
Changing layer 3's weights from 
0.539937038880877 to 0.545763995524909
0.453382034761004 to 0.459208991405036
0.710779448014788 to 0.71660640465882
0.490530510407977 to 0.496357467052008
0.819360573751025 to 0.825187530395057
0.744914789659075 to 0.750741746303107
0.252664078933291 to 0.258491035577323
0.0227566575381084 to 0.0285836141821401
0.294482936126284 to 0.300309892770316
0.288208593589358 to 0.29403555023339
Changing layer 4's weights from 
0.488371153337054 to 0.494198109981086
0.36553167269855 to 0.371358629342582
0.0979612739506288 to 0.103788230594661
0.874710996610217 to 0.880537953254248
0.991323907834582 to 0.997150864478614
0.660450001222186 to 0.666276957866217
0.752745528680377 to 0.758572485324408
0.460222740632586 to 0.466049697276618
0.604733009797625 to 0.610559966441657
0.46922578380733 to 0.475052740451361
Changing layer 5's weights from 
0.632896085244708 to 0.638723041888739
0.4617741303268 to 0.467601086970832
0.649678369027667 to 0.655505325671698
0.812923033696703 to 0.818749990340735
0.46749283836513 to 0.473319795009162
0.267782201034121 to 0.273609157678153
0.513655205232195 to 0.519482161876227
0.839956124288134 to 0.845783080932166
0.101532195669703 to 0.107359152313735
0.901781638127856 to 0.907608594771888
Trying to learn from memory 2, 0, 0.12
sum 0.747824030347492 distri 0.252360622128341
Using diff 0.308507400632278 and condRate 0.166666666666667
Changed category 0 weights from 
0.509997665882111 to 0.516167814139936
0.424197256565094 to 0.430367404822919
0.5810427069664 to 0.587212855224225
0.421492636203766 to 0.427662784461591
Changing layer 0's weights from 
1.01222082602647 to 1.01839097428429
0.0541017949846046 to 0.0602719432424298
0.214123960610892 to 0.220294108868717
0.306546565171744 to 0.312716713429569
0.244817491647269 to 0.250987639905094
0.745970275279548 to 0.752140423537373
0.404918577548529 to 0.411088725806355
0.878173436996009 to 0.884343585253834
0.947218026992347 to 0.953388175250172
0.331331130143668 to 0.337501278401493
Changing layer 1's weights from 
0.485342409488227 to 0.491512557746052
0.538348343249823 to 0.544518491507649
0.962463583823706 to 0.968633732081532
0.486018564578559 to 0.492188712836384
0.8148954771724 to 0.821065625430226
0.217512246247794 to 0.223682394505619
0.35488223659661 to 0.361052384854435
0.0895691849556224 to 0.0957393332134476
0.189685564514662 to 0.195855712772488
0.19036815690663 to 0.196538305164455
Changing layer 2's weights from 
0.311787601586844 to 0.317957749844669
0.194656949516799 to 0.200827097774624
0.180866222855117 to 0.187036371112942
0.151390176292922 to 0.157560324550747
0.560852196094062 to 0.567022344351887
0.0609330870476024 to 0.0671032353054276
0.63705637681153 to 0.643226525069355
0.998453464385535 to 1.00462361264336
0.844645466681983 to 0.850815614939808
0.0739750601616161 to 0.0801452084194413
Changing layer 3's weights from 
0.545763995524909 to 0.551934143782734
0.459208991405036 to 0.465379139662861
0.71660640465882 to 0.722776552916645
0.496357467052008 to 0.502527615309834
0.825187530395057 to 0.831357678652882
0.750741746303107 to 0.756911894560932
0.258491035577323 to 0.264661183835148
0.0285836141821401 to 0.0347537624399653
0.300309892770316 to 0.306480041028141
0.29403555023339 to 0.300205698491215
Changing layer 4's weights from 
0.494198109981086 to 0.500368258238911
0.371358629342582 to 0.377528777600407
0.103788230594661 to 0.109958378852486
0.880537953254248 to 0.886708101512074
0.997150864478614 to 1.00332101273644
0.666276957866217 to 0.672447106124043
0.758572485324408 to 0.764742633582234
0.466049697276618 to 0.472219845534443
0.610559966441657 to 0.616730114699482
0.475052740451361 to 0.481222888709187
Changing layer 5's weights from 
0.638723041888739 to 0.644893190146565
0.467601086970832 to 0.473771235228657
0.655505325671698 to 0.661675473929524
0.818749990340735 to 0.824920138598561
0.473319795009162 to 0.479489943266987
0.273609157678153 to 0.279779305935978
0.519482161876227 to 0.525652310134052
0.845783080932166 to 0.851953229189991
0.107359152313735 to 0.11352930057156
0.907608594771888 to 0.913778743029713
Trying to learn from memory 3, 0, 0.12
sum 0.747889936582078 distri 0.252382711352678
Using diff 0.308534741083881 and condRate 0.166666666666667
Changed category 0 weights from 
0.516167814139936 to 0.522338509206815
0.430367404822919 to 0.436538099889798
0.587212855224225 to 0.593383550291104
0.427662784461591 to 0.43383347952847
Changing layer 0's weights from 
1.01839097428429 to 1.02456166935117
0.0602719432424298 to 0.0664426383093088
0.220294108868717 to 0.226464803935596
0.312716713429569 to 0.318887408496448
0.250987639905094 to 0.257158334971973
0.752140423537373 to 0.758311118604252
0.411088725806355 to 0.417259420873234
0.884343585253834 to 0.890514280320713
0.953388175250172 to 0.959558870317051
0.337501278401493 to 0.343671973468372
Changing layer 1's weights from 
0.491512557746052 to 0.497683252812931
0.544518491507649 to 0.550689186574528
0.968633732081532 to 0.974804427148411
0.492188712836384 to 0.498359407903263
0.821065625430226 to 0.827236320497104
0.223682394505619 to 0.229853089572498
0.361052384854435 to 0.367223079921314
0.0957393332134476 to 0.101910028280327
0.195855712772488 to 0.202026407839367
0.196538305164455 to 0.202709000231334
Changing layer 2's weights from 
0.317957749844669 to 0.324128444911548
0.200827097774624 to 0.206997792841503
0.187036371112942 to 0.193207066179821
0.157560324550747 to 0.163731019617626
0.567022344351887 to 0.573193039418766
0.0671032353054276 to 0.0732739303723066
0.643226525069355 to 0.649397220136234
1.00462361264336 to 1.01079430771024
0.850815614939808 to 0.856986310006687
0.0801452084194413 to 0.0863159034863203
Changing layer 3's weights from 
0.551934143782734 to 0.558104838849613
0.465379139662861 to 0.47154983472974
0.722776552916645 to 0.728947247983524
0.502527615309834 to 0.508698310376713
0.831357678652882 to 0.837528373719761
0.756911894560932 to 0.763082589627811
0.264661183835148 to 0.270831878902027
0.0347537624399653 to 0.0409244575068443
0.306480041028141 to 0.31265073609502
0.300205698491215 to 0.306376393558094
Changing layer 4's weights from 
0.500368258238911 to 0.50653895330579
0.377528777600407 to 0.383699472667286
0.109958378852486 to 0.116129073919365
0.886708101512074 to 0.892878796578953
1.00332101273644 to 1.00949170780332
0.672447106124043 to 0.678617801190922
0.764742633582234 to 0.770913328649113
0.472219845534443 to 0.478390540601322
0.616730114699482 to 0.622900809766361
0.481222888709187 to 0.487393583776066
Changing layer 5's weights from 
0.644893190146565 to 0.651063885213444
0.473771235228657 to 0.479941930295536
0.661675473929524 to 0.667846168996403
0.824920138598561 to 0.831090833665439
0.479489943266987 to 0.485660638333866
0.279779305935978 to 0.285950001002857
0.525652310134052 to 0.531823005200931
0.851953229189991 to 0.85812392425687
0.11352930057156 to 0.119699995638439
0.913778743029713 to 0.919949438096592
Trying to learn from memory 4, 2, 0.12
sum 0.747866225806548 distri 0.226014932988985
Using diff 0.334884736365926 and condRate 0.166666666666667
Changed category 2 weights from 
0.62992598579555 to 0.636623680789011
0.268664111358218 to 0.275361806351679
0.0643757255497499 to 0.0710734205432109
0.792788465482287 to 0.799486160475748
Changing layer 0's weights from 
1.02456166935117 to 1.03125936434463
0.0664426383093088 to 0.0731403333027698
0.226464803935596 to 0.233162498929057
0.318887408496448 to 0.325585103489909
0.257158334971973 to 0.263856029965434
0.758311118604252 to 0.765008813597713
0.417259420873234 to 0.423957115866695
0.890514280320713 to 0.897211975314174
0.959558870317051 to 0.966256565310512
0.343671973468372 to 0.350369668461833
Changing layer 1's weights from 
0.497683252812931 to 0.504380947806392
0.550689186574528 to 0.557386881567989
0.974804427148411 to 0.981502122141872
0.498359407903263 to 0.505057102896724
0.827236320497104 to 0.833934015490565
0.229853089572498 to 0.236550784565959
0.367223079921314 to 0.373920774914775
0.101910028280327 to 0.108607723273788
0.202026407839367 to 0.208724102832828
0.202709000231334 to 0.209406695224795
Changing layer 2's weights from 
0.324128444911548 to 0.330826139905009
0.206997792841503 to 0.213695487834964
0.193207066179821 to 0.199904761173282
0.163731019617626 to 0.170428714611087
0.573193039418766 to 0.579890734412227
0.0732739303723066 to 0.0799716253657676
0.649397220136234 to 0.656094915129695
1.01079430771024 to 1.0174920027037
0.856986310006687 to 0.863684005000148
0.0863159034863203 to 0.0930135984797813
Changing layer 3's weights from 
0.558104838849613 to 0.564802533843074
0.47154983472974 to 0.478247529723201
0.728947247983524 to 0.735644942976985
0.508698310376713 to 0.515396005370173
0.837528373719761 to 0.844226068713222
0.763082589627811 to 0.769780284621272
0.270831878902027 to 0.277529573895488
0.0409244575068443 to 0.0476221525003053
0.31265073609502 to 0.319348431088481
0.306376393558094 to 0.313074088551555
Changing layer 4's weights from 
0.50653895330579 to 0.513236648299251
0.383699472667286 to 0.390397167660747
0.116129073919365 to 0.122826768912826
0.892878796578953 to 0.899576491572414
1.00949170780332 to 1.01618940279678
0.678617801190922 to 0.685315496184383
0.770913328649113 to 0.777611023642574
0.478390540601322 to 0.485088235594783
0.622900809766361 to 0.629598504759822
0.487393583776066 to 0.494091278769527
Changing layer 5's weights from 
0.651063885213444 to 0.657761580206905
0.479941930295536 to 0.486639625288997
0.667846168996403 to 0.674543863989864
0.831090833665439 to 0.8377885286589
0.485660638333866 to 0.492358333327327
0.285950001002857 to 0.292647695996318
0.531823005200931 to 0.538520700194392
0.85812392425687 to 0.864821619250331
0.119699995638439 to 0.1263976906319
0.919949438096592 to 0.926647133090053
Trying to learn from memory 5, 0, 0.12
sum 0.748243115517098 distri 0.252505784885232
Using diff 0.308676551752591 and condRate 0.166666666666667
Changed category 0 weights from 
0.522338509206815 to 0.528512040487181
0.436538099889798 to 0.442711631170164
0.593383550291104 to 0.59955708157147
0.43383347952847 to 0.440007010808836
Changing layer 0's weights from 
1.03125936434463 to 1.037432895625
0.0731403333027698 to 0.0793138645831357
0.233162498929057 to 0.239336030209423
0.325585103489909 to 0.331758634770275
0.263856029965434 to 0.2700295612458
0.765008813597713 to 0.771182344878079
0.423957115866695 to 0.430130647147061
0.897211975314174 to 0.90338550659454
0.966256565310512 to 0.972430096590878
0.350369668461833 to 0.356543199742199
Changing layer 1's weights from 
0.504380947806392 to 0.510554479086758
0.557386881567989 to 0.563560412848354
0.981502122141872 to 0.987675653422238
0.505057102896724 to 0.51123063417709
0.833934015490565 to 0.840107546770931
0.236550784565959 to 0.242724315846325
0.373920774914775 to 0.380094306195141
0.108607723273788 to 0.114781254554154
0.208724102832828 to 0.214897634113194
0.209406695224795 to 0.215580226505161
Changing layer 2's weights from 
0.330826139905009 to 0.336999671185375
0.213695487834964 to 0.21986901911533
0.199904761173282 to 0.206078292453648
0.170428714611087 to 0.176602245891453
0.579890734412227 to 0.586064265692593
0.0799716253657676 to 0.0861451566461335
0.656094915129695 to 0.662268446410061
1.0174920027037 to 1.02366553398407
0.863684005000148 to 0.869857536280514
0.0930135984797813 to 0.0991871297601472
Changing layer 3's weights from 
0.564802533843074 to 0.57097606512344
0.478247529723201 to 0.484421061003567
0.735644942976985 to 0.741818474257351
0.515396005370173 to 0.521569536650539
0.844226068713222 to 0.850399599993588
0.769780284621272 to 0.775953815901638
0.277529573895488 to 0.283703105175854
0.0476221525003053 to 0.0537956837806712
0.319348431088481 to 0.325521962368847
0.313074088551555 to 0.319247619831921
Changing layer 4's weights from 
0.513236648299251 to 0.519410179579617
0.390397167660747 to 0.396570698941113
0.122826768912826 to 0.129000300193192
0.899576491572414 to 0.90575002285278
1.01618940279678 to 1.02236293407714
0.685315496184383 to 0.691489027464749
0.777611023642574 to 0.783784554922939
0.485088235594783 to 0.491261766875149
0.629598504759822 to 0.635772036040188
0.494091278769527 to 0.500264810049893
Changing layer 5's weights from 
0.657761580206905 to 0.66393511148727
0.486639625288997 to 0.492813156569363
0.674543863989864 to 0.680717395270229
0.8377885286589 to 0.843962059939266
0.492358333327327 to 0.498531864607693
0.292647695996318 to 0.298821227276684
0.538520700194392 to 0.544694231474758
0.864821619250331 to 0.870995150530697
0.1263976906319 to 0.132571221912266
0.926647133090053 to 0.932820664370419
Trying to learn from memory 6, 1, 0.12
sum 0.754211482348533 distri 0.271251084963356
Using diff 0.294407526798043 and condRate 0.166666666666667
Changed category 1 weights from 
0.205286420001957 to 0.211174570771892
0.953879005327198 to 0.959767156097133
0.927050597085926 to 0.932938747855861
0.0127470568337771 to 0.0186352076037121
Changing layer 0's weights from 
1.037432895625 to 1.04332104639493
0.0793138645831357 to 0.0852020153530706
0.239336030209423 to 0.245224180979358
0.331758634770275 to 0.33764678554021
0.2700295612458 to 0.275917712015735
0.771182344878079 to 0.777070495648014
0.430130647147061 to 0.436018797916995
0.90338550659454 to 0.909273657364475
0.972430096590878 to 0.978318247360813
0.356543199742199 to 0.362431350512134
Changing layer 1's weights from 
0.510554479086758 to 0.516442629856693
0.563560412848354 to 0.569448563618289
0.987675653422238 to 0.993563804192173
0.51123063417709 to 0.517118784947025
0.840107546770931 to 0.845995697540866
0.242724315846325 to 0.24861246661626
0.380094306195141 to 0.385982456965076
0.114781254554154 to 0.120669405324088
0.214897634113194 to 0.220785784883129
0.215580226505161 to 0.221468377275096
Changing layer 2's weights from 
0.336999671185375 to 0.34288782195531
0.21986901911533 to 0.225757169885265
0.206078292453648 to 0.211966443223583
0.176602245891453 to 0.182490396661388
0.586064265692593 to 0.591952416462528
0.0861451566461335 to 0.0920333074160685
0.662268446410061 to 0.668156597179996
1.02366553398407 to 1.029553684754
0.869857536280514 to 0.875745687050449
0.0991871297601472 to 0.105075280530082
Changing layer 3's weights from 
0.57097606512344 to 0.576864215893375
0.484421061003567 to 0.490309211773502
0.741818474257351 to 0.747706625027286
0.521569536650539 to 0.527457687420474
0.850399599993588 to 0.856287750763523
0.775953815901638 to 0.781841966671573
0.283703105175854 to 0.289591255945789
0.0537956837806712 to 0.0596838345506061
0.325521962368847 to 0.331410113138782
0.319247619831921 to 0.325135770601856
Changing layer 4's weights from 
0.519410179579617 to 0.525298330349552
0.396570698941113 to 0.402458849711048
0.129000300193192 to 0.134888450963127
0.90575002285278 to 0.911638173622714
1.02236293407714 to 1.02825108484708
0.691489027464749 to 0.697377178234683
0.783784554922939 to 0.789672705692874
0.491261766875149 to 0.497149917645084
0.635772036040188 to 0.641660186810123
0.500264810049893 to 0.506152960819828
Changing layer 5's weights from 
0.66393511148727 to 0.669823262257205
0.492813156569363 to 0.498701307339298
0.680717395270229 to 0.686605546040164
0.843962059939266 to 0.849850210709201
0.498531864607693 to 0.504420015377628
0.298821227276684 to 0.304709378046619
0.544694231474758 to 0.550582382244693
0.870995150530697 to 0.876883301300632
0.132571221912266 to 0.138459372682201
0.932820664370419 to 0.938708815140354
Trying to learn from memory 7, 1, 0.12
sum 0.757241213114854 distri 0.272116623814886
Using diff 0.295814286021254 and condRate 0.166666666666667
Changed category 1 weights from 
0.211174570771892 to 0.217090856727409
0.959767156097133 to 0.96568344205265
0.932938747855861 to 0.938855033811378
0.0186352076037121 to 0.0245514935592292
Changing layer 0's weights from 
1.04332104639493 to 1.04923733235045
0.0852020153530706 to 0.0911183013085878
0.245224180979358 to 0.251140466934875
0.33764678554021 to 0.343563071495727
0.275917712015735 to 0.281833997971252
0.777070495648014 to 0.782986781603531
0.436018797916995 to 0.441935083872513
0.909273657364475 to 0.915189943319992
0.978318247360813 to 0.98423453331633
0.362431350512134 to 0.368347636467651
Changing layer 1's weights from 
0.516442629856693 to 0.52235891581221
0.569448563618289 to 0.575364849573807
0.993563804192173 to 0.99948009014769
0.517118784947025 to 0.523035070902542
0.845995697540866 to 0.851911983496384
0.24861246661626 to 0.254528752571777
0.385982456965076 to 0.391898742920593
0.120669405324088 to 0.126585691279606
0.220785784883129 to 0.226702070838646
0.221468377275096 to 0.227384663230613
Changing layer 2's weights from 
0.34288782195531 to 0.348804107910827
0.225757169885265 to 0.231673455840782
0.211966443223583 to 0.2178827291791
0.182490396661388 to 0.188406682616905
0.591952416462528 to 0.597868702418045
0.0920333074160685 to 0.0979495933715856
0.668156597179996 to 0.674072883135513
1.029553684754 to 1.03546997070952
0.875745687050449 to 0.881661973005966
0.105075280530082 to 0.110991566485599
Changing layer 3's weights from 
0.576864215893375 to 0.582780501848892
0.490309211773502 to 0.496225497729019
0.747706625027286 to 0.753622910982803
0.527457687420474 to 0.533373973375992
0.856287750763523 to 0.86220403671904
0.781841966671573 to 0.78775825262709
0.289591255945789 to 0.295507541901306
0.0596838345506061 to 0.0656001205061233
0.331410113138782 to 0.337326399094299
0.325135770601856 to 0.331052056557373
Changing layer 4's weights from 
0.525298330349552 to 0.531214616305069
0.402458849711048 to 0.408375135666565
0.134888450963127 to 0.140804736918644
0.911638173622714 to 0.917554459578232
1.02825108484708 to 1.0341673708026
0.697377178234683 to 0.703293464190201
0.789672705692874 to 0.795588991648392
0.497149917645084 to 0.503066203600601
0.641660186810123 to 0.64757647276564
0.506152960819828 to 0.512069246775345
Changing layer 5's weights from 
0.669823262257205 to 0.675739548212723
0.498701307339298 to 0.504617593294815
0.686605546040164 to 0.692521831995682
0.849850210709201 to 0.855766496664718
0.504420015377628 to 0.510336301333145
0.304709378046619 to 0.310625664002136
0.550582382244693 to 0.55649866820021
0.876883301300632 to 0.882799587256149
0.138459372682201 to 0.144375658637718
0.938708815140354 to 0.944625101095871
Trying to learn from memory 7, 0, 0.12
sum 0.757241213114854 distri 0.255608881047999
Using diff 0.312322028788142 and condRate 0.166666666666667
Changed category 0 weights from 
0.528512040487181 to 0.534758481311155
0.442711631170164 to 0.448958071994138
0.59955708157147 to 0.605803522395444
0.440007010808836 to 0.44625345163281
Changing layer 0's weights from 
1.04923733235045 to 1.05548377317442
0.0911183013085878 to 0.0973647421325619
0.251140466934875 to 0.257386907758849
0.343563071495727 to 0.349809512319701
0.281833997971252 to 0.288080438795226
0.782986781603531 to 0.789233222427505
0.441935083872513 to 0.448181524696487
0.915189943319992 to 0.921436384143966
0.98423453331633 to 0.990480974140304
0.368347636467651 to 0.374594077291625
Changing layer 1's weights from 
0.52235891581221 to 0.528605356636184
0.575364849573807 to 0.581611290397781
0.99948009014769 to 1.00572653097166
0.523035070902542 to 0.529281511726516
0.851911983496384 to 0.858158424320358
0.254528752571777 to 0.260775193395751
0.391898742920593 to 0.398145183744567
0.126585691279606 to 0.13283213210358
0.226702070838646 to 0.23294851166262
0.227384663230613 to 0.233631104054588
Changing layer 2's weights from 
0.348804107910827 to 0.355050548734802
0.231673455840782 to 0.237919896664756
0.2178827291791 to 0.224129170003074
0.188406682616905 to 0.194653123440879
0.597868702418045 to 0.604115143242019
0.0979495933715856 to 0.10419603419556
0.674072883135513 to 0.680319323959487
1.03546997070952 to 1.04171641153349
0.881661973005966 to 0.88790841382994
0.110991566485599 to 0.117238007309573
Changing layer 3's weights from 
0.582780501848892 to 0.589026942672866
0.496225497729019 to 0.502471938552993
0.753622910982803 to 0.759869351806777
0.533373973375992 to 0.539620414199966
0.86220403671904 to 0.868450477543014
0.78775825262709 to 0.794004693451064
0.295507541901306 to 0.30175398272528
0.0656001205061233 to 0.0718465613300974
0.337326399094299 to 0.343572839918273
0.331052056557373 to 0.337298497381347
Changing layer 4's weights from 
0.531214616305069 to 0.537461057129043
0.408375135666565 to 0.414621576490539
0.140804736918644 to 0.147051177742618
0.917554459578232 to 0.923800900402206
1.0341673708026 to 1.04041381162657
0.703293464190201 to 0.709539905014175
0.795588991648392 to 0.801835432472366
0.503066203600601 to 0.509312644424575
0.64757647276564 to 0.653822913589614
0.512069246775345 to 0.518315687599319
Changing layer 5's weights from 
0.675739548212723 to 0.681985989036697
0.504617593294815 to 0.510864034118789
0.692521831995682 to 0.698768272819656
0.855766496664718 to 0.862012937488693
0.510336301333145 to 0.516582742157119
0.310625664002136 to 0.31687210482611
0.55649866820021 to 0.562745109024185
0.882799587256149 to 0.889046028080123
0.144375658637718 to 0.150622099461692
0.944625101095871 to 0.950871541919845
Trying to learn from memory 7, 0, 0.12
sum 0.757241213114854 distri 0.255608881047999
Using diff 0.312322028788142 and condRate 0.166666666666667
Changed category 0 weights from 
0.534758481311155 to 0.541004922135129
0.448958071994138 to 0.455204512818112
0.605803522395444 to 0.612049963219418
0.44625345163281 to 0.452499892456784
Changing layer 0's weights from 
1.05548377317442 to 1.0617302139984
0.0973647421325619 to 0.103611182956536
0.257386907758849 to 0.263633348582823
0.349809512319701 to 0.356055953143676
0.288080438795226 to 0.2943268796192
0.789233222427505 to 0.795479663251479
0.448181524696487 to 0.454427965520461
0.921436384143966 to 0.92768282496794
0.990480974140304 to 0.996727414964278
0.374594077291625 to 0.380840518115599
Changing layer 1's weights from 
0.528605356636184 to 0.534851797460158
0.581611290397781 to 0.587857731221755
1.00572653097166 to 1.01197297179564
0.529281511726516 to 0.53552795255049
0.858158424320358 to 0.864404865144332
0.260775193395751 to 0.267021634219725
0.398145183744567 to 0.404391624568541
0.13283213210358 to 0.139078572927554
0.23294851166262 to 0.239194952486594
0.233631104054588 to 0.239877544878562
Changing layer 2's weights from 
0.355050548734802 to 0.361296989558776
0.237919896664756 to 0.24416633748873
0.224129170003074 to 0.230375610827048
0.194653123440879 to 0.200899564264853
0.604115143242019 to 0.610361584065993
0.10419603419556 to 0.110442475019534
0.680319323959487 to 0.686565764783461
1.04171641153349 to 1.04796285235747
0.88790841382994 to 0.894154854653914
0.117238007309573 to 0.123484448133547
Changing layer 3's weights from 
0.589026942672866 to 0.59527338349684
0.502471938552993 to 0.508718379376967
0.759869351806777 to 0.766115792630751
0.539620414199966 to 0.54586685502394
0.868450477543014 to 0.874696918366988
0.794004693451064 to 0.800251134275038
0.30175398272528 to 0.308000423549254
0.0718465613300974 to 0.0780930021540715
0.343572839918273 to 0.349819280742247
0.337298497381347 to 0.343544938205321
Changing layer 4's weights from 
0.537461057129043 to 0.543707497953017
0.414621576490539 to 0.420868017314513
0.147051177742618 to 0.153297618566592
0.923800900402206 to 0.93004734122618
1.04041381162657 to 1.04666025245055
0.709539905014175 to 0.715786345838149
0.801835432472366 to 0.80808187329634
0.509312644424575 to 0.515559085248549
0.653822913589614 to 0.660069354413588
0.518315687599319 to 0.524562128423293
Changing layer 5's weights from 
0.681985989036697 to 0.688232429860671
0.510864034118789 to 0.517110474942763
0.698768272819656 to 0.70501471364363
0.862012937488693 to 0.868259378312667
0.516582742157119 to 0.522829182981093
0.31687210482611 to 0.323118545650084
0.562745109024185 to 0.568991549848159
0.889046028080123 to 0.895292468904097
0.150622099461692 to 0.156868540285666
0.950871541919845 to 0.957117982743819
10/5/2016 11:21:34 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 7, 0, 0.12
sum 0.757241213114854 distri 0.255608881047999
Using diff 0.312322028788142 and condRate 0.166666666666667
Changed category 0 weights from 
0.541004922135129 to 0.547251362959103
0.455204512818112 to 0.461450953642086
0.612049963219418 to 0.618296404043392
0.452499892456784 to 0.458746333280758
Changing layer 0's weights from 
1.0617302139984 to 1.06797665482237
0.103611182956536 to 0.10985762378051
0.263633348582823 to 0.269879789406798
0.356055953143676 to 0.36230239396765
0.2943268796192 to 0.300573320443174
0.795479663251479 to 0.801726104075453
0.454427965520461 to 0.460674406344435
0.92768282496794 to 0.933929265791914
0.996727414964278 to 1.00297385578825
0.380840518115599 to 0.387086958939573
Changing layer 1's weights from 
0.534851797460158 to 0.541098238284132
0.587857731221755 to 0.594104172045729
1.01197297179564 to 1.01821941261961
0.53552795255049 to 0.541774393374464
0.864404865144332 to 0.870651305968306
0.267021634219725 to 0.273268075043699
0.404391624568541 to 0.410638065392515
0.139078572927554 to 0.145325013751528
0.239194952486594 to 0.245441393310568
0.239877544878562 to 0.246123985702536
Changing layer 2's weights from 
0.361296989558776 to 0.36754343038275
0.24416633748873 to 0.250412778312704
0.230375610827048 to 0.236622051651022
0.200899564264853 to 0.207146005088827
0.610361584065993 to 0.616608024889967
0.110442475019534 to 0.116688915843508
0.686565764783461 to 0.692812205607435
1.04796285235747 to 1.05420929318144
0.894154854653914 to 0.900401295477888
0.123484448133547 to 0.129730888957522
Changing layer 3's weights from 
0.59527338349684 to 0.601519824320814
0.508718379376967 to 0.514964820200941
0.766115792630751 to 0.772362233454725
0.54586685502394 to 0.552113295847914
0.874696918366988 to 0.880943359190962
0.800251134275038 to 0.806497575099013
0.308000423549254 to 0.314246864373228
0.0780930021540715 to 0.0843394429780456
0.349819280742247 to 0.356065721566221
0.343544938205321 to 0.349791379029295
Changing layer 4's weights from 
0.543707497953017 to 0.549953938776991
0.420868017314513 to 0.427114458138487
0.153297618566592 to 0.159544059390566
0.93004734122618 to 0.936293782050154
1.04666025245055 to 1.05290669327452
0.715786345838149 to 0.722032786662123
0.80808187329634 to 0.814328314120314
0.515559085248549 to 0.521805526072523
0.660069354413588 to 0.666315795237562
0.524562128423293 to 0.530808569247267
Changing layer 5's weights from 
0.688232429860671 to 0.694478870684645
0.517110474942763 to 0.523356915766737
0.70501471364363 to 0.711261154467604
0.868259378312667 to 0.874505819136641
0.522829182981093 to 0.529075623805067
0.323118545650084 to 0.329364986474058
0.568991549848159 to 0.575237990672133
0.895292468904097 to 0.901538909728071
0.156868540285666 to 0.16311498110964
0.957117982743819 to 0.963364423567793
Trying to learn from memory 7, 1, 0.12
sum 0.757241213114854 distri 0.272116623814886
Using diff 0.295814286021254 and condRate 0.166666666666667
Changed category 1 weights from 
0.217090856727409 to 0.223007142682926
0.96568344205265 to 0.971599728008168
0.938855033811378 to 0.944771319766896
0.0245514935592292 to 0.0304677795147464
Changing layer 0's weights from 
1.06797665482237 to 1.07389294077789
0.10985762378051 to 0.115773909736027
0.269879789406798 to 0.275796075362315
0.36230239396765 to 0.368218679923167
0.300573320443174 to 0.306489606398692
0.801726104075453 to 0.80764239003097
0.460674406344435 to 0.466590692299952
0.933929265791914 to 0.939845551747431
1.00297385578825 to 1.00889014174377
0.387086958939573 to 0.393003244895091
Changing layer 1's weights from 
0.541098238284132 to 0.547014524239649
0.594104172045729 to 0.600020458001246
1.01821941261961 to 1.02413569857513
0.541774393374464 to 0.547690679329981
0.870651305968306 to 0.876567591923823
0.273268075043699 to 0.279184360999217
0.410638065392515 to 0.416554351348032
0.145325013751528 to 0.151241299707045
0.245441393310568 to 0.251357679266085
0.246123985702536 to 0.252040271658053
Changing layer 2's weights from 
0.36754343038275 to 0.373459716338267
0.250412778312704 to 0.256329064268221
0.236622051651022 to 0.242538337606539
0.207146005088827 to 0.213062291044344
0.616608024889967 to 0.622524310845484
0.116688915843508 to 0.122605201799025
0.692812205607435 to 0.698728491562953
1.05420929318144 to 1.06012557913696
0.900401295477888 to 0.906317581433405
0.129730888957522 to 0.135647174913039
Changing layer 3's weights from 
0.601519824320814 to 0.607436110276331
0.514964820200941 to 0.520881106156458
0.772362233454725 to 0.778278519410243
0.552113295847914 to 0.558029581803431
0.880943359190962 to 0.886859645146479
0.806497575099013 to 0.81241386105453
0.314246864373228 to 0.320163150328745
0.0843394429780456 to 0.0902557289335627
0.356065721566221 to 0.361982007521739
0.349791379029295 to 0.355707664984812
Changing layer 4's weights from 
0.549953938776991 to 0.555870224732508
0.427114458138487 to 0.433030744094004
0.159544059390566 to 0.165460345346083
0.936293782050154 to 0.942210068005671
1.05290669327452 to 1.05882297923004
0.722032786662123 to 0.72794907261764
0.814328314120314 to 0.820244600075831
0.521805526072523 to 0.52772181202804
0.666315795237562 to 0.67223208119308
0.530808569247267 to 0.536724855202784
Changing layer 5's weights from 
0.694478870684645 to 0.700395156640162
0.523356915766737 to 0.529273201722254
0.711261154467604 to 0.717177440423121
0.874505819136641 to 0.880422105092158
0.529075623805067 to 0.534991909760584
0.329364986474058 to 0.335281272429575
0.575237990672133 to 0.58115427662765
0.901538909728071 to 0.907455195683589
0.16311498110964 to 0.169031267065157
0.963364423567793 to 0.96928070952331
Trying to learn from memory 8, 2, 0.12
sum 0.770064627300038 distri 0.233434699367701
Using diff 0.344113771107328 and condRate 0.166666666666667
Changed category 2 weights from 
0.636623680789011 to 0.643505956484634
0.275361806351679 to 0.282244082047302
0.0710734205432109 to 0.0779556962388345
0.799486160475748 to 0.806368436171372
Changing layer 0's weights from 
1.07389294077789 to 1.08077521647351
0.115773909736027 to 0.122656185431651
0.275796075362315 to 0.282678351057938
0.368218679923167 to 0.37510095561879
0.306489606398692 to 0.313371882094315
0.80764239003097 to 0.814524665726594
0.466590692299952 to 0.473472967995576
0.939845551747431 to 0.946727827443055
1.00889014174377 to 1.01577241743939
0.393003244895091 to 0.399885520590714
Changing layer 1's weights from 
0.547014524239649 to 0.553896799935273
0.600020458001246 to 0.60690273369687
1.02413569857513 to 1.03101797427075
0.547690679329981 to 0.554572955025605
0.876567591923823 to 0.883449867619447
0.279184360999217 to 0.28606663669484
0.416554351348032 to 0.423436627043656
0.151241299707045 to 0.158123575402669
0.251357679266085 to 0.258239954961709
0.252040271658053 to 0.258922547353676
Changing layer 2's weights from 
0.373459716338267 to 0.38034199203389
0.256329064268221 to 0.263211339963845
0.242538337606539 to 0.249420613302163
0.213062291044344 to 0.219944566739968
0.622524310845484 to 0.629406586541108
0.122605201799025 to 0.129487477494649
0.698728491562953 to 0.705610767258576
1.06012557913696 to 1.06700785483258
0.906317581433405 to 0.913199857129029
0.135647174913039 to 0.142529450608662
Changing layer 3's weights from 
0.607436110276331 to 0.614318385971955
0.520881106156458 to 0.527763381852082
0.778278519410243 to 0.785160795105866
0.558029581803431 to 0.564911857499055
0.886859645146479 to 0.893741920842103
0.81241386105453 to 0.819296136750153
0.320163150328745 to 0.327045426024369
0.0902557289335627 to 0.0971380046291863
0.361982007521739 to 0.368864283217362
0.355707664984812 to 0.362589940680436
Changing layer 4's weights from 
0.555870224732508 to 0.562752500428132
0.433030744094004 to 0.439913019789628
0.165460345346083 to 0.172342621041707
0.942210068005671 to 0.949092343701295
1.05882297923004 to 1.06570525492566
0.72794907261764 to 0.734831348313264
0.820244600075831 to 0.827126875771455
0.52772181202804 to 0.534604087723664
0.67223208119308 to 0.679114356888703
0.536724855202784 to 0.543607130898408
Changing layer 5's weights from 
0.700395156640162 to 0.707277432335786
0.529273201722254 to 0.536155477417878
0.717177440423121 to 0.724059716118745
0.880422105092158 to 0.887304380787782
0.534991909760584 to 0.541874185456208
0.335281272429575 to 0.342163548125199
0.58115427662765 to 0.588036552323273
0.907455195683589 to 0.914337471379212
0.169031267065157 to 0.175913542760781
0.96928070952331 to 0.976162985218934
10/5/2016 11:21:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:36 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:37 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:42 PMStarting learning phase with deltaScore: 0.7333333
Modified index 0's learning in memoryPool to 0.1466667
Modified index 1's learning in memoryPool to 0.1466667
Modified index 2's learning in memoryPool to 0.1466667
Modified index 3's learning in memoryPool to 0.1466667
Modified index 4's learning in memoryPool to 0.1466667
10/5/2016 11:21:43 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 9, 1, 0.1466667
sum 1.06829193247618 distri 0.379228882041917
Using diff 0.421990067315215 and condRate 0.166666666666667
Changed category 1 weights from 
0.223007142682926 to 0.233322456082308
0.971599728008168 to 0.98191504140755
0.944771319766896 to 0.955086633166278
0.0304677795147464 to 0.0407830929141284
Changing layer 0's weights from 
1.08077521647351 to 1.09109052987289
0.122656185431651 to 0.132971498831033
0.282678351057938 to 0.29299366445732
0.37510095561879 to 0.385416269018172
0.313371882094315 to 0.323687195493697
0.814524665726594 to 0.824839979125976
0.473472967995576 to 0.483788281394958
0.946727827443055 to 0.957043140842437
1.01577241743939 to 1.02608773083878
0.399885520590714 to 0.410200833990096
Changing layer 1's weights from 
0.553896799935273 to 0.564212113334655
0.60690273369687 to 0.617218047096252
1.03101797427075 to 1.04133328767013
0.554572955025605 to 0.564888268424987
0.883449867619447 to 0.893765181018829
0.28606663669484 to 0.296381950094222
0.423436627043656 to 0.433751940443038
0.158123575402669 to 0.168438888802051
0.258239954961709 to 0.268555268361091
0.258922547353676 to 0.269237860753058
Changing layer 2's weights from 
0.38034199203389 to 0.390657305433272
0.263211339963845 to 0.273526653363227
0.249420613302163 to 0.259735926701545
0.219944566739968 to 0.23025988013935
0.629406586541108 to 0.63972189994049
0.129487477494649 to 0.139802790894031
0.705610767258576 to 0.715926080657958
1.06700785483258 to 1.07732316823196
0.913199857129029 to 0.923515170528411
0.142529450608662 to 0.152844764008044
Changing layer 3's weights from 
0.614318385971955 to 0.624633699371337
0.527763381852082 to 0.538078695251464
0.785160795105866 to 0.795476108505248
0.564911857499055 to 0.575227170898437
0.893741920842103 to 0.904057234241485
0.819296136750153 to 0.829611450149535
0.327045426024369 to 0.337360739423751
0.0971380046291863 to 0.107453318028568
0.368864283217362 to 0.379179596616744
0.362589940680436 to 0.372905254079818
Changing layer 4's weights from 
0.562752500428132 to 0.573067813827514
0.439913019789628 to 0.45022833318901
0.172342621041707 to 0.182657934441089
0.949092343701295 to 0.959407657100677
1.06570525492566 to 1.07602056832504
0.734831348313264 to 0.745146661712646
0.827126875771455 to 0.837442189170837
0.534604087723664 to 0.544919401123046
0.679114356888703 to 0.689429670288085
0.543607130898408 to 0.55392244429779
Changing layer 5's weights from 
0.707277432335786 to 0.717592745735168
0.536155477417878 to 0.54647079081726
0.724059716118745 to 0.734375029518127
0.887304380787782 to 0.897619694187163
0.541874185456208 to 0.55218949885559
0.342163548125199 to 0.352478861524581
0.588036552323273 to 0.598351865722655
0.914337471379212 to 0.924652784778594
0.175913542760781 to 0.186228856160163
0.976162985218934 to 0.986478298618316
Trying to learn from memory 10, 2, 0.1466667
sum 1.04818892617811 distri 0.315046643006628
Using diff 0.471095051626957 and condRate 0.166666666666667
Changed category 2 weights from 
0.643505956484634 to 0.655021614019768
0.282244082047302 to 0.293759739582436
0.0779556962388345 to 0.0894713537739684
0.806368436171372 to 0.817884093706505
Changing layer 0's weights from 
1.09109052987289 to 1.10260618740803
0.132971498831033 to 0.144487156366167
0.29299366445732 to 0.304509321992454
0.385416269018172 to 0.396931926553306
0.323687195493697 to 0.335202853028831
0.824839979125976 to 0.83635563666111
0.483788281394958 to 0.495303938930091
0.957043140842437 to 0.968558798377571
1.02608773083878 to 1.03760338837391
0.410200833990096 to 0.42171649152523
Changing layer 1's weights from 
0.564212113334655 to 0.575727770869789
0.617218047096252 to 0.628733704631385
1.04133328767013 to 1.05284894520527
0.564888268424987 to 0.576403925960121
0.893765181018829 to 0.905280838553962
0.296381950094222 to 0.307897607629356
0.433751940443038 to 0.445267597978172
0.168438888802051 to 0.179954546337185
0.268555268361091 to 0.280070925896225
0.269237860753058 to 0.280753518288192
Changing layer 2's weights from 
0.390657305433272 to 0.402172962968406
0.273526653363227 to 0.285042310898361
0.259735926701545 to 0.271251584236679
0.23025988013935 to 0.241775537674484
0.63972189994049 to 0.651237557475624
0.139802790894031 to 0.151318448429165
0.715926080657958 to 0.727441738193092
1.07732316823196 to 1.0888388257671
0.923515170528411 to 0.935030828063545
0.152844764008044 to 0.164360421543178
Changing layer 3's weights from 
0.624633699371337 to 0.636149356906471
0.538078695251464 to 0.549594352786598
0.795476108505248 to 0.806991766040382
0.575227170898437 to 0.58674282843357
0.904057234241485 to 0.915572891776619
0.829611450149535 to 0.841127107684669
0.337360739423751 to 0.348876396958885
0.107453318028568 to 0.118968975563702
0.379179596616744 to 0.390695254151878
0.372905254079818 to 0.384420911614952
Changing layer 4's weights from 
0.573067813827514 to 0.584583471362648
0.45022833318901 to 0.461743990724144
0.182657934441089 to 0.194173591976223
0.959407657100677 to 0.970923314635811
1.07602056832504 to 1.08753622586018
0.745146661712646 to 0.756662319247779
0.837442189170837 to 0.84895784670597
0.544919401123046 to 0.55643505865818
0.689429670288085 to 0.700945327823219
0.55392244429779 to 0.565438101832924
Changing layer 5's weights from 
0.717592745735168 to 0.729108403270301
0.54647079081726 to 0.557986448352394
0.734375029518127 to 0.74589068705326
0.897619694187163 to 0.909135351722297
0.55218949885559 to 0.563705156390724
0.352478861524581 to 0.363994519059715
0.598351865722655 to 0.609867523257789
0.924652784778594 to 0.936168442313728
0.186228856160163 to 0.197744513695297
0.986478298618316 to 0.99799395615345
Trying to learn from memory 11, 2, 0.1466667
sum 1.03662369587503 distri 0.311401285042455
Using diff 0.466066486863817 and condRate 0.166666666666667
Changed category 2 weights from 
0.655021614019768 to 0.666414351075255
0.293759739582436 to 0.305152476637923
0.0894713537739684 to 0.100864090829455
0.817884093706505 to 0.829276830761992
Changing layer 0's weights from 
1.10260618740803 to 1.11399892446351
0.144487156366167 to 0.155879893421653
0.304509321992454 to 0.31590205904794
0.396931926553306 to 0.408324663608792
0.335202853028831 to 0.346595590084317
0.83635563666111 to 0.847748373716596
0.495303938930091 to 0.506696675985578
0.968558798377571 to 0.979951535433057
1.03760338837391 to 1.0489961254294
0.42171649152523 to 0.433109228580716
Changing layer 1's weights from 
0.575727770869789 to 0.587120507925275
0.628733704631385 to 0.640126441686872
1.05284894520527 to 1.06424168226076
0.576403925960121 to 0.587796663015607
0.905280838553962 to 0.916673575609449
0.307897607629356 to 0.319290344684842
0.445267597978172 to 0.456660335033658
0.179954546337185 to 0.191347283392671
0.280070925896225 to 0.291463662951711
0.280753518288192 to 0.292146255343679
Changing layer 2's weights from 
0.402172962968406 to 0.413565700023893
0.285042310898361 to 0.296435047953847
0.271251584236679 to 0.282644321292165
0.241775537674484 to 0.25316827472997
0.651237557475624 to 0.66263029453111
0.151318448429165 to 0.162711185484651
0.727441738193092 to 0.738834475248578
1.0888388257671 to 1.10023156282258
0.935030828063545 to 0.946423565119031
0.164360421543178 to 0.175753158598665
Changing layer 3's weights from 
0.636149356906471 to 0.647542093961957
0.549594352786598 to 0.560987089842084
0.806991766040382 to 0.818384503095868
0.58674282843357 to 0.598135565489057
0.915572891776619 to 0.926965628832105
0.841127107684669 to 0.852519844740156
0.348876396958885 to 0.360269134014371
0.118968975563702 to 0.130361712619189
0.390695254151878 to 0.402087991207364
0.384420911614952 to 0.395813648670438
Changing layer 4's weights from 
0.584583471362648 to 0.595976208418134
0.461743990724144 to 0.47313672777963
0.194173591976223 to 0.205566329031709
0.970923314635811 to 0.982316051691297
1.08753622586018 to 1.09892896291566
0.756662319247779 to 0.768055056303266
0.84895784670597 to 0.860350583761457
0.55643505865818 to 0.567827795713666
0.700945327823219 to 0.712338064878705
0.565438101832924 to 0.57683083888841
Changing layer 5's weights from 
0.729108403270301 to 0.740501140325788
0.557986448352394 to 0.56937918540788
0.74589068705326 to 0.757283424108747
0.909135351722297 to 0.920528088777784
0.563705156390724 to 0.57509789344621
0.363994519059715 to 0.375387256115201
0.609867523257789 to 0.621260260313276
0.936168442313728 to 0.947561179369214
0.197744513695297 to 0.209137250750783
0.99799395615345 to 1.00938669320894
Trying to learn from memory 12, 1, 0.1466667
sum 1.07650909168946 distri 0.383849760728317
Using diff 0.423532058038776 and condRate 0.166666666666667
Changed category 1 weights from 
0.233322456082308 to 0.243675462590615
0.98191504140755 to 0.992268047915856
0.955086633166278 to 0.965439639674584
0.0407830929141284 to 0.0511360994224351
Changing layer 0's weights from 
1.11399892446351 to 1.12435193097182
0.155879893421653 to 0.16623289992996
0.31590205904794 to 0.326255065556247
0.408324663608792 to 0.418677670117099
0.346595590084317 to 0.356948596592624
0.847748373716596 to 0.858101380224903
0.506696675985578 to 0.517049682493885
0.979951535433057 to 0.990304541941364
1.0489961254294 to 1.0593491319377
0.433109228580716 to 0.443462235089023
Changing layer 1's weights from 
0.587120507925275 to 0.597473514433582
0.640126441686872 to 0.650479448195179
1.06424168226076 to 1.07459468876906
0.587796663015607 to 0.598149669523914
0.916673575609449 to 0.927026582117755
0.319290344684842 to 0.329643351193149
0.456660335033658 to 0.467013341541965
0.191347283392671 to 0.201700289900978
0.291463662951711 to 0.301816669460018
0.292146255343679 to 0.302499261851985
Changing layer 2's weights from 
0.413565700023893 to 0.423918706532199
0.296435047953847 to 0.306788054462154
0.282644321292165 to 0.292997327800472
0.25316827472997 to 0.263521281238277
0.66263029453111 to 0.672983301039417
0.162711185484651 to 0.173064191992958
0.738834475248578 to 0.749187481756885
1.10023156282258 to 1.11058456933089
0.946423565119031 to 0.956776571627338
0.175753158598665 to 0.186106165106971
Changing layer 3's weights from 
0.647542093961957 to 0.657895100470264
0.560987089842084 to 0.571340096350391
0.818384503095868 to 0.828737509604175
0.598135565489057 to 0.608488571997364
0.926965628832105 to 0.937318635340412
0.852519844740156 to 0.862872851248462
0.360269134014371 to 0.370622140522678
0.130361712619189 to 0.140714719127495
0.402087991207364 to 0.412440997715671
0.395813648670438 to 0.406166655178745
Changing layer 4's weights from 
0.595976208418134 to 0.606329214926441
0.47313672777963 to 0.483489734287937
0.205566329031709 to 0.215919335540016
0.982316051691297 to 0.992669058199604
1.09892896291566 to 1.10928196942397
0.768055056303266 to 0.778408062811573
0.860350583761457 to 0.870703590269764
0.567827795713666 to 0.578180802221973
0.712338064878705 to 0.722691071387012
0.57683083888841 to 0.587183845396717
Changing layer 5's weights from 
0.740501140325788 to 0.750854146834095
0.56937918540788 to 0.579732191916187
0.757283424108747 to 0.767636430617054
0.920528088777784 to 0.93088109528609
0.57509789344621 to 0.585450899954517
0.375387256115201 to 0.385740262623508
0.621260260313276 to 0.631613266821582
0.947561179369214 to 0.957914185877521
0.209137250750783 to 0.21949025725909
1.00938669320894 to 1.01973969971724
Trying to learn from memory 13, 2, 0.1466667
sum 1.03784635461283 distri 0.312180732536686
Using diff 0.466204033422935 and condRate 0.166666666666667
Changed category 2 weights from 
0.666414351075255 to 0.677810450380173
0.305152476637923 to 0.316548575942841
0.100864090829455 to 0.112260190134373
0.829276830761992 to 0.84067293006691
Changing layer 0's weights from 
1.12435193097182 to 1.13574803027674
0.16623289992996 to 0.177628999234879
0.326255065556247 to 0.337651164861166
0.418677670117099 to 0.430073769422018
0.356948596592624 to 0.368344695897543
0.858101380224903 to 0.869497479529821
0.517049682493885 to 0.528445781798803
0.990304541941364 to 1.00170064124628
1.0593491319377 to 1.07074523124262
0.443462235089023 to 0.454858334393942
Changing layer 1's weights from 
0.597473514433582 to 0.608869613738501
0.650479448195179 to 0.661875547500097
1.07459468876906 to 1.08599078807398
0.598149669523914 to 0.609545768828833
0.927026582117755 to 0.938422681422674
0.329643351193149 to 0.341039450498068
0.467013341541965 to 0.478409440846884
0.201700289900978 to 0.213096389205896
0.301816669460018 to 0.313212768764936
0.302499261851985 to 0.313895361156904
Changing layer 2's weights from 
0.423918706532199 to 0.435314805837118
0.306788054462154 to 0.318184153767073
0.292997327800472 to 0.30439342710539
0.263521281238277 to 0.274917380543196
0.672983301039417 to 0.684379400344336
0.173064191992958 to 0.184460291297876
0.749187481756885 to 0.760583581061804
1.11058456933089 to 1.12198066863581
0.956776571627338 to 0.968172670932257
0.186106165106971 to 0.19750226441189
Changing layer 3's weights from 
0.657895100470264 to 0.669291199775183
0.571340096350391 to 0.58273619565531
0.828737509604175 to 0.840133608909094
0.608488571997364 to 0.619884671302282
0.937318635340412 to 0.94871473464533
0.862872851248462 to 0.874268950553381
0.370622140522678 to 0.382018239827597
0.140714719127495 to 0.152110818432414
0.412440997715671 to 0.42383709702059
0.406166655178745 to 0.417562754483663
Changing layer 4's weights from 
0.606329214926441 to 0.617725314231359
0.483489734287937 to 0.494885833592855
0.215919335540016 to 0.227315434844934
0.992669058199604 to 1.00406515750452
1.10928196942397 to 1.12067806872889
0.778408062811573 to 0.789804162116491
0.870703590269764 to 0.882099689574682
0.578180802221973 to 0.589576901526892
0.722691071387012 to 0.734087170691931
0.587183845396717 to 0.598579944701635
Changing layer 5's weights from 
0.750854146834095 to 0.762250246139013
0.579732191916187 to 0.591128291221105
0.767636430617054 to 0.779032529921972
0.93088109528609 to 0.942277194591009
0.585450899954517 to 0.596846999259436
0.385740262623508 to 0.397136361928427
0.631613266821582 to 0.643009366126501
0.957914185877521 to 0.96931028518244
0.21949025725909 to 0.230886356564009
1.01973969971724 to 1.03113579902216
10/5/2016 11:21:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:43 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:44 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:47 PMStarting learning phase with deltaScore: 4.666667
Modified index 0's learning in memoryPool to 0.9333333
10/5/2016 11:21:47 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 14, 2, 0.9333333
sum 1.3219724059326 distri 0.413573898430539
Using diff 0.577905406018913 and condRate 0.166666666666667
Changed category 2 weights from 
0.677810450380173 to 0.767706841513762
0.316548575942841 to 0.406444967076429
0.112260190134373 to 0.202156581267962
0.84067293006691 to 0.930569321200499
Changing layer 0's weights from 
1.13574803027674 to 1.22564442141033
0.177628999234879 to 0.267525390368467
0.337651164861166 to 0.427547555994754
0.430073769422018 to 0.519970160555606
0.368344695897543 to 0.458241087031131
0.869497479529821 to 0.95939387066341
0.528445781798803 to 0.618342172932391
1.00170064124628 to 1.09159703237987
1.07074523124262 to 1.16064162237621
0.454858334393942 to 0.54475472552753
Changing layer 1's weights from 
0.608869613738501 to 0.698766004872089
0.661875547500097 to 0.751771938633685
1.08599078807398 to 1.17588717920757
0.609545768828833 to 0.699442159962421
0.938422681422674 to 1.02831907255626
0.341039450498068 to 0.430935841631656
0.478409440846884 to 0.568305831980472
0.213096389205896 to 0.302992780339484
0.313212768764936 to 0.403109159898524
0.313895361156904 to 0.403791752290492
Changing layer 2's weights from 
0.435314805837118 to 0.525211196970706
0.318184153767073 to 0.408080544900661
0.30439342710539 to 0.394289818238979
0.274917380543196 to 0.364813771676784
0.684379400344336 to 0.774275791477924
0.184460291297876 to 0.274356682431464
0.760583581061804 to 0.850479972195392
1.12198066863581 to 1.2118770597694
0.968172670932257 to 1.05806906206584
0.19750226441189 to 0.287398655545478
Changing layer 3's weights from 
0.669291199775183 to 0.759187590908771
0.58273619565531 to 0.672632586788898
0.840133608909094 to 0.930030000042682
0.619884671302282 to 0.70978106243587
0.94871473464533 to 1.03861112577892
0.874268950553381 to 0.964165341686969
0.382018239827597 to 0.471914630961185
0.152110818432414 to 0.242007209566002
0.42383709702059 to 0.513733488154178
0.417562754483663 to 0.507459145617252
Changing layer 4's weights from 
0.617725314231359 to 0.707621705364947
0.494885833592855 to 0.584782224726443
0.227315434844934 to 0.317211825978522
1.00406515750452 to 1.09396154863811
1.12067806872889 to 1.21057445986248
0.789804162116491 to 0.879700553250079
0.882099689574682 to 0.97199608070827
0.589576901526892 to 0.67947329266048
0.734087170691931 to 0.823983561825519
0.598579944701635 to 0.688476335835224
Changing layer 5's weights from 
0.762250246139013 to 0.852146637272601
0.591128291221105 to 0.681024682354693
0.779032529921972 to 0.86892892105556
0.942277194591009 to 1.0321735857246
0.596846999259436 to 0.686743390393024
0.397136361928427 to 0.487032753062015
0.643009366126501 to 0.732905757260089
0.96931028518244 to 1.05920667631603
0.230886356564009 to 0.320782747697597
1.03113579902216 to 1.12103219015575
10/5/2016 11:21:47 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:48 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:49 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:50 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:51 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:52 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:53 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:54 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:55 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:56 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:57 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:58 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:21:59 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:00 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:01 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:02 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:03 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:06 PMStarting learning phase with deltaScore: 2.333333
Modified index 0's learning in memoryPool to 0.4666666
Modified index 1's learning in memoryPool to 0.4666666
Modified index 2's learning in memoryPool to 0.4666666
Modified index 3's learning in memoryPool to 0.4666666
Modified index 4's learning in memoryPool to 0.4666666
Modified index 5's learning in memoryPool to 0.4666666
Modified index 6's learning in memoryPool to 0.4666666
Modified index 7's learning in memoryPool to 0.4666666
Modified index 8's learning in memoryPool to 0.4666666
Modified index 9's learning in memoryPool to 0.4666666
Modified index 10's learning in memoryPool to 0.4666666
Modified index 11's learning in memoryPool to 0.4666666
Modified index 12's learning in memoryPool to 0.4666666
Modified index 13's learning in memoryPool to 0.4666666
10/5/2016 11:22:06 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 15, 0, 0.4666666
sum 2.08031676365259 distri 0.654001072101067
Using diff 0.906236500638376 and condRate 0.166666666666667
Changed category 0 weights from 
0.547251362959103 to 0.617736419918628
0.461450953642086 to 0.531936010601612
0.618296404043392 to 0.688781461002918
0.458746333280758 to 0.529231390240284
Changing layer 0's weights from 
1.22564442141033 to 1.29612947836985
0.267525390368467 to 0.338010447327992
0.427547555994754 to 0.49803261295428
0.519970160555606 to 0.590455217515132
0.458241087031131 to 0.528726143990657
0.95939387066341 to 1.02987892762294
0.618342172932391 to 0.688827229891917
1.09159703237987 to 1.1620820893394
1.16064162237621 to 1.23112667933573
0.54475472552753 to 0.615239782487055
Changing layer 1's weights from 
0.698766004872089 to 0.769251061831614
0.751771938633685 to 0.822256995593211
1.17588717920757 to 1.24637223616709
0.699442159962421 to 0.769927216921946
1.02831907255626 to 1.09880412951579
0.430935841631656 to 0.501420898591181
0.568305831980472 to 0.638790888939997
0.302992780339484 to 0.37347783729901
0.403109159898524 to 0.47359421685805
0.403791752290492 to 0.474276809250018
Changing layer 2's weights from 
0.525211196970706 to 0.595696253930232
0.408080544900661 to 0.478565601860186
0.394289818238979 to 0.464774875198504
0.364813771676784 to 0.435298828636309
0.774275791477924 to 0.844760848437449
0.274356682431464 to 0.34484173939099
0.850479972195392 to 0.920965029154918
1.2118770597694 to 1.28236211672892
1.05806906206584 to 1.12855411902537
0.287398655545478 to 0.357883712505004
Changing layer 3's weights from 
0.759187590908771 to 0.829672647868296
0.672632586788898 to 0.743117643748423
0.930030000042682 to 1.00051505700221
0.70978106243587 to 0.780266119395396
1.03861112577892 to 1.10909618273844
0.964165341686969 to 1.03465039864649
0.471914630961185 to 0.54239968792071
0.242007209566002 to 0.312492266525528
0.513733488154178 to 0.584218545113703
0.507459145617252 to 0.577944202576777
Changing layer 4's weights from 
0.707621705364947 to 0.778106762324473
0.584782224726443 to 0.655267281685969
0.317211825978522 to 0.387696882938048
1.09396154863811 to 1.16444660559764
1.21057445986248 to 1.281059516822
0.879700553250079 to 0.950185610209605
0.97199608070827 to 1.0424811376678
0.67947329266048 to 0.749958349620005
0.823983561825519 to 0.894468618785044
0.688476335835224 to 0.758961392794749
Changing layer 5's weights from 
0.852146637272601 to 0.922631694232127
0.681024682354693 to 0.751509739314219
0.86892892105556 to 0.939413978015086
1.0321735857246 to 1.10265864268412
0.686743390393024 to 0.757228447352549
0.487032753062015 to 0.55751781002154
0.732905757260089 to 0.803390814219615
1.05920667631603 to 1.12969173327555
0.320782747697597 to 0.391267804657122
1.12103219015575 to 1.19151724711528
Trying to learn from memory 16, 0, 0.4666666
sum 2.08032022895709 distri 0.654002190813442
Using diff 0.906237980904372 and condRate 0.166666666666667
Changed category 0 weights from 
0.617736419918628 to 0.688221592009947
0.531936010601612 to 0.60242118269293
0.688781461002918 to 0.759266633094236
0.529231390240284 to 0.599716562331602
Changing layer 0's weights from 
1.29612947836985 to 1.36661465046117
0.338010447327992 to 0.408495619419311
0.49803261295428 to 0.568517785045598
0.590455217515132 to 0.66094038960645
0.528726143990657 to 0.599211316081975
1.02987892762294 to 1.10036409971425
0.688827229891917 to 0.759312401983235
1.1620820893394 to 1.23256726143071
1.23112667933573 to 1.30161185142705
0.615239782487055 to 0.685724954578374
Changing layer 1's weights from 
0.769251061831614 to 0.839736233922933
0.822256995593211 to 0.892742167684529
1.24637223616709 to 1.31685740825841
0.769927216921946 to 0.840412389013265
1.09880412951579 to 1.16928930160711
0.501420898591181 to 0.5719060706825
0.638790888939997 to 0.709276061031316
0.37347783729901 to 0.443963009390328
0.47359421685805 to 0.544079388949368
0.474276809250018 to 0.544761981341336
Changing layer 2's weights from 
0.595696253930232 to 0.66618142602155
0.478565601860186 to 0.549050773951505
0.464774875198504 to 0.535260047289822
0.435298828636309 to 0.505784000727628
0.844760848437449 to 0.915246020528768
0.34484173939099 to 0.415326911482308
0.920965029154918 to 0.991450201246236
1.28236211672892 to 1.35284728882024
1.12855411902537 to 1.19903929111669
0.357883712505004 to 0.428368884596322
Changing layer 3's weights from 
0.829672647868296 to 0.900157819959615
0.743117643748423 to 0.813602815839742
1.00051505700221 to 1.07100022909353
0.780266119395396 to 0.850751291486714
1.10909618273844 to 1.17958135482976
1.03465039864649 to 1.10513557073781
0.54239968792071 to 0.612884860012029
0.312492266525528 to 0.382977438616846
0.584218545113703 to 0.654703717205022
0.577944202576777 to 0.648429374668096
Changing layer 4's weights from 
0.778106762324473 to 0.848591934415791
0.655267281685969 to 0.725752453777287
0.387696882938048 to 0.458182055029366
1.16444660559764 to 1.23493177768895
1.281059516822 to 1.35154468891332
0.950185610209605 to 1.02067078230092
1.0424811376678 to 1.11296630975911
0.749958349620005 to 0.820443521711324
0.894468618785044 to 0.964953790876363
0.758961392794749 to 0.829446564886067
Changing layer 5's weights from 
0.922631694232127 to 0.993116866323445
0.751509739314219 to 0.821994911405537
0.939413978015086 to 1.0098991501064
1.10265864268412 to 1.17314381477544
0.757228447352549 to 0.827713619443868
0.55751781002154 to 0.628002982112859
0.803390814219615 to 0.873875986310933
1.12969173327555 to 1.20017690536687
0.391267804657122 to 0.461752976748441
1.19151724711528 to 1.26200241920659
Trying to learn from memory 17, 0, 0.4666666
sum 2.080468083176 distri 0.654049506887505
Using diff 0.906301555494495 and condRate 0.166666666666667
Changed category 0 weights from 
0.688221592009947 to 0.758711708791313
0.60242118269293 to 0.672911299474297
0.759266633094236 to 0.829756749875603
0.599716562331602 to 0.670206679112969
Changing layer 0's weights from 
1.36661465046117 to 1.43710476724254
0.408495619419311 to 0.478985736200677
0.568517785045598 to 0.639007901826965
0.66094038960645 to 0.731430506387817
0.599211316081975 to 0.669701432863341
1.10036409971425 to 1.17085421649562
0.759312401983235 to 0.829802518764602
1.23256726143071 to 1.30305737821208
1.30161185142705 to 1.37210196820842
0.685724954578374 to 0.75621507135974
Changing layer 1's weights from 
0.839736233922933 to 0.910226350704299
0.892742167684529 to 0.963232284465896
1.31685740825841 to 1.38734752503978
0.840412389013265 to 0.910902505794631
1.16928930160711 to 1.23977941838847
0.5719060706825 to 0.642396187463866
0.709276061031316 to 0.779766177812682
0.443963009390328 to 0.514453126171695
0.544079388949368 to 0.614569505730735
0.544761981341336 to 0.615252098122703
Changing layer 2's weights from 
0.66618142602155 to 0.736671542802917
0.549050773951505 to 0.619540890732871
0.535260047289822 to 0.605750164071189
0.505784000727628 to 0.576274117508994
0.915246020528768 to 0.985736137310134
0.415326911482308 to 0.485817028263675
0.991450201246236 to 1.0619403180276
1.35284728882024 to 1.42333740560161
1.19903929111669 to 1.26952940789806
0.428368884596322 to 0.498859001377689
Changing layer 3's weights from 
0.900157819959615 to 0.970647936740981
0.813602815839742 to 0.884092932621108
1.07100022909353 to 1.14149034587489
0.850751291486714 to 0.921241408268081
1.17958135482976 to 1.25007147161113
1.10513557073781 to 1.17562568751918
0.612884860012029 to 0.683374976793395
0.382977438616846 to 0.453467555398213
0.654703717205022 to 0.725193833986388
0.648429374668096 to 0.718919491449462
Changing layer 4's weights from 
0.848591934415791 to 0.919082051197158
0.725752453777287 to 0.796242570558654
0.458182055029366 to 0.528672171810733
1.23493177768895 to 1.30542189447032
1.35154468891332 to 1.42203480569469
1.02067078230092 to 1.09116089908229
1.11296630975911 to 1.18345642654048
0.820443521711324 to 0.89093363849269
0.964953790876363 to 1.03544390765773
0.829446564886067 to 0.899936681667434
Changing layer 5's weights from 
0.993116866323445 to 1.06360698310481
0.821994911405537 to 0.892485028186904
1.0098991501064 to 1.08038926688777
1.17314381477544 to 1.24363393155681
0.827713619443868 to 0.898203736225234
0.628002982112859 to 0.698493098894225
0.873875986310933 to 0.9443661030923
1.20017690536687 to 1.27066702214824
0.461752976748441 to 0.532243093529807
1.26200241920659 to 1.33249253598796
Trying to learn from memory 18, 1, 0.4666666
sum 2.09194125952289 distri 0.697578874014149
Using diff 0.871377070628015 and condRate 0.166666666666667
Changed category 1 weights from 
0.243675462590615 to 0.31144923071094
0.992268047915856 to 1.06004181603618
0.965439639674584 to 1.03321340779491
0.0511360994224351 to 0.11890986754276
Changing layer 0's weights from 
1.43710476724254 to 1.50487853536286
0.478985736200677 to 0.546759504321002
0.639007901826965 to 0.70678166994729
0.731430506387817 to 0.799204274508142
0.669701432863341 to 0.737475200983667
1.17085421649562 to 1.23862798461595
0.829802518764602 to 0.897576286884927
1.30305737821208 to 1.37083114633241
1.37210196820842 to 1.43987573632874
0.75621507135974 to 0.823988839480066
Changing layer 1's weights from 
0.910226350704299 to 0.978000118824624
0.963232284465896 to 1.03100605258622
1.38734752503978 to 1.4551212931601
0.910902505794631 to 0.978676273914956
1.23977941838847 to 1.3075531865088
0.642396187463866 to 0.710169955584191
0.779766177812682 to 0.847539945933007
0.514453126171695 to 0.58222689429202
0.614569505730735 to 0.68234327385106
0.615252098122703 to 0.683025866243028
Changing layer 2's weights from 
0.736671542802917 to 0.804445310923242
0.619540890732871 to 0.687314658853196
0.605750164071189 to 0.673523932191514
0.576274117508994 to 0.644047885629319
0.985736137310134 to 1.05350990543046
0.485817028263675 to 0.553590796384
1.0619403180276 to 1.12971408614793
1.42333740560161 to 1.49111117372193
1.26952940789806 to 1.33730317601838
0.498859001377689 to 0.566632769498014
Changing layer 3's weights from 
0.970647936740981 to 1.03842170486131
0.884092932621108 to 0.951866700741433
1.14149034587489 to 1.20926411399522
0.921241408268081 to 0.989015176388406
1.25007147161113 to 1.31784523973145
1.17562568751918 to 1.2433994556395
0.683374976793395 to 0.75114874491372
0.453467555398213 to 0.521241323518538
0.725193833986388 to 0.792967602106713
0.718919491449462 to 0.786693259569787
Changing layer 4's weights from 
0.919082051197158 to 0.986855819317483
0.796242570558654 to 0.864016338678979
0.528672171810733 to 0.596445939931058
1.30542189447032 to 1.37319566259065
1.42203480569469 to 1.48980857381501
1.09116089908229 to 1.15893466720262
1.18345642654048 to 1.25123019466081
0.89093363849269 to 0.958707406613015
1.03544390765773 to 1.10321767577805
0.899936681667434 to 0.967710449787759
Changing layer 5's weights from 
1.06360698310481 to 1.13138075122514
0.892485028186904 to 0.960258796307229
1.08038926688777 to 1.1481630350081
1.24363393155681 to 1.31140769967713
0.898203736225234 to 0.965977504345559
0.698493098894225 to 0.76626686701455
0.9443661030923 to 1.01213987121262
1.27066702214824 to 1.33844079026856
0.532243093529807 to 0.600016861650132
1.33249253598796 to 1.40026630410829
Trying to learn from memory 18, 0, 0.4666666
sum 2.09194125952289 distri 0.657709241967601
Using diff 0.911246702674563 and condRate 0.166666666666667
Changed category 0 weights from 
0.758711708791313 to 0.829586448108204
0.672911299474297 to 0.743786038791188
0.829756749875603 to 0.900631489192494
0.670206679112969 to 0.74108141842986
Changing layer 0's weights from 
1.50487853536286 to 1.57575327467975
0.546759504321002 to 0.617634243637893
0.70678166994729 to 0.777656409264181
0.799204274508142 to 0.870079013825033
0.737475200983667 to 0.808349940300558
1.23862798461595 to 1.30950272393284
0.897576286884927 to 0.968451026201818
1.37083114633241 to 1.4417058856493
1.43987573632874 to 1.51075047564564
0.823988839480066 to 0.894863578796957
Changing layer 1's weights from 
0.978000118824624 to 1.04887485814152
1.03100605258622 to 1.10188079190311
1.4551212931601 to 1.525996032477
0.978676273914956 to 1.04955101323185
1.3075531865088 to 1.37842792582569
0.710169955584191 to 0.781044694901083
0.847539945933007 to 0.918414685249898
0.58222689429202 to 0.653101633608911
0.68234327385106 to 0.753218013167951
0.683025866243028 to 0.753900605559919
Changing layer 2's weights from 
0.804445310923242 to 0.875320050240133
0.687314658853196 to 0.758189398170087
0.673523932191514 to 0.744398671508405
0.644047885629319 to 0.71492262494621
1.05350990543046 to 1.12438464474735
0.553590796384 to 0.624465535700891
1.12971408614793 to 1.20058882546482
1.49111117372193 to 1.56198591303882
1.33730317601838 to 1.40817791533527
0.566632769498014 to 0.637507508814905
Changing layer 3's weights from 
1.03842170486131 to 1.1092964441782
0.951866700741433 to 1.02274144005832
1.20926411399522 to 1.28013885331211
0.989015176388406 to 1.0598899157053
1.31784523973145 to 1.38871997904835
1.2433994556395 to 1.3142741949564
0.75114874491372 to 0.822023484230611
0.521241323518538 to 0.592116062835429
0.792967602106713 to 0.863842341423605
0.786693259569787 to 0.857567998886678
Changing layer 4's weights from 
0.986855819317483 to 1.05773055863437
0.864016338678979 to 0.93489107799587
0.596445939931058 to 0.667320679247949
1.37319566259065 to 1.44407040190754
1.48980857381501 to 1.5606833131319
1.15893466720262 to 1.22980940651951
1.25123019466081 to 1.3221049339777
0.958707406613015 to 1.02958214592991
1.10321767577805 to 1.17409241509495
0.967710449787759 to 1.03858518910465
Changing layer 5's weights from 
1.13138075122514 to 1.20225549054203
0.960258796307229 to 1.03113353562412
1.1481630350081 to 1.21903777432499
1.31140769967713 to 1.38228243899402
0.965977504345559 to 1.03685224366245
0.76626686701455 to 0.837141606331441
1.01213987121262 to 1.08301461052952
1.33844079026856 to 1.40931552958545
0.600016861650132 to 0.670891600967023
1.40026630410829 to 1.47114104342518
Trying to learn from memory 19, 2, 0.4666666
sum 2.16158481387161 distri 0.764941764017181
Using diff 0.856246846386529 and condRate 0.166666666666667
Changed category 2 weights from 
0.767706841513762 to 0.834303814485447
0.406444967076429 to 0.473041940048115
0.202156581267962 to 0.268753554239647
0.930569321200499 to 0.997166294172184
Changing layer 0's weights from 
1.57575327467975 to 1.64235024765144
0.617634243637893 to 0.684231216609579
0.777656409264181 to 0.844253382235866
0.870079013825033 to 0.936675986796718
0.808349940300558 to 0.874946913272243
1.30950272393284 to 1.37609969690452
0.968451026201818 to 1.0350479991735
1.4417058856493 to 1.50830285862098
1.51075047564564 to 1.57734744861732
0.894863578796957 to 0.961460551768642
Changing layer 1's weights from 
1.04887485814152 to 1.1154718311132
1.10188079190311 to 1.1684777648748
1.525996032477 to 1.59259300544868
1.04955101323185 to 1.11614798620353
1.37842792582569 to 1.44502489879737
0.781044694901083 to 0.847641667872768
0.918414685249898 to 0.985011658221584
0.653101633608911 to 0.719698606580596
0.753218013167951 to 0.819814986139636
0.753900605559919 to 0.820497578531604
Changing layer 2's weights from 
0.875320050240133 to 0.941917023211818
0.758189398170087 to 0.824786371141773
0.744398671508405 to 0.81099564448009
0.71492262494621 to 0.781519597917896
1.12438464474735 to 1.19098161771904
0.624465535700891 to 0.691062508672576
1.20058882546482 to 1.2671857984365
1.56198591303882 to 1.62858288601051
1.40817791533527 to 1.47477488830696
0.637507508814905 to 0.70410448178659
Changing layer 3's weights from 
1.1092964441782 to 1.17589341714988
1.02274144005832 to 1.08933841303001
1.28013885331211 to 1.34673582628379
1.0598899157053 to 1.12648688867698
1.38871997904835 to 1.45531695202003
1.3142741949564 to 1.38087116792808
0.822023484230611 to 0.888620457202297
0.592116062835429 to 0.658713035807114
0.863842341423605 to 0.93043931439529
0.857567998886678 to 0.924164971858364
Changing layer 4's weights from 
1.05773055863437 to 1.12432753160606
0.93489107799587 to 1.00148805096756
0.667320679247949 to 0.733917652219634
1.44407040190754 to 1.51066737487922
1.5606833131319 to 1.62728028610359
1.22980940651951 to 1.29640637949119
1.3221049339777 to 1.38870190694938
1.02958214592991 to 1.09617911890159
1.17409241509495 to 1.24068938806663
1.03858518910465 to 1.10518216207634
Changing layer 5's weights from 
1.20225549054203 to 1.26885246351371
1.03113353562412 to 1.09773050859581
1.21903777432499 to 1.28563474729667
1.38228243899402 to 1.44887941196571
1.03685224366245 to 1.10344921663414
0.837141606331441 to 0.903738579303127
1.08301461052952 to 1.1496115835012
1.40931552958545 to 1.47591250255714
0.670891600967023 to 0.737488573938709
1.47114104342518 to 1.53773801639686
Trying to learn from memory 20, 1, 0.4666666
sum 2.19817211530812 distri 0.726760904793663
Using diff 0.921868181687429 and condRate 0.166666666666667
Changed category 1 weights from 
0.31144923071094 to 0.383150085012925
1.06004181603618 to 1.13174267033817
1.03321340779491 to 1.10491426209689
0.11890986754276 to 0.190610721844745
Changing layer 0's weights from 
1.64235024765144 to 1.71405110195342
0.684231216609579 to 0.755932070911563
0.844253382235866 to 0.915954236537851
0.936675986796718 to 1.0083768410987
0.874946913272243 to 0.946647767574228
1.37609969690452 to 1.44780055120651
1.0350479991735 to 1.10674885347549
1.50830285862098 to 1.58000371292297
1.57734744861732 to 1.64904830291931
0.961460551768642 to 1.03316140607063
Changing layer 1's weights from 
1.1154718311132 to 1.18717268541519
1.1684777648748 to 1.24017861917678
1.59259300544868 to 1.66429385975067
1.11614798620353 to 1.18784884050552
1.44502489879737 to 1.51672575309936
0.847641667872768 to 0.919342522174753
0.985011658221584 to 1.05671251252357
0.719698606580596 to 0.791399460882581
0.819814986139636 to 0.891515840441621
0.820497578531604 to 0.892198432833589
Changing layer 2's weights from 
0.941917023211818 to 1.0136178775138
0.824786371141773 to 0.896487225443757
0.81099564448009 to 0.882696498782075
0.781519597917896 to 0.85322045221988
1.19098161771904 to 1.26268247202102
0.691062508672576 to 0.762763362974561
1.2671857984365 to 1.33888665273849
1.62858288601051 to 1.70028374031249
1.47477488830696 to 1.54647574260894
0.70410448178659 to 0.775805336088575
Changing layer 3's weights from 
1.17589341714988 to 1.24759427145187
1.08933841303001 to 1.16103926733199
1.34673582628379 to 1.41843668058578
1.12648688867698 to 1.19818774297897
1.45531695202003 to 1.52701780632202
1.38087116792808 to 1.45257202223007
0.888620457202297 to 0.960321311504281
0.658713035807114 to 0.730413890109099
0.93043931439529 to 1.00214016869727
0.924164971858364 to 0.995865826160348
Changing layer 4's weights from 
1.12432753160606 to 1.19602838590804
1.00148805096756 to 1.07318890526954
0.733917652219634 to 0.805618506521619
1.51066737487922 to 1.58236822918121
1.62728028610359 to 1.69898114040557
1.29640637949119 to 1.36810723379318
1.38870190694938 to 1.46040276125137
1.09617911890159 to 1.16787997320358
1.24068938806663 to 1.31239024236862
1.10518216207634 to 1.17688301637832
Changing layer 5's weights from 
1.26885246351371 to 1.3405533178157
1.09773050859581 to 1.16943136289779
1.28563474729667 to 1.35733560159866
1.44887941196571 to 1.52058026626769
1.10344921663414 to 1.17515007093612
0.903738579303127 to 0.975439433605111
1.1496115835012 to 1.22131243780319
1.47591250255714 to 1.54761335685912
0.737488573938709 to 0.809189428240693
1.53773801639686 to 1.60943887069885
Trying to learn from memory 21, 2, 0.4666666
sum 2.14456404366267 distri 0.758421369177569
Using diff 0.850001663569437 and condRate 0.166666666666667
Changed category 2 weights from 
0.834303814485447 to 0.900415051044755
0.473041940048115 to 0.539153176607423
0.268753554239647 to 0.334864790798955
0.997166294172184 to 1.06327753073149
Changing layer 0's weights from 
1.71405110195342 to 1.78016233851273
0.755932070911563 to 0.822043307470871
0.915954236537851 to 0.982065473097159
1.0083768410987 to 1.07448807765801
0.946647767574228 to 1.01275900413354
1.44780055120651 to 1.51391178776581
1.10674885347549 to 1.1728600900348
1.58000371292297 to 1.64611494948228
1.64904830291931 to 1.71515953947861
1.03316140607063 to 1.09927264262993
Changing layer 1's weights from 
1.18717268541519 to 1.25328392197449
1.24017861917678 to 1.30628985573609
1.66429385975067 to 1.73040509630997
1.18784884050552 to 1.25396007706483
1.51672575309936 to 1.58283698965867
0.919342522174753 to 0.985453758734061
1.05671251252357 to 1.12282374908288
0.791399460882581 to 0.857510697441889
0.891515840441621 to 0.957627077000929
0.892198432833589 to 0.958309669392897
Changing layer 2's weights from 
1.0136178775138 to 1.07972911407311
0.896487225443757 to 0.962598462003065
0.882696498782075 to 0.948807735341383
0.85322045221988 to 0.919331688779188
1.26268247202102 to 1.32879370858033
0.762763362974561 to 0.828874599533869
1.33888665273849 to 1.4049978892978
1.70028374031249 to 1.7663949768718
1.54647574260894 to 1.61258697916825
0.775805336088575 to 0.841916572647883
Changing layer 3's weights from 
1.24759427145187 to 1.31370550801118
1.16103926733199 to 1.2271505038913
1.41843668058578 to 1.48454791714509
1.19818774297897 to 1.26429897953827
1.52701780632202 to 1.59312904288132
1.45257202223007 to 1.51868325878937
0.960321311504281 to 1.02643254806359
0.730413890109099 to 0.796525126668407
1.00214016869727 to 1.06825140525658
0.995865826160348 to 1.06197706271966
Changing layer 4's weights from 
1.19602838590804 to 1.26213962246735
1.07318890526954 to 1.13930014182885
0.805618506521619 to 0.871729743080927
1.58236822918121 to 1.64847946574052
1.69898114040557 to 1.76509237696488
1.36810723379318 to 1.43421847035248
1.46040276125137 to 1.52651399781068
1.16787997320358 to 1.23399120976288
1.31239024236862 to 1.37850147892792
1.17688301637832 to 1.24299425293763
Changing layer 5's weights from 
1.3405533178157 to 1.40666455437501
1.16943136289779 to 1.2355425994571
1.35733560159866 to 1.42344683815797
1.52058026626769 to 1.586691502827
1.17515007093612 to 1.24126130749543
0.975439433605111 to 1.04155067016442
1.22131243780319 to 1.28742367436249
1.54761335685912 to 1.61372459341843
0.809189428240693 to 0.875300664800001
1.60943887069885 to 1.67555010725815
Trying to learn from memory 22, 1, 0.4666666
sum 2.08685374513063 distri 0.695762348087996
Using diff 0.86937796075998 and condRate 0.166666666666667
Changed category 1 weights from 
0.383150085012925 to 0.450768366819448
1.13174267033817 to 1.19936095214469
1.10491426209689 to 1.17253254390342
0.190610721844745 to 0.258229003651268
Changing layer 0's weights from 
1.78016233851273 to 1.84778062031926
0.822043307470871 to 0.889661589277395
0.982065473097159 to 1.04968375490368
1.07448807765801 to 1.14210635946453
1.01275900413354 to 1.08037728594006
1.51391178776581 to 1.58153006957234
1.1728600900348 to 1.24047837184132
1.64611494948228 to 1.7137332312888
1.71515953947861 to 1.78277782128514
1.09927264262993 to 1.16689092443646
Changing layer 1's weights from 
1.25328392197449 to 1.32090220378102
1.30628985573609 to 1.37390813754261
1.73040509630997 to 1.7980233781165
1.25396007706483 to 1.32157835887135
1.58283698965867 to 1.65045527146519
0.985453758734061 to 1.05307204054058
1.12282374908288 to 1.1904420308894
0.857510697441889 to 0.925128979248413
0.957627077000929 to 1.02524535880745
0.958309669392897 to 1.02592795119942
Changing layer 2's weights from 
1.07972911407311 to 1.14734739587963
0.962598462003065 to 1.03021674380959
0.948807735341383 to 1.01642601714791
0.919331688779188 to 0.986949970585712
1.32879370858033 to 1.39641199038685
0.828874599533869 to 0.896492881340393
1.4049978892978 to 1.47261617110432
1.7663949768718 to 1.83401325867833
1.61258697916825 to 1.68020526097477
0.841916572647883 to 0.909534854454406
Changing layer 3's weights from 
1.31370550801118 to 1.3813237898177
1.2271505038913 to 1.29476878569783
1.48454791714509 to 1.55216619895161
1.26429897953827 to 1.3319172613448
1.59312904288132 to 1.66074732468785
1.51868325878937 to 1.5863015405959
1.02643254806359 to 1.09405082987011
0.796525126668407 to 0.86414340847493
1.06825140525658 to 1.13586968706311
1.06197706271966 to 1.12959534452618
Changing layer 4's weights from 
1.26213962246735 to 1.32975790427388
1.13930014182885 to 1.20691842363537
0.871729743080927 to 0.939348024887451
1.64847946574052 to 1.71609774754704
1.76509237696488 to 1.8327106587714
1.43421847035248 to 1.50183675215901
1.52651399781068 to 1.5941322796172
1.23399120976288 to 1.30160949156941
1.37850147892792 to 1.44611976073445
1.24299425293763 to 1.31061253474415
Changing layer 5's weights from 
1.40666455437501 to 1.47428283618153
1.2355425994571 to 1.30316088126362
1.42344683815797 to 1.49106511996449
1.586691502827 to 1.65430978463353
1.24126130749543 to 1.30887958930195
1.04155067016442 to 1.10916895197094
1.28742367436249 to 1.35504195616902
1.61372459341843 to 1.68134287522496
0.875300664800001 to 0.942918946606525
1.67555010725815 to 1.74316838906468
Trying to learn from memory 23, 1, 0.4666666
sum 2.08987159148117 distri 0.696529440116572
Using diff 0.870874253494308 and condRate 0.166666666666667
Changed category 1 weights from 
0.450768366819448 to 0.518503026942816
1.19936095214469 to 1.26709561226806
1.17253254390342 to 1.24026720402679
0.258229003651268 to 0.325963663774636
Changing layer 0's weights from 
1.84778062031926 to 1.91551528044262
0.889661589277395 to 0.957396249400763
1.04968375490368 to 1.11741841502705
1.14210635946453 to 1.2098410195879
1.08037728594006 to 1.14811194606343
1.58153006957234 to 1.64926472969571
1.24047837184132 to 1.30821303196469
1.7137332312888 to 1.78146789141217
1.78277782128514 to 1.8505124814085
1.16689092443646 to 1.23462558455983
Changing layer 1's weights from 
1.32090220378102 to 1.38863686390438
1.37390813754261 to 1.44164279766598
1.7980233781165 to 1.86575803823986
1.32157835887135 to 1.38931301899472
1.65045527146519 to 1.71818993158856
1.05307204054058 to 1.12080670066395
1.1904420308894 to 1.25817669101277
0.925128979248413 to 0.99286363937178
1.02524535880745 to 1.09298001893082
1.02592795119942 to 1.09366261132279
Changing layer 2's weights from 
1.14734739587963 to 1.215082056003
1.03021674380959 to 1.09795140393296
1.01642601714791 to 1.08416067727127
0.986949970585712 to 1.05468463070908
1.39641199038685 to 1.46414665051022
0.896492881340393 to 0.96422754146376
1.47261617110432 to 1.54035083122769
1.83401325867833 to 1.90174791880169
1.68020526097477 to 1.74793992109814
0.909534854454406 to 0.977269514577774
Changing layer 3's weights from 
1.3813237898177 to 1.44905844994107
1.29476878569783 to 1.36250344582119
1.55216619895161 to 1.61990085907498
1.3319172613448 to 1.39965192146817
1.66074732468785 to 1.72848198481121
1.5863015405959 to 1.65403620071927
1.09405082987011 to 1.16178548999348
0.86414340847493 to 0.931878068598298
1.13586968706311 to 1.20360434718647
1.12959534452618 to 1.19733000464955
Changing layer 4's weights from 
1.32975790427388 to 1.39749256439724
1.20691842363537 to 1.27465308375874
0.939348024887451 to 1.00708268501082
1.71609774754704 to 1.78383240767041
1.8327106587714 to 1.90044531889477
1.50183675215901 to 1.56957141228238
1.5941322796172 to 1.66186693974057
1.30160949156941 to 1.36934415169278
1.44611976073445 to 1.51385442085781
1.31061253474415 to 1.37834719486752
Changing layer 5's weights from 
1.47428283618153 to 1.5420174963049
1.30316088126362 to 1.37089554138699
1.49106511996449 to 1.55879978008786
1.65430978463353 to 1.72204444475689
1.30887958930195 to 1.37661424942532
1.10916895197094 to 1.17690361209431
1.35504195616902 to 1.42277661629238
1.68134287522496 to 1.74907753534832
0.942918946606525 to 1.01065360672989
1.74316838906468 to 1.81090304918805
10/5/2016 11:22:06 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 23, 0, 0.4666666
sum 2.08987159148117 distri 0.657071898519613
Using diff 0.910331795091267 and condRate 0.166666666666667
Changed category 0 weights from 
0.829586448108204 to 0.900390027950636
0.743786038791188 to 0.814589618633619
0.900631489192494 to 0.971435069034926
0.74108141842986 to 0.811884998272291
Changing layer 0's weights from 
1.91551528044262 to 1.98631886028505
0.957396249400763 to 1.02819982924319
1.11741841502705 to 1.18822199486948
1.2098410195879 to 1.28064459943033
1.14811194606343 to 1.21891552590586
1.64926472969571 to 1.72006830953814
1.30821303196469 to 1.37901661180712
1.78146789141217 to 1.8522714712546
1.8505124814085 to 1.92131606125094
1.23462558455983 to 1.30542916440226
Changing layer 1's weights from 
1.38863686390438 to 1.45944044374682
1.44164279766598 to 1.51244637750841
1.86575803823986 to 1.9365616180823
1.38931301899472 to 1.46011659883715
1.71818993158856 to 1.78899351143099
1.12080670066395 to 1.19161028050638
1.25817669101277 to 1.3289802708552
0.99286363937178 to 1.06366721921421
1.09298001893082 to 1.16378359877325
1.09366261132279 to 1.16446619116522
Changing layer 2's weights from 
1.215082056003 to 1.28588563584543
1.09795140393296 to 1.16875498377539
1.08416067727127 to 1.15496425711371
1.05468463070908 to 1.12548821055151
1.46414665051022 to 1.53495023035265
0.96422754146376 to 1.03503112130619
1.54035083122769 to 1.61115441107012
1.90174791880169 to 1.97255149864412
1.74793992109814 to 1.81874350094057
0.977269514577774 to 1.04807309442021
Changing layer 3's weights from 
1.44905844994107 to 1.5198620297835
1.36250344582119 to 1.43330702566363
1.61990085907498 to 1.69070443891741
1.39965192146817 to 1.4704555013106
1.72848198481121 to 1.79928556465365
1.65403620071927 to 1.7248397805617
1.16178548999348 to 1.23258906983591
0.931878068598298 to 1.00268164844073
1.20360434718647 to 1.27440792702891
1.19733000464955 to 1.26813358449198
Changing layer 4's weights from 
1.39749256439724 to 1.46829614423967
1.27465308375874 to 1.34545666360117
1.00708268501082 to 1.07788626485325
1.78383240767041 to 1.85463598751284
1.90044531889477 to 1.9712488987372
1.56957141228238 to 1.64037499212481
1.66186693974057 to 1.732670519583
1.36934415169278 to 1.44014773153521
1.51385442085781 to 1.58465800070025
1.37834719486752 to 1.44915077470995
Changing layer 5's weights from 
1.5420174963049 to 1.61282107614733
1.37089554138699 to 1.44169912122942
1.55879978008786 to 1.62960335993029
1.72204444475689 to 1.79284802459932
1.37661424942532 to 1.44741782926775
1.17690361209431 to 1.24770719193674
1.42277661629238 to 1.49358019613482
1.74907753534832 to 1.81988111519076
1.01065360672989 to 1.08145718657232
1.81090304918805 to 1.88170662903048
Trying to learn from memory 23, 0, 0.4666666
sum 2.08987159148117 distri 0.657071898519613
Using diff 0.910331795091267 and condRate 0.166666666666667
Changed category 0 weights from 
0.900390027950636 to 0.971193607793068
0.814589618633619 to 0.885393198476051
0.971435069034926 to 1.04223864887736
0.811884998272291 to 0.882688578114723
Changing layer 0's weights from 
1.98631886028505 to 2.05712244012749
1.02819982924319 to 1.09900340908563
1.18822199486948 to 1.25902557471191
1.28064459943033 to 1.35144817927277
1.21891552590586 to 1.28971910574829
1.72006830953814 to 1.79087188938057
1.37901661180712 to 1.44982019164955
1.8522714712546 to 1.92307505109703
1.92131606125094 to 1.99211964109337
1.30542916440226 to 1.37623274424469
Changing layer 1's weights from 
1.45944044374682 to 1.53024402358925
1.51244637750841 to 1.58324995735084
1.9365616180823 to 2.00736519792473
1.46011659883715 to 1.53092017867958
1.78899351143099 to 1.85979709127342
1.19161028050638 to 1.26241386034881
1.3289802708552 to 1.39978385069763
1.06366721921421 to 1.13447079905664
1.16378359877325 to 1.23458717861568
1.16446619116522 to 1.23526977100765
Changing layer 2's weights from 
1.28588563584543 to 1.35668921568787
1.16875498377539 to 1.23955856361782
1.15496425711371 to 1.22576783695614
1.12548821055151 to 1.19629179039394
1.53495023035265 to 1.60575381019508
1.03503112130619 to 1.10583470114862
1.61115441107012 to 1.68195799091255
1.97255149864412 to 2.04335507848656
1.81874350094057 to 1.889547080783
1.04807309442021 to 1.11887667426264
Changing layer 3's weights from 
1.5198620297835 to 1.59066560962593
1.43330702566363 to 1.50411060550606
1.69070443891741 to 1.76150801875984
1.4704555013106 to 1.54125908115303
1.79928556465365 to 1.87008914449608
1.7248397805617 to 1.79564336040413
1.23258906983591 to 1.30339264967834
1.00268164844073 to 1.07348522828316
1.27440792702891 to 1.34521150687134
1.26813358449198 to 1.33893716433441
Changing layer 4's weights from 
1.46829614423967 to 1.53909972408211
1.34545666360117 to 1.4162602434436
1.07788626485325 to 1.14868984469568
1.85463598751284 to 1.92543956735527
1.9712488987372 to 2.04205247857964
1.64037499212481 to 1.71117857196724
1.732670519583 to 1.80347409942543
1.44014773153521 to 1.51095131137764
1.58465800070025 to 1.65546158054268
1.44915077470995 to 1.51995435455238
Changing layer 5's weights from 
1.61282107614733 to 1.68362465598976
1.44169912122942 to 1.51250270107185
1.62960335993029 to 1.70040693977272
1.79284802459932 to 1.86365160444176
1.44741782926775 to 1.51822140911018
1.24770719193674 to 1.31851077177917
1.49358019613482 to 1.56438377597725
1.81988111519076 to 1.89068469503319
1.08145718657232 to 1.15226076641476
1.88170662903048 to 1.95251020887291
Trying to learn from memory 23, 0, 0.4666666
sum 2.08987159148117 distri 0.657071898519613
Using diff 0.910331795091267 and condRate 0.166666666666667
Changed category 0 weights from 
0.971193607793068 to 1.0419971876355
0.885393198476051 to 0.956196778318483
1.04223864887736 to 1.11304222871979
0.882688578114723 to 0.953492157957154
Changing layer 0's weights from 
2.05712244012749 to 2.12792601996992
1.09900340908563 to 1.16980698892806
1.25902557471191 to 1.32982915455434
1.35144817927277 to 1.4222517591152
1.28971910574829 to 1.36052268559072
1.79087188938057 to 1.861675469223
1.44982019164955 to 1.52062377149198
1.92307505109703 to 1.99387863093946
1.99211964109337 to 2.0629232209358
1.37623274424469 to 1.44703632408712
Changing layer 1's weights from 
1.53024402358925 to 1.60104760343168
1.58324995735084 to 1.65405353719328
2.00736519792473 to 2.07816877776716
1.53092017867958 to 1.60172375852201
1.85979709127342 to 1.93060067111585
1.26241386034881 to 1.33321744019125
1.39978385069763 to 1.47058743054006
1.13447079905664 to 1.20527437889908
1.23458717861568 to 1.30539075845812
1.23526977100765 to 1.30607335085008
Changing layer 2's weights from 
1.35668921568787 to 1.4274927955303
1.23955856361782 to 1.31036214346025
1.22576783695614 to 1.29657141679857
1.19629179039394 to 1.26709537023637
1.60575381019508 to 1.67655739003751
1.10583470114862 to 1.17663828099106
1.68195799091255 to 1.75276157075498
2.04335507848656 to 2.11415865832899
1.889547080783 to 1.96035066062544
1.11887667426264 to 1.18968025410507
Changing layer 3's weights from 
1.59066560962593 to 1.66146918946836
1.50411060550606 to 1.57491418534849
1.76150801875984 to 1.83231159860227
1.54125908115303 to 1.61206266099546
1.87008914449608 to 1.94089272433851
1.79564336040413 to 1.86644694024656
1.30339264967834 to 1.37419622952078
1.07348522828316 to 1.14428880812559
1.34521150687134 to 1.41601508671377
1.33893716433441 to 1.40974074417684
Changing layer 4's weights from 
1.53909972408211 to 1.60990330392454
1.4162602434436 to 1.48706382328603
1.14868984469568 to 1.21949342453811
1.92543956735527 to 1.9962431471977
2.04205247857964 to 2.11285605842207
1.71117857196724 to 1.78198215180967
1.80347409942543 to 1.87427767926786
1.51095131137764 to 1.58175489122007
1.65546158054268 to 1.72626516038511
1.51995435455238 to 1.59075793439481
Changing layer 5's weights from 
1.68362465598976 to 1.75442823583219
1.51250270107185 to 1.58330628091428
1.70040693977272 to 1.77121051961515
1.86365160444176 to 1.93445518428419
1.51822140911018 to 1.58902498895261
1.31851077177917 to 1.38931435162161
1.56438377597725 to 1.63518735581968
1.89068469503319 to 1.96148827487562
1.15226076641476 to 1.22306434625719
1.95251020887291 to 2.02331378871534
Trying to learn from memory 23, 1, 0.4666666
sum 2.08987159148117 distri 0.696529440116572
Using diff 0.870874253494308 and condRate 0.166666666666667
Changed category 1 weights from 
0.518503026942816 to 0.586237687066184
1.26709561226806 to 1.33483027239143
1.24026720402679 to 1.30800186415015
0.325963663774636 to 0.393698323898004
Changing layer 0's weights from 
2.12792601996992 to 2.19566068009329
1.16980698892806 to 1.23754164905143
1.32982915455434 to 1.39756381467771
1.4222517591152 to 1.48998641923856
1.36052268559072 to 1.42825734571409
1.861675469223 to 1.92941012934637
1.52062377149198 to 1.58835843161535
1.99387863093946 to 2.06161329106283
2.0629232209358 to 2.13065788105917
1.44703632408712 to 1.51477098421049
Changing layer 1's weights from 
1.60104760343168 to 1.66878226355505
1.65405353719328 to 1.72178819731664
2.07816877776716 to 2.14590343789053
1.60172375852201 to 1.66945841864538
1.93060067111585 to 1.99833533123922
1.33321744019125 to 1.40095210031461
1.47058743054006 to 1.53832209066343
1.20527437889908 to 1.27300903902244
1.30539075845812 to 1.37312541858148
1.30607335085008 to 1.37380801097345
Changing layer 2's weights from 
1.4274927955303 to 1.49522745565366
1.31036214346025 to 1.37809680358362
1.29657141679857 to 1.36430607692194
1.26709537023637 to 1.33483003035974
1.67655739003751 to 1.74429205016088
1.17663828099106 to 1.24437294111442
1.75276157075498 to 1.82049623087835
2.11415865832899 to 2.18189331845236
1.96035066062544 to 2.0280853207488
1.18968025410507 to 1.25741491422844
Changing layer 3's weights from 
1.66146918946836 to 1.72920384959173
1.57491418534849 to 1.64264884547186
1.83231159860227 to 1.90004625872564
1.61206266099546 to 1.67979732111883
1.94089272433851 to 2.00862738446188
1.86644694024656 to 1.93418160036993
1.37419622952078 to 1.44193088964414
1.14428880812559 to 1.21202346824896
1.41601508671377 to 1.48374974683714
1.40974074417684 to 1.47747540430021
Changing layer 4's weights from 
1.60990330392454 to 1.67763796404791
1.48706382328603 to 1.5547984834094
1.21949342453811 to 1.28722808466148
1.9962431471977 to 2.06397780732107
2.11285605842207 to 2.18059071854543
1.78198215180967 to 1.84971681193304
1.87427767926786 to 1.94201233939123
1.58175489122007 to 1.64948955134344
1.72626516038511 to 1.79399982050848
1.59075793439481 to 1.65849259451818
Changing layer 5's weights from 
1.75442823583219 to 1.82216289595556
1.58330628091428 to 1.65104094103765
1.77121051961515 to 1.83894517973852
1.93445518428419 to 2.00218984440756
1.58902498895261 to 1.65675964907598
1.38931435162161 to 1.45704901174497
1.63518735581968 to 1.70292201594305
1.96148827487562 to 2.02922293499899
1.22306434625719 to 1.29079900638056
2.02331378871534 to 2.09104844883871
10/5/2016 11:22:06 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:07 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:08 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:09 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:10 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:11 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:12 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:13 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:18 PMStarting learning phase with deltaScore: 0.8
Modified index 0's learning in memoryPool to 0.16
Modified index 1's learning in memoryPool to 0.16
Modified index 2's learning in memoryPool to 0.16
Modified index 3's learning in memoryPool to 0.16
Modified index 4's learning in memoryPool to 0.16
Modified index 5's learning in memoryPool to 0.16
Modified index 6's learning in memoryPool to 0.16
Modified index 7's learning in memoryPool to 0.16
Modified index 8's learning in memoryPool to 0.16
Modified index 9's learning in memoryPool to 0.16
10/5/2016 11:22:18 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 24, 0, 0.16
sum 223.637005043123 distri 86.6191646414056
Using diff 81.1085891409363 and condRate 0.166666666666667
Changed category 0 weights from 
1.0419971876355 to 3.20489284971598
0.956196778318483 to 3.11909244039896
1.11304222871979 to 3.27593789080027
0.953492157957154 to 3.11638782003764
Changing layer 0's weights from 
2.19566068009329 to 4.35855634217377
1.23754164905143 to 3.40043731113191
1.39756381467771 to 3.56045947675819
1.48998641923856 to 3.65288208131905
1.42825734571409 to 3.59115300779457
1.92941012934637 to 4.09230579142685
1.58835843161535 to 3.75125409369583
2.06161329106283 to 4.22450895314331
2.13065788105917 to 4.29355354313965
1.51477098421049 to 3.67766664629097
Changing layer 1's weights from 
1.66878226355505 to 3.83167792563553
1.72178819731664 to 3.88468385939712
2.14590343789053 to 4.30879909997101
1.66945841864538 to 3.83235408072586
1.99833533123922 to 4.1612309933197
1.40095210031461 to 3.5638477623951
1.53832209066343 to 3.70121775274391
1.27300903902244 to 3.43590470110292
1.37312541858148 to 3.53602108066196
1.37380801097345 to 3.53670367305393
Changing layer 2's weights from 
1.49522745565366 to 3.65812311773415
1.37809680358362 to 3.5409924656641
1.36430607692194 to 3.52720173900242
1.33483003035974 to 3.49772569244022
1.74429205016088 to 3.90718771224136
1.24437294111442 to 3.4072686031949
1.82049623087835 to 3.98339189295883
2.18189331845236 to 4.34478898053284
2.0280853207488 to 4.19098098282928
1.25741491422844 to 3.42031057630892
Changing layer 3's weights from 
1.72920384959173 to 3.89209951167221
1.64264884547186 to 3.80554450755234
1.90004625872564 to 4.06294192080612
1.67979732111883 to 3.84269298319931
2.00862738446188 to 4.17152304654236
1.93418160036993 to 4.09707726245041
1.44193088964414 to 3.60482655172462
1.21202346824896 to 3.37491913032944
1.48374974683714 to 3.64664540891762
1.47747540430021 to 3.64037106638069
Changing layer 4's weights from 
1.67763796404791 to 3.84053362612839
1.5547984834094 to 3.71769414548988
1.28722808466148 to 3.45012374674196
2.06397780732107 to 4.22687346940155
2.18059071854543 to 4.34348638062592
1.84971681193304 to 4.01261247401352
1.94201233939123 to 4.10490800147171
1.64948955134344 to 3.81238521342392
1.79399982050848 to 3.95689548258896
1.65849259451818 to 3.82138825659866
Changing layer 5's weights from 
1.82216289595556 to 3.98505855803604
1.65104094103765 to 3.81393660311813
1.83894517973852 to 4.001840841819
2.00218984440756 to 4.16508550648804
1.65675964907598 to 3.81965531115646
1.45704901174497 to 3.61994467382545
1.70292201594305 to 3.86581767802353
2.02922293499899 to 4.19211859707947
1.29079900638056 to 3.45369466846104
2.09104844883871 to 4.25394411091919
Trying to learn from memory 25, 1, 0.16
sum 223.667341627222 distri 74.9252190659195
Using diff 92.8252871544971 and condRate 0.166666666666667
Changed category 1 weights from 
0.586237687066184 to 3.06157862252459
1.33483027239143 to 3.81017120784983
1.30800186415015 to 3.78334279960856
0.393698323898004 to 2.86903925935641
Changing layer 0's weights from 
4.35855634217377 to 6.83389727763217
3.40043731113191 to 5.87577824659031
3.56045947675819 to 6.0358004122166
3.65288208131905 to 6.12822301677745
3.59115300779457 to 6.06649394325298
4.09230579142685 to 6.56764672688526
3.75125409369583 to 6.22659502915424
4.22450895314331 to 6.69984988860172
4.29355354313965 to 6.76889447859805
3.67766664629097 to 6.15300758174938
Changing layer 1's weights from 
3.83167792563553 to 6.30701886109394
3.88468385939712 to 6.36002479485553
4.30879909997101 to 6.78414003542941
3.83235408072586 to 6.30769501618427
4.1612309933197 to 6.63657192877811
3.5638477623951 to 6.0391886978535
3.70121775274391 to 6.17655868820232
3.43590470110292 to 5.91124563656133
3.53602108066196 to 6.01136201612037
3.53670367305393 to 6.01204460851234
Changing layer 2's weights from 
3.65812311773415 to 6.13346405319255
3.5409924656641 to 6.01633340112251
3.52720173900242 to 6.00254267446083
3.49772569244022 to 5.97306662789863
3.90718771224136 to 6.38252864769977
3.4072686031949 to 5.88260953865331
3.98339189295883 to 6.45873282841724
4.34478898053284 to 6.82012991599124
4.19098098282928 to 6.66632191828769
3.42031057630892 to 5.89565151176732
Changing layer 3's weights from 
3.89209951167221 to 6.36744044713062
3.80554450755234 to 6.28088544301074
4.06294192080612 to 6.53828285626453
3.84269298319931 to 6.31803391865772
4.17152304654236 to 6.64686398200076
4.09707726245041 to 6.57241819790882
3.60482655172462 to 6.08016748718303
3.37491913032944 to 5.85026006578785
3.64664540891762 to 6.12198634437602
3.64037106638069 to 6.1157120018391
Changing layer 4's weights from 
3.84053362612839 to 6.31587456158679
3.71769414548988 to 6.19303508094829
3.45012374674196 to 5.92546468220037
4.22687346940155 to 6.70221440485996
4.34348638062592 to 6.81882731608432
4.01261247401352 to 6.48795340947193
4.10490800147171 to 6.58024893693012
3.81238521342392 to 6.28772614888233
3.95689548258896 to 6.43223641804737
3.82138825659866 to 6.29672919205707
Changing layer 5's weights from 
3.98505855803604 to 6.46039949349445
3.81393660311813 to 6.28927753857654
4.001840841819 to 6.47718177727741
4.16508550648804 to 6.64042644194644
3.81965531115646 to 6.29499624661487
3.61994467382545 to 6.09528560928386
3.86581767802353 to 6.34115861348194
4.19211859707947 to 6.66745953253787
3.45369466846104 to 5.92903560391944
4.25394411091919 to 6.7292850463776
Trying to learn from memory 26, 2, 0.16
sum 223.904951256852 distri 62.1768252346256
Using diff 105.751888208013 and condRate 0.166666666666667
Changed category 2 weights from 
0.900415051044755 to 3.7204653402254
0.539153176607423 to 3.35920346578807
0.334864790798955 to 3.1549150799796
1.06327753073149 to 3.88332781991214
Changing layer 0's weights from 
6.83389727763217 to 9.65394756681282
5.87577824659031 to 8.69582853577096
6.0358004122166 to 8.85585070139724
6.12822301677745 to 8.9482733059581
6.06649394325298 to 8.88654423243362
6.56764672688526 to 9.3876970160659
6.22659502915424 to 9.04664531833488
6.69984988860172 to 9.51990017778236
6.76889447859805 to 9.5889447677787
6.15300758174938 to 8.97305787093002
Changing layer 1's weights from 
6.30701886109394 to 9.12706915027458
6.36002479485553 to 9.18007508403618
6.78414003542941 to 9.60419032461006
6.30769501618427 to 9.12774530536491
6.63657192877811 to 9.45662221795875
6.0391886978535 to 8.85923898703415
6.17655868820232 to 8.99660897738296
5.91124563656133 to 8.73129592574197
6.01136201612037 to 8.83141230530102
6.01204460851234 to 8.83209489769298
Changing layer 2's weights from 
6.13346405319255 to 8.9535143423732
6.01633340112251 to 8.83638369030315
6.00254267446083 to 8.82259296364147
5.97306662789863 to 8.79311691707927
6.38252864769977 to 9.20257893688041
5.88260953865331 to 8.70265982783395
6.45873282841724 to 9.27878311759788
6.82012991599124 to 9.64018020517189
6.66632191828769 to 9.48637220746834
5.89565151176732 to 8.71570180094797
Changing layer 3's weights from 
6.36744044713062 to 9.18749073631126
6.28088544301074 to 9.10093573219139
6.53828285626453 to 9.35833314544517
6.31803391865772 to 9.13808420783836
6.64686398200076 to 9.46691427118141
6.57241819790882 to 9.39246848708946
6.08016748718303 to 8.90021777636368
5.85026006578785 to 8.67031035496849
6.12198634437602 to 8.94203663355667
6.1157120018391 to 8.93576229101974
Changing layer 4's weights from 
6.31587456158679 to 9.13592485076744
6.19303508094829 to 9.01308537012893
5.92546468220037 to 8.74551497138101
6.70221440485996 to 9.5222646940406
6.81882731608432 to 9.63887760526497
6.48795340947193 to 9.30800369865257
6.58024893693012 to 9.40029922611076
6.28772614888233 to 9.10777643806297
6.43223641804737 to 9.25228670722801
6.29672919205707 to 9.11677948123771
Changing layer 5's weights from 
6.46039949349445 to 9.28044978267509
6.28927753857654 to 9.10932782775718
6.47718177727741 to 9.29723206645805
6.64042644194644 to 9.46047673112709
6.29499624661487 to 9.11504653579551
6.09528560928386 to 8.91533589846451
6.34115861348194 to 9.16120890266258
6.66745953253787 to 9.48750982171852
5.92903560391944 to 8.74908589310009
6.7292850463776 to 9.54933533555824
Trying to learn from memory 27, 1, 0.16
sum 224.9151881396 distri 75.3671690998647
Using diff 93.3192220048354 and condRate 0.166666666666667
Changed category 1 weights from 
3.06157862252459 to 5.55009115369761
3.81017120784983 to 6.29868373902285
3.78334279960856 to 6.27185533078158
2.86903925935641 to 5.35755179052943
Changing layer 0's weights from 
9.65394756681282 to 12.1424600979858
8.69582853577096 to 11.184341066944
8.85585070139724 to 11.3443632325703
8.9482733059581 to 11.4367858371311
8.88654423243362 to 11.3750567636066
9.3876970160659 to 11.8762095472389
9.04664531833488 to 11.5351578495079
9.51990017778236 to 12.0084127089554
9.5889447677787 to 12.0774572989517
8.97305787093002 to 11.461570402103
Changing layer 1's weights from 
9.12706915027458 to 11.6155816814476
9.18007508403618 to 11.6685876152092
9.60419032461006 to 12.0927028557831
9.12774530536491 to 11.6162578365379
9.45662221795875 to 11.9451347491318
8.85923898703415 to 11.3477515182072
8.99660897738296 to 11.485121508556
8.73129592574197 to 11.219808456915
8.83141230530102 to 11.319924836474
8.83209489769298 to 11.320607428866
Changing layer 2's weights from 
8.9535143423732 to 11.4420268735462
8.83638369030315 to 11.3248962214762
8.82259296364147 to 11.3111054948145
8.79311691707927 to 11.2816294482523
9.20257893688041 to 11.6910914680534
8.70265982783395 to 11.191172359007
9.27878311759788 to 11.7672956487709
9.64018020517189 to 12.1286927363449
9.48637220746834 to 11.9748847386414
8.71570180094797 to 11.204214332121
Changing layer 3's weights from 
9.18749073631126 to 11.6760032674843
9.10093573219139 to 11.5894482633644
9.35833314544517 to 11.8468456766182
9.13808420783836 to 11.6265967390114
9.46691427118141 to 11.9554268023544
9.39246848708946 to 11.8809810182625
8.90021777636368 to 11.3887303075367
8.67031035496849 to 11.1588228861415
8.94203663355667 to 11.4305491647297
8.93576229101974 to 11.4242748221928
Changing layer 4's weights from 
9.13592485076744 to 11.6244373819405
9.01308537012893 to 11.501597901302
8.74551497138101 to 11.234027502554
9.5222646940406 to 12.0107772252136
9.63887760526497 to 12.127390136438
9.30800369865257 to 11.7965162298256
9.40029922611076 to 11.8888117572838
9.10777643806297 to 11.596288969236
9.25228670722801 to 11.740799238401
9.11677948123771 to 11.6052920124107
Changing layer 5's weights from 
9.28044978267509 to 11.7689623138481
9.10932782775718 to 11.5978403589302
9.29723206645805 to 11.7857445976311
9.46047673112709 to 11.9489892623001
9.11504653579551 to 11.6035590669685
8.91533589846451 to 11.4038484296375
9.16120890266258 to 11.6497214338356
9.48750982171852 to 11.9760223528915
8.74908589310009 to 11.2375984242731
9.54933533555824 to 12.0378478667313
Trying to learn from memory 28, 0, 0.16
sum 226.402298865051 distri 87.6820912846919
Using diff 82.1196328640967 and condRate 0.166666666666667
Changed category 0 weights from 
3.20489284971598 to 5.39474967714478
3.11909244039896 to 5.30894926782776
3.27593789080027 to 5.46579471822907
3.11638782003764 to 5.30624464746643
Changing layer 0's weights from 
12.1424600979858 to 14.3323169254146
11.184341066944 to 13.3741978943728
11.3443632325703 to 13.5342200599991
11.4367858371311 to 13.6266426645599
11.3750567636066 to 13.5649135910354
11.8762095472389 to 14.0660663746677
11.5351578495079 to 13.7250146769367
12.0084127089554 to 14.1982695363842
12.0774572989517 to 14.2673141263805
11.461570402103 to 13.6514272295318
Changing layer 1's weights from 
11.6155816814476 to 13.8054385088764
11.6685876152092 to 13.858444442638
12.0927028557831 to 14.2825596832119
11.6162578365379 to 13.8061146639667
11.9451347491318 to 14.1349915765606
11.3477515182072 to 13.537608345636
11.485121508556 to 13.6749783359848
11.219808456915 to 13.4096652843438
11.319924836474 to 13.5097816639028
11.320607428866 to 13.5104642562948
Changing layer 2's weights from 
11.4420268735462 to 13.631883700975
11.3248962214762 to 13.514753048905
11.3111054948145 to 13.5009623222433
11.2816294482523 to 13.4714862756811
11.6910914680534 to 13.8809482954822
11.191172359007 to 13.3810291864358
11.7672956487709 to 13.9571524761997
12.1286927363449 to 14.3185495637737
11.9748847386414 to 14.1647415660702
11.204214332121 to 13.3940711595498
Changing layer 3's weights from 
11.6760032674843 to 13.8658600949131
11.5894482633644 to 13.7793050907932
11.8468456766182 to 14.036702504047
11.6265967390114 to 13.8164535664402
11.9554268023544 to 14.1452836297832
11.8809810182625 to 14.0708378456913
11.3887303075367 to 13.5785871349655
11.1588228861415 to 13.3486797135703
11.4305491647297 to 13.6204059921585
11.4242748221928 to 13.6141316496216
Changing layer 4's weights from 
11.6244373819405 to 13.8142942093693
11.501597901302 to 13.6914547287308
11.234027502554 to 13.4238843299828
12.0107772252136 to 14.2006340526424
12.127390136438 to 14.3172469638668
11.7965162298256 to 13.9863730572544
11.8888117572838 to 14.0786685847126
11.596288969236 to 13.7861457966648
11.740799238401 to 13.9306560658298
11.6052920124107 to 13.7951488398395
Changing layer 5's weights from 
11.7689623138481 to 13.9588191412769
11.5978403589302 to 13.787697186359
11.7857445976311 to 13.9756014250599
11.9489892623001 to 14.1388460897289
11.6035590669685 to 13.7934158943973
11.4038484296375 to 13.5937052570663
11.6497214338356 to 13.8395782612644
11.9760223528915 to 14.1658791803203
11.2375984242731 to 13.4274552517019
12.0378478667313 to 14.2277046941601
Trying to learn from memory 29, 1, 0.16
sum 224.383149204245 distri 75.1631246885155
Using diff 93.1242372146682 and condRate 0.166666666666667
Changed category 1 weights from 
5.55009115369761 to 8.03340409058239
6.29868373902285 to 8.78199667590763
6.27185533078158 to 8.75516826766636
5.35755179052943 to 7.84086472741421
Changing layer 0's weights from 
14.3323169254146 to 16.8156298622994
13.3741978943728 to 15.8575108312576
13.5342200599991 to 16.0175329968838
13.6266426645599 to 16.1099556014447
13.5649135910354 to 16.0482265279202
14.0660663746677 to 16.5493793115525
13.7250146769367 to 16.2083276138215
14.1982695363842 to 16.681582473269
14.2673141263805 to 16.7506270632653
13.6514272295318 to 16.1347401664166
Changing layer 1's weights from 
13.8054385088764 to 16.2887514457612
13.858444442638 to 16.3417573795228
14.2825596832119 to 16.7658726200967
13.8061146639667 to 16.2894276008515
14.1349915765606 to 16.6183045134453
13.537608345636 to 16.0209212825207
13.6749783359848 to 16.1582912728696
13.4096652843438 to 15.8929782212286
13.5097816639028 to 15.9930946007876
13.5104642562948 to 15.9937771931796
Changing layer 2's weights from 
13.631883700975 to 16.1151966378598
13.514753048905 to 15.9980659857897
13.5009623222433 to 15.9842752591281
13.4714862756811 to 15.9547992125659
13.8809482954822 to 16.364261232367
13.3810291864358 to 15.8643421233206
13.9571524761997 to 16.4404654130845
14.3185495637737 to 16.8018625006585
14.1647415660702 to 16.6480545029549
13.3940711595498 to 15.8773840964346
Changing layer 3's weights from 
13.8658600949131 to 16.3491730317979
13.7793050907932 to 16.262618027678
14.036702504047 to 16.5200154409318
13.8164535664402 to 16.299766503325
14.1452836297832 to 16.628596566668
14.0708378456913 to 16.5541507825761
13.5785871349655 to 16.0619000718503
13.3486797135703 to 15.8319926504551
13.6204059921585 to 16.1037189290433
13.6141316496216 to 16.0974445865063
Changing layer 4's weights from 
13.8142942093693 to 16.297607146254
13.6914547287308 to 16.1747676656155
13.4238843299828 to 15.9071972668676
14.2006340526424 to 16.6839469895272
14.3172469638668 to 16.8005599007516
13.9863730572544 to 16.4696859941392
14.0786685847126 to 16.5619815215974
13.7861457966648 to 16.2694587335496
13.9306560658298 to 16.4139690027146
13.7951488398395 to 16.2784617767243
Changing layer 5's weights from 
13.9588191412769 to 16.4421320781617
13.787697186359 to 16.2710101232438
13.9756014250599 to 16.4589143619446
14.1388460897289 to 16.6221590266137
13.7934158943973 to 16.2767288312821
13.5937052570663 to 16.0770181939511
13.8395782612644 to 16.3228911981492
14.1658791803203 to 16.6491921172051
13.4274552517019 to 15.9107681885867
14.2277046941601 to 16.7110176310448
Trying to learn from memory 29, 0, 0.16
sum 224.383149204245 distri 86.9084228182232
Using diff 81.3789390849604 and condRate 0.166666666666667
Changed category 0 weights from 
5.39474967714478 to 7.56485467090476
5.30894926782776 to 7.47905426158774
5.46579471822907 to 7.63589971198905
5.30624464746643 to 7.47634964122641
Changing layer 0's weights from 
16.8156298622994 to 18.9857348560594
15.8575108312576 to 18.0276158250175
16.0175329968838 to 18.1876379906438
16.1099556014447 to 18.2800605952047
16.0482265279202 to 18.2183315216802
16.5493793115525 to 18.7194843053125
16.2083276138215 to 18.3784326075815
16.681582473269 to 18.8516874670289
16.7506270632653 to 18.9207320570253
16.1347401664166 to 18.3048451601766
Changing layer 1's weights from 
16.2887514457612 to 18.4588564395212
16.3417573795228 to 18.5118623732828
16.7658726200967 to 18.9359776138566
16.2894276008515 to 18.4595325946115
16.6183045134453 to 18.7884095072053
16.0209212825207 to 18.1910262762807
16.1582912728696 to 18.3283962666295
15.8929782212286 to 18.0630832149886
15.9930946007876 to 18.1631995945476
15.9937771931796 to 18.1638821869396
Changing layer 2's weights from 
16.1151966378598 to 18.2853016316198
15.9980659857897 to 18.1681709795497
15.9842752591281 to 18.1543802528881
15.9547992125659 to 18.1249042063259
16.364261232367 to 18.534366226127
15.8643421233206 to 18.0344471170805
16.4404654130845 to 18.6105704068445
16.8018625006585 to 18.9719674944185
16.6480545029549 to 18.8181594967149
15.8773840964346 to 18.0474890901945
Changing layer 3's weights from 
16.3491730317979 to 18.5192780255578
16.262618027678 to 18.432723021438
16.5200154409318 to 18.6901204346918
16.299766503325 to 18.4698714970849
16.628596566668 to 18.798701560428
16.5541507825761 to 18.724255776336
16.0619000718503 to 18.2320050656103
15.8319926504551 to 18.0020976442151
16.1037189290433 to 18.2738239228032
16.0974445865063 to 18.2675495802663
Changing layer 4's weights from 
16.297607146254 to 18.467712140014
16.1747676656155 to 18.3448726593755
15.9071972668676 to 18.0773022606276
16.6839469895272 to 18.8540519832872
16.8005599007516 to 18.9706648945115
16.4696859941392 to 18.6397909878991
16.5619815215974 to 18.7320865153573
16.2694587335496 to 18.4395637273095
16.4139690027146 to 18.5840739964746
16.2784617767243 to 18.4485667704843
Changing layer 5's weights from 
16.4421320781617 to 18.6122370719217
16.2710101232438 to 18.4411151170038
16.4589143619446 to 18.6290193557046
16.6221590266137 to 18.7922640203737
16.2767288312821 to 18.4468338250421
16.0770181939511 to 18.2471231877111
16.3228911981492 to 18.4929961919092
16.6491921172051 to 18.8192971109651
15.9107681885867 to 18.0808731823467
16.7110176310448 to 18.8811226248048
Trying to learn from memory 29, 0, 0.16
sum 224.383149204245 distri 86.9084228182232
Using diff 81.3789390849604 and condRate 0.166666666666667
Changed category 0 weights from 
7.56485467090476 to 9.73495966466474
7.47905426158774 to 9.64915925534773
7.63589971198905 to 9.80600470574903
7.47634964122641 to 9.6464546349864
Changing layer 0's weights from 
18.9857348560594 to 21.1558398498194
18.0276158250175 to 20.1977208187775
18.1876379906438 to 20.3577429844038
18.2800605952047 to 20.4501655889647
18.2183315216802 to 20.3884365154402
18.7194843053125 to 20.8895892990725
18.3784326075815 to 20.5485376013414
18.8516874670289 to 21.0217924607889
18.9207320570253 to 21.0908370507853
18.3048451601766 to 20.4749501539366
Changing layer 1's weights from 
18.4588564395212 to 20.6289614332811
18.5118623732828 to 20.6819673670427
18.9359776138566 to 21.1060826076166
18.4595325946115 to 20.6296375883715
18.7884095072053 to 20.9585145009653
18.1910262762807 to 20.3611312700407
18.3283962666295 to 20.4985012603895
18.0630832149886 to 20.2331882087485
18.1631995945476 to 20.3333045883076
18.1638821869396 to 20.3339871806995
Changing layer 2's weights from 
18.2853016316198 to 20.4554066253798
18.1681709795497 to 20.3382759733097
18.1543802528881 to 20.324485246648
18.1249042063259 to 20.2950092000858
18.534366226127 to 20.704471219887
18.0344471170805 to 20.2045521108405
18.6105704068445 to 20.7806754006044
18.9719674944185 to 21.1420724881784
18.8181594967149 to 20.9882644904749
18.0474890901945 to 20.2175940839545
Changing layer 3's weights from 
18.5192780255578 to 20.6893830193178
18.432723021438 to 20.602828015198
18.6901204346918 to 20.8602254284517
18.4698714970849 to 20.6399764908449
18.798701560428 to 20.968806554188
18.724255776336 to 20.894360770096
18.2320050656103 to 20.4021100593702
18.0020976442151 to 20.1722026379751
18.2738239228032 to 20.4439289165632
18.2675495802663 to 20.4376545740263
Changing layer 4's weights from 
18.467712140014 to 20.637817133774
18.3448726593755 to 20.5149776531355
18.0773022606276 to 20.2474072543876
18.8540519832872 to 21.0241569770472
18.9706648945115 to 21.1407698882715
18.6397909878991 to 20.8098959816591
18.7320865153573 to 20.9021915091173
18.4395637273095 to 20.6096687210695
18.5840739964746 to 20.7541789902346
18.4485667704843 to 20.6186717642443
Changing layer 5's weights from 
18.6122370719217 to 20.7823420656817
18.4411151170038 to 20.6112201107637
18.6290193557046 to 20.7991243494646
18.7922640203737 to 20.9623690141337
18.4468338250421 to 20.6169388188021
18.2471231877111 to 20.4172281814711
18.4929961919092 to 20.6631011856691
18.8192971109651 to 20.9894021047251
18.0808731823467 to 20.2509781761066
18.8811226248048 to 21.0512276185648
Trying to learn from memory 29, 1, 0.16
sum 224.383149204245 distri 75.1631246885155
Using diff 93.1242372146682 and condRate 0.166666666666667
Changed category 1 weights from 
8.03340409058239 to 10.5167170274672
8.78199667590763 to 11.2653096127924
8.75516826766636 to 11.2384812045511
7.84086472741421 to 10.324177664299
Changing layer 0's weights from 
21.1558398498194 to 23.6391527867042
20.1977208187775 to 22.6810337556623
20.3577429844038 to 22.8410559212886
20.4501655889647 to 22.9334785258494
20.3884365154402 to 22.871749452325
20.8895892990725 to 23.3729022359572
20.5485376013414 to 23.0318505382262
21.0217924607889 to 23.5051053976737
21.0908370507853 to 23.57414998767
20.4749501539366 to 22.9582630908214
Changing layer 1's weights from 
20.6289614332811 to 23.1122743701659
20.6819673670427 to 23.1652803039275
21.1060826076166 to 23.5893955445014
20.6296375883715 to 23.1129505252563
20.9585145009653 to 23.4418274378501
20.3611312700407 to 22.8444442069255
20.4985012603895 to 22.9818141972743
20.2331882087485 to 22.7165011456333
20.3333045883076 to 22.8166175251924
20.3339871806995 to 22.8173001175843
Changing layer 2's weights from 
20.4554066253798 to 22.9387195622645
20.3382759733097 to 22.8215889101945
20.324485246648 to 22.8077981835328
20.2950092000858 to 22.7783221369706
20.704471219887 to 23.1877841567718
20.2045521108405 to 22.6878650477253
20.7806754006044 to 23.2639883374892
21.1420724881784 to 23.6253854250632
20.9882644904749 to 23.4715774273597
20.2175940839545 to 22.7009070208393
Changing layer 3's weights from 
20.6893830193178 to 23.1726959562026
20.602828015198 to 23.0861409520827
20.8602254284517 to 23.3435383653365
20.6399764908449 to 23.1232894277297
20.968806554188 to 23.4521194910728
20.894360770096 to 23.3776737069808
20.4021100593702 to 22.885422996255
20.1722026379751 to 22.6555155748598
20.4439289165632 to 22.927241853448
20.4376545740263 to 22.9209675109111
Changing layer 4's weights from 
20.637817133774 to 23.1211300706588
20.5149776531355 to 22.9982905900203
20.2474072543876 to 22.7307201912724
21.0241569770472 to 23.5074699139319
21.1407698882715 to 23.6240828251563
20.8098959816591 to 23.2932089185439
20.9021915091173 to 23.3855044460021
20.6096687210695 to 23.0929816579543
20.7541789902346 to 23.2374919271194
20.6186717642443 to 23.1019847011291
Changing layer 5's weights from 
20.7823420656817 to 23.2656550025664
20.6112201107637 to 23.0945330476485
20.7991243494646 to 23.2824372863494
20.9623690141337 to 23.4456819510184
20.6169388188021 to 23.1002517556869
20.4172281814711 to 22.9005411183558
20.6631011856691 to 23.1464141225539
20.9894021047251 to 23.4727150416099
20.2509781761066 to 22.7342911129914
21.0512276185648 to 23.5345405554496
Trying to learn from memory 30, 2, 0.16
sum 225.627716264372 distri 62.6314640791011
Using diff 106.589323119178 and condRate 0.166666666666667
Changed category 2 weights from 
3.7204653402254 to 6.56284722653795
3.35920346578807 to 6.20158535210061
3.1549150799796 to 5.99729696629215
3.88332781991214 to 6.72570970622468
Changing layer 0's weights from 
23.6391527867042 to 26.4815346730167
22.6810337556623 to 25.5234156419748
22.8410559212886 to 25.6834378076011
22.9334785258494 to 25.775860412162
22.871749452325 to 25.7141313386375
23.3729022359572 to 26.2152841222698
23.0318505382262 to 25.8742324245388
23.5051053976737 to 26.3474872839863
23.57414998767 to 26.4165318739826
22.9582630908214 to 25.8006449771339
Changing layer 1's weights from 
23.1122743701659 to 25.9546562564785
23.1652803039275 to 26.0076621902401
23.5893955445014 to 26.4317774308139
23.1129505252563 to 25.9553324115688
23.4418274378501 to 26.2842093241626
22.8444442069255 to 25.686826093238
22.9818141972743 to 25.8241960835869
22.7165011456333 to 25.5588830319459
22.8166175251924 to 25.6589994115049
22.8173001175843 to 25.6596820038969
Changing layer 2's weights from 
22.9387195622645 to 25.7811014485771
22.8215889101945 to 25.663970796507
22.8077981835328 to 25.6501800698454
22.7783221369706 to 25.6207040232832
23.1877841567718 to 26.0301660430843
22.6878650477253 to 25.5302469340378
23.2639883374892 to 26.1063702238018
23.6253854250632 to 26.4677673113758
23.4715774273597 to 26.3139593136722
22.7009070208393 to 25.5432889071519
Changing layer 3's weights from 
23.1726959562026 to 26.0150778425152
23.0861409520827 to 25.9285228383953
23.3435383653365 to 26.1859202516491
23.1232894277297 to 25.9656713140423
23.4521194910728 to 26.2945013773853
23.3776737069808 to 26.2200555932933
22.885422996255 to 25.7278048825676
22.6555155748598 to 25.4978974611724
22.927241853448 to 25.7696237397606
22.9209675109111 to 25.7633493972236
Changing layer 4's weights from 
23.1211300706588 to 25.9635119569713
22.9982905900203 to 25.8406724763328
22.7307201912724 to 25.5731020775849
23.5074699139319 to 26.3498518002445
23.6240828251563 to 26.4664647114689
23.2932089185439 to 26.1355908048565
23.3855044460021 to 26.2278863323147
23.0929816579543 to 25.9353635442669
23.2374919271194 to 26.0798738134319
23.1019847011291 to 25.9443665874416
Changing layer 5's weights from 
23.2656550025664 to 26.108036888879
23.0945330476485 to 25.9369149339611
23.2824372863494 to 26.1248191726619
23.4456819510184 to 26.288063837331
23.1002517556869 to 25.9426336419994
22.9005411183558 to 25.7429230046684
23.1464141225539 to 25.9887960088665
23.4727150416099 to 26.3150969279224
22.7342911129914 to 25.576672999304
23.5345405554496 to 26.3769224417621
10/5/2016 11:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:23 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:37 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
Modified index 9's learning in memoryPool to -0.2
Modified index 10's learning in memoryPool to -0.2
Modified index 11's learning in memoryPool to -0.2
Modified index 12's learning in memoryPool to -0.2
Modified index 13's learning in memoryPool to -0.2
Modified index 14's learning in memoryPool to -0.2
10/5/2016 11:22:37 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 31, 0, -0.2
sum 26852254690.2753 distri 9688872016.74386
Using diff 10450319000.9626 and condRate 0.166666666666667
Changed category 0 weights from 
9.73495966466474 to -348343962.154523
9.64915925534773 to -348343962.240324
9.80600470574903 to -348343962.083478
9.6464546349864 to -348343962.243028
Changing layer 0's weights from 
26.4815346730167 to -348343945.407948
25.5234156419748 to -348343946.366067
25.6834378076011 to -348343946.206045
25.775860412162 to -348343946.113623
25.7141313386375 to -348343946.175352
26.2152841222698 to -348343945.674199
25.8742324245388 to -348343946.015251
26.3474872839863 to -348343945.541996
26.4165318739826 to -348343945.472951
25.8006449771339 to -348343946.088838
Changing layer 1's weights from 
25.9546562564785 to -348343945.934827
26.0076621902401 to -348343945.881821
26.4317774308139 to -348343945.457706
25.9553324115688 to -348343945.934151
26.2842093241626 to -348343945.605274
25.686826093238 to -348343946.202657
25.8241960835869 to -348343946.065287
25.5588830319459 to -348343946.3306
25.6589994115049 to -348343946.230484
25.6596820038969 to -348343946.229801
Changing layer 2's weights from 
25.7811014485771 to -348343946.108382
25.663970796507 to -348343946.225512
25.6501800698454 to -348343946.239303
25.6207040232832 to -348343946.268779
26.0301660430843 to -348343945.859317
25.5302469340378 to -348343946.359236
26.1063702238018 to -348343945.783113
26.4677673113758 to -348343945.421716
26.3139593136722 to -348343945.575524
25.5432889071519 to -348343946.346194
Changing layer 3's weights from 
26.0150778425152 to -348343945.874405
25.9285228383953 to -348343945.96096
26.1859202516491 to -348343945.703563
25.9656713140423 to -348343945.923812
26.2945013773853 to -348343945.594982
26.2200555932933 to -348343945.669427
25.7278048825676 to -348343946.161678
25.4978974611724 to -348343946.391586
25.7696237397606 to -348343946.119859
25.7633493972236 to -348343946.126134
Changing layer 4's weights from 
25.9635119569713 to -348343945.925971
25.8406724763328 to -348343946.04881
25.5731020775849 to -348343946.316381
26.3498518002445 to -348343945.539631
26.4664647114689 to -348343945.423018
26.1355908048565 to -348343945.753892
26.2278863323147 to -348343945.661597
25.9353635442669 to -348343945.954119
26.0798738134319 to -348343945.809609
25.9443665874416 to -348343945.945116
Changing layer 5's weights from 
26.108036888879 to -348343945.781446
25.9369149339611 to -348343945.952568
26.1248191726619 to -348343945.764664
26.288063837331 to -348343945.601419
25.9426336419994 to -348343945.946849
25.7429230046684 to -348343946.14656
25.9887960088665 to -348343945.900687
26.3150969279224 to -348343945.574386
25.576672999304 to -348343946.31281
26.3769224417621 to -348343945.512561
Trying to learn from memory 32, 2, -0.2
sum 30140207720.7004 distri 7155097827.26816
Using diff 15450057963.2571 and condRate 0.166666666666667
Changed category 2 weights from 
6.56284722653795 to -515001933.21985
6.20158535210061 to -515001933.581112
5.99729696629215 to -515001933.7854
6.72570970622468 to -515001933.056987
Changing layer 0's weights from 
-348343945.407948 to -863345885.190645
-348343946.366067 to -863345886.148764
-348343946.206045 to -863345885.988742
-348343946.113623 to -863345885.89632
-348343946.175352 to -863345885.958049
-348343945.674199 to -863345885.456896
-348343946.015251 to -863345885.797948
-348343945.541996 to -863345885.324693
-348343945.472951 to -863345885.255648
-348343946.088838 to -863345885.871535
Changing layer 1's weights from 
-348343945.934827 to -863345885.717524
-348343945.881821 to -863345885.664518
-348343945.457706 to -863345885.240403
-348343945.934151 to -863345885.716848
-348343945.605274 to -863345885.387971
-348343946.202657 to -863345885.985354
-348343946.065287 to -863345885.847984
-348343946.3306 to -863345886.113297
-348343946.230484 to -863345886.013181
-348343946.229801 to -863345886.012498
Changing layer 2's weights from 
-348343946.108382 to -863345885.891079
-348343946.225512 to -863345886.008209
-348343946.239303 to -863345886.022
-348343946.268779 to -863345886.051476
-348343945.859317 to -863345885.642014
-348343946.359236 to -863345886.141933
-348343945.783113 to -863345885.56581
-348343945.421716 to -863345885.204413
-348343945.575524 to -863345885.358221
-348343946.346194 to -863345886.128891
Changing layer 3's weights from 
-348343945.874405 to -863345885.657102
-348343945.96096 to -863345885.743657
-348343945.703563 to -863345885.48626
-348343945.923812 to -863345885.706509
-348343945.594982 to -863345885.377679
-348343945.669427 to -863345885.452124
-348343946.161678 to -863345885.944375
-348343946.391586 to -863345886.174283
-348343946.119859 to -863345885.902556
-348343946.126134 to -863345885.908831
Changing layer 4's weights from 
-348343945.925971 to -863345885.708668
-348343946.04881 to -863345885.831508
-348343946.316381 to -863345886.099078
-348343945.539631 to -863345885.322328
-348343945.423018 to -863345885.205715
-348343945.753892 to -863345885.536589
-348343945.661597 to -863345885.444294
-348343945.954119 to -863345885.736817
-348343945.809609 to -863345885.592306
-348343945.945116 to -863345885.727813
Changing layer 5's weights from 
-348343945.781446 to -863345885.564143
-348343945.952568 to -863345885.735265
-348343945.764664 to -863345885.547361
-348343945.601419 to -863345885.384116
-348343945.946849 to -863345885.729546
-348343946.14656 to -863345885.929257
-348343945.900687 to -863345885.683384
-348343945.574386 to -863345885.357083
-348343946.31281 to -863345886.095507
-348343945.512561 to -863345885.295258
Trying to learn from memory 33, 0, -0.2
sum 31936208371.9158 distri 11521647900.726
Using diff 12430508378.2108 and condRate 0.166666666666667
Changed category 0 weights from 
-348343962.154523 to -762694247.602518
-348343962.240324 to -762694247.688318
-348343962.083478 to -762694247.531473
-348343962.243028 to -762694247.691023
Changing layer 0's weights from 
-863345885.190645 to -1277696170.63864
-863345886.148764 to -1277696171.59676
-863345885.988742 to -1277696171.43674
-863345885.89632 to -1277696171.34431
-863345885.958049 to -1277696171.40604
-863345885.456896 to -1277696170.90489
-863345885.797948 to -1277696171.24594
-863345885.324693 to -1277696170.77269
-863345885.255648 to -1277696170.70364
-863345885.871535 to -1277696171.31953
Changing layer 1's weights from 
-863345885.717524 to -1277696171.16552
-863345885.664518 to -1277696171.11251
-863345885.240403 to -1277696170.6884
-863345885.716848 to -1277696171.16484
-863345885.387971 to -1277696170.83597
-863345885.985354 to -1277696171.43335
-863345885.847984 to -1277696171.29598
-863345886.113297 to -1277696171.56129
-863345886.013181 to -1277696171.46118
-863345886.012498 to -1277696171.46049
Changing layer 2's weights from 
-863345885.891079 to -1277696171.33907
-863345886.008209 to -1277696171.4562
-863345886.022 to -1277696171.46999
-863345886.051476 to -1277696171.49947
-863345885.642014 to -1277696171.09001
-863345886.141933 to -1277696171.58993
-863345885.56581 to -1277696171.0138
-863345885.204413 to -1277696170.65241
-863345885.358221 to -1277696170.80622
-863345886.128891 to -1277696171.57689
Changing layer 3's weights from 
-863345885.657102 to -1277696171.1051
-863345885.743657 to -1277696171.19165
-863345885.48626 to -1277696170.93425
-863345885.706509 to -1277696171.1545
-863345885.377679 to -1277696170.82567
-863345885.452124 to -1277696170.90012
-863345885.944375 to -1277696171.39237
-863345886.174283 to -1277696171.62228
-863345885.902556 to -1277696171.35055
-863345885.908831 to -1277696171.35683
Changing layer 4's weights from 
-863345885.708668 to -1277696171.15666
-863345885.831508 to -1277696171.2795
-863345886.099078 to -1277696171.54707
-863345885.322328 to -1277696170.77032
-863345885.205715 to -1277696170.65371
-863345885.536589 to -1277696170.98458
-863345885.444294 to -1277696170.89229
-863345885.736817 to -1277696171.18481
-863345885.592306 to -1277696171.0403
-863345885.727813 to -1277696171.17581
Changing layer 5's weights from 
-863345885.564143 to -1277696171.01214
-863345885.735265 to -1277696171.18326
-863345885.547361 to -1277696170.99536
-863345885.384116 to -1277696170.83211
-863345885.729546 to -1277696171.17754
-863345885.929257 to -1277696171.37725
-863345885.683384 to -1277696171.13138
-863345885.357083 to -1277696170.80508
-863345886.095507 to -1277696171.5435
-863345885.295258 to -1277696170.74325
Trying to learn from memory 33, 1, -0.2
sum 31936208371.9158 distri 12836460401.2789
Using diff 11115695877.658 and condRate 0.166666666666667
Changed category 1 weights from 
10.5167170274672 to -370523190.926441
11.2653096127924 to -370523190.177849
11.2384812045511 to -370523190.204677
10.324177664299 to -370523191.118981
Changing layer 0's weights from 
-1277696170.63864 to -1648219372.0818
-1277696171.59676 to -1648219373.03992
-1277696171.43674 to -1648219372.8799
-1277696171.34431 to -1648219372.78747
-1277696171.40604 to -1648219372.8492
-1277696170.90489 to -1648219372.34805
-1277696171.24594 to -1648219372.6891
-1277696170.77269 to -1648219372.21585
-1277696170.70364 to -1648219372.1468
-1277696171.31953 to -1648219372.76269
Changing layer 1's weights from 
-1277696171.16552 to -1648219372.60868
-1277696171.11251 to -1648219372.55567
-1277696170.6884 to -1648219372.13156
-1277696171.16484 to -1648219372.608
-1277696170.83597 to -1648219372.27912
-1277696171.43335 to -1648219372.87651
-1277696171.29598 to -1648219372.73914
-1277696171.56129 to -1648219373.00445
-1277696171.46118 to -1648219372.90433
-1277696171.46049 to -1648219372.90365
Changing layer 2's weights from 
-1277696171.33907 to -1648219372.78223
-1277696171.4562 to -1648219372.89936
-1277696171.46999 to -1648219372.91315
-1277696171.49947 to -1648219372.94263
-1277696171.09001 to -1648219372.53317
-1277696171.58993 to -1648219373.03309
-1277696171.0138 to -1648219372.45696
-1277696170.65241 to -1648219372.09557
-1277696170.80622 to -1648219372.24937
-1277696171.57689 to -1648219373.02004
Changing layer 3's weights from 
-1277696171.1051 to -1648219372.54826
-1277696171.19165 to -1648219372.63481
-1277696170.93425 to -1648219372.37741
-1277696171.1545 to -1648219372.59766
-1277696170.82567 to -1648219372.26883
-1277696170.90012 to -1648219372.34328
-1277696171.39237 to -1648219372.83553
-1277696171.62228 to -1648219373.06544
-1277696171.35055 to -1648219372.79371
-1277696171.35683 to -1648219372.79998
Changing layer 4's weights from 
-1277696171.15666 to -1648219372.59982
-1277696171.2795 to -1648219372.72266
-1277696171.54707 to -1648219372.99023
-1277696170.77032 to -1648219372.21348
-1277696170.65371 to -1648219372.09687
-1277696170.98458 to -1648219372.42774
-1277696170.89229 to -1648219372.33545
-1277696171.18481 to -1648219372.62797
-1277696171.0403 to -1648219372.48346
-1277696171.17581 to -1648219372.61897
Changing layer 5's weights from 
-1277696171.01214 to -1648219372.4553
-1277696171.18326 to -1648219372.62642
-1277696170.99536 to -1648219372.43851
-1277696170.83211 to -1648219372.27527
-1277696171.17754 to -1648219372.6207
-1277696171.37725 to -1648219372.82041
-1277696171.13138 to -1648219372.57454
-1277696170.80508 to -1648219372.24824
-1277696171.5435 to -1648219372.98666
-1277696170.74325 to -1648219372.18641
Trying to learn from memory 34, 1, -0.2
sum 30128756139.6201 distri 12106302310.1899
Using diff 10490264794.5251 and condRate 0.166666666666667
Changed category 1 weights from 
-370523190.926441 to -720198689.28785
-370523190.177849 to -720198688.539257
-370523190.204677 to -720198688.566086
-370523191.118981 to -720198689.480389
Changing layer 0's weights from 
-1648219372.0818 to -1997894870.44321
-1648219373.03992 to -1997894871.40133
-1648219372.8799 to -1997894871.2413
-1648219372.78747 to -1997894871.14888
-1648219372.8492 to -1997894871.21061
-1648219372.34805 to -1997894870.70946
-1648219372.6891 to -1997894871.05051
-1648219372.21585 to -1997894870.57725
-1648219372.1468 to -1997894870.50821
-1648219372.76269 to -1997894871.1241
Changing layer 1's weights from 
-1648219372.60868 to -1997894870.97009
-1648219372.55567 to -1997894870.91708
-1648219372.13156 to -1997894870.49296
-1648219372.608 to -1997894870.96941
-1648219372.27912 to -1997894870.64053
-1648219372.87651 to -1997894871.23792
-1648219372.73914 to -1997894871.10055
-1648219373.00445 to -1997894871.36586
-1648219372.90433 to -1997894871.26574
-1648219372.90365 to -1997894871.26506
Changing layer 2's weights from 
-1648219372.78223 to -1997894871.14364
-1648219372.89936 to -1997894871.26077
-1648219372.91315 to -1997894871.27456
-1648219372.94263 to -1997894871.30404
-1648219372.53317 to -1997894870.89458
-1648219373.03309 to -1997894871.39449
-1648219372.45696 to -1997894870.81837
-1648219372.09557 to -1997894870.45697
-1648219372.24937 to -1997894870.61078
-1648219373.02004 to -1997894871.38145
Changing layer 3's weights from 
-1648219372.54826 to -1997894870.90966
-1648219372.63481 to -1997894870.99622
-1648219372.37741 to -1997894870.73882
-1648219372.59766 to -1997894870.95907
-1648219372.26883 to -1997894870.63024
-1648219372.34328 to -1997894870.70469
-1648219372.83553 to -1997894871.19694
-1648219373.06544 to -1997894871.42684
-1648219372.79371 to -1997894871.15512
-1648219372.79998 to -1997894871.16139
Changing layer 4's weights from 
-1648219372.59982 to -1997894870.96123
-1648219372.72266 to -1997894871.08407
-1648219372.99023 to -1997894871.35164
-1648219372.21348 to -1997894870.57489
-1648219372.09687 to -1997894870.45828
-1648219372.42774 to -1997894870.78915
-1648219372.33545 to -1997894870.69686
-1648219372.62797 to -1997894870.98938
-1648219372.48346 to -1997894870.84487
-1648219372.61897 to -1997894870.98037
Changing layer 5's weights from 
-1648219372.4553 to -1997894870.8167
-1648219372.62642 to -1997894870.98783
-1648219372.43851 to -1997894870.79992
-1648219372.27527 to -1997894870.63668
-1648219372.6207 to -1997894870.98211
-1648219372.82041 to -1997894871.18182
-1648219372.57454 to -1997894870.93595
-1648219372.24824 to -1997894870.60964
-1648219372.98666 to -1997894871.34807
-1648219372.18641 to -1997894870.54782
Trying to learn from memory 35, 2, -0.2
sum 28825322996.4787 distri 6848455717.93463
Using diff 14770536529.4244 and condRate 0.166666666666667
Changed category 2 weights from 
-515001933.21985 to -1007353158.20393
-515001933.581112 to -1007353158.5652
-515001933.7854 to -1007353158.76949
-515001933.056987 to -1007353158.04107
Changing layer 0's weights from 
-1997894870.44321 to -2490246095.42729
-1997894871.40133 to -2490246096.38541
-1997894871.2413 to -2490246096.22539
-1997894871.14888 to -2490246096.13297
-1997894871.21061 to -2490246096.1947
-1997894870.70946 to -2490246095.69354
-1997894871.05051 to -2490246096.03459
-1997894870.57725 to -2490246095.56134
-1997894870.50821 to -2490246095.49229
-1997894871.1241 to -2490246096.10818
Changing layer 1's weights from 
-1997894870.97009 to -2490246095.95417
-1997894870.91708 to -2490246095.90116
-1997894870.49296 to -2490246095.47705
-1997894870.96941 to -2490246095.95349
-1997894870.64053 to -2490246095.62462
-1997894871.23792 to -2490246096.222
-1997894871.10055 to -2490246096.08463
-1997894871.36586 to -2490246096.34994
-1997894871.26574 to -2490246096.24983
-1997894871.26506 to -2490246096.24914
Changing layer 2's weights from 
-1997894871.14364 to -2490246096.12773
-1997894871.26077 to -2490246096.24486
-1997894871.27456 to -2490246096.25865
-1997894871.30404 to -2490246096.28812
-1997894870.89458 to -2490246095.87866
-1997894871.39449 to -2490246096.37858
-1997894870.81837 to -2490246095.80246
-1997894870.45697 to -2490246095.44106
-1997894870.61078 to -2490246095.59487
-1997894871.38145 to -2490246096.36554
Changing layer 3's weights from 
-1997894870.90966 to -2490246095.89375
-1997894870.99622 to -2490246095.9803
-1997894870.73882 to -2490246095.72291
-1997894870.95907 to -2490246095.94316
-1997894870.63024 to -2490246095.61433
-1997894870.70469 to -2490246095.68877
-1997894871.19694 to -2490246096.18102
-1997894871.42684 to -2490246096.41093
-1997894871.15512 to -2490246096.1392
-1997894871.16139 to -2490246096.14548
Changing layer 4's weights from 
-1997894870.96123 to -2490246095.94531
-1997894871.08407 to -2490246096.06815
-1997894871.35164 to -2490246096.33572
-1997894870.57489 to -2490246095.55897
-1997894870.45828 to -2490246095.44236
-1997894870.78915 to -2490246095.77324
-1997894870.69686 to -2490246095.68094
-1997894870.98938 to -2490246095.97346
-1997894870.84487 to -2490246095.82895
-1997894870.98037 to -2490246095.96446
Changing layer 5's weights from 
-1997894870.8167 to -2490246095.80079
-1997894870.98783 to -2490246095.97191
-1997894870.79992 to -2490246095.78401
-1997894870.63668 to -2490246095.62076
-1997894870.98211 to -2490246095.96619
-1997894871.18182 to -2490246096.1659
-1997894870.93595 to -2490246095.92003
-1997894870.60964 to -2490246095.59373
-1997894871.34807 to -2490246096.33215
-1997894870.54782 to -2490246095.5319
Trying to learn from memory 36, 1, -0.2
sum 26943589002.7631 distri 10819267398.5907
Using diff 9388424353.48168 and condRate 0.166666666666667
Changed category 1 weights from 
-720198689.28785 to -1033146172.40052
-720198688.539257 to -1033146171.65193
-720198688.566086 to -1033146171.67876
-720198689.480389 to -1033146172.59306
Changing layer 0's weights from 
-2490246095.42729 to -2803193578.53996
-2490246096.38541 to -2803193579.49808
-2490246096.22539 to -2803193579.33806
-2490246096.13297 to -2803193579.24564
-2490246096.1947 to -2803193579.30736
-2490246095.69354 to -2803193578.80621
-2490246096.03459 to -2803193579.14726
-2490246095.56134 to -2803193578.67401
-2490246095.49229 to -2803193578.60496
-2490246096.10818 to -2803193579.22085
Changing layer 1's weights from 
-2490246095.95417 to -2803193579.06684
-2490246095.90116 to -2803193579.01383
-2490246095.47705 to -2803193578.58972
-2490246095.95349 to -2803193579.06616
-2490246095.62462 to -2803193578.73729
-2490246096.222 to -2803193579.33467
-2490246096.08463 to -2803193579.1973
-2490246096.34994 to -2803193579.46261
-2490246096.24983 to -2803193579.3625
-2490246096.24914 to -2803193579.36181
Changing layer 2's weights from 
-2490246096.12773 to -2803193579.2404
-2490246096.24486 to -2803193579.35753
-2490246096.25865 to -2803193579.37132
-2490246096.28812 to -2803193579.40079
-2490246095.87866 to -2803193578.99133
-2490246096.37858 to -2803193579.49125
-2490246095.80246 to -2803193578.91513
-2490246095.44106 to -2803193578.55373
-2490246095.59487 to -2803193578.70754
-2490246096.36554 to -2803193579.47821
Changing layer 3's weights from 
-2490246095.89375 to -2803193579.00642
-2490246095.9803 to -2803193579.09297
-2490246095.72291 to -2803193578.83558
-2490246095.94316 to -2803193579.05583
-2490246095.61433 to -2803193578.727
-2490246095.68877 to -2803193578.80144
-2490246096.18102 to -2803193579.29369
-2490246096.41093 to -2803193579.5236
-2490246096.1392 to -2803193579.25187
-2490246096.14548 to -2803193579.25815
Changing layer 4's weights from 
-2490246095.94531 to -2803193579.05798
-2490246096.06815 to -2803193579.18082
-2490246096.33572 to -2803193579.44839
-2490246095.55897 to -2803193578.67164
-2490246095.44236 to -2803193578.55503
-2490246095.77324 to -2803193578.88591
-2490246095.68094 to -2803193578.79361
-2490246095.97346 to -2803193579.08613
-2490246095.82895 to -2803193578.94162
-2490246095.96446 to -2803193579.07713
Changing layer 5's weights from 
-2490246095.80079 to -2803193578.91346
-2490246095.97191 to -2803193579.08458
-2490246095.78401 to -2803193578.89668
-2490246095.62076 to -2803193578.73343
-2490246095.96619 to -2803193579.07886
-2490246096.1659 to -2803193579.27857
-2490246095.92003 to -2803193579.0327
-2490246095.59373 to -2803193578.7064
-2490246096.33215 to -2803193579.44482
-2490246095.5319 to -2803193578.64457
Trying to learn from memory 37, 1, -0.2
sum 26830632209.2553 distri 10773925782.3043
Using diff 9349048374.63719 and condRate 0.166666666666667
Changed category 1 weights from 
-1033146172.40052 to -1344781122.86548
-1033146171.65193 to -1344781122.11689
-1033146171.67876 to -1344781122.14372
-1033146172.59306 to -1344781123.05802
Changing layer 0's weights from 
-2803193578.53996 to -3114828529.00492
-2803193579.49808 to -3114828529.96304
-2803193579.33806 to -3114828529.80302
-2803193579.24564 to -3114828529.7106
-2803193579.30736 to -3114828529.77233
-2803193578.80621 to -3114828529.27117
-2803193579.14726 to -3114828529.61223
-2803193578.67401 to -3114828529.13897
-2803193578.60496 to -3114828529.06993
-2803193579.22085 to -3114828529.68581
Changing layer 1's weights from 
-2803193579.06684 to -3114828529.5318
-2803193579.01383 to -3114828529.4788
-2803193578.58972 to -3114828529.05468
-2803193579.06616 to -3114828529.53113
-2803193578.73729 to -3114828529.20225
-2803193579.33467 to -3114828529.79963
-2803193579.1973 to -3114828529.66226
-2803193579.46261 to -3114828529.92758
-2803193579.3625 to -3114828529.82746
-2803193579.36181 to -3114828529.82678
Changing layer 2's weights from 
-2803193579.2404 to -3114828529.70536
-2803193579.35753 to -3114828529.82249
-2803193579.37132 to -3114828529.83628
-2803193579.40079 to -3114828529.86575
-2803193578.99133 to -3114828529.45629
-2803193579.49125 to -3114828529.95621
-2803193578.91513 to -3114828529.38009
-2803193578.55373 to -3114828529.01869
-2803193578.70754 to -3114828529.1725
-2803193579.47821 to -3114828529.94317
Changing layer 3's weights from 
-2803193579.00642 to -3114828529.47138
-2803193579.09297 to -3114828529.55794
-2803193578.83558 to -3114828529.30054
-2803193579.05583 to -3114828529.52079
-2803193578.727 to -3114828529.19196
-2803193578.80144 to -3114828529.2664
-2803193579.29369 to -3114828529.75865
-2803193579.5236 to -3114828529.98856
-2803193579.25187 to -3114828529.71683
-2803193579.25815 to -3114828529.72311
Changing layer 4's weights from 
-2803193579.05798 to -3114828529.52295
-2803193579.18082 to -3114828529.64579
-2803193579.44839 to -3114828529.91336
-2803193578.67164 to -3114828529.13661
-2803193578.55503 to -3114828529.01999
-2803193578.88591 to -3114828529.35087
-2803193578.79361 to -3114828529.25857
-2803193579.08613 to -3114828529.5511
-2803193578.94162 to -3114828529.40658
-2803193579.07713 to -3114828529.54209
Changing layer 5's weights from 
-2803193578.91346 to -3114828529.37842
-2803193579.08458 to -3114828529.54954
-2803193578.89668 to -3114828529.36164
-2803193578.73343 to -3114828529.19839
-2803193579.07886 to -3114828529.54382
-2803193579.27857 to -3114828529.74354
-2803193579.0327 to -3114828529.49766
-2803193578.7064 to -3114828529.17136
-2803193579.44482 to -3114828529.90979
-2803193578.64457 to -3114828529.10954
Trying to learn from memory 38, 2, -0.2
sum 26831429637.0409 distri 6375829697.59962
Using diff 13747742530.181 and condRate 0.166666666666667
Changed category 2 weights from 
-1007353158.20393 to -1465611249.37188
-1007353158.5652 to -1465611249.73314
-1007353158.76949 to -1465611249.93743
-1007353158.04107 to -1465611249.20902
Changing layer 0's weights from 
-3114828529.00492 to -3573086620.17287
-3114828529.96304 to -3573086621.13099
-3114828529.80302 to -3573086620.97097
-3114828529.7106 to -3573086620.87854
-3114828529.77233 to -3573086620.94027
-3114828529.27117 to -3573086620.43912
-3114828529.61223 to -3573086620.78017
-3114828529.13897 to -3573086620.30692
-3114828529.06993 to -3573086620.23787
-3114828529.68581 to -3573086620.85376
Changing layer 1's weights from 
-3114828529.5318 to -3573086620.69975
-3114828529.4788 to -3573086620.64674
-3114828529.05468 to -3573086620.22263
-3114828529.53113 to -3573086620.69907
-3114828529.20225 to -3573086620.37019
-3114828529.79963 to -3573086620.96758
-3114828529.66226 to -3573086620.83021
-3114828529.92758 to -3573086621.09552
-3114828529.82746 to -3573086620.9954
-3114828529.82678 to -3573086620.99472
Changing layer 2's weights from 
-3114828529.70536 to -3573086620.8733
-3114828529.82249 to -3573086620.99043
-3114828529.83628 to -3573086621.00422
-3114828529.86575 to -3573086621.0337
-3114828529.45629 to -3573086620.62424
-3114828529.95621 to -3573086621.12416
-3114828529.38009 to -3573086620.54803
-3114828529.01869 to -3573086620.18664
-3114828529.1725 to -3573086620.34044
-3114828529.94317 to -3573086621.11111
Changing layer 3's weights from 
-3114828529.47138 to -3573086620.63933
-3114828529.55794 to -3573086620.72588
-3114828529.30054 to -3573086620.46848
-3114828529.52079 to -3573086620.68873
-3114828529.19196 to -3573086620.3599
-3114828529.2664 to -3573086620.43435
-3114828529.75865 to -3573086620.9266
-3114828529.98856 to -3573086621.15651
-3114828529.71683 to -3573086620.88478
-3114828529.72311 to -3573086620.89105
Changing layer 4's weights from 
-3114828529.52295 to -3573086620.69089
-3114828529.64579 to -3573086620.81373
-3114828529.91336 to -3573086621.0813
-3114828529.13661 to -3573086620.30455
-3114828529.01999 to -3573086620.18794
-3114828529.35087 to -3573086620.51881
-3114828529.25857 to -3573086620.42652
-3114828529.5511 to -3573086620.71904
-3114828529.40658 to -3573086620.57453
-3114828529.54209 to -3573086620.71004
Changing layer 5's weights from 
-3114828529.37842 to -3573086620.54637
-3114828529.54954 to -3573086620.71749
-3114828529.36164 to -3573086620.52958
-3114828529.19839 to -3573086620.36634
-3114828529.54382 to -3573086620.71177
-3114828529.74354 to -3573086620.91148
-3114828529.49766 to -3573086620.66561
-3114828529.17136 to -3573086620.33931
-3114828529.90979 to -3573086621.07773
-3114828529.10954 to -3573086620.27748
Trying to learn from memory 39, 1, -0.2
sum 26828643482.6879 distri 10773143422.1313
Using diff 9348339189.88462 and condRate 0.166666666666667
Changed category 1 weights from 
-1344781122.86548 to -1656392433.83834
-1344781122.11689 to -1656392433.08975
-1344781122.14372 to -1656392433.11658
-1344781123.05802 to -1656392434.03088
Changing layer 0's weights from 
-3573086620.17287 to -3884697931.14573
-3573086621.13099 to -3884697932.10385
-3573086620.97097 to -3884697931.94382
-3573086620.87854 to -3884697931.8514
-3573086620.94027 to -3884697931.91313
-3573086620.43912 to -3884697931.41198
-3573086620.78017 to -3884697931.75303
-3573086620.30692 to -3884697931.27977
-3573086620.23787 to -3884697931.21073
-3573086620.85376 to -3884697931.82662
Changing layer 1's weights from 
-3573086620.69975 to -3884697931.67261
-3573086620.64674 to -3884697931.6196
-3573086620.22263 to -3884697931.19548
-3573086620.69907 to -3884697931.67193
-3573086620.37019 to -3884697931.34305
-3573086620.96758 to -3884697931.94043
-3573086620.83021 to -3884697931.80307
-3573086621.09552 to -3884697932.06838
-3573086620.9954 to -3884697931.96826
-3573086620.99472 to -3884697931.96758
Changing layer 2's weights from 
-3573086620.8733 to -3884697931.84616
-3573086620.99043 to -3884697931.96329
-3573086621.00422 to -3884697931.97708
-3573086621.0337 to -3884697932.00656
-3573086620.62424 to -3884697931.5971
-3573086621.12416 to -3884697932.09701
-3573086620.54803 to -3884697931.52089
-3573086620.18664 to -3884697931.15949
-3573086620.34044 to -3884697931.3133
-3573086621.11111 to -3884697932.08397
Changing layer 3's weights from 
-3573086620.63933 to -3884697931.61218
-3573086620.72588 to -3884697931.69874
-3573086620.46848 to -3884697931.44134
-3573086620.68873 to -3884697931.66159
-3573086620.3599 to -3884697931.33276
-3573086620.43435 to -3884697931.40721
-3573086620.9266 to -3884697931.89946
-3573086621.15651 to -3884697932.12936
-3573086620.88478 to -3884697931.85764
-3573086620.89105 to -3884697931.86391
Changing layer 4's weights from 
-3573086620.69089 to -3884697931.66375
-3573086620.81373 to -3884697931.78659
-3573086621.0813 to -3884697932.05416
-3573086620.30455 to -3884697931.27741
-3573086620.18794 to -3884697931.1608
-3573086620.51881 to -3884697931.49167
-3573086620.42652 to -3884697931.39938
-3573086620.71904 to -3884697931.6919
-3573086620.57453 to -3884697931.54739
-3573086620.71004 to -3884697931.68289
Changing layer 5's weights from 
-3573086620.54637 to -3884697931.51922
-3573086620.71749 to -3884697931.69035
-3573086620.52958 to -3884697931.50244
-3573086620.36634 to -3884697931.3392
-3573086620.71177 to -3884697931.68463
-3573086620.91148 to -3884697931.88434
-3573086620.66561 to -3884697931.63847
-3573086620.33931 to -3884697931.31216
-3573086621.07773 to -3884697932.05059
-3573086620.27748 to -3884697931.25034
10/5/2016 11:22:37 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 40, 2, -0.2
sum 26828643482.6879 distri 6375139937.26564
Using diff 13746342674.7503 and condRate 0.166666666666667
Changed category 2 weights from 
-1465611249.37188 to -1923822678.69144
-1465611249.73314 to -1923822679.0527
-1465611249.93743 to -1923822679.25699
-1465611249.20902 to -1923822678.52857
Changing layer 0's weights from 
-3884697931.14573 to -4342909360.46528
-3884697932.10385 to -4342909361.4234
-3884697931.94382 to -4342909361.26338
-3884697931.8514 to -4342909361.17096
-3884697931.91313 to -4342909361.23269
-3884697931.41198 to -4342909360.73154
-3884697931.75303 to -4342909361.07259
-3884697931.27977 to -4342909360.59933
-3884697931.21073 to -4342909360.53029
-3884697931.82662 to -4342909361.14617
Changing layer 1's weights from 
-3884697931.67261 to -4342909360.99216
-3884697931.6196 to -4342909360.93916
-3884697931.19548 to -4342909360.51504
-3884697931.67193 to -4342909360.99149
-3884697931.34305 to -4342909360.66261
-3884697931.94043 to -4342909361.25999
-3884697931.80307 to -4342909361.12262
-3884697932.06838 to -4342909361.38794
-3884697931.96826 to -4342909361.28782
-3884697931.96758 to -4342909361.28714
Changing layer 2's weights from 
-3884697931.84616 to -4342909361.16572
-3884697931.96329 to -4342909361.28285
-3884697931.97708 to -4342909361.29664
-3884697932.00656 to -4342909361.32611
-3884697931.5971 to -4342909360.91665
-3884697932.09701 to -4342909361.41657
-3884697931.52089 to -4342909360.84045
-3884697931.15949 to -4342909360.47905
-3884697931.3133 to -4342909360.63286
-3884697932.08397 to -4342909361.40353
Changing layer 3's weights from 
-3884697931.61218 to -4342909360.93174
-3884697931.69874 to -4342909361.0183
-3884697931.44134 to -4342909360.7609
-3884697931.66159 to -4342909360.98115
-3884697931.33276 to -4342909360.65232
-3884697931.40721 to -4342909360.72676
-3884697931.89946 to -4342909361.21901
-3884697932.12936 to -4342909361.44892
-3884697931.85764 to -4342909361.17719
-3884697931.86391 to -4342909361.18347
Changing layer 4's weights from 
-3884697931.66375 to -4342909360.98331
-3884697931.78659 to -4342909361.10615
-3884697932.05416 to -4342909361.37372
-3884697931.27741 to -4342909360.59697
-3884697931.1608 to -4342909360.48035
-3884697931.49167 to -4342909360.81123
-3884697931.39938 to -4342909360.71893
-3884697931.6919 to -4342909361.01146
-3884697931.54739 to -4342909360.86695
-3884697931.68289 to -4342909361.00245
Changing layer 5's weights from 
-3884697931.51922 to -4342909360.83878
-3884697931.69035 to -4342909361.0099
-3884697931.50244 to -4342909360.822
-3884697931.3392 to -4342909360.65875
-3884697931.68463 to -4342909361.00418
-3884697931.88434 to -4342909361.2039
-3884697931.63847 to -4342909360.95802
-3884697931.31216 to -4342909360.63172
-3884697932.05059 to -4342909361.37015
-3884697931.25034 to -4342909360.5699
Trying to learn from memory 40, 2, -0.2
sum 26828643482.6879 distri 6375139937.26564
Using diff 13746342674.7503 and condRate 0.166666666666667
Changed category 2 weights from 
-1923822678.69144 to -2382034108.01099
-1923822679.0527 to -2382034108.37226
-1923822679.25699 to -2382034108.57655
-1923822678.52857 to -2382034107.84813
Changing layer 0's weights from 
-4342909360.46528 to -4801120789.78484
-4342909361.4234 to -4801120790.74296
-4342909361.26338 to -4801120790.58294
-4342909361.17096 to -4801120790.49052
-4342909361.23269 to -4801120790.55224
-4342909360.73154 to -4801120790.05109
-4342909361.07259 to -4801120790.39214
-4342909360.59933 to -4801120789.91889
-4342909360.53029 to -4801120789.84984
-4342909361.14617 to -4801120790.46573
Changing layer 1's weights from 
-4342909360.99216 to -4801120790.31172
-4342909360.93916 to -4801120790.25871
-4342909360.51504 to -4801120789.8346
-4342909360.99149 to -4801120790.31104
-4342909360.66261 to -4801120789.98217
-4342909361.25999 to -4801120790.57955
-4342909361.12262 to -4801120790.44218
-4342909361.38794 to -4801120790.70749
-4342909361.28782 to -4801120790.60738
-4342909361.28714 to -4801120790.60669
Changing layer 2's weights from 
-4342909361.16572 to -4801120790.48527
-4342909361.28285 to -4801120790.6024
-4342909361.29664 to -4801120790.6162
-4342909361.32611 to -4801120790.64567
-4342909360.91665 to -4801120790.23621
-4342909361.41657 to -4801120790.73613
-4342909360.84045 to -4801120790.16001
-4342909360.47905 to -4801120789.79861
-4342909360.63286 to -4801120789.95242
-4342909361.40353 to -4801120790.72309
Changing layer 3's weights from 
-4342909360.93174 to -4801120790.2513
-4342909361.0183 to -4801120790.33785
-4342909360.7609 to -4801120790.08046
-4342909360.98115 to -4801120790.30071
-4342909360.65232 to -4801120789.97187
-4342909360.72676 to -4801120790.04632
-4342909361.21901 to -4801120790.53857
-4342909361.44892 to -4801120790.76848
-4342909361.17719 to -4801120790.49675
-4342909361.18347 to -4801120790.50303
Changing layer 4's weights from 
-4342909360.98331 to -4801120790.30286
-4342909361.10615 to -4801120790.4257
-4342909361.37372 to -4801120790.69327
-4342909360.59697 to -4801120789.91652
-4342909360.48035 to -4801120789.79991
-4342909360.81123 to -4801120790.13079
-4342909360.71893 to -4801120790.03849
-4342909361.01146 to -4801120790.33101
-4342909360.86695 to -4801120790.1865
-4342909361.00245 to -4801120790.32201
Changing layer 5's weights from 
-4342909360.83878 to -4801120790.15834
-4342909361.0099 to -4801120790.32946
-4342909360.822 to -4801120790.14156
-4342909360.65875 to -4801120789.97831
-4342909361.00418 to -4801120790.32374
-4342909361.2039 to -4801120790.52345
-4342909360.95802 to -4801120790.27758
-4342909360.63172 to -4801120789.95128
-4342909361.37015 to -4801120790.6897
-4342909360.5699 to -4801120789.88945
Trying to learn from memory 40, 1, -0.2
sum 26828643482.6879 distri 10773143422.1313
Using diff 9348339189.88462 and condRate 0.166666666666667
Changed category 1 weights from 
-1656392433.83834 to -1968003744.8112
-1656392433.08975 to -1968003744.0626
-1656392433.11658 to -1968003744.08943
-1656392434.03088 to -1968003745.00374
Changing layer 0's weights from 
-4801120789.78484 to -5112732100.7577
-4801120790.74296 to -5112732101.71582
-4801120790.58294 to -5112732101.5558
-4801120790.49052 to -5112732101.46337
-4801120790.55224 to -5112732101.5251
-4801120790.05109 to -5112732101.02395
-4801120790.39214 to -5112732101.365
-4801120789.91889 to -5112732100.89175
-4801120789.84984 to -5112732100.8227
-4801120790.46573 to -5112732101.43859
Changing layer 1's weights from 
-4801120790.31172 to -5112732101.28458
-4801120790.25871 to -5112732101.23157
-4801120789.8346 to -5112732100.80746
-4801120790.31104 to -5112732101.2839
-4801120789.98217 to -5112732100.95502
-4801120790.57955 to -5112732101.55241
-4801120790.44218 to -5112732101.41504
-4801120790.70749 to -5112732101.68035
-4801120790.60738 to -5112732101.58023
-4801120790.60669 to -5112732101.57955
Changing layer 2's weights from 
-4801120790.48527 to -5112732101.45813
-4801120790.6024 to -5112732101.57526
-4801120790.6162 to -5112732101.58905
-4801120790.64567 to -5112732101.61853
-4801120790.23621 to -5112732101.20907
-4801120790.73613 to -5112732101.70899
-4801120790.16001 to -5112732101.13286
-4801120789.79861 to -5112732100.77147
-4801120789.95242 to -5112732100.92527
-4801120790.72309 to -5112732101.69594
Changing layer 3's weights from 
-4801120790.2513 to -5112732101.22416
-4801120790.33785 to -5112732101.31071
-4801120790.08046 to -5112732101.05331
-4801120790.30071 to -5112732101.27356
-4801120789.97187 to -5112732100.94473
-4801120790.04632 to -5112732101.01918
-4801120790.53857 to -5112732101.51143
-4801120790.76848 to -5112732101.74134
-4801120790.49675 to -5112732101.46961
-4801120790.50303 to -5112732101.47588
Changing layer 4's weights from 
-4801120790.30286 to -5112732101.27572
-4801120790.4257 to -5112732101.39856
-4801120790.69327 to -5112732101.66613
-4801120789.91652 to -5112732100.88938
-4801120789.79991 to -5112732100.77277
-4801120790.13079 to -5112732101.10364
-4801120790.03849 to -5112732101.01135
-4801120790.33101 to -5112732101.30387
-4801120790.1865 to -5112732101.15936
-4801120790.32201 to -5112732101.29487
Changing layer 5's weights from 
-4801120790.15834 to -5112732101.1312
-4801120790.32946 to -5112732101.30232
-4801120790.14156 to -5112732101.11441
-4801120789.97831 to -5112732100.95117
-4801120790.32374 to -5112732101.2966
-4801120790.52345 to -5112732101.49631
-4801120790.27758 to -5112732101.25044
-4801120789.95128 to -5112732100.92414
-4801120790.6897 to -5112732101.66256
-4801120789.88945 to -5112732100.86231
Trying to learn from memory 40, 2, -0.2
sum 26828643482.6879 distri 6375139937.26564
Using diff 13746342674.7503 and condRate 0.166666666666667
Changed category 2 weights from 
-2382034108.01099 to -2840245537.33055
-2382034108.37226 to -2840245537.69181
-2382034108.57655 to -2840245537.8961
-2382034107.84813 to -2840245537.16769
Changing layer 0's weights from 
-5112732100.7577 to -5570943530.07726
-5112732101.71582 to -5570943531.03537
-5112732101.5558 to -5570943530.87535
-5112732101.46337 to -5570943530.78293
-5112732101.5251 to -5570943530.84466
-5112732101.02395 to -5570943530.34351
-5112732101.365 to -5570943530.68456
-5112732100.89175 to -5570943530.2113
-5112732100.8227 to -5570943530.14226
-5112732101.43859 to -5570943530.75815
Changing layer 1's weights from 
-5112732101.28458 to -5570943530.60413
-5112732101.23157 to -5570943530.55113
-5112732100.80746 to -5570943530.12701
-5112732101.2839 to -5570943530.60346
-5112732100.95502 to -5570943530.27458
-5112732101.55241 to -5570943530.87196
-5112732101.41504 to -5570943530.73459
-5112732101.68035 to -5570943530.99991
-5112732101.58023 to -5570943530.89979
-5112732101.57955 to -5570943530.89911
Changing layer 2's weights from 
-5112732101.45813 to -5570943530.77769
-5112732101.57526 to -5570943530.89482
-5112732101.58905 to -5570943530.90861
-5112732101.61853 to -5570943530.93809
-5112732101.20907 to -5570943530.52862
-5112732101.70899 to -5570943531.02854
-5112732101.13286 to -5570943530.45242
-5112732100.77147 to -5570943530.09102
-5112732100.92527 to -5570943530.24483
-5112732101.69594 to -5570943531.0155
Changing layer 3's weights from 
-5112732101.22416 to -5570943530.54371
-5112732101.31071 to -5570943530.63027
-5112732101.05331 to -5570943530.37287
-5112732101.27356 to -5570943530.59312
-5112732100.94473 to -5570943530.26429
-5112732101.01918 to -5570943530.33873
-5112732101.51143 to -5570943530.83099
-5112732101.74134 to -5570943531.06089
-5112732101.46961 to -5570943530.78917
-5112732101.47588 to -5570943530.79544
Changing layer 4's weights from 
-5112732101.27572 to -5570943530.59528
-5112732101.39856 to -5570943530.71812
-5112732101.66613 to -5570943530.98569
-5112732100.88938 to -5570943530.20894
-5112732100.77277 to -5570943530.09233
-5112732101.10364 to -5570943530.4232
-5112732101.01135 to -5570943530.3309
-5112732101.30387 to -5570943530.62343
-5112732101.15936 to -5570943530.47892
-5112732101.29487 to -5570943530.61442
Changing layer 5's weights from 
-5112732101.1312 to -5570943530.45075
-5112732101.30232 to -5570943530.62188
-5112732101.11441 to -5570943530.43397
-5112732100.95117 to -5570943530.27073
-5112732101.2966 to -5570943530.61616
-5112732101.49631 to -5570943530.81587
-5112732101.25044 to -5570943530.56999
-5112732100.92414 to -5570943530.24369
-5112732101.66256 to -5570943530.98212
-5112732100.86231 to -5570943530.18187
Trying to learn from memory 40, 0, -0.2
sum 26828643482.6879 distri 9680360123.29095
Using diff 10441122488.7249 and condRate 0.166666666666667
Changed category 0 weights from 
-762694247.602518 to -1110731669.07951
-762694247.688318 to -1110731669.16531
-762694247.531473 to -1110731669.00847
-762694247.691023 to -1110731669.16802
Changing layer 0's weights from 
-5570943530.07726 to -5918980951.55425
-5570943531.03537 to -5918980952.51237
-5570943530.87535 to -5918980952.35235
-5570943530.78293 to -5918980952.25992
-5570943530.84466 to -5918980952.32165
-5570943530.34351 to -5918980951.8205
-5570943530.68456 to -5918980952.16155
-5570943530.2113 to -5918980951.6883
-5570943530.14226 to -5918980951.61925
-5570943530.75815 to -5918980952.23514
Changing layer 1's weights from 
-5570943530.60413 to -5918980952.08113
-5570943530.55113 to -5918980952.02812
-5570943530.12701 to -5918980951.60401
-5570943530.60346 to -5918980952.08045
-5570943530.27458 to -5918980951.75157
-5570943530.87196 to -5918980952.34896
-5570943530.73459 to -5918980952.21159
-5570943530.99991 to -5918980952.4769
-5570943530.89979 to -5918980952.37678
-5570943530.89911 to -5918980952.3761
Changing layer 2's weights from 
-5570943530.77769 to -5918980952.25468
-5570943530.89482 to -5918980952.37181
-5570943530.90861 to -5918980952.3856
-5570943530.93809 to -5918980952.41508
-5570943530.52862 to -5918980952.00562
-5570943531.02854 to -5918980952.50554
-5570943530.45242 to -5918980951.92941
-5570943530.09102 to -5918980951.56802
-5570943530.24483 to -5918980951.72182
-5570943531.0155 to -5918980952.49249
Changing layer 3's weights from 
-5570943530.54371 to -5918980952.02071
-5570943530.63027 to -5918980952.10726
-5570943530.37287 to -5918980951.84986
-5570943530.59312 to -5918980952.07011
-5570943530.26429 to -5918980951.74128
-5570943530.33873 to -5918980951.81573
-5570943530.83099 to -5918980952.30798
-5570943531.06089 to -5918980952.53789
-5570943530.78917 to -5918980952.26616
-5570943530.79544 to -5918980952.27243
Changing layer 4's weights from 
-5570943530.59528 to -5918980952.07227
-5570943530.71812 to -5918980952.19511
-5570943530.98569 to -5918980952.46268
-5570943530.20894 to -5918980951.68593
-5570943530.09233 to -5918980951.56932
-5570943530.4232 to -5918980951.90019
-5570943530.3309 to -5918980951.8079
-5570943530.62343 to -5918980952.10042
-5570943530.47892 to -5918980951.95591
-5570943530.61442 to -5918980952.09142
Changing layer 5's weights from 
-5570943530.45075 to -5918980951.92775
-5570943530.62188 to -5918980952.09887
-5570943530.43397 to -5918980951.91096
-5570943530.27073 to -5918980951.74772
-5570943530.61616 to -5918980952.09315
-5570943530.81587 to -5918980952.29286
-5570943530.56999 to -5918980952.04699
-5570943530.24369 to -5918980951.72069
-5570943530.98212 to -5918980952.45911
-5570943530.18187 to -5918980951.65886
10/5/2016 11:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:39 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:40 PMStarting learning phase with deltaScore: 0
10/5/2016 11:22:40 PMStarting learning phase with deltaScore: 0
