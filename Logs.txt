10/5/2016 1:22:13 PMStarting AI
Weights.txt is empty, so generating random weights now
Layer #0 weights: 0.733110368251801 0.361183911561966 0.126834645867348 0.161393895745277 0.909261465072632 0.226619511842728 0.380449235439301 0.352994829416275 0.115502968430519 0.0717529132962227 
Layer #1 weights: 0.0768498256802559 0.678018510341644 0.552315771579742 0.542410433292389 0.357357650995255 0.219119817018509 0.489838540554047 0.104035511612892 0.515698492527008 0.153051629662514 
Layer #2 weights: 0.962239861488342 0.408186376094818 0.692775666713715 0.863975763320923 0.686723053455353 0.480713427066803 0.999526143074036 0.927104115486145 0.195105820894241 0.240063339471817 
Layer #3 weights: 0.929189562797546 0.665636479854584 0.908246397972107 0.000807166215963662 0.785570502281189 0.655321896076202 0.325137048959732 0.783649921417236 0.451953947544098 0.924783825874329 
Layer #4 weights: 0.825510025024414 0.616677224636078 0.416199624538422 0.70313435792923 0.463881552219391 0.695346772670746 0.69098824262619 0.229090720415115 0.822786808013916 0.390484869480133 
Layer #5 weights: 0.978289842605591 0.53621631860733 0.96140468120575 0.0258924998342991 0.563161313533783 0.164693370461464 0.464658677577972 0.537104904651642 0.926706910133362 0.920198798179626 
Categorical Layer #0 weights: 0.860626578330994 0.460591614246368 0.178070917725563 0.329576283693314 
Categorical Layer #1 weights: 0.481941282749176 0.715256869792938 0.29033163189888 0.294424444437027 
Categorical Layer #2 weights: 0.512860238552094 0.79130220413208 0.704261839389801 0.0496901348233223 
10/5/2016 1:22:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:23 PMStarting learning phase with deltaScore: 0.4
Modified index 0's learning in memoryPool to 0.08
Modified index 1's learning in memoryPool to 0.08
Modified index 2's learning in memoryPool to 0.08
Modified index 3's learning in memoryPool to 0.08
Modified index 4's learning in memoryPool to 0.08
Modified index 5's learning in memoryPool to 0.08
Modified index 6's learning in memoryPool to 0.08
Modified index 7's learning in memoryPool to 0.08
Modified index 8's learning in memoryPool to 0.08
10/5/2016 1:22:23 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 0, 0, 0.08
sum 0.891381931474596 distri 0.289020189454061
Using diff 0.379516259151886 and condRate 0.166666666666667
Changed category 0 weights from 
0.860626578330994 to 0.865686795006581
0.460591614246368 to 0.465651830921956
0.178070917725563 to 0.18313113440115
0.329576283693314 to 0.334636500368901
Changing layer 0's weights from 
0.733110368251801 to 0.738170584927388
0.361183911561966 to 0.366244128237553
0.126834645867348 to 0.131894862542935
0.161393895745277 to 0.166454112420865
0.909261465072632 to 0.914321681748219
0.226619511842728 to 0.231679728518315
0.380449235439301 to 0.385509452114888
0.352994829416275 to 0.358055046091862
0.115502968430519 to 0.120563185106106
0.0717529132962227 to 0.0768131299718098
Changing layer 1's weights from 
0.0768498256802559 to 0.0819100423558431
0.678018510341644 to 0.683078727017231
0.552315771579742 to 0.55737598825533
0.542410433292389 to 0.547470649967976
0.357357650995255 to 0.362417867670842
0.219119817018509 to 0.224180033694096
0.489838540554047 to 0.494898757229634
0.104035511612892 to 0.109095728288479
0.515698492527008 to 0.520758709202595
0.153051629662514 to 0.158111846338101
Changing layer 2's weights from 
0.962239861488342 to 0.967300078163929
0.408186376094818 to 0.413246592770405
0.692775666713715 to 0.697835883389302
0.863975763320923 to 0.86903597999651
0.686723053455353 to 0.69178327013094
0.480713427066803 to 0.48577364374239
0.999526143074036 to 1.00458635974962
0.927104115486145 to 0.932164332161732
0.195105820894241 to 0.200166037569828
0.240063339471817 to 0.245123556147404
Changing layer 3's weights from 
0.929189562797546 to 0.934249779473134
0.665636479854584 to 0.670696696530171
0.908246397972107 to 0.913306614647694
0.000807166215963662 to 0.00586738289155082
0.785570502281189 to 0.790630718956776
0.655321896076202 to 0.66038211275179
0.325137048959732 to 0.330197265635319
0.783649921417236 to 0.788710138092824
0.451953947544098 to 0.457014164219685
0.924783825874329 to 0.929844042549916
Changing layer 4's weights from 
0.825510025024414 to 0.830570241700001
0.616677224636078 to 0.621737441311665
0.416199624538422 to 0.421259841214009
0.70313435792923 to 0.708194574604817
0.463881552219391 to 0.468941768894978
0.695346772670746 to 0.700406989346333
0.69098824262619 to 0.696048459301777
0.229090720415115 to 0.234150937090703
0.822786808013916 to 0.827847024689503
0.390484869480133 to 0.39554508615572
Changing layer 5's weights from 
0.978289842605591 to 0.983350059281178
0.53621631860733 to 0.541276535282918
0.96140468120575 to 0.966464897881337
0.0258924998342991 to 0.0309527165098862
0.563161313533783 to 0.56822153020937
0.164693370461464 to 0.169753587137051
0.464658677577972 to 0.46971889425356
0.537104904651642 to 0.542165121327229
0.926706910133362 to 0.931767126808949
0.920198798179626 to 0.925259014855214
Trying to learn from memory 1, 2, 0.08
sum 0.891740318127947 distri 0.322603642401986
Using diff 0.346201596193974 and condRate 0.166666666666667
Changed category 2 weights from 
0.512860238552094 to 0.517476259731504
0.79130220413208 to 0.79591822531149
0.704261839389801 to 0.708877860569211
0.0496901348233223 to 0.0543061560027325
Changing layer 0's weights from 
0.738170584927388 to 0.742786606106798
0.366244128237553 to 0.370860149416963
0.131894862542935 to 0.136510883722345
0.166454112420865 to 0.171070133600275
0.914321681748219 to 0.918937702927629
0.231679728518315 to 0.236295749697725
0.385509452114888 to 0.390125473294298
0.358055046091862 to 0.362671067271272
0.120563185106106 to 0.125179206285516
0.0768131299718098 to 0.0814291511512201
Changing layer 1's weights from 
0.0819100423558431 to 0.0865260635352533
0.683078727017231 to 0.687694748196642
0.55737598825533 to 0.56199200943474
0.547470649967976 to 0.552086671147386
0.362417867670842 to 0.367033888850252
0.224180033694096 to 0.228796054873506
0.494898757229634 to 0.499514778409044
0.109095728288479 to 0.11371174946789
0.520758709202595 to 0.525374730382005
0.158111846338101 to 0.162727867517511
Changing layer 2's weights from 
0.967300078163929 to 0.97191609934334
0.413246592770405 to 0.417862613949815
0.697835883389302 to 0.702451904568712
0.86903597999651 to 0.87365200117592
0.69178327013094 to 0.69639929131035
0.48577364374239 to 0.4903896649218
1.00458635974962 to 1.00920238092903
0.932164332161732 to 0.936780353341142
0.200166037569828 to 0.204782058749239
0.245123556147404 to 0.249739577326814
Changing layer 3's weights from 
0.934249779473134 to 0.938865800652544
0.670696696530171 to 0.675312717709581
0.913306614647694 to 0.917922635827104
0.00586738289155082 to 0.010483404070961
0.790630718956776 to 0.795246740136186
0.66038211275179 to 0.6649981339312
0.330197265635319 to 0.334813286814729
0.788710138092824 to 0.793326159272234
0.457014164219685 to 0.461630185399095
0.929844042549916 to 0.934460063729326
Changing layer 4's weights from 
0.830570241700001 to 0.835186262879411
0.621737441311665 to 0.626353462491075
0.421259841214009 to 0.425875862393419
0.708194574604817 to 0.712810595784227
0.468941768894978 to 0.473557790074388
0.700406989346333 to 0.705023010525743
0.696048459301777 to 0.700664480481188
0.234150937090703 to 0.238766958270113
0.827847024689503 to 0.832463045868913
0.39554508615572 to 0.40016110733513
Changing layer 5's weights from 
0.983350059281178 to 0.987966080460588
0.541276535282918 to 0.545892556462328
0.966464897881337 to 0.971080919060747
0.0309527165098862 to 0.0355687376892964
0.56822153020937 to 0.57283755138878
0.169753587137051 to 0.174369608316461
0.46971889425356 to 0.47433491543297
0.542165121327229 to 0.546781142506639
0.931767126808949 to 0.936383147988359
0.925259014855214 to 0.929875036034624
Trying to learn from memory 2, 2, 0.08
sum 0.914650535228611 distri 0.330613268141467
Using diff 0.355374633279991 and condRate 0.166666666666667
Changed category 2 weights from 
0.517476259731504 to 0.522214588069327
0.79591822531149 to 0.800656553649314
0.708877860569211 to 0.713616188907034
0.0543061560027325 to 0.0590444843405558
Changing layer 0's weights from 
0.742786606106798 to 0.747524934444621
0.370860149416963 to 0.375598477754787
0.136510883722345 to 0.141249212060168
0.171070133600275 to 0.175808461938098
0.918937702927629 to 0.923676031265453
0.236295749697725 to 0.241034078035548
0.390125473294298 to 0.394863801632121
0.362671067271272 to 0.367409395609096
0.125179206285516 to 0.12991753462334
0.0814291511512201 to 0.0861674794890434
Changing layer 1's weights from 
0.0865260635352533 to 0.0912643918730766
0.687694748196642 to 0.692433076534465
0.56199200943474 to 0.566730337772563
0.552086671147386 to 0.55682499948521
0.367033888850252 to 0.371772217188075
0.228796054873506 to 0.23353438321133
0.499514778409044 to 0.504253106746867
0.11371174946789 to 0.118450077805713
0.525374730382005 to 0.530113058719829
0.162727867517511 to 0.167466195855334
Changing layer 2's weights from 
0.97191609934334 to 0.976654427681163
0.417862613949815 to 0.422600942287639
0.702451904568712 to 0.707190232906535
0.87365200117592 to 0.878390329513744
0.69639929131035 to 0.701137619648173
0.4903896649218 to 0.495127993259624
1.00920238092903 to 1.01394070926686
0.936780353341142 to 0.941518681678966
0.204782058749239 to 0.209520387087062
0.249739577326814 to 0.254477905664638
Changing layer 3's weights from 
0.938865800652544 to 0.943604128990367
0.675312717709581 to 0.680051046047404
0.917922635827104 to 0.922660964164928
0.010483404070961 to 0.0152217324087843
0.795246740136186 to 0.79998506847401
0.6649981339312 to 0.669736462269023
0.334813286814729 to 0.339551615152553
0.793326159272234 to 0.798064487610057
0.461630185399095 to 0.466368513736919
0.934460063729326 to 0.939198392067149
Changing layer 4's weights from 
0.835186262879411 to 0.839924591217235
0.626353462491075 to 0.631091790828899
0.425875862393419 to 0.430614190731242
0.712810595784227 to 0.71754892412205
0.473557790074388 to 0.478296118412212
0.705023010525743 to 0.709761338863567
0.700664480481188 to 0.705402808819011
0.238766958270113 to 0.243505286607936
0.832463045868913 to 0.837201374206737
0.40016110733513 to 0.404899435672954
Changing layer 5's weights from 
0.987966080460588 to 0.992704408798411
0.545892556462328 to 0.550630884800151
0.971080919060747 to 0.97581924739857
0.0355687376892964 to 0.0403070660271198
0.57283755138878 to 0.577575879726604
0.174369608316461 to 0.179107936654285
0.47433491543297 to 0.479073243770793
0.546781142506639 to 0.551519470844463
0.936383147988359 to 0.941121476326182
0.929875036034624 to 0.934613364372447
Trying to learn from memory 3, 0, 0.08
sum 0.899160181978086 distri 0.291512340656308
Using diff 0.382857795827256 and condRate 0.166666666666667
Changed category 0 weights from 
0.865686795006581 to 0.87079156550351
0.465651830921956 to 0.470756601418885
0.18313113440115 to 0.18823590489808
0.334636500368901 to 0.33974127086583
Changing layer 0's weights from 
0.747524934444621 to 0.752629704941551
0.375598477754787 to 0.380703248251716
0.141249212060168 to 0.146353982557098
0.175808461938098 to 0.180913232435028
0.923676031265453 to 0.928780801762382
0.241034078035548 to 0.246138848532478
0.394863801632121 to 0.399968572129051
0.367409395609096 to 0.372514166106025
0.12991753462334 to 0.135022305120269
0.0861674794890434 to 0.0912722499859729
Changing layer 1's weights from 
0.0912643918730766 to 0.0963691623700061
0.692433076534465 to 0.697537847031395
0.566730337772563 to 0.571835108269493
0.55682499948521 to 0.561929769982139
0.371772217188075 to 0.376876987685005
0.23353438321133 to 0.238639153708259
0.504253106746867 to 0.509357877243797
0.118450077805713 to 0.123554848302642
0.530113058719829 to 0.535217829216758
0.167466195855334 to 0.172570966352264
Changing layer 2's weights from 
0.976654427681163 to 0.981759198178093
0.422600942287639 to 0.427705712784568
0.707190232906535 to 0.712295003403465
0.878390329513744 to 0.883495100010673
0.701137619648173 to 0.706242390145103
0.495127993259624 to 0.500232763756553
1.01394070926686 to 1.01904547976379
0.941518681678966 to 0.946623452175895
0.209520387087062 to 0.214625157583992
0.254477905664638 to 0.259582676161567
Changing layer 3's weights from 
0.943604128990367 to 0.948708899487297
0.680051046047404 to 0.685155816544334
0.922660964164928 to 0.927765734661857
0.0152217324087843 to 0.0203265029057139
0.79998506847401 to 0.805089838970939
0.669736462269023 to 0.674841232765953
0.339551615152553 to 0.344656385649482
0.798064487610057 to 0.803169258106987
0.466368513736919 to 0.471473284233848
0.939198392067149 to 0.944303162564079
Changing layer 4's weights from 
0.839924591217235 to 0.845029361714164
0.631091790828899 to 0.636196561325828
0.430614190731242 to 0.435718961228172
0.71754892412205 to 0.72265369461898
0.478296118412212 to 0.483400888909141
0.709761338863567 to 0.714866109360496
0.705402808819011 to 0.71050757931594
0.243505286607936 to 0.248610057104866
0.837201374206737 to 0.842306144703666
0.404899435672954 to 0.410004206169883
Changing layer 5's weights from 
0.992704408798411 to 0.997809179295341
0.550630884800151 to 0.555735655297081
0.97581924739857 to 0.9809240178955
0.0403070660271198 to 0.0454118365240493
0.577575879726604 to 0.582680650223533
0.179107936654285 to 0.184212707151214
0.479073243770793 to 0.484178014267723
0.551519470844463 to 0.556624241341392
0.941121476326182 to 0.946226246823112
0.934613364372447 to 0.939718134869377
Trying to learn from memory 3, 0, 0.08
sum 0.899160181978086 distri 0.291512340656308
Using diff 0.382857795827256 and condRate 0.166666666666667
Changed category 0 weights from 
0.87079156550351 to 0.87589633600044
0.470756601418885 to 0.475861371915815
0.18823590489808 to 0.193340675395009
0.33974127086583 to 0.34484604136276
Changing layer 0's weights from 
0.752629704941551 to 0.75773447543848
0.380703248251716 to 0.385808018748646
0.146353982557098 to 0.151458753054028
0.180913232435028 to 0.186018002931957
0.928780801762382 to 0.933885572259312
0.246138848532478 to 0.251243619029407
0.399968572129051 to 0.40507334262598
0.372514166106025 to 0.377618936602955
0.135022305120269 to 0.140127075617199
0.0912722499859729 to 0.0963770204829025
Changing layer 1's weights from 
0.0963691623700061 to 0.101473932866936
0.697537847031395 to 0.702642617528324
0.571835108269493 to 0.576939878766422
0.561929769982139 to 0.567034540479069
0.376876987685005 to 0.381981758181934
0.238639153708259 to 0.243743924205189
0.509357877243797 to 0.514462647740726
0.123554848302642 to 0.128659618799572
0.535217829216758 to 0.540322599713688
0.172570966352264 to 0.177675736849194
Changing layer 2's weights from 
0.981759198178093 to 0.986863968675022
0.427705712784568 to 0.432810483281498
0.712295003403465 to 0.717399773900394
0.883495100010673 to 0.888599870507603
0.706242390145103 to 0.711347160642033
0.500232763756553 to 0.505337534253483
1.01904547976379 to 1.02415025026072
0.946623452175895 to 0.951728222672825
0.214625157583992 to 0.219729928080921
0.259582676161567 to 0.264687446658497
Changing layer 3's weights from 
0.948708899487297 to 0.953813669984226
0.685155816544334 to 0.690260587041264
0.927765734661857 to 0.932870505158787
0.0203265029057139 to 0.0254312734026435
0.805089838970939 to 0.810194609467869
0.674841232765953 to 0.679946003262882
0.344656385649482 to 0.349761156146412
0.803169258106987 to 0.808274028603916
0.471473284233848 to 0.476578054730778
0.944303162564079 to 0.949407933061008
Changing layer 4's weights from 
0.845029361714164 to 0.850134132211094
0.636196561325828 to 0.641301331822758
0.435718961228172 to 0.440823731725101
0.72265369461898 to 0.72775846511591
0.483400888909141 to 0.488505659406071
0.714866109360496 to 0.719970879857426
0.71050757931594 to 0.71561234981287
0.248610057104866 to 0.253714827601795
0.842306144703666 to 0.847410915200596
0.410004206169883 to 0.415108976666813
Changing layer 5's weights from 
0.997809179295341 to 1.00291394979227
0.555735655297081 to 0.56084042579401
0.9809240178955 to 0.986028788392429
0.0454118365240493 to 0.0505166070209789
0.582680650223533 to 0.587785420720463
0.184212707151214 to 0.189317477648144
0.484178014267723 to 0.489282784764652
0.556624241341392 to 0.561729011838322
0.946226246823112 to 0.951331017320042
0.939718134869377 to 0.944822905366306
Trying to learn from memory 3, 1, 0.08
sum 0.899160181978086 distri 0.28242659591926
Using diff 0.391943540564304 and condRate 0.166666666666667
Changed category 1 weights from 
0.481941282749176 to 0.487167196506558
0.715256869792938 to 0.720482783550321
0.29033163189888 to 0.295557545656262
0.294424444437027 to 0.299650358194409
Changing layer 0's weights from 
0.75773447543848 to 0.762960389195863
0.385808018748646 to 0.391033932506028
0.151458753054028 to 0.15668466681141
0.186018002931957 to 0.19124391668934
0.933885572259312 to 0.939111486016694
0.251243619029407 to 0.25646953278679
0.40507334262598 to 0.410299256383363
0.377618936602955 to 0.382844850360337
0.140127075617199 to 0.145352989374581
0.0963770204829025 to 0.101602934240285
Changing layer 1's weights from 
0.101473932866936 to 0.106699846624318
0.702642617528324 to 0.707868531285707
0.576939878766422 to 0.582165792523805
0.567034540479069 to 0.572260454236451
0.381981758181934 to 0.387207671939317
0.243743924205189 to 0.248969837962571
0.514462647740726 to 0.519688561498109
0.128659618799572 to 0.133885532556954
0.540322599713688 to 0.54554851347107
0.177675736849194 to 0.182901650606576
Changing layer 2's weights from 
0.986863968675022 to 0.992089882432405
0.432810483281498 to 0.43803639703888
0.717399773900394 to 0.722625687657777
0.888599870507603 to 0.893825784264985
0.711347160642033 to 0.716573074399415
0.505337534253483 to 0.510563448010865
1.02415025026072 to 1.0293761640181
0.951728222672825 to 0.956954136430207
0.219729928080921 to 0.224955841838304
0.264687446658497 to 0.269913360415879
Changing layer 3's weights from 
0.953813669984226 to 0.959039583741609
0.690260587041264 to 0.695486500798646
0.932870505158787 to 0.938096418916169
0.0254312734026435 to 0.0306571871600259
0.810194609467869 to 0.815420523225251
0.679946003262882 to 0.685171917020265
0.349761156146412 to 0.354987069903794
0.808274028603916 to 0.813499942361299
0.476578054730778 to 0.48180396848816
0.949407933061008 to 0.954633846818391
Changing layer 4's weights from 
0.850134132211094 to 0.855360045968476
0.641301331822758 to 0.64652724558014
0.440823731725101 to 0.446049645482484
0.72775846511591 to 0.732984378873292
0.488505659406071 to 0.493731573163453
0.719970879857426 to 0.725196793614808
0.71561234981287 to 0.720838263570252
0.253714827601795 to 0.258940741359178
0.847410915200596 to 0.852636828957978
0.415108976666813 to 0.420334890424195
Changing layer 5's weights from 
1.00291394979227 to 1.00813986354965
0.56084042579401 to 0.566066339551393
0.986028788392429 to 0.991254702149812
0.0505166070209789 to 0.0557425207783614
0.587785420720463 to 0.593011334477845
0.189317477648144 to 0.194543391405526
0.489282784764652 to 0.494508698522035
0.561729011838322 to 0.566954925595704
0.951331017320042 to 0.956556931077424
0.944822905366306 to 0.950048819123689
Trying to learn from memory 4, 1, 0.08
sum 0.905019905003734 distri 0.284482175699207
Using diff 0.394282753053593 and condRate 0.166666666666667
Changed category 1 weights from 
0.487167196506558 to 0.492424299763101
0.720482783550321 to 0.725739886806863
0.295557545656262 to 0.300814648912805
0.299650358194409 to 0.304907461450952
Changing layer 0's weights from 
0.762960389195863 to 0.768217492452405
0.391033932506028 to 0.396291035762571
0.15668466681141 to 0.161941770067952
0.19124391668934 to 0.196501019945882
0.939111486016694 to 0.944368589273237
0.25646953278679 to 0.261726636043332
0.410299256383363 to 0.415556359639905
0.382844850360337 to 0.38810195361688
0.145352989374581 to 0.150610092631124
0.101602934240285 to 0.106860037496827
Changing layer 1's weights from 
0.106699846624318 to 0.111956949880861
0.707868531285707 to 0.713125634542249
0.582165792523805 to 0.587422895780347
0.572260454236451 to 0.577517557492994
0.387207671939317 to 0.392464775195859
0.248969837962571 to 0.254226941219114
0.519688561498109 to 0.524945664754651
0.133885532556954 to 0.139142635813497
0.54554851347107 to 0.550805616727613
0.182901650606576 to 0.188158753863118
Changing layer 2's weights from 
0.992089882432405 to 0.997346985688947
0.43803639703888 to 0.443293500295423
0.722625687657777 to 0.727882790914319
0.893825784264985 to 0.899082887521528
0.716573074399415 to 0.721830177655958
0.510563448010865 to 0.515820551267408
1.0293761640181 to 1.03463326727464
0.956954136430207 to 0.96221123968675
0.224955841838304 to 0.230212945094846
0.269913360415879 to 0.275170463672422
Changing layer 3's weights from 
0.959039583741609 to 0.964296686998151
0.695486500798646 to 0.700743604055189
0.938096418916169 to 0.943353522172712
0.0306571871600259 to 0.0359142904165684
0.815420523225251 to 0.820677626481794
0.685171917020265 to 0.690429020276807
0.354987069903794 to 0.360244173160337
0.813499942361299 to 0.818757045617841
0.48180396848816 to 0.487061071744703
0.954633846818391 to 0.959890950074933
Changing layer 4's weights from 
0.855360045968476 to 0.860617149225019
0.64652724558014 to 0.651784348836683
0.446049645482484 to 0.451306748739026
0.732984378873292 to 0.738241482129835
0.493731573163453 to 0.498988676419996
0.725196793614808 to 0.730453896871351
0.720838263570252 to 0.726095366826795
0.258940741359178 to 0.26419784461572
0.852636828957978 to 0.857893932214521
0.420334890424195 to 0.425591993680738
Changing layer 5's weights from 
1.00813986354965 to 1.0133969668062
0.566066339551393 to 0.571323442807935
0.991254702149812 to 0.996511805406354
0.0557425207783614 to 0.0609996240349038
0.593011334477845 to 0.598268437734388
0.194543391405526 to 0.199800494662069
0.494508698522035 to 0.499765801778577
0.566954925595704 to 0.572212028852247
0.956556931077424 to 0.961814034333967
0.950048819123689 to 0.955305922380231
Trying to learn from memory 5, 0, 0.08
sum 0.927305939016647 distri 0.300022078704861
Using diff 0.395457375557624 and condRate 0.166666666666667
Changed category 0 weights from 
0.87589633600044 to 0.88116910089002
0.475861371915815 to 0.481134136805394
0.193340675395009 to 0.198613440284589
0.34484604136276 to 0.350118806252339
Changing layer 0's weights from 
0.768217492452405 to 0.773490257341985
0.396291035762571 to 0.40156380065215
0.161941770067952 to 0.167214534957532
0.196501019945882 to 0.201773784835462
0.944368589273237 to 0.949641354162816
0.261726636043332 to 0.266999400932912
0.415556359639905 to 0.420829124529485
0.38810195361688 to 0.393374718506459
0.150610092631124 to 0.155882857520703
0.106860037496827 to 0.112132802386407
Changing layer 1's weights from 
0.111956949880861 to 0.11722971477044
0.713125634542249 to 0.718398399431829
0.587422895780347 to 0.592695660669927
0.577517557492994 to 0.582790322382573
0.392464775195859 to 0.397737540085439
0.254226941219114 to 0.259499706108693
0.524945664754651 to 0.530218429644231
0.139142635813497 to 0.144415400703076
0.550805616727613 to 0.556078381617192
0.188158753863118 to 0.193431518752698
Changing layer 2's weights from 
0.997346985688947 to 1.00261975057853
0.443293500295423 to 0.448566265185002
0.727882790914319 to 0.733155555803899
0.899082887521528 to 0.904355652411107
0.721830177655958 to 0.727102942545537
0.515820551267408 to 0.521093316156987
1.03463326727464 to 1.03990603216422
0.96221123968675 to 0.967484004576329
0.230212945094846 to 0.235485709984426
0.275170463672422 to 0.280443228562001
Changing layer 3's weights from 
0.964296686998151 to 0.969569451887731
0.700743604055189 to 0.706016368944768
0.943353522172712 to 0.948626287062291
0.0359142904165684 to 0.0411870553061479
0.820677626481794 to 0.825950391371373
0.690429020276807 to 0.695701785166387
0.360244173160337 to 0.365516938049916
0.818757045617841 to 0.824029810507421
0.487061071744703 to 0.492333836634282
0.959890950074933 to 0.965163714964513
Changing layer 4's weights from 
0.860617149225019 to 0.865889914114598
0.651784348836683 to 0.657057113726262
0.451306748739026 to 0.456579513628606
0.738241482129835 to 0.743514247019414
0.498988676419996 to 0.504261441309575
0.730453896871351 to 0.73572666176093
0.726095366826795 to 0.731368131716374
0.26419784461572 to 0.2694706095053
0.857893932214521 to 0.8631666971041
0.425591993680738 to 0.430864758570317
Changing layer 5's weights from 
1.0133969668062 to 1.01866973169578
0.571323442807935 to 0.576596207697515
0.996511805406354 to 1.00178457029593
0.0609996240349038 to 0.0662723889244834
0.598268437734388 to 0.603541202623967
0.199800494662069 to 0.205073259551648
0.499765801778577 to 0.505038566668157
0.572212028852247 to 0.577484793741826
0.961814034333967 to 0.967086799223546
0.955305922380231 to 0.960578687269811
Trying to learn from memory 6, 2, 0.08
sum 0.955658036939844 distri 0.346334419485386
Using diff 0.370409108219497 and condRate 0.166666666666667
Changed category 2 weights from 
0.522214588069327 to 0.52715337606853
0.800656553649314 to 0.805595341648516
0.713616188907034 to 0.718554976906237
0.0590444843405558 to 0.0639832723397586
Changing layer 0's weights from 
0.773490257341985 to 0.778429045341188
0.40156380065215 to 0.406502588651353
0.167214534957532 to 0.172153322956735
0.201773784835462 to 0.206712572834664
0.949641354162816 to 0.954580142162019
0.266999400932912 to 0.271938188932115
0.420829124529485 to 0.425767912528688
0.393374718506459 to 0.398313506505662
0.155882857520703 to 0.160821645519906
0.112132802386407 to 0.11707159038561
Changing layer 1's weights from 
0.11722971477044 to 0.122168502769643
0.718398399431829 to 0.723337187431031
0.592695660669927 to 0.59763444866913
0.582790322382573 to 0.587729110381776
0.397737540085439 to 0.402676328084642
0.259499706108693 to 0.264438494107896
0.530218429644231 to 0.535157217643434
0.144415400703076 to 0.149354188702279
0.556078381617192 to 0.561017169616395
0.193431518752698 to 0.198370306751901
Changing layer 2's weights from 
1.00261975057853 to 1.00755853857773
0.448566265185002 to 0.453505053184205
0.733155555803899 to 0.738094343803102
0.904355652411107 to 0.90929444041031
0.727102942545537 to 0.73204173054474
0.521093316156987 to 0.52603210415619
1.03990603216422 to 1.04484482016342
0.967484004576329 to 0.972422792575532
0.235485709984426 to 0.240424497983628
0.280443228562001 to 0.285382016561204
Changing layer 3's weights from 
0.969569451887731 to 0.974508239886933
0.706016368944768 to 0.710955156943971
0.948626287062291 to 0.953565075061494
0.0411870553061479 to 0.0461258433053507
0.825950391371373 to 0.830889179370576
0.695701785166387 to 0.700640573165589
0.365516938049916 to 0.370455726049119
0.824029810507421 to 0.828968598506623
0.492333836634282 to 0.497272624633485
0.965163714964513 to 0.970102502963716
Changing layer 4's weights from 
0.865889914114598 to 0.870828702113801
0.657057113726262 to 0.661995901725465
0.456579513628606 to 0.461518301627809
0.743514247019414 to 0.748453035018617
0.504261441309575 to 0.509200229308778
0.73572666176093 to 0.740665449760133
0.731368131716374 to 0.736306919715577
0.2694706095053 to 0.274409397504502
0.8631666971041 to 0.868105485103303
0.430864758570317 to 0.43580354656952
Changing layer 5's weights from 
1.01866973169578 to 1.02360851969498
0.576596207697515 to 0.581534995696717
1.00178457029593 to 1.00672335829514
0.0662723889244834 to 0.0712111769236861
0.603541202623967 to 0.60847999062317
0.205073259551648 to 0.210012047550851
0.505038566668157 to 0.509977354667359
0.577484793741826 to 0.582423581741029
0.967086799223546 to 0.972025587222749
0.960578687269811 to 0.965517475269014
10/5/2016 1:22:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:52 PMStarting learning phase with deltaScore: -5
Modified index 0's learning in memoryPool to -1
Modified index 1's learning in memoryPool to -1
Modified index 2's learning in memoryPool to -1
Modified index 3's learning in memoryPool to -1
Modified index 4's learning in memoryPool to -1
Modified index 5's learning in memoryPool to -1
Modified index 6's learning in memoryPool to -1
Modified index 7's learning in memoryPool to -1
Modified index 8's learning in memoryPool to -1
Modified index 9's learning in memoryPool to -1
Modified index 10's learning in memoryPool to -1
Modified index 11's learning in memoryPool to -1
Modified index 12's learning in memoryPool to -1
Modified index 13's learning in memoryPool to -1
Modified index 14's learning in memoryPool to -1
Modified index 15's learning in memoryPool to -1
Modified index 16's learning in memoryPool to -1
Modified index 17's learning in memoryPool to -1
10/5/2016 1:22:52 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 7, 0, -1
sum 1.1275806108138 distri 0.371461555895052
Using diff 0.4742239022153 and condRate 0.166666666666667
Changed category 0 weights from 
0.88116910089002 to 0.802131783854136
0.481134136805394 to 0.402096819769511
0.198613440284589 to 0.119576123248705
0.350118806252339 to 0.271081489216456
Changing layer 0's weights from 
0.778429045341188 to 0.699391728305304
0.406502588651353 to 0.32746527161547
0.172153322956735 to 0.0931160059208513
0.206712572834664 to 0.127675255798781
0.954580142162019 to 0.875542825126135
0.271938188932115 to 0.192900871896231
0.425767912528688 to 0.346730595492804
0.398313506505662 to 0.319276189469779
0.160821645519906 to 0.0817843284840227
0.11707159038561 to 0.0380342733497263
Changing layer 1's weights from 
0.122168502769643 to 0.0431311857337595
0.723337187431031 to 0.644299870395148
0.59763444866913 to 0.518597131633246
0.587729110381776 to 0.508691793345893
0.402676328084642 to 0.323639011048758
0.264438494107896 to 0.185401177072012
0.535157217643434 to 0.45611990060755
0.149354188702279 to 0.0703168716663958
0.561017169616395 to 0.481979852580512
0.198370306751901 to 0.119332989716017
Changing layer 2's weights from 
1.00755853857773 to 0.928521221541846
0.453505053184205 to 0.374467736148322
0.738094343803102 to 0.659057026767218
0.90929444041031 to 0.830257123374426
0.73204173054474 to 0.653004413508856
0.52603210415619 to 0.446994787120307
1.04484482016342 to 0.965807503127539
0.972422792575532 to 0.893385475539649
0.240424497983628 to 0.161387180947745
0.285382016561204 to 0.206344699525321
Changing layer 3's weights from 
0.974508239886933 to 0.89547092285105
0.710955156943971 to 0.631917839908087
0.953565075061494 to 0.874527758025611
0.0461258433053507 to -0.0329114737305327
0.830889179370576 to 0.751851862334693
0.700640573165589 to 0.621603256129706
0.370455726049119 to 0.291418409013236
0.828968598506623 to 0.74993128147074
0.497272624633485 to 0.418235307597602
0.970102502963716 to 0.891065185927832
Changing layer 4's weights from 
0.870828702113801 to 0.791791385077918
0.661995901725465 to 0.582958584689582
0.461518301627809 to 0.382480984591925
0.748453035018617 to 0.669415717982733
0.509200229308778 to 0.430162912272895
0.740665449760133 to 0.661628132724249
0.736306919715577 to 0.657269602679694
0.274409397504502 to 0.195372080468619
0.868105485103303 to 0.78906816806742
0.43580354656952 to 0.356766229533637
Changing layer 5's weights from 
1.02360851969498 to 0.944571202659095
0.581534995696717 to 0.502497678660834
1.00672335829514 to 0.927686041259253
0.0712111769236861 to -0.00782614011219727
0.60847999062317 to 0.529442673587287
0.210012047550851 to 0.130974730514968
0.509977354667359 to 0.430940037631476
0.582423581741029 to 0.503386264705145
0.972025587222749 to 0.892988270186865
0.965517475269014 to 0.88648015823313
Trying to learn from memory 8, 0, -1
sum 1.07680059631608 distri 0.354348301371219
Using diff 0.453252145865841 and condRate 0.166666666666667
Changed category 0 weights from 
0.802131783854136 to 0.726589759543163
0.402096819769511 to 0.326554795458537
0.119576123248705 to 0.0440340989377319
0.271081489216456 to 0.195539464905483
Changing layer 0's weights from 
0.699391728305304 to 0.623849703994331
0.32746527161547 to 0.251923247304496
0.0931160059208513 to 0.0175739816098778
0.127675255798781 to 0.0521332314878075
0.875542825126135 to 0.800000800815162
0.192900871896231 to 0.117358847585258
0.346730595492804 to 0.271188571181831
0.319276189469779 to 0.243734165158805
0.0817843284840227 to 0.00624230417304922
0.0380342733497263 to -0.0375077509612471
Changing layer 1's weights from 
0.0431311857337595 to -0.0324108385772139
0.644299870395148 to 0.568757846084174
0.518597131633246 to 0.443055107322273
0.508691793345893 to 0.433149769034919
0.323639011048758 to 0.248096986737785
0.185401177072012 to 0.109859152761039
0.45611990060755 to 0.380577876296577
0.0703168716663958 to -0.00522515264457771
0.481979852580512 to 0.406437828269538
0.119332989716017 to 0.0437909654050438
Changing layer 2's weights from 
0.928521221541846 to 0.852979197230873
0.374467736148322 to 0.298925711837348
0.659057026767218 to 0.583515002456245
0.830257123374426 to 0.754715099063453
0.653004413508856 to 0.577462389197883
0.446994787120307 to 0.371452762809333
0.965807503127539 to 0.890265478816566
0.893385475539649 to 0.817843451228675
0.161387180947745 to 0.0858451566367714
0.206344699525321 to 0.130802675214347
Changing layer 3's weights from 
0.89547092285105 to 0.819928898540077
0.631917839908087 to 0.556375815597114
0.874527758025611 to 0.798985733714637
-0.0329114737305327 to -0.108453498041506
0.751851862334693 to 0.676309838023719
0.621603256129706 to 0.546061231818733
0.291418409013236 to 0.215876384702262
0.74993128147074 to 0.674389257159767
0.418235307597602 to 0.342693283286628
0.891065185927832 to 0.815523161616859
Changing layer 4's weights from 
0.791791385077918 to 0.716249360766944
0.582958584689582 to 0.507416560378608
0.382480984591925 to 0.306938960280952
0.669415717982733 to 0.59387369367176
0.430162912272895 to 0.354620887961921
0.661628132724249 to 0.586086108413276
0.657269602679694 to 0.58172757836872
0.195372080468619 to 0.119830056157646
0.78906816806742 to 0.713526143756446
0.356766229533637 to 0.281224205222663
Changing layer 5's weights from 
0.944571202659095 to 0.869029178348121
0.502497678660834 to 0.42695565434986
0.927686041259253 to 0.85214401694828
-0.00782614011219727 to -0.0833681644231708
0.529442673587287 to 0.453900649276313
0.130974730514968 to 0.055432706203994
0.430940037631476 to 0.355398013320503
0.503386264705145 to 0.427844240394172
0.892988270186865 to 0.817446245875892
0.88648015823313 to 0.810938133922157
Trying to learn from memory 8, 0, -1
sum 1.07680059631608 distri 0.354348301371219
Using diff 0.453252145865841 and condRate 0.166666666666667
Changed category 0 weights from 
0.726589759543163 to 0.651047735232189
0.326554795458537 to 0.251012771147564
0.0440340989377319 to -0.0315079253732415
0.195539464905483 to 0.119997440594509
Changing layer 0's weights from 
0.623849703994331 to 0.548307679683357
0.251923247304496 to 0.176381222993523
0.0175739816098778 to -0.0579680427010957
0.0521332314878075 to -0.023408792823166
0.800000800815162 to 0.724458776504189
0.117358847585258 to 0.0418168232742843
0.271188571181831 to 0.195646546870857
0.243734165158805 to 0.168192140847832
0.00624230417304922 to -0.0692997201379243
-0.0375077509612471 to -0.113049775272221
Changing layer 1's weights from 
-0.0324108385772139 to -0.107952862888187
0.568757846084174 to 0.493215821773201
0.443055107322273 to 0.367513083011299
0.433149769034919 to 0.357607744723946
0.248096986737785 to 0.172554962426811
0.109859152761039 to 0.0343171284500655
0.380577876296577 to 0.305035851985603
-0.00522515264457771 to -0.0807671769555512
0.406437828269538 to 0.330895803958565
0.0437909654050438 to -0.0317510589059296
Changing layer 2's weights from 
0.852979197230873 to 0.777437172919899
0.298925711837348 to 0.223383687526375
0.583515002456245 to 0.507972978145271
0.754715099063453 to 0.67917307475248
0.577462389197883 to 0.50192036488691
0.371452762809333 to 0.29591073849836
0.890265478816566 to 0.814723454505593
0.817843451228675 to 0.742301426917702
0.0858451566367714 to 0.010303132325798
0.130802675214347 to 0.0552606509033737
Changing layer 3's weights from 
0.819928898540077 to 0.744386874229103
0.556375815597114 to 0.48083379128614
0.798985733714637 to 0.723443709403664
-0.108453498041506 to -0.18399552235248
0.676309838023719 to 0.600767813712746
0.546061231818733 to 0.470519207507759
0.215876384702262 to 0.140334360391289
0.674389257159767 to 0.598847232848793
0.342693283286628 to 0.267151258975655
0.815523161616859 to 0.739981137305885
Changing layer 4's weights from 
0.716249360766944 to 0.640707336455971
0.507416560378608 to 0.431874536067635
0.306938960280952 to 0.231396935969978
0.59387369367176 to 0.518331669360786
0.354620887961921 to 0.279078863650948
0.586086108413276 to 0.510544084102303
0.58172757836872 to 0.506185554057747
0.119830056157646 to 0.044288031846672
0.713526143756446 to 0.637984119445473
0.281224205222663 to 0.20568218091169
Changing layer 5's weights from 
0.869029178348121 to 0.793487154037148
0.42695565434986 to 0.351413630038887
0.85214401694828 to 0.776601992637306
-0.0833681644231708 to -0.158910188734144
0.453900649276313 to 0.37835862496534
0.055432706203994 to -0.0201093181069794
0.355398013320503 to 0.279855989009529
0.427844240394172 to 0.352302216083199
0.817446245875892 to 0.741904221564919
0.810938133922157 to 0.735396109611183
Trying to learn from memory 8, 0, -1
sum 1.07680059631608 distri 0.354348301371219
Using diff 0.453252145865841 and condRate 0.166666666666667
Changed category 0 weights from 
0.651047735232189 to 0.575505710921216
0.251012771147564 to 0.17547074683659
-0.0315079253732415 to -0.107049949684215
0.119997440594509 to 0.0444554162835356
Changing layer 0's weights from 
0.548307679683357 to 0.472765655372384
0.176381222993523 to 0.100839198682549
-0.0579680427010957 to -0.133510067012069
-0.023408792823166 to -0.0989508171341394
0.724458776504189 to 0.648916752193215
0.0418168232742843 to -0.0337252010366892
0.195646546870857 to 0.120104522559884
0.168192140847832 to 0.0926501165368583
-0.0692997201379243 to -0.144841744448898
-0.113049775272221 to -0.188591799583194
Changing layer 1's weights from 
-0.107952862888187 to -0.183494887199161
0.493215821773201 to 0.417673797462228
0.367513083011299 to 0.291971058700326
0.357607744723946 to 0.282065720412972
0.172554962426811 to 0.0970129381158377
0.0343171284500655 to -0.041224895860908
0.305035851985603 to 0.22949382767463
-0.0807671769555512 to -0.156309201266525
0.330895803958565 to 0.255353779647591
-0.0317510589059296 to -0.107293083216903
Changing layer 2's weights from 
0.777437172919899 to 0.701895148608926
0.223383687526375 to 0.147841663215401
0.507972978145271 to 0.432430953834298
0.67917307475248 to 0.603631050441506
0.50192036488691 to 0.426378340575936
0.29591073849836 to 0.220368714187386
0.814723454505593 to 0.739181430194619
0.742301426917702 to 0.666759402606728
0.010303132325798 to -0.0652388919851755
0.0552606509033737 to -0.0202813734075998
Changing layer 3's weights from 
0.744386874229103 to 0.66884484991813
0.48083379128614 to 0.405291766975167
0.723443709403664 to 0.64790168509269
-0.18399552235248 to -0.259537546663453
0.600767813712746 to 0.525225789401772
0.470519207507759 to 0.394977183196786
0.140334360391289 to 0.0647923360803153
0.598847232848793 to 0.52330520853782
0.267151258975655 to 0.191609234664681
0.739981137305885 to 0.664439112994912
Changing layer 4's weights from 
0.640707336455971 to 0.565165312144997
0.431874536067635 to 0.356332511756661
0.231396935969978 to 0.155854911659005
0.518331669360786 to 0.442789645049813
0.279078863650948 to 0.203536839339974
0.510544084102303 to 0.435002059791329
0.506185554057747 to 0.430643529746773
0.044288031846672 to -0.0312539924643015
0.637984119445473 to 0.562442095134499
0.20568218091169 to 0.130140156600716
Changing layer 5's weights from 
0.793487154037148 to 0.717945129726174
0.351413630038887 to 0.275871605727914
0.776601992637306 to 0.701059968326333
-0.158910188734144 to -0.234452213045118
0.37835862496534 to 0.302816600654366
-0.0201093181069794 to -0.0956513424179529
0.279855989009529 to 0.204313964698556
0.352302216083199 to 0.276760191772225
0.741904221564919 to 0.666362197253945
0.735396109611183 to 0.65985408530021
Trying to learn from memory 8, 2, -1
sum 1.07680059631608 distri 0.387339081365257
Using diff 0.420261365871802 and condRate 0.166666666666667
Changed category 2 weights from 
0.52715337606853 to 0.457109815089896
0.805595341648516 to 0.735551780669883
0.718554976906237 to 0.648511415927604
0.0639832723397586 to -0.00606028863887509
Changing layer 0's weights from 
0.472765655372384 to 0.40272209439375
0.100839198682549 to 0.0307956377039155
-0.133510067012069 to -0.203553627990703
-0.0989508171341394 to -0.168994378112773
0.648916752193215 to 0.578873191214581
-0.0337252010366892 to -0.103768762015323
0.120104522559884 to 0.05006096158125
0.0926501165368583 to 0.0226065555582246
-0.144841744448898 to -0.214885305427531
-0.188591799583194 to -0.258635360561828
Changing layer 1's weights from 
-0.183494887199161 to -0.253538448177795
0.417673797462228 to 0.347630236483594
0.291971058700326 to 0.221927497721692
0.282065720412972 to 0.212022159434339
0.0970129381158377 to 0.0269693771372041
-0.041224895860908 to -0.111268456839542
0.22949382767463 to 0.159450266695996
-0.156309201266525 to -0.226352762245158
0.255353779647591 to 0.185310218668958
-0.107293083216903 to -0.177336644195537
Changing layer 2's weights from 
0.701895148608926 to 0.631851587630292
0.147841663215401 to 0.0777981022367676
0.432430953834298 to 0.362387392855664
0.603631050441506 to 0.533587489462872
0.426378340575936 to 0.356334779597302
0.220368714187386 to 0.150325153208753
0.739181430194619 to 0.669137869215985
0.666759402606728 to 0.596715841628095
-0.0652388919851755 to -0.135282452963809
-0.0202813734075998 to -0.0903249343862334
Changing layer 3's weights from 
0.66884484991813 to 0.598801288939496
0.405291766975167 to 0.335248205996533
0.64790168509269 to 0.577858124114056
-0.259537546663453 to -0.329581107642087
0.525225789401772 to 0.455182228423139
0.394977183196786 to 0.324933622218152
0.0647923360803153 to -0.0052512248983184
0.52330520853782 to 0.453261647559186
0.191609234664681 to 0.121565673686047
0.664439112994912 to 0.594395552016278
Changing layer 4's weights from 
0.565165312144997 to 0.495121751166364
0.356332511756661 to 0.286288950778027
0.155854911659005 to 0.0858113506803711
0.442789645049813 to 0.372746084071179
0.203536839339974 to 0.13349327836134
0.435002059791329 to 0.364958498812695
0.430643529746773 to 0.36059996876814
-0.0312539924643015 to -0.101297553442935
0.562442095134499 to 0.492398534155866
0.130140156600716 to 0.0600965956220826
Changing layer 5's weights from 
0.717945129726174 to 0.647901568747541
0.275871605727914 to 0.20582804474928
0.701059968326333 to 0.631016407347699
-0.234452213045118 to -0.304495774023751
0.302816600654366 to 0.232773039675733
-0.0956513424179529 to -0.165694903396587
0.204313964698556 to 0.134270403719922
0.276760191772225 to 0.206716630793591
0.666362197253945 to 0.596318636275311
0.65985408530021 to 0.589810524321576
Trying to learn from memory 9, 2, -1
sum 1.07437877395822 distri 0.386592366330058
Using diff 0.419191714138605 and condRate 0.166666666666667
Changed category 2 weights from 
0.457109815089896 to 0.387244529400128
0.735551780669883 to 0.665686494980115
0.648511415927604 to 0.578646130237836
-0.00606028863887509 to -0.0759255743286427
Changing layer 0's weights from 
0.40272209439375 to 0.332856808703983
0.0307956377039155 to -0.0390696479858521
-0.203553627990703 to -0.27341891368047
-0.168994378112773 to -0.238859663802541
0.578873191214581 to 0.509007905524814
-0.103768762015323 to -0.17363404770509
0.05006096158125 to -0.0198043241085175
0.0226065555582246 to -0.047258730131543
-0.214885305427531 to -0.284750591117299
-0.258635360561828 to -0.328500646251595
Changing layer 1's weights from 
-0.253538448177795 to -0.323403733867562
0.347630236483594 to 0.277764950793826
0.221927497721692 to 0.152062212031924
0.212022159434339 to 0.142156873744571
0.0269693771372041 to -0.0428959085525635
-0.111268456839542 to -0.181133742529309
0.159450266695996 to 0.0895849810062286
-0.226352762245158 to -0.296218047934926
0.185310218668958 to 0.11544493297919
-0.177336644195537 to -0.247201929885304
Changing layer 2's weights from 
0.631851587630292 to 0.561986301940524
0.0777981022367676 to 0.00793281654700007
0.362387392855664 to 0.292522107165897
0.533587489462872 to 0.463722203773105
0.356334779597302 to 0.286469493907535
0.150325153208753 to 0.0804598675189849
0.669137869215985 to 0.599272583526218
0.596715841628095 to 0.526850555938327
-0.135282452963809 to -0.205147738653577
-0.0903249343862334 to -0.160190220076001
Changing layer 3's weights from 
0.598801288939496 to 0.528936003249728
0.335248205996533 to 0.265382920306766
0.577858124114056 to 0.507992838424289
-0.329581107642087 to -0.399446393331854
0.455182228423139 to 0.385316942733371
0.324933622218152 to 0.255068336528384
-0.0052512248983184 to -0.075116510588086
0.453261647559186 to 0.383396361869418
0.121565673686047 to 0.0517003879962799
0.594395552016278 to 0.524530266326511
Changing layer 4's weights from 
0.495121751166364 to 0.425256465476596
0.286288950778027 to 0.21642366508826
0.0858113506803711 to 0.0159460649906036
0.372746084071179 to 0.302880798381412
0.13349327836134 to 0.0636279926715728
0.364958498812695 to 0.295093213122928
0.36059996876814 to 0.290734683078372
-0.101297553442935 to -0.171162839132703
0.492398534155866 to 0.422533248466098
0.0600965956220826 to -0.00976869006768499
Changing layer 5's weights from 
0.647901568747541 to 0.578036283057773
0.20582804474928 to 0.135962759059512
0.631016407347699 to 0.561151121657932
-0.304495774023751 to -0.374361059713519
0.232773039675733 to 0.162907753985965
-0.165694903396587 to -0.235560189086354
0.134270403719922 to 0.0644051180301544
0.206716630793591 to 0.136851345103824
0.596318636275311 to 0.526453350585544
0.589810524321576 to 0.519945238631808
Trying to learn from memory 10, 1, -1
sum 1.07435774539746 distri 0.334429985439069
Using diff 0.471338323609028 and condRate 0.166666666666667
Changed category 1 weights from 
0.492424299763101 to 0.41386791249493
0.725739886806863 to 0.647183499538692
0.300814648912805 to 0.222258261644634
0.304907461450952 to 0.226351074182781
Changing layer 0's weights from 
0.332856808703983 to 0.254300421435811
-0.0390696479858521 to -0.117626035254023
-0.27341891368047 to -0.351975300948642
-0.238859663802541 to -0.317416051070712
0.509007905524814 to 0.430451518256642
-0.17363404770509 to -0.252190434973262
-0.0198043241085175 to -0.0983607113766889
-0.047258730131543 to -0.125815117399714
-0.284750591117299 to -0.36330697838547
-0.328500646251595 to -0.407057033519767
Changing layer 1's weights from 
-0.323403733867562 to -0.401960121135734
0.277764950793826 to 0.199208563525655
0.152062212031924 to 0.0735058247637531
0.142156873744571 to 0.0636004864763996
-0.0428959085525635 to -0.121452295820735
-0.181133742529309 to -0.259690129797481
0.0895849810062286 to 0.0110285937380572
-0.296218047934926 to -0.374774435203097
0.11544493297919 to 0.0368885457110187
-0.247201929885304 to -0.325758317153476
Changing layer 2's weights from 
0.561986301940524 to 0.483429914672353
0.00793281654700007 to -0.0706235707211713
0.292522107165897 to 0.213965719897725
0.463722203773105 to 0.385165816504933
0.286469493907535 to 0.207913106639363
0.0804598675189849 to 0.00190348025081359
0.599272583526218 to 0.520716196258046
0.526850555938327 to 0.448294168670156
-0.205147738653577 to -0.283704125921748
-0.160190220076001 to -0.238746607344172
Changing layer 3's weights from 
0.528936003249728 to 0.450379615981557
0.265382920306766 to 0.186826533038594
0.507992838424289 to 0.429436451156118
-0.399446393331854 to -0.478002780600026
0.385316942733371 to 0.3067605554652
0.255068336528384 to 0.176511949260213
-0.075116510588086 to -0.153672897856257
0.383396361869418 to 0.304839974601247
0.0517003879962799 to -0.0268559992718915
0.524530266326511 to 0.445973879058339
Changing layer 4's weights from 
0.425256465476596 to 0.346700078208425
0.21642366508826 to 0.137867277820089
0.0159460649906036 to -0.0626103222775678
0.302880798381412 to 0.22432441111324
0.0636279926715728 to -0.0149283945965985
0.295093213122928 to 0.216536825854757
0.290734683078372 to 0.212178295810201
-0.171162839132703 to -0.249719226400874
0.422533248466098 to 0.343976861197927
-0.00976869006768499 to -0.0883250773358563
Changing layer 5's weights from 
0.578036283057773 to 0.499479895789602
0.135962759059512 to 0.057406371791341
0.561151121657932 to 0.48259473438976
-0.374361059713519 to -0.45291744698169
0.162907753985965 to 0.0843513667177936
-0.235560189086354 to -0.314116576354526
0.0644051180301544 to -0.014151269238017
0.136851345103824 to 0.0582949578356525
0.526453350585544 to 0.447896963317372
0.519945238631808 to 0.441388851363637
Trying to learn from memory 11, 1, -1
sum 1.07430440167014 distri 0.334415718178214
Using diff 0.471312583074388 and condRate 0.166666666666667
Changed category 1 weights from 
0.41386791249493 to 0.335315815315865
0.647183499538692 to 0.568631402359627
0.222258261644634 to 0.143706164465569
0.226351074182781 to 0.147798977003716
Changing layer 0's weights from 
0.254300421435811 to 0.175748324256747
-0.117626035254023 to -0.196178132433088
-0.351975300948642 to -0.430527398127706
-0.317416051070712 to -0.395968148249777
0.430451518256642 to 0.351899421077578
-0.252190434973262 to -0.330742532152326
-0.0983607113766889 to -0.176912808555754
-0.125815117399714 to -0.204367214578779
-0.36330697838547 to -0.441859075564535
-0.407057033519767 to -0.485609130698831
Changing layer 1's weights from 
-0.401960121135734 to -0.480512218314798
0.199208563525655 to 0.12065646634659
0.0735058247637531 to -0.00504627241531158
0.0636004864763996 to -0.0149516107026651
-0.121452295820735 to -0.2000043929998
-0.259690129797481 to -0.338242226976545
0.0110285937380572 to -0.0675235034410074
-0.374774435203097 to -0.453326532382162
0.0368885457110187 to -0.0416635514680459
-0.325758317153476 to -0.40431041433254
Changing layer 2's weights from 
0.483429914672353 to 0.404877817493288
-0.0706235707211713 to -0.149175667900236
0.213965719897725 to 0.135413622718661
0.385165816504933 to 0.306613719325869
0.207913106639363 to 0.129361009460299
0.00190348025081359 to -0.0766486169282511
0.520716196258046 to 0.442164099078982
0.448294168670156 to 0.369742071491091
-0.283704125921748 to -0.362256223100813
-0.238746607344172 to -0.317298704523237
Changing layer 3's weights from 
0.450379615981557 to 0.371827518802492
0.186826533038594 to 0.10827443585953
0.429436451156118 to 0.350884353977053
-0.478002780600026 to -0.55655487777909
0.3067605554652 to 0.228208458286135
0.176511949260213 to 0.0979598520811484
-0.153672897856257 to -0.232224995035322
0.304839974601247 to 0.226287877422182
-0.0268559992718915 to -0.105408096450956
0.445973879058339 to 0.367421781879275
Changing layer 4's weights from 
0.346700078208425 to 0.26814798102936
0.137867277820089 to 0.0593151806410239
-0.0626103222775678 to -0.141162419456632
0.22432441111324 to 0.145772313934176
-0.0149283945965985 to -0.0934804917756632
0.216536825854757 to 0.137984728675692
0.212178295810201 to 0.133626198631136
-0.249719226400874 to -0.328271323579939
0.343976861197927 to 0.265424764018862
-0.0883250773358563 to -0.166877174514921
Changing layer 5's weights from 
0.499479895789602 to 0.420927798610537
0.057406371791341 to -0.0211457253877237
0.48259473438976 to 0.404042637210696
-0.45291744698169 to -0.531469544160755
0.0843513667177936 to 0.00579926953872895
-0.314116576354526 to -0.39266867353359
-0.014151269238017 to -0.0927033664170816
0.0582949578356525 to -0.0202571393434122
0.447896963317372 to 0.369344866138308
0.441388851363637 to 0.362836754184572
Trying to learn from memory 12, 1, -1
sum 1.07429055915417 distri 0.334411955075339
Using diff 0.471305964290291 and condRate 0.166666666666667
Changed category 1 weights from 
0.335315815315865 to 0.256764821267483
0.568631402359627 to 0.490080408311245
0.143706164465569 to 0.0651551704171871
0.147798977003716 to 0.0692479829553341
Changing layer 0's weights from 
0.175748324256747 to 0.0971973302083647
-0.196178132433088 to -0.27472912648147
-0.430527398127706 to -0.509078392176088
-0.395968148249777 to -0.474519142298159
0.351899421077578 to 0.273348427029196
-0.330742532152326 to -0.409293526200708
-0.176912808555754 to -0.255463802604135
-0.204367214578779 to -0.282918208627161
-0.441859075564535 to -0.520410069612917
-0.485609130698831 to -0.564160124747213
Changing layer 1's weights from 
-0.480512218314798 to -0.55906321236318
0.12065646634659 to 0.0421054722982085
-0.00504627241531158 to -0.0835972664636934
-0.0149516107026651 to -0.0935026047510469
-0.2000043929998 to -0.278555387048181
-0.338242226976545 to -0.416793221024927
-0.0675235034410074 to -0.146074497489389
-0.453326532382162 to -0.531877526430544
-0.0416635514680459 to -0.120214545516428
-0.40431041433254 to -0.482861408380922
Changing layer 2's weights from 
0.404877817493288 to 0.326326823444907
-0.149175667900236 to -0.227726661948618
0.135413622718661 to 0.0568626286702788
0.306613719325869 to 0.228062725277487
0.129361009460299 to 0.050810015411917
-0.0766486169282511 to -0.155199610976633
0.442164099078982 to 0.3636131050306
0.369742071491091 to 0.291191077442709
-0.362256223100813 to -0.440807217149195
-0.317298704523237 to -0.395849698571619
Changing layer 3's weights from 
0.371827518802492 to 0.29327652475411
0.10827443585953 to 0.0297234418111479
0.350884353977053 to 0.272333359928671
-0.55655487777909 to -0.635105871827472
0.228208458286135 to 0.149657464237753
0.0979598520811484 to 0.0194088580327666
-0.232224995035322 to -0.310775989083704
0.226287877422182 to 0.147736883373801
-0.105408096450956 to -0.183959090499338
0.367421781879275 to 0.288870787830893
Changing layer 4's weights from 
0.26814798102936 to 0.189596986980978
0.0593151806410239 to -0.0192358134073579
-0.141162419456632 to -0.219713413505014
0.145772313934176 to 0.0672213198857939
-0.0934804917756632 to -0.172031485824045
0.137984728675692 to 0.05943373462731
0.133626198631136 to 0.0550752045827544
-0.328271323579939 to -0.406822317628321
0.265424764018862 to 0.18687376997048
-0.166877174514921 to -0.245428168563303
Changing layer 5's weights from 
0.420927798610537 to 0.342376804562155
-0.0211457253877237 to -0.0996967194361055
0.404042637210696 to 0.325491643162314
-0.531469544160755 to -0.610020538209137
0.00579926953872895 to -0.0727517245096529
-0.39266867353359 to -0.471219667581972
-0.0927033664170816 to -0.171254360465463
-0.0202571393434122 to -0.098808133391794
0.369344866138308 to 0.290793872089926
0.362836754184572 to 0.284285760136191
Trying to learn from memory 12, 2, -1
sum 1.07429055915417 distri 0.386567958189354
Using diff 0.419149961176276 and condRate 0.166666666666667
Changed category 2 weights from 
0.387244529400128 to 0.317386202537416
0.665686494980115 to 0.595828168117402
0.578646130237836 to 0.508787803375123
-0.0759255743286427 to -0.145783901191355
Changing layer 0's weights from 
0.0971973302083647 to 0.0273390033456521
-0.27472912648147 to -0.344587453344183
-0.509078392176088 to -0.578936719038801
-0.474519142298159 to -0.544377469160871
0.273348427029196 to 0.203490100166483
-0.409293526200708 to -0.479151853063421
-0.255463802604135 to -0.325322129466848
-0.282918208627161 to -0.352776535489873
-0.520410069612917 to -0.590268396475629
-0.564160124747213 to -0.634018451609926
Changing layer 1's weights from 
-0.55906321236318 to -0.628921539225893
0.0421054722982085 to -0.0277528545645042
-0.0835972664636934 to -0.153455593326406
-0.0935026047510469 to -0.16336093161376
-0.278555387048181 to -0.348413713910894
-0.416793221024927 to -0.48665154788764
-0.146074497489389 to -0.215932824352102
-0.531877526430544 to -0.601735853293256
-0.120214545516428 to -0.19007287237914
-0.482861408380922 to -0.552719735243635
Changing layer 2's weights from 
0.326326823444907 to 0.256468496582194
-0.227726661948618 to -0.29758498881133
0.0568626286702788 to -0.0129956981924339
0.228062725277487 to 0.158204398414774
0.050810015411917 to -0.0190483114507957
-0.155199610976633 to -0.225057937839346
0.3636131050306 to 0.293754778167887
0.291191077442709 to 0.221332750579996
-0.440807217149195 to -0.510665544011907
-0.395849698571619 to -0.465708025434332
Changing layer 3's weights from 
0.29327652475411 to 0.223418197891398
0.0297234418111479 to -0.0401348850515647
0.272333359928671 to 0.202475033065958
-0.635105871827472 to -0.704964198690185
0.149657464237753 to 0.0797991373750405
0.0194088580327666 to -0.0504494688299461
-0.310775989083704 to -0.380634315946416
0.147736883373801 to 0.0778785565110879
-0.183959090499338 to -0.253817417362051
0.288870787830893 to 0.21901246096818
Changing layer 4's weights from 
0.189596986980978 to 0.119738660118266
-0.0192358134073579 to -0.0890941402700706
-0.219713413505014 to -0.289571740367727
0.0672213198857939 to -0.00263700697691875
-0.172031485824045 to -0.241889812686758
0.05943373462731 to -0.0104245922354026
0.0550752045827544 to -0.0147831222799583
-0.406822317628321 to -0.476680644491033
0.18687376997048 to 0.117015443107768
-0.245428168563303 to -0.315286495426016
Changing layer 5's weights from 
0.342376804562155 to 0.272518477699442
-0.0996967194361055 to -0.169555046298818
0.325491643162314 to 0.255633316299601
-0.610020538209137 to -0.679878865071849
-0.0727517245096529 to -0.142610051372366
-0.471219667581972 to -0.541077994444685
-0.171254360465463 to -0.241112687328176
-0.098808133391794 to -0.168666460254507
0.290793872089926 to 0.220935545227213
0.284285760136191 to 0.214427433273478
10/5/2016 1:22:52 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 12, 0, -1
sum 1.07429055915417 distri 0.35331064588948
Using diff 0.45240727347615 and condRate 0.166666666666667
Changed category 0 weights from 
0.575505710921216 to 0.500104498675191
0.17547074683659 to 0.100069534590566
-0.107049949684215 to -0.18245116193024
0.0444554162835356 to -0.0309457959624893
Changing layer 0's weights from 
0.0273390033456521 to -0.0480622089003728
-0.344587453344183 to -0.419988665590208
-0.578936719038801 to -0.654337931284826
-0.544377469160871 to -0.619778681406896
0.203490100166483 to 0.128088887920458
-0.479151853063421 to -0.554553065309446
-0.325322129466848 to -0.400723341712873
-0.352776535489873 to -0.428177747735898
-0.590268396475629 to -0.665669608721654
-0.634018451609926 to -0.709419663855951
Changing layer 1's weights from 
-0.628921539225893 to -0.704322751471918
-0.0277528545645042 to -0.103154066810529
-0.153455593326406 to -0.228856805572431
-0.16336093161376 to -0.238762143859785
-0.348413713910894 to -0.423814926156919
-0.48665154788764 to -0.562052760133665
-0.215932824352102 to -0.291334036598127
-0.601735853293256 to -0.677137065539281
-0.19007287237914 to -0.265474084625165
-0.552719735243635 to -0.62812094748966
Changing layer 2's weights from 
0.256468496582194 to 0.181067284336169
-0.29758498881133 to -0.372986201057355
-0.0129956981924339 to -0.0883969104384588
0.158204398414774 to 0.0828031861687494
-0.0190483114507957 to -0.0944495236968206
-0.225057937839346 to -0.30045915008537
0.293754778167887 to 0.218353565921862
0.221332750579996 to 0.145931538333972
-0.510665544011907 to -0.586066756257932
-0.465708025434332 to -0.541109237680356
Changing layer 3's weights from 
0.223418197891398 to 0.148016985645373
-0.0401348850515647 to -0.11553609729759
0.202475033065958 to 0.127073820819933
-0.704964198690185 to -0.78036541093621
0.0797991373750405 to 0.00439792512901559
-0.0504494688299461 to -0.125850681075971
-0.380634315946416 to -0.456035528192441
0.0778785565110879 to 0.00247734426506295
-0.253817417362051 to -0.329218629608076
0.21901246096818 to 0.143611248722155
Changing layer 4's weights from 
0.119738660118266 to 0.0443374478722407
-0.0890941402700706 to -0.164495352516096
-0.289571740367727 to -0.364972952613752
-0.00263700697691875 to -0.0780382192229437
-0.241889812686758 to -0.317291024932783
-0.0104245922354026 to -0.0858258044814276
-0.0147831222799583 to -0.0901843345259832
-0.476680644491033 to -0.552081856737058
0.117015443107768 to 0.0416142308617427
-0.315286495426016 to -0.39068770767204
Changing layer 5's weights from 
0.272518477699442 to 0.197117265453418
-0.169555046298818 to -0.244956258544843
0.255633316299601 to 0.180232104053576
-0.679878865071849 to -0.755280077317874
-0.142610051372366 to -0.21801126361839
-0.541077994444685 to -0.61647920669071
-0.241112687328176 to -0.316513899574201
-0.168666460254507 to -0.244067672500532
0.220935545227213 to 0.145534332981188
0.214427433273478 to 0.139026221027453
Trying to learn from memory 12, 2, -1
sum 1.07429055915417 distri 0.386567958189354
Using diff 0.419149961176276 and condRate 0.166666666666667
Changed category 2 weights from 
0.317386202537416 to 0.247527875674703
0.595828168117402 to 0.52596984125469
0.508787803375123 to 0.438929476512411
-0.145783901191355 to -0.215642228054068
Changing layer 0's weights from 
-0.0480622089003728 to -0.117920535763086
-0.419988665590208 to -0.48984699245292
-0.654337931284826 to -0.724196258147538
-0.619778681406896 to -0.689637008269609
0.128088887920458 to 0.0582305610577457
-0.554553065309446 to -0.624411392172158
-0.400723341712873 to -0.470581668575586
-0.428177747735898 to -0.498036074598611
-0.665669608721654 to -0.735527935584367
-0.709419663855951 to -0.779277990718663
Changing layer 1's weights from 
-0.704322751471918 to -0.77418107833463
-0.103154066810529 to -0.173012393673242
-0.228856805572431 to -0.298715132435144
-0.238762143859785 to -0.308620470722497
-0.423814926156919 to -0.493673253019632
-0.562052760133665 to -0.631911086996377
-0.291334036598127 to -0.361192363460839
-0.677137065539281 to -0.746995392401994
-0.265474084625165 to -0.335332411487878
-0.62812094748966 to -0.697979274352372
Changing layer 2's weights from 
0.181067284336169 to 0.111208957473456
-0.372986201057355 to -0.442844527920068
-0.0883969104384588 to -0.158255237301171
0.0828031861687494 to 0.0129448593060367
-0.0944495236968206 to -0.164307850559533
-0.30045915008537 to -0.370317476948083
0.218353565921862 to 0.14849523905915
0.145931538333972 to 0.0760732114712589
-0.586066756257932 to -0.655925083120645
-0.541109237680356 to -0.610967564543069
Changing layer 3's weights from 
0.148016985645373 to 0.0781586587826602
-0.11553609729759 to -0.185394424160302
0.127073820819933 to 0.0572154939572208
-0.78036541093621 to -0.850223737798922
0.00439792512901559 to -0.0654604017336971
-0.125850681075971 to -0.195709007938684
-0.456035528192441 to -0.525893855055154
0.00247734426506295 to -0.0673809825976497
-0.329218629608076 to -0.399076956470788
0.143611248722155 to 0.0737529218594425
Changing layer 4's weights from 
0.0443374478722407 to -0.025520878990472
-0.164495352516096 to -0.234353679378808
-0.364972952613752 to -0.434831279476464
-0.0780382192229437 to -0.147896546085656
-0.317291024932783 to -0.387149351795495
-0.0858258044814276 to -0.15568413134414
-0.0901843345259832 to -0.160042661388696
-0.552081856737058 to -0.621940183599771
0.0416142308617427 to -0.02824409600097
-0.39068770767204 to -0.460546034534753
Changing layer 5's weights from 
0.197117265453418 to 0.127258938590705
-0.244956258544843 to -0.314814585407556
0.180232104053576 to 0.110373777190864
-0.755280077317874 to -0.825138404180587
-0.21801126361839 to -0.287869590481103
-0.61647920669071 to -0.686337533553422
-0.316513899574201 to -0.386372226436914
-0.244067672500532 to -0.313925999363244
0.145534332981188 to 0.0756760061184757
0.139026221027453 to 0.0691678941647403
Trying to learn from memory 12, 0, -1
sum 1.07429055915417 distri 0.35331064588948
Using diff 0.45240727347615 and condRate 0.166666666666667
Changed category 0 weights from 
0.500104498675191 to 0.424703286429166
0.100069534590566 to 0.0246683223445406
-0.18245116193024 to -0.257852374176265
-0.0309457959624893 to -0.106347008208514
Changing layer 0's weights from 
-0.117920535763086 to -0.19332174800911
-0.48984699245292 to -0.565248204698945
-0.724196258147538 to -0.799597470393563
-0.689637008269609 to -0.765038220515634
0.0582305610577457 to -0.0171706511882792
-0.624411392172158 to -0.699812604418183
-0.470581668575586 to -0.545982880821611
-0.498036074598611 to -0.573437286844636
-0.735527935584367 to -0.810929147830392
-0.779277990718663 to -0.854679202964688
Changing layer 1's weights from 
-0.77418107833463 to -0.849582290580655
-0.173012393673242 to -0.248413605919267
-0.298715132435144 to -0.374116344681169
-0.308620470722497 to -0.384021682968522
-0.493673253019632 to -0.569074465265657
-0.631911086996377 to -0.707312299242402
-0.361192363460839 to -0.436593575706864
-0.746995392401994 to -0.822396604648019
-0.335332411487878 to -0.410733623733903
-0.697979274352372 to -0.773380486598397
Changing layer 2's weights from 
0.111208957473456 to 0.0358077452274315
-0.442844527920068 to -0.518245740166093
-0.158255237301171 to -0.233656449547196
0.0129448593060367 to -0.0624563529399882
-0.164307850559533 to -0.239709062805558
-0.370317476948083 to -0.445718689194108
0.14849523905915 to 0.0730940268131248
0.0760732114712589 to 0.000671999225233963
-0.655925083120645 to -0.73132629536667
-0.610967564543069 to -0.686368776789094
Changing layer 3's weights from 
0.0781586587826602 to 0.00275744653663533
-0.185394424160302 to -0.260795636406327
0.0572154939572208 to -0.0181857182888041
-0.850223737798922 to -0.925624950044947
-0.0654604017336971 to -0.140861613979722
-0.195709007938684 to -0.271110220184709
-0.525893855055154 to -0.601295067301179
-0.0673809825976497 to -0.142782194843675
-0.399076956470788 to -0.474478168716813
0.0737529218594425 to -0.00164829038658244
Changing layer 4's weights from 
-0.025520878990472 to -0.100922091236497
-0.234353679378808 to -0.309754891624833
-0.434831279476464 to -0.510232491722489
-0.147896546085656 to -0.223297758331681
-0.387149351795495 to -0.46255056404152
-0.15568413134414 to -0.231085343590165
-0.160042661388696 to -0.235443873634721
-0.621940183599771 to -0.697341395845796
-0.02824409600097 to -0.103645308246995
-0.460546034534753 to -0.535947246780778
Changing layer 5's weights from 
0.127258938590705 to 0.05185772634468
-0.314814585407556 to -0.390215797653581
0.110373777190864 to 0.0349725649448387
-0.825138404180587 to -0.900539616426612
-0.287869590481103 to -0.363270802727128
-0.686337533553422 to -0.761738745799447
-0.386372226436914 to -0.461773438682939
-0.313925999363244 to -0.389327211609269
0.0756760061184757 to 0.00027479387245076
0.0691678941647403 to -0.00623331808128459
Trying to learn from memory 12, 0, -1
sum 1.07429055915417 distri 0.35331064588948
Using diff 0.45240727347615 and condRate 0.166666666666667
Changed category 0 weights from 
0.424703286429166 to 0.349302074183141
0.0246683223445406 to -0.0507328899014843
-0.257852374176265 to -0.33325358642229
-0.106347008208514 to -0.181748220454539
Changing layer 0's weights from 
-0.19332174800911 to -0.268722960255135
-0.565248204698945 to -0.64064941694497
-0.799597470393563 to -0.874998682639588
-0.765038220515634 to -0.840439432761659
-0.0171706511882792 to -0.0925718634343041
-0.699812604418183 to -0.775213816664208
-0.545982880821611 to -0.621384093067635
-0.573437286844636 to -0.648838499090661
-0.810929147830392 to -0.886330360076417
-0.854679202964688 to -0.930080415210713
Changing layer 1's weights from 
-0.849582290580655 to -0.92498350282668
-0.248413605919267 to -0.323814818165292
-0.374116344681169 to -0.449517556927193
-0.384021682968522 to -0.459422895214547
-0.569074465265657 to -0.644475677511681
-0.707312299242402 to -0.782713511488427
-0.436593575706864 to -0.511994787952889
-0.822396604648019 to -0.897797816894044
-0.410733623733903 to -0.486134835979928
-0.773380486598397 to -0.848781698844422
Changing layer 2's weights from 
0.0358077452274315 to -0.0395934670185935
-0.518245740166093 to -0.593646952412118
-0.233656449547196 to -0.309057661793221
-0.0624563529399882 to -0.137857565186013
-0.239709062805558 to -0.315110275051583
-0.445718689194108 to -0.521119901440133
0.0730940268131248 to -0.00230718543290009
0.000671999225233963 to -0.074729213020791
-0.73132629536667 to -0.806727507612695
-0.686368776789094 to -0.761769989035119
Changing layer 3's weights from 
0.00275744653663533 to -0.0726437657093896
-0.260795636406327 to -0.336196848652352
-0.0181857182888041 to -0.093586930534829
-0.925624950044947 to -1.00102616229097
-0.140861613979722 to -0.216262826225747
-0.271110220184709 to -0.346511432430734
-0.601295067301179 to -0.676696279547204
-0.142782194843675 to -0.2181834070897
-0.474478168716813 to -0.549879380962838
-0.00164829038658244 to -0.0770495026326074
Changing layer 4's weights from 
-0.100922091236497 to -0.176323303482522
-0.309754891624833 to -0.385156103870858
-0.510232491722489 to -0.585633703968514
-0.223297758331681 to -0.298698970577706
-0.46255056404152 to -0.537951776287545
-0.231085343590165 to -0.30648655583619
-0.235443873634721 to -0.310845085880746
-0.697341395845796 to -0.772742608091821
-0.103645308246995 to -0.17904652049302
-0.535947246780778 to -0.611348459026803
Changing layer 5's weights from 
0.05185772634468 to -0.0235434859013449
-0.390215797653581 to -0.465617009899606
0.0349725649448387 to -0.0404286473011862
-0.900539616426612 to -0.975940828672637
-0.363270802727128 to -0.438672014973153
-0.761738745799447 to -0.837139958045472
-0.461773438682939 to -0.537174650928963
-0.389327211609269 to -0.464728423855294
0.00027479387245076 to -0.0751264183735742
-0.00623331808128459 to -0.0816345303273095
Trying to learn from memory 12, 2, -1
sum 1.07429055915417 distri 0.386567958189354
Using diff 0.419149961176276 and condRate 0.166666666666667
Changed category 2 weights from 
0.247527875674703 to 0.177669548811991
0.52596984125469 to 0.456111514391977
0.438929476512411 to 0.369071149649698
-0.215642228054068 to -0.285500554916781
Changing layer 0's weights from 
-0.268722960255135 to -0.338581287117848
-0.64064941694497 to -0.710507743807683
-0.874998682639588 to -0.944857009502301
-0.840439432761659 to -0.910297759624371
-0.0925718634343041 to -0.162430190297017
-0.775213816664208 to -0.845072143526921
-0.621384093067635 to -0.691242419930348
-0.648838499090661 to -0.718696825953374
-0.886330360076417 to -0.95618868693913
-0.930080415210713 to -0.999938742073426
Changing layer 1's weights from 
-0.92498350282668 to -0.994841829689393
-0.323814818165292 to -0.393673145028004
-0.449517556927193 to -0.519375883789906
-0.459422895214547 to -0.52928122207726
-0.644475677511681 to -0.714334004374394
-0.782713511488427 to -0.85257183835114
-0.511994787952889 to -0.581853114815602
-0.897797816894044 to -0.967656143756756
-0.486134835979928 to -0.55599316284264
-0.848781698844422 to -0.918640025707135
Changing layer 2's weights from 
-0.0395934670185935 to -0.109451793881306
-0.593646952412118 to -0.663505279274831
-0.309057661793221 to -0.378915988655934
-0.137857565186013 to -0.207715892048726
-0.315110275051583 to -0.384968601914296
-0.521119901440133 to -0.590978228302846
-0.00230718543290009 to -0.0721655122956128
-0.074729213020791 to -0.144587539883504
-0.806727507612695 to -0.876585834475407
-0.761769989035119 to -0.831628315897832
Changing layer 3's weights from 
-0.0726437657093896 to -0.142502092572102
-0.336196848652352 to -0.406055175515065
-0.093586930534829 to -0.163445257397542
-1.00102616229097 to -1.07088448915368
-0.216262826225747 to -0.28612115308846
-0.346511432430734 to -0.416369759293446
-0.676696279547204 to -0.746554606409917
-0.2181834070897 to -0.288041733952412
-0.549879380962838 to -0.619737707825551
-0.0770495026326074 to -0.14690782949532
Changing layer 4's weights from 
-0.176323303482522 to -0.246181630345234
-0.385156103870858 to -0.455014430733571
-0.585633703968514 to -0.655492030831227
-0.298698970577706 to -0.368557297440419
-0.537951776287545 to -0.607810103150258
-0.30648655583619 to -0.376344882698903
-0.310845085880746 to -0.380703412743458
-0.772742608091821 to -0.842600934954533
-0.17904652049302 to -0.248904847355732
-0.611348459026803 to -0.681206785889516
Changing layer 5's weights from 
-0.0235434859013449 to -0.0934018127640576
-0.465617009899606 to -0.535475336762318
-0.0404286473011862 to -0.110286974163899
-0.975940828672637 to -1.04579915553535
-0.438672014973153 to -0.508530341835866
-0.837139958045472 to -0.906998284908185
-0.537174650928963 to -0.607032977791676
-0.464728423855294 to -0.534586750718007
-0.0751264183735742 to -0.144984745236287
-0.0816345303273095 to -0.151492857190022
Trying to learn from memory 13, 0, -1
sum 1.07428749390503 distri 0.353309329610296
Using diff 0.452406290818477 and condRate 0.166666666666667
Changed category 0 weights from 
0.349302074183141 to 0.273901025713395
-0.0507328899014843 to -0.12613393837123
-0.33325358642229 to -0.408654634892036
-0.181748220454539 to -0.257149268924285
Changing layer 0's weights from 
-0.338581287117848 to -0.413982335587594
-0.710507743807683 to -0.785908792277429
-0.944857009502301 to -1.02025805797205
-0.910297759624371 to -0.985698808094117
-0.162430190297017 to -0.237831238766763
-0.845072143526921 to -0.920473191996667
-0.691242419930348 to -0.766643468400094
-0.718696825953374 to -0.79409787442312
-0.95618868693913 to -1.03158973540888
-0.999938742073426 to -1.07533979054317
Changing layer 1's weights from 
-0.994841829689393 to -1.07024287815914
-0.393673145028004 to -0.46907419349775
-0.519375883789906 to -0.594776932259652
-0.52928122207726 to -0.604682270547006
-0.714334004374394 to -0.78973505284414
-0.85257183835114 to -0.927972886820886
-0.581853114815602 to -0.657254163285348
-0.967656143756756 to -1.0430571922265
-0.55599316284264 to -0.631394211312387
-0.918640025707135 to -0.994041074176881
Changing layer 2's weights from 
-0.109451793881306 to -0.184852842351052
-0.663505279274831 to -0.738906327744577
-0.378915988655934 to -0.45431703712568
-0.207715892048726 to -0.283116940518472
-0.384968601914296 to -0.460369650384042
-0.590978228302846 to -0.666379276772592
-0.0721655122956128 to -0.147566560765359
-0.144587539883504 to -0.21998858835325
-0.876585834475407 to -0.951986882945153
-0.831628315897832 to -0.907029364367578
Changing layer 3's weights from 
-0.142502092572102 to -0.217903141041848
-0.406055175515065 to -0.481456223984811
-0.163445257397542 to -0.238846305867288
-1.07088448915368 to -1.14628553762343
-0.28612115308846 to -0.361522201558206
-0.416369759293446 to -0.491770807763192
-0.746554606409917 to -0.821955654879663
-0.288041733952412 to -0.363442782422158
-0.619737707825551 to -0.695138756295297
-0.14690782949532 to -0.222308877965066
Changing layer 4's weights from 
-0.246181630345234 to -0.321582678814981
-0.455014430733571 to -0.530415479203317
-0.655492030831227 to -0.730893079300973
-0.368557297440419 to -0.443958345910165
-0.607810103150258 to -0.683211151620004
-0.376344882698903 to -0.451745931168649
-0.380703412743458 to -0.456104461213205
-0.842600934954533 to -0.918001983424279
-0.248904847355732 to -0.324305895825479
-0.681206785889516 to -0.756607834359262
Changing layer 5's weights from 
-0.0934018127640576 to -0.168802861233804
-0.535475336762318 to -0.610876385232064
-0.110286974163899 to -0.185688022633645
-1.04579915553535 to -1.1212002040051
-0.508530341835866 to -0.583931390305612
-0.906998284908185 to -0.982399333377931
-0.607032977791676 to -0.682434026261422
-0.534586750718007 to -0.609987799187753
-0.144984745236287 to -0.220385793706033
-0.151492857190022 to -0.226893905659768
Trying to learn from memory 14, 0, -1
sum 1.07428749390503 distri 0.353309329610296
Using diff 0.452406290818477 and condRate 0.166666666666667
Changed category 0 weights from 
0.273901025713395 to 0.198499977243649
-0.12613393837123 to -0.201534986840977
-0.408654634892036 to -0.484055683361782
-0.257149268924285 to -0.332550317394032
Changing layer 0's weights from 
-0.413982335587594 to -0.48938338405734
-0.785908792277429 to -0.861309840747175
-1.02025805797205 to -1.09565910644179
-0.985698808094117 to -1.06109985656386
-0.237831238766763 to -0.313232287236509
-0.920473191996667 to -0.995874240466413
-0.766643468400094 to -0.84204451686984
-0.79409787442312 to -0.869498922892866
-1.03158973540888 to -1.10699078387862
-1.07533979054317 to -1.15074083901292
Changing layer 1's weights from 
-1.07024287815914 to -1.14564392662888
-0.46907419349775 to -0.544475241967497
-0.594776932259652 to -0.670177980729398
-0.604682270547006 to -0.680083319016752
-0.78973505284414 to -0.865136101313886
-0.927972886820886 to -1.00337393529063
-0.657254163285348 to -0.732655211755094
-1.0430571922265 to -1.11845824069625
-0.631394211312387 to -0.706795259782133
-0.994041074176881 to -1.06944212264663
Changing layer 2's weights from 
-0.184852842351052 to -0.260253890820798
-0.738906327744577 to -0.814307376214323
-0.45431703712568 to -0.529718085595426
-0.283116940518472 to -0.358517988988218
-0.460369650384042 to -0.535770698853788
-0.666379276772592 to -0.741780325242338
-0.147566560765359 to -0.222967609235105
-0.21998858835325 to -0.295389636822996
-0.951986882945153 to -1.0273879314149
-0.907029364367578 to -0.982430412837324
Changing layer 3's weights from 
-0.217903141041848 to -0.293304189511595
-0.481456223984811 to -0.556857272454557
-0.238846305867288 to -0.314247354337034
-1.14628553762343 to -1.22168658609318
-0.361522201558206 to -0.436923250027952
-0.491770807763192 to -0.567171856232938
-0.821955654879663 to -0.897356703349409
-0.363442782422158 to -0.438843830891905
-0.695138756295297 to -0.770539804765043
-0.222308877965066 to -0.297709926434812
Changing layer 4's weights from 
-0.321582678814981 to -0.396983727284727
-0.530415479203317 to -0.605816527673063
-0.730893079300973 to -0.806294127770719
-0.443958345910165 to -0.519359394379911
-0.683211151620004 to -0.75861220008975
-0.451745931168649 to -0.527146979638395
-0.456104461213205 to -0.531505509682951
-0.918001983424279 to -0.993403031894026
-0.324305895825479 to -0.399706944295225
-0.756607834359262 to -0.832008882829008
Changing layer 5's weights from 
-0.168802861233804 to -0.24420390970355
-0.610876385232064 to -0.686277433701811
-0.185688022633645 to -0.261089071103391
-1.1212002040051 to -1.19660125247484
-0.583931390305612 to -0.659332438775358
-0.982399333377931 to -1.05780038184768
-0.682434026261422 to -0.757835074731168
-0.609987799187753 to -0.685388847657499
-0.220385793706033 to -0.295786842175779
-0.226893905659768 to -0.302294954129514
Trying to learn from memory 15, 2, -1
sum 1.07428749390503 distri 0.38656705070738
Using diff 0.419148569721393 and condRate 0.166666666666667
Changed category 2 weights from 
0.177669548811991 to 0.107811453858425
0.456111514391977 to 0.386253419438412
0.369071149649698 to 0.299213054696133
-0.285500554916781 to -0.355358649870346
Changing layer 0's weights from 
-0.48938338405734 to -0.559241479010906
-0.861309840747175 to -0.93116793570074
-1.09565910644179 to -1.16551720139536
-1.06109985656386 to -1.13095795151743
-0.313232287236509 to -0.383090382190075
-0.995874240466413 to -1.06573233541998
-0.84204451686984 to -0.911902611823406
-0.869498922892866 to -0.939357017846431
-1.10699078387862 to -1.17684887883219
-1.15074083901292 to -1.22059893396648
Changing layer 1's weights from 
-1.14564392662888 to -1.21550202158245
-0.544475241967497 to -0.614333336921062
-0.670177980729398 to -0.740036075682964
-0.680083319016752 to -0.749941413970317
-0.865136101313886 to -0.934994196267452
-1.00337393529063 to -1.0732320302442
-0.732655211755094 to -0.80251330670866
-1.11845824069625 to -1.18831633564981
-0.706795259782133 to -0.776653354735698
-1.06944212264663 to -1.13930021760019
Changing layer 2's weights from 
-0.260253890820798 to -0.330111985774364
-0.814307376214323 to -0.884165471167888
-0.529718085595426 to -0.599576180548992
-0.358517988988218 to -0.428376083941784
-0.535770698853788 to -0.605628793807353
-0.741780325242338 to -0.811638420195903
-0.222967609235105 to -0.29282570418867
-0.295389636822996 to -0.365247731776561
-1.0273879314149 to -1.09724602636846
-0.982430412837324 to -1.05228850779089
Changing layer 3's weights from 
-0.293304189511595 to -0.36316228446516
-0.556857272454557 to -0.626715367408123
-0.314247354337034 to -0.384105449290599
-1.22168658609318 to -1.29154468104674
-0.436923250027952 to -0.506781344981517
-0.567171856232938 to -0.637029951186504
-0.897356703349409 to -0.967214798302974
-0.438843830891905 to -0.50870192584547
-0.770539804765043 to -0.840397899718608
-0.297709926434812 to -0.367568021388378
Changing layer 4's weights from 
-0.396983727284727 to -0.466841822238292
-0.605816527673063 to -0.675674622626628
-0.806294127770719 to -0.876152222724285
-0.519359394379911 to -0.589217489333477
-0.75861220008975 to -0.828470295043315
-0.527146979638395 to -0.59700507459196
-0.531505509682951 to -0.601363604636516
-0.993403031894026 to -1.06326112684759
-0.399706944295225 to -0.46956503924879
-0.832008882829008 to -0.901866977782573
Changing layer 5's weights from 
-0.24420390970355 to -0.314062004657115
-0.686277433701811 to -0.756135528655376
-0.261089071103391 to -0.330947166056957
-1.19660125247484 to -1.26645934742841
-0.659332438775358 to -0.729190533728923
-1.05780038184768 to -1.12765847680124
-0.757835074731168 to -0.827693169684734
-0.685388847657499 to -0.755246942611065
-0.295786842175779 to -0.365644937129345
-0.302294954129514 to -0.37215304908308
10/5/2016 1:22:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:22:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:22 PMStarting learning phase with deltaScore: 1.933333
Modified index 0's learning in memoryPool to 0.3866667
Modified index 1's learning in memoryPool to 0.3866667
Modified index 2's learning in memoryPool to 0.3866667
Modified index 3's learning in memoryPool to 0.3866667
Modified index 4's learning in memoryPool to 0.3866667
Modified index 5's learning in memoryPool to 0.3866667
Modified index 6's learning in memoryPool to 0.3866667
Modified index 7's learning in memoryPool to 0.3866667
10/5/2016 1:23:22 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 16, 0, 0.3866667
sum 0.0724962518899922 distri 0.0175354640020089
Using diff 0.0368367249154853 and condRate 0.166666666666667
Changed category 0 weights from 
0.198499977243649 to 0.200873899447671
-0.201534986840977 to -0.199161064636954
-0.484055683361782 to -0.48168176115776
-0.332550317394032 to -0.330176395190009
Changing layer 0's weights from 
-0.559241479010906 to -0.556867556806883
-0.93116793570074 to -0.928794013496718
-1.16551720139536 to -1.16314327919134
-1.13095795151743 to -1.12858402931341
-0.383090382190075 to -0.380716459986052
-1.06573233541998 to -1.06335841321596
-0.911902611823406 to -0.909528689619383
-0.939357017846431 to -0.936983095642409
-1.17684887883219 to -1.17447495662816
-1.22059893396648 to -1.21822501176246
Changing layer 1's weights from 
-1.21550202158245 to -1.21312809937843
-0.614333336921062 to -0.61195941471704
-0.740036075682964 to -0.737662153478941
-0.749941413970317 to -0.747567491766295
-0.934994196267452 to -0.932620274063429
-1.0732320302442 to -1.07085810804018
-0.80251330670866 to -0.800139384504637
-1.18831633564981 to -1.18594241344579
-0.776653354735698 to -0.774279432531676
-1.13930021760019 to -1.13692629539617
Changing layer 2's weights from 
-0.330111985774364 to -0.327738063570341
-0.884165471167888 to -0.881791548963866
-0.599576180548992 to -0.597202258344969
-0.428376083941784 to -0.426002161737761
-0.605628793807353 to -0.603254871603331
-0.811638420195903 to -0.809264497991881
-0.29282570418867 to -0.290451781984648
-0.365247731776561 to -0.362873809572539
-1.09724602636846 to -1.09487210416444
-1.05228850779089 to -1.04991458558687
Changing layer 3's weights from 
-0.36316228446516 to -0.360788362261137
-0.626715367408123 to -0.6243414452041
-0.384105449290599 to -0.381731527086577
-1.29154468104674 to -1.28917075884272
-0.506781344981517 to -0.504407422777495
-0.637029951186504 to -0.634656028982481
-0.967214798302974 to -0.964840876098952
-0.50870192584547 to -0.506328003641448
-0.840397899718608 to -0.838023977514586
-0.367568021388378 to -0.365194099184355
Changing layer 4's weights from 
-0.466841822238292 to -0.46446790003427
-0.675674622626628 to -0.673300700422606
-0.876152222724285 to -0.873778300520262
-0.589217489333477 to -0.586843567129454
-0.828470295043315 to -0.826096372839293
-0.59700507459196 to -0.594631152387938
-0.601363604636516 to -0.598989682432494
-1.06326112684759 to -1.06088720464357
-0.46956503924879 to -0.467191117044768
-0.901866977782573 to -0.899493055578551
Changing layer 5's weights from 
-0.314062004657115 to -0.311688082453093
-0.756135528655376 to -0.753761606451354
-0.330947166056957 to -0.328573243852934
-1.26645934742841 to -1.26408542522439
-0.729190533728923 to -0.726816611524901
-1.12765847680124 to -1.12528455459722
-0.827693169684734 to -0.825319247480711
-0.755246942611065 to -0.752873020407042
-0.365644937129345 to -0.363271014925322
-0.37215304908308 to -0.369779126879057
Trying to learn from memory 17, 2, 0.3866667
sum 0.0787807638398236 distri 0.0603107802096834
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.107811453858425 to 0.172255896448503
0.386253419438412 to 0.450697862028489
0.299213054696133 to 0.36365749728621
-0.355358649870346 to -0.290914207280268
Changing layer 0's weights from 
-0.556867556806883 to -0.492423114216806
-0.928794013496718 to -0.86434957090664
-1.16314327919134 to -1.09869883660126
-1.12858402931341 to -1.06413958672333
-0.380716459986052 to -0.316272017395974
-1.06335841321596 to -0.998913970625879
-0.909528689619383 to -0.845084247029306
-0.936983095642409 to -0.872538653052331
-1.17447495662816 to -1.11003051403809
-1.21822501176246 to -1.15378056917238
Changing layer 1's weights from 
-1.21312809937843 to -1.14868365678835
-0.61195941471704 to -0.547514972126962
-0.737662153478941 to -0.673217710888864
-0.747567491766295 to -0.683123049176217
-0.932620274063429 to -0.868175831473352
-1.07085810804018 to -1.0064136654501
-0.800139384504637 to -0.735694941914559
-1.18594241344579 to -1.12149797085571
-0.774279432531676 to -0.709834989941598
-1.13692629539617 to -1.07248185280609
Changing layer 2's weights from 
-0.327738063570341 to -0.263293620980264
-0.881791548963866 to -0.817347106373788
-0.597202258344969 to -0.532757815754891
-0.426002161737761 to -0.361557719147683
-0.603254871603331 to -0.538810429013253
-0.809264497991881 to -0.744820055401803
-0.290451781984648 to -0.22600733939457
-0.362873809572539 to -0.298429366982461
-1.09487210416444 to -1.03042766157436
-1.04991458558687 to -0.985470142996789
Changing layer 3's weights from 
-0.360788362261137 to -0.29634391967106
-0.6243414452041 to -0.559897002614022
-0.381731527086577 to -0.317287084496499
-1.28917075884272 to -1.22472631625264
-0.504407422777495 to -0.439962980187417
-0.634656028982481 to -0.570211586392404
-0.964840876098952 to -0.900396433508874
-0.506328003641448 to -0.44188356105137
-0.838023977514586 to -0.773579534924508
-0.365194099184355 to -0.300749656594278
Changing layer 4's weights from 
-0.46446790003427 to -0.400023457444192
-0.673300700422606 to -0.608856257832528
-0.873778300520262 to -0.809333857930185
-0.586843567129454 to -0.522399124539376
-0.826096372839293 to -0.761651930249215
-0.594631152387938 to -0.53018670979786
-0.598989682432494 to -0.534545239842416
-1.06088720464357 to -0.996442762053491
-0.467191117044768 to -0.40274667445469
-0.899493055578551 to -0.835048612988473
Changing layer 5's weights from 
-0.311688082453093 to -0.247243639863015
-0.753761606451354 to -0.689317163861276
-0.328573243852934 to -0.264128801262856
-1.26408542522439 to -1.19964098263431
-0.726816611524901 to -0.662372168934823
-1.12528455459722 to -1.06084011200714
-0.825319247480711 to -0.760874804890634
-0.752873020407042 to -0.688428577816964
-0.363271014925322 to -0.298826572335244
-0.369779126879057 to -0.30533468428898
Trying to learn from memory 18, 1, 0.3866667
sum 0.079168334233009 distri 0.13659682222531
Using diff 1 and condRate 0.166666666666667
Changed category 1 weights from 
0.256764821267483 to 0.321209263857561
0.490080408311245 to 0.554524850901323
0.0651551704171871 to 0.129599613007265
0.0692479829553341 to 0.133692425545412
Changing layer 0's weights from 
-0.492423114216806 to -0.427978671626728
-0.86434957090664 to -0.799905128316562
-1.09869883660126 to -1.03425439401118
-1.06413958672333 to -0.999695144133251
-0.316272017395974 to -0.251827574805897
-0.998913970625879 to -0.934469528035801
-0.845084247029306 to -0.780639804439228
-0.872538653052331 to -0.808094210462253
-1.11003051403809 to -1.04558607144801
-1.15378056917238 to -1.08933612658231
Changing layer 1's weights from 
-1.14868365678835 to -1.08423921419827
-0.547514972126962 to -0.483070529536884
-0.673217710888864 to -0.608773268298786
-0.683123049176217 to -0.618678606586139
-0.868175831473352 to -0.803731388883274
-1.0064136654501 to -0.94196922286002
-0.735694941914559 to -0.671250499324482
-1.12149797085571 to -1.05705352826564
-0.709834989941598 to -0.64539054735152
-1.07248185280609 to -1.00803741021601
Changing layer 2's weights from 
-0.263293620980264 to -0.198849178390186
-0.817347106373788 to -0.75290266378371
-0.532757815754891 to -0.468313373164814
-0.361557719147683 to -0.297113276557606
-0.538810429013253 to -0.474365986423176
-0.744820055401803 to -0.680375612811725
-0.22600733939457 to -0.161562896804493
-0.298429366982461 to -0.233984924392383
-1.03042766157436 to -0.965983218984287
-0.985470142996789 to -0.921025700406712
Changing layer 3's weights from 
-0.29634391967106 to -0.231899477080982
-0.559897002614022 to -0.495452560023945
-0.317287084496499 to -0.252842641906422
-1.22472631625264 to -1.16028187366256
-0.439962980187417 to -0.37551853759734
-0.570211586392404 to -0.505767143802326
-0.900396433508874 to -0.835951990918796
-0.44188356105137 to -0.377439118461292
-0.773579534924508 to -0.70913509233443
-0.300749656594278 to -0.2363052140042
Changing layer 4's weights from 
-0.400023457444192 to -0.335579014854114
-0.608856257832528 to -0.544411815242451
-0.809333857930185 to -0.744889415340107
-0.522399124539376 to -0.457954681949299
-0.761651930249215 to -0.697207487659137
-0.53018670979786 to -0.465742267207783
-0.534545239842416 to -0.470100797252338
-0.996442762053491 to -0.931998319463413
-0.40274667445469 to -0.338302231864612
-0.835048612988473 to -0.770604170398395
Changing layer 5's weights from 
-0.247243639863015 to -0.182799197272937
-0.689317163861276 to -0.624872721271198
-0.264128801262856 to -0.199684358672779
-1.19964098263431 to -1.13519654004423
-0.662372168934823 to -0.597927726344745
-1.06084011200714 to -0.996395669417064
-0.760874804890634 to -0.696430362300556
-0.688428577816964 to -0.623984135226887
-0.298826572335244 to -0.234382129745167
-0.30533468428898 to -0.240890241698902
Trying to learn from memory 19, 2, 0.3866667
sum 0.0794730935810481 distri 0.0605250520323528
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.172255896448503 to 0.236700339038581
0.450697862028489 to 0.515142304618567
0.36365749728621 to 0.428101939876288
-0.290914207280268 to -0.226469764690191
Changing layer 0's weights from 
-0.427978671626728 to -0.36353422903665
-0.799905128316562 to -0.735460685726485
-1.03425439401118 to -0.969809951421103
-0.999695144133251 to -0.935250701543173
-0.251827574805897 to -0.187383132215819
-0.934469528035801 to -0.870025085445723
-0.780639804439228 to -0.71619536184915
-0.808094210462253 to -0.743649767872176
-1.04558607144801 to -0.981141628857932
-1.08933612658231 to -1.02489168399223
Changing layer 1's weights from 
-1.08423921419827 to -1.0197947716082
-0.483070529536884 to -0.418626086946806
-0.608773268298786 to -0.544328825708708
-0.618678606586139 to -0.554234163996062
-0.803731388883274 to -0.739286946293196
-0.94196922286002 to -0.877524780269942
-0.671250499324482 to -0.606806056734404
-1.05705352826564 to -0.992609085675559
-0.64539054735152 to -0.580946104761442
-1.00803741021601 to -0.943592967625937
Changing layer 2's weights from 
-0.198849178390186 to -0.134404735800108
-0.75290266378371 to -0.688458221193633
-0.468313373164814 to -0.403868930574736
-0.297113276557606 to -0.232668833967528
-0.474365986423176 to -0.409921543833098
-0.680375612811725 to -0.615931170221648
-0.161562896804493 to -0.0971184542144149
-0.233984924392383 to -0.169540481802306
-0.965983218984287 to -0.901538776394209
-0.921025700406712 to -0.856581257816634
Changing layer 3's weights from 
-0.231899477080982 to -0.167455034490904
-0.495452560023945 to -0.431008117433867
-0.252842641906422 to -0.188398199316344
-1.16028187366256 to -1.09583743107249
-0.37551853759734 to -0.311074095007262
-0.505767143802326 to -0.441322701212248
-0.835951990918796 to -0.771507548328719
-0.377439118461292 to -0.312994675871214
-0.70913509233443 to -0.644690649744353
-0.2363052140042 to -0.171860771414122
Changing layer 4's weights from 
-0.335579014854114 to -0.271134572264037
-0.544411815242451 to -0.479967372652373
-0.744889415340107 to -0.680444972750029
-0.457954681949299 to -0.393510239359221
-0.697207487659137 to -0.63276304506906
-0.465742267207783 to -0.401297824617705
-0.470100797252338 to -0.40565635466226
-0.931998319463413 to -0.867553876873335
-0.338302231864612 to -0.273857789274535
-0.770604170398395 to -0.706159727808318
Changing layer 5's weights from 
-0.182799197272937 to -0.11835475468286
-0.624872721271198 to -0.56042827868112
-0.199684358672779 to -0.135239916082701
-1.13519654004423 to -1.07075209745415
-0.597927726344745 to -0.533483283754668
-0.996395669417064 to -0.931951226826987
-0.696430362300556 to -0.631985919710478
-0.623984135226887 to -0.559539692636809
-0.234382129745167 to -0.169937687155089
-0.240890241698902 to -0.176445799108824
Trying to learn from memory 20, 2, 0.3866667
sum 0.0722519877239092 distri 0.0593319753516316
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.236700339038581 to 0.301144781628658
0.515142304618567 to 0.579586747208645
0.428101939876288 to 0.492546382466366
-0.226469764690191 to -0.162025322100113
Changing layer 0's weights from 
-0.36353422903665 to -0.299089786446572
-0.735460685726485 to -0.671016243136407
-0.969809951421103 to -0.905365508831025
-0.935250701543173 to -0.870806258953095
-0.187383132215819 to -0.122938689625741
-0.870025085445723 to -0.805580642855645
-0.71619536184915 to -0.651750919259072
-0.743649767872176 to -0.679205325282098
-0.981141628857932 to -0.916697186267854
-1.02489168399223 to -0.96044724140215
Changing layer 1's weights from 
-1.0197947716082 to -0.955350329018117
-0.418626086946806 to -0.354181644356729
-0.544328825708708 to -0.479884383118631
-0.554234163996062 to -0.489789721405984
-0.739286946293196 to -0.674842503703118
-0.877524780269942 to -0.813080337679864
-0.606806056734404 to -0.542361614144326
-0.992609085675559 to -0.928164643085481
-0.580946104761442 to -0.516501662171365
-0.943592967625937 to -0.879148525035859
Changing layer 2's weights from 
-0.134404735800108 to -0.0699602932100305
-0.688458221193633 to -0.624013778603555
-0.403868930574736 to -0.339424487984658
-0.232668833967528 to -0.16822439137745
-0.409921543833098 to -0.34547710124302
-0.615931170221648 to -0.55148672763157
-0.0971184542144149 to -0.0326740116243372
-0.169540481802306 to -0.105096039212228
-0.901538776394209 to -0.837094333804132
-0.856581257816634 to -0.792136815226556
Changing layer 3's weights from 
-0.167455034490904 to -0.103010591900827
-0.431008117433867 to -0.366563674843789
-0.188398199316344 to -0.123953756726266
-1.09583743107249 to -1.03139298848241
-0.311074095007262 to -0.246629652417184
-0.441322701212248 to -0.376878258622171
-0.771507548328719 to -0.707063105738641
-0.312994675871214 to -0.248550233281137
-0.644690649744353 to -0.580246207154275
-0.171860771414122 to -0.107416328824044
Changing layer 4's weights from 
-0.271134572264037 to -0.206690129673959
-0.479967372652373 to -0.415522930062295
-0.680444972750029 to -0.616000530159951
-0.393510239359221 to -0.329065796769143
-0.63276304506906 to -0.568318602478982
-0.401297824617705 to -0.336853382027627
-0.40565635466226 to -0.341211912072183
-0.867553876873335 to -0.803109434283258
-0.273857789274535 to -0.209413346684457
-0.706159727808318 to -0.64171528521824
Changing layer 5's weights from 
-0.11835475468286 to -0.053910312092782
-0.56042827868112 to -0.495983836091043
-0.135239916082701 to -0.0707954734926233
-1.07075209745415 to -1.00630765486407
-0.533483283754668 to -0.46903884116459
-0.931951226826987 to -0.867506784236909
-0.631985919710478 to -0.5675414771204
-0.559539692636809 to -0.495095250046731
-0.169937687155089 to -0.105493244565011
-0.176445799108824 to -0.112001356518747
Trying to learn from memory 20, 2, 0.3866667
sum 0.0722519877239092 distri 0.0593319753516316
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.301144781628658 to 0.365589224218736
0.579586747208645 to 0.644031189798723
0.492546382466366 to 0.556990825056443
-0.162025322100113 to -0.0975808795100352
Changing layer 0's weights from 
-0.299089786446572 to -0.234645343856495
-0.671016243136407 to -0.606571800546329
-0.905365508831025 to -0.840921066240948
-0.870806258953095 to -0.806361816363018
-0.122938689625741 to -0.0584942470356635
-0.805580642855645 to -0.741136200265568
-0.651750919259072 to -0.587306476668995
-0.679205325282098 to -0.61476088269202
-0.916697186267854 to -0.852252743677776
-0.96044724140215 to -0.896002798812073
Changing layer 1's weights from 
-0.955350329018117 to -0.890905886428039
-0.354181644356729 to -0.289737201766651
-0.479884383118631 to -0.415439940528553
-0.489789721405984 to -0.425345278815906
-0.674842503703118 to -0.610398061113041
-0.813080337679864 to -0.748635895089786
-0.542361614144326 to -0.477917171554248
-0.928164643085481 to -0.863720200495403
-0.516501662171365 to -0.452057219581287
-0.879148525035859 to -0.814704082445782
Changing layer 2's weights from 
-0.0699602932100305 to -0.00551585061995283
-0.624013778603555 to -0.559569336013477
-0.339424487984658 to -0.274980045394581
-0.16822439137745 to -0.103779948787373
-0.34547710124302 to -0.281032658652942
-0.55148672763157 to -0.487042285041492
-0.0326740116243372 to 0.0317704309657405
-0.105096039212228 to -0.0406515966221503
-0.837094333804132 to -0.772649891214054
-0.792136815226556 to -0.727692372636478
Changing layer 3's weights from 
-0.103010591900827 to -0.0385661493107489
-0.366563674843789 to -0.302119232253712
-0.123953756726266 to -0.0595093141361884
-1.03139298848241 to -0.966948545892332
-0.246629652417184 to -0.182185209827106
-0.376878258622171 to -0.312433816032093
-0.707063105738641 to -0.642618663148563
-0.248550233281137 to -0.184105790691059
-0.580246207154275 to -0.515801764564197
-0.107416328824044 to -0.0429718862339667
Changing layer 4's weights from 
-0.206690129673959 to -0.142245687083881
-0.415522930062295 to -0.351078487472217
-0.616000530159951 to -0.551556087569874
-0.329065796769143 to -0.264621354179066
-0.568318602478982 to -0.503874159888904
-0.336853382027627 to -0.272408939437549
-0.341211912072183 to -0.276767469482105
-0.803109434283258 to -0.73866499169318
-0.209413346684457 to -0.144968904094379
-0.64171528521824 to -0.577270842628162
Changing layer 5's weights from 
-0.053910312092782 to 0.0105341304972957
-0.495983836091043 to -0.431539393500965
-0.0707954734926233 to -0.0063510309025456
-1.00630765486407 to -0.941863212273997
-0.46903884116459 to -0.404594398574512
-0.867506784236909 to -0.803062341646831
-0.5675414771204 to -0.503097034530323
-0.495095250046731 to -0.430650807456653
-0.105493244565011 to -0.0410488019749335
-0.112001356518747 to -0.0475569139286689
Trying to learn from memory 20, 2, 0.3866667
sum 0.0722519877239092 distri 0.0593319753516316
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.365589224218736 to 0.430033666808814
0.644031189798723 to 0.7084756323888
0.556990825056443 to 0.621435267646521
-0.0975808795100352 to -0.0331364369199575
Changing layer 0's weights from 
-0.234645343856495 to -0.170200901266417
-0.606571800546329 to -0.542127357956251
-0.840921066240948 to -0.77647662365087
-0.806361816363018 to -0.74191737377294
-0.0584942470356635 to 0.00595019555441421
-0.741136200265568 to -0.67669175767549
-0.587306476668995 to -0.522862034078917
-0.61476088269202 to -0.550316440101942
-0.852252743677776 to -0.787808301087698
-0.896002798812073 to -0.831558356221995
Changing layer 1's weights from 
-0.890905886428039 to -0.826461443837962
-0.289737201766651 to -0.225292759176573
-0.415439940528553 to -0.350995497938475
-0.425345278815906 to -0.360900836225829
-0.610398061113041 to -0.545953618522963
-0.748635895089786 to -0.684191452499709
-0.477917171554248 to -0.413472728964171
-0.863720200495403 to -0.799275757905325
-0.452057219581287 to -0.387612776991209
-0.814704082445782 to -0.750259639855704
Changing layer 2's weights from 
-0.00551585061995283 to 0.0589285919701249
-0.559569336013477 to -0.495124893423399
-0.274980045394581 to -0.210535602804503
-0.103779948787373 to -0.0393355061972948
-0.281032658652942 to -0.216588216062865
-0.487042285041492 to -0.422597842451414
0.0317704309657405 to 0.0962148735558183
-0.0406515966221503 to 0.0237928459679274
-0.772649891214054 to -0.708205448623976
-0.727692372636478 to -0.6632479300464
Changing layer 3's weights from 
-0.0385661493107489 to 0.0258782932793288
-0.302119232253712 to -0.237674789663634
-0.0595093141361884 to 0.00493512845388931
-0.966948545892332 to -0.902504103302254
-0.182185209827106 to -0.117740767237029
-0.312433816032093 to -0.247989373442015
-0.642618663148563 to -0.578174220558485
-0.184105790691059 to -0.119661348100981
-0.515801764564197 to -0.451357321974119
-0.0429718862339667 to 0.021472556356111
Changing layer 4's weights from 
-0.142245687083881 to -0.0778012444938035
-0.351078487472217 to -0.28663404488214
-0.551556087569874 to -0.487111644979796
-0.264621354179066 to -0.200176911588988
-0.503874159888904 to -0.439429717298826
-0.272408939437549 to -0.207964496847472
-0.276767469482105 to -0.212323026892027
-0.73866499169318 to -0.674220549103102
-0.144968904094379 to -0.0805244615043015
-0.577270842628162 to -0.512826400038084
Changing layer 5's weights from 
0.0105341304972957 to 0.0749785730873734
-0.431539393500965 to -0.367094950910887
-0.0063510309025456 to 0.0580934116875321
-0.941863212273997 to -0.877418769683919
-0.404594398574512 to -0.340149955984435
-0.803062341646831 to -0.738617899056753
-0.503097034530323 to -0.438652591940245
-0.430650807456653 to -0.366206364866576
-0.0410488019749335 to 0.0233956406151442
-0.0475569139286689 to 0.0168875286614088
Trying to learn from memory 21, 2, 0.3866667
sum 0.0732998629820056 distri 0.0684801780297162
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.430033666808814 to 0.494478109398891
0.7084756323888 to 0.772920074978878
0.621435267646521 to 0.685879710236599
-0.0331364369199575 to 0.0313080056701202
Changing layer 0's weights from 
-0.170200901266417 to -0.105756458676339
-0.542127357956251 to -0.477682915366174
-0.77647662365087 to -0.712032181060792
-0.74191737377294 to -0.677472931182862
0.00595019555441421 to 0.0703946381444919
-0.67669175767549 to -0.612247315085412
-0.522862034078917 to -0.458417591488839
-0.550316440101942 to -0.485871997511865
-0.787808301087698 to -0.723363858497621
-0.831558356221995 to -0.767113913631917
Changing layer 1's weights from 
-0.826461443837962 to -0.762017001247884
-0.225292759176573 to -0.160848316586496
-0.350995497938475 to -0.286551055348397
-0.360900836225829 to -0.296456393635751
-0.545953618522963 to -0.481509175932885
-0.684191452499709 to -0.619747009909631
-0.413472728964171 to -0.349028286374093
-0.799275757905325 to -0.734831315315248
-0.387612776991209 to -0.323168334401132
-0.750259639855704 to -0.685815197265626
Changing layer 2's weights from 
0.0589285919701249 to 0.123373034560203
-0.495124893423399 to -0.430680450833322
-0.210535602804503 to -0.146091160214425
-0.0393355061972948 to 0.0251089363927829
-0.216588216062865 to -0.152143773472787
-0.422597842451414 to -0.358153399861337
0.0962148735558183 to 0.160659316145896
0.0237928459679274 to 0.0882372885580051
-0.708205448623976 to -0.643761006033898
-0.6632479300464 to -0.598803487456323
Changing layer 3's weights from 
0.0258782932793288 to 0.0903227358694065
-0.237674789663634 to -0.173230347073556
0.00493512845388931 to 0.069379571043967
-0.902504103302254 to -0.838059660712176
-0.117740767237029 to -0.053296324646951
-0.247989373442015 to -0.183544930851937
-0.578174220558485 to -0.513729777968408
-0.119661348100981 to -0.0552169055109037
-0.451357321974119 to -0.386912879384042
0.021472556356111 to 0.0859169989461887
Changing layer 4's weights from 
-0.0778012444938035 to -0.0133568019037258
-0.28663404488214 to -0.222189602292062
-0.487111644979796 to -0.422667202389718
-0.200176911588988 to -0.13573246899891
-0.439429717298826 to -0.374985274708749
-0.207964496847472 to -0.143520054257394
-0.212323026892027 to -0.14787858430195
-0.674220549103102 to -0.609776106513024
-0.0805244615043015 to -0.0160800189142238
-0.512826400038084 to -0.448381957448007
Changing layer 5's weights from 
0.0749785730873734 to 0.139423015677451
-0.367094950910887 to -0.30265050832081
0.0580934116875321 to 0.12253785427761
-0.877418769683919 to -0.812974327093841
-0.340149955984435 to -0.275705513394357
-0.738617899056753 to -0.674173456466676
-0.438652591940245 to -0.374208149350167
-0.366206364866576 to -0.301761922276498
0.0233956406151442 to 0.0878400832052219
0.0168875286614088 to 0.0813319712514866
10/5/2016 1:23:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:43 PMStarting learning phase with deltaScore: 0.8
Modified index 0's learning in memoryPool to 0.16
Modified index 1's learning in memoryPool to 0.16
Modified index 2's learning in memoryPool to 0.16
Modified index 3's learning in memoryPool to 0.16
Modified index 4's learning in memoryPool to 0.16
Modified index 5's learning in memoryPool to 0.16
Modified index 6's learning in memoryPool to 0.16
Modified index 7's learning in memoryPool to 0.16
Modified index 8's learning in memoryPool to 0.16
10/5/2016 1:23:44 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 22, 2, 0.16
sum 0.0709722104903954 distri 0.0627964568933477
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.494478109398891 to 0.521144775469512
0.772920074978878 to 0.799586741049498
0.685879710236599 to 0.712546376307219
0.0313080056701202 to 0.0579746717407404
Changing layer 0's weights from 
-0.105756458676339 to -0.0790897926057191
-0.477682915366174 to -0.451016249295554
-0.712032181060792 to -0.685365514990172
-0.677472931182862 to -0.650806265112242
0.0703946381444919 to 0.0970613042151121
-0.612247315085412 to -0.585580649014792
-0.458417591488839 to -0.431750925418219
-0.485871997511865 to -0.459205331441244
-0.723363858497621 to -0.696697192427
-0.767113913631917 to -0.740447247561297
Changing layer 1's weights from 
-0.762017001247884 to -0.735350335177264
-0.160848316586496 to -0.134181650515875
-0.286551055348397 to -0.259884389277777
-0.296456393635751 to -0.269789727565131
-0.481509175932885 to -0.454842509862265
-0.619747009909631 to -0.593080343839011
-0.349028286374093 to -0.322361620303473
-0.734831315315248 to -0.708164649244627
-0.323168334401132 to -0.296501668330511
-0.685815197265626 to -0.659148531195006
Changing layer 2's weights from 
0.123373034560203 to 0.150039700630823
-0.430680450833322 to -0.404013784762701
-0.146091160214425 to -0.119424494143805
0.0251089363927829 to 0.0517756024634031
-0.152143773472787 to -0.125477107402167
-0.358153399861337 to -0.331486733790716
0.160659316145896 to 0.187325982216516
0.0882372885580051 to 0.114903954628625
-0.643761006033898 to -0.617094339963278
-0.598803487456323 to -0.572136821385703
Changing layer 3's weights from 
0.0903227358694065 to 0.116989401940027
-0.173230347073556 to -0.146563681002936
0.069379571043967 to 0.0960462371145872
-0.838059660712176 to -0.811392994641556
-0.053296324646951 to -0.0266296585763308
-0.183544930851937 to -0.156878264781317
-0.513729777968408 to -0.487063111897787
-0.0552169055109037 to -0.0285502394402834
-0.386912879384042 to -0.360246213313421
0.0859169989461887 to 0.112583665016809
Changing layer 4's weights from 
-0.0133568019037258 to 0.0133098641668945
-0.222189602292062 to -0.195522936221442
-0.422667202389718 to -0.396000536319098
-0.13573246899891 to -0.10906580292829
-0.374985274708749 to -0.348318608638129
-0.143520054257394 to -0.116853388186774
-0.14787858430195 to -0.121211918231329
-0.609776106513024 to -0.583109440442404
-0.0160800189142238 to 0.0105866471563964
-0.448381957448007 to -0.421715291377386
Changing layer 5's weights from 
0.139423015677451 to 0.166089681748071
-0.30265050832081 to -0.275983842250189
0.12253785427761 to 0.14920452034823
-0.812974327093841 to -0.786307661023221
-0.275705513394357 to -0.249038847323737
-0.674173456466676 to -0.647506790396056
-0.374208149350167 to -0.347541483279547
-0.301761922276498 to -0.275095256205878
0.0878400832052219 to 0.114506749275842
0.0813319712514866 to 0.107998637322107
Trying to learn from memory 23, 2, 0.16
sum 0.046760148710584 distri 0.0401723697767095
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.521144775469512 to 0.547811441540132
0.799586741049498 to 0.826253407120118
0.712546376307219 to 0.739213042377839
0.0579746717407404 to 0.0846413378113606
Changing layer 0's weights from 
-0.0790897926057191 to -0.0524231265350989
-0.451016249295554 to -0.424349583224933
-0.685365514990172 to -0.658698848919552
-0.650806265112242 to -0.624139599041622
0.0970613042151121 to 0.123727970285732
-0.585580649014792 to -0.558913982944172
-0.431750925418219 to -0.405084259347599
-0.459205331441244 to -0.432538665370624
-0.696697192427 to -0.67003052635638
-0.740447247561297 to -0.713780581490677
Changing layer 1's weights from 
-0.735350335177264 to -0.708683669106644
-0.134181650515875 to -0.107514984445255
-0.259884389277777 to -0.233217723207157
-0.269789727565131 to -0.24312306149451
-0.454842509862265 to -0.428175843791645
-0.593080343839011 to -0.566413677768391
-0.322361620303473 to -0.295694954232853
-0.708164649244627 to -0.681497983174007
-0.296501668330511 to -0.269835002259891
-0.659148531195006 to -0.632481865124386
Changing layer 2's weights from 
0.150039700630823 to 0.176706366701443
-0.404013784762701 to -0.377347118692081
-0.119424494143805 to -0.0927578280731848
0.0517756024634031 to 0.0784422685340234
-0.125477107402167 to -0.0988104413315467
-0.331486733790716 to -0.304820067720096
0.187325982216516 to 0.213992648287136
0.114903954628625 to 0.141570620699246
-0.617094339963278 to -0.590427673892658
-0.572136821385703 to -0.545470155315082
Changing layer 3's weights from 
0.116989401940027 to 0.143656068010647
-0.146563681002936 to -0.119897014932316
0.0960462371145872 to 0.122712903185207
-0.811392994641556 to -0.784726328570936
-0.0266296585763308 to 3.70074942894066E-05
-0.156878264781317 to -0.130211598710697
-0.487063111897787 to -0.460396445827167
-0.0285502394402834 to -0.00188357336966323
-0.360246213313421 to -0.333579547242801
0.112583665016809 to 0.139250331087429
Changing layer 4's weights from 
0.0133098641668945 to 0.0399765302375147
-0.195522936221442 to -0.168856270150822
-0.396000536319098 to -0.369333870248478
-0.10906580292829 to -0.0823991368576697
-0.348318608638129 to -0.321651942567508
-0.116853388186774 to -0.0901867221161536
-0.121211918231329 to -0.0945452521607092
-0.583109440442404 to -0.556442774371784
0.0105866471563964 to 0.0372533132270166
-0.421715291377386 to -0.395048625306766
Changing layer 5's weights from 
0.166089681748071 to 0.192756347818692
-0.275983842250189 to -0.249317176179569
0.14920452034823 to 0.17587118641885
-0.786307661023221 to -0.759640994952601
-0.249038847323737 to -0.222372181253116
-0.647506790396056 to -0.620840124325435
-0.347541483279547 to -0.320874817208927
-0.275095256205878 to -0.248428590135258
0.114506749275842 to 0.141173415346462
0.107998637322107 to 0.134665303392727
Trying to learn from memory 24, 1, 0.16
sum 0.0467442717689955 distri 0.0230519616044114
Using diff 0.0120062422223352 and condRate 0.166666666666667
Changed category 1 weights from 
0.321209263857561 to 0.321529430309667
0.554524850901323 to 0.554845017353429
0.129599613007265 to 0.129919779459371
0.133692425545412 to 0.134012591997518
Changing layer 0's weights from 
-0.0524231265350989 to -0.0521029600829929
-0.424349583224933 to -0.424029416772827
-0.658698848919552 to -0.658378682467446
-0.624139599041622 to -0.623819432589516
0.123727970285732 to 0.124048136737838
-0.558913982944172 to -0.558593816492066
-0.405084259347599 to -0.404764092895493
-0.432538665370624 to -0.432218498918518
-0.67003052635638 to -0.669710359904274
-0.713780581490677 to -0.713460415038571
Changing layer 1's weights from 
-0.708683669106644 to -0.708363502654538
-0.107514984445255 to -0.107194817993149
-0.233217723207157 to -0.232897556755051
-0.24312306149451 to -0.242802895042404
-0.428175843791645 to -0.427855677339539
-0.566413677768391 to -0.566093511316285
-0.295694954232853 to -0.295374787780747
-0.681497983174007 to -0.681177816721901
-0.269835002259891 to -0.269514835807785
-0.632481865124386 to -0.63216169867228
Changing layer 2's weights from 
0.176706366701443 to 0.177026533153549
-0.377347118692081 to -0.377026952239975
-0.0927578280731848 to -0.0924376616210788
0.0784422685340234 to 0.0787624349861294
-0.0988104413315467 to -0.0984902748794407
-0.304820067720096 to -0.30449990126799
0.213992648287136 to 0.214312814739242
0.141570620699246 to 0.141890787151352
-0.590427673892658 to -0.590107507440552
-0.545470155315082 to -0.545149988862976
Changing layer 3's weights from 
0.143656068010647 to 0.143976234462753
-0.119897014932316 to -0.11957684848021
0.122712903185207 to 0.123033069637313
-0.784726328570936 to -0.78440616211883
3.70074942894066E-05 to 0.0003571739463954
-0.130211598710697 to -0.129891432258591
-0.460396445827167 to -0.460076279375061
-0.00188357336966323 to -0.00156340691755724
-0.333579547242801 to -0.333259380790695
0.139250331087429 to 0.139570497539535
Changing layer 4's weights from 
0.0399765302375147 to 0.0402966966896207
-0.168856270150822 to -0.168536103698716
-0.369333870248478 to -0.369013703796372
-0.0823991368576697 to -0.0820789704055637
-0.321651942567508 to -0.321331776115402
-0.0901867221161536 to -0.0898665556640476
-0.0945452521607092 to -0.0942250857086033
-0.556442774371784 to -0.556122607919678
0.0372533132270166 to 0.0375734796791226
-0.395048625306766 to -0.39472845885466
Changing layer 5's weights from 
0.192756347818692 to 0.193076514270798
-0.249317176179569 to -0.248997009727463
0.17587118641885 to 0.176191352870956
-0.759640994952601 to -0.759320828500495
-0.222372181253116 to -0.22205201480101
-0.620840124325435 to -0.620519957873329
-0.320874817208927 to -0.320554650756821
-0.248428590135258 to -0.248108423683152
0.141173415346462 to 0.141493581798568
0.134665303392727 to 0.134985469844833
Trying to learn from memory 25, 2, 0.16
sum 0.0466879729178735 distri 0.0400885333617711
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.547811441540132 to 0.574478107610752
0.826253407120118 to 0.852920073190739
0.739213042377839 to 0.765879708448459
0.0846413378113606 to 0.111308003881981
Changing layer 0's weights from 
-0.0521029600829929 to -0.0254362940123727
-0.424029416772827 to -0.397362750702207
-0.658378682467446 to -0.631712016396826
-0.623819432589516 to -0.597152766518896
0.124048136737838 to 0.150714802808459
-0.558593816492066 to -0.531927150421446
-0.404764092895493 to -0.378097426824872
-0.432218498918518 to -0.405551832847898
-0.669710359904274 to -0.643043693833654
-0.713460415038571 to -0.686793748967951
Changing layer 1's weights from 
-0.708363502654538 to -0.681696836583917
-0.107194817993149 to -0.0805281519225289
-0.232897556755051 to -0.206230890684431
-0.242802895042404 to -0.216136228971784
-0.427855677339539 to -0.401189011268918
-0.566093511316285 to -0.539426845245664
-0.295374787780747 to -0.268708121710126
-0.681177816721901 to -0.654511150651281
-0.269514835807785 to -0.242848169737165
-0.63216169867228 to -0.605495032601659
Changing layer 2's weights from 
0.177026533153549 to 0.203693199224169
-0.377026952239975 to -0.350360286169355
-0.0924376616210788 to -0.0657709955504586
0.0787624349861294 to 0.10542910105675
-0.0984902748794407 to -0.0718236088088204
-0.30449990126799 to -0.27783323519737
0.214312814739242 to 0.240979480809863
0.141890787151352 to 0.168557453221972
-0.590107507440552 to -0.563440841369932
-0.545149988862976 to -0.518483322792356
Changing layer 3's weights from 
0.143976234462753 to 0.170642900533373
-0.11957684848021 to -0.0929101824095895
0.123033069637313 to 0.149699735707934
-0.78440616211883 to -0.75773949604821
0.0003571739463954 to 0.0270238400170156
-0.129891432258591 to -0.103224766187971
-0.460076279375061 to -0.433409613304441
-0.00156340691755724 to 0.025103259153063
-0.333259380790695 to -0.306592714720075
0.139570497539535 to 0.166237163610155
Changing layer 4's weights from 
0.0402966966896207 to 0.0669633627602409
-0.168536103698716 to -0.141869437628095
-0.369013703796372 to -0.342347037725751
-0.0820789704055637 to -0.0554123043349435
-0.321331776115402 to -0.294665110044782
-0.0898665556640476 to -0.0631998895934274
-0.0942250857086033 to -0.067558419637983
-0.556122607919678 to -0.529455941849058
0.0375734796791226 to 0.0642401457497428
-0.39472845885466 to -0.36806179278404
Changing layer 5's weights from 
0.193076514270798 to 0.219743180341418
-0.248997009727463 to -0.222330343656843
0.176191352870956 to 0.202858018941576
-0.759320828500495 to -0.732654162429875
-0.22205201480101 to -0.19538534873039
-0.620519957873329 to -0.593853291802709
-0.320554650756821 to -0.293887984686201
-0.248108423683152 to -0.221441757612531
0.141493581798568 to 0.168160247869189
0.134985469844833 to 0.161652135915453
Trying to learn from memory 26, 1, 0.16
sum 0.0466779021685231 distri 0.0230029952361356
Using diff 0.0120054313902567 and condRate 0.166666666666667
Changed category 1 weights from 
0.321529430309667 to 0.321849575139585
0.554845017353429 to 0.555165162183347
0.129919779459371 to 0.130239924289289
0.134012591997518 to 0.134332736827436
Changing layer 0's weights from 
-0.0254362940123727 to -0.025116149182455
-0.397362750702207 to -0.397042605872289
-0.631712016396826 to -0.631391871566908
-0.597152766518896 to -0.596832621688978
0.150714802808459 to 0.151034947638376
-0.531927150421446 to -0.531607005591528
-0.378097426824872 to -0.377777281994955
-0.405551832847898 to -0.40523168801798
-0.643043693833654 to -0.642723549003736
-0.686793748967951 to -0.686473604138033
Changing layer 1's weights from 
-0.681696836583917 to -0.681376691754
-0.0805281519225289 to -0.0802080070926112
-0.206230890684431 to -0.205910745854513
-0.216136228971784 to -0.215816084141867
-0.401189011268918 to -0.400868866439001
-0.539426845245664 to -0.539106700415747
-0.268708121710126 to -0.268387976880209
-0.654511150651281 to -0.654191005821363
-0.242848169737165 to -0.242528024907247
-0.605495032601659 to -0.605174887771742
Changing layer 2's weights from 
0.203693199224169 to 0.204013344054087
-0.350360286169355 to -0.350040141339437
-0.0657709955504586 to -0.0654508507205409
0.10542910105675 to 0.105749245886667
-0.0718236088088204 to -0.0715034639789027
-0.27783323519737 to -0.277513090367452
0.240979480809863 to 0.24129962563978
0.168557453221972 to 0.168877598051889
-0.563440841369932 to -0.563120696540014
-0.518483322792356 to -0.518163177962438
Changing layer 3's weights from 
0.170642900533373 to 0.170963045363291
-0.0929101824095895 to -0.0925900375796718
0.149699735707934 to 0.150019880537851
-0.75773949604821 to -0.757419351218292
0.0270238400170156 to 0.0273439848469333
-0.103224766187971 to -0.102904621358053
-0.433409613304441 to -0.433089468474523
0.025103259153063 to 0.0254234039829807
-0.306592714720075 to -0.306272569890157
0.166237163610155 to 0.166557308440073
Changing layer 4's weights from 
0.0669633627602409 to 0.0672835075901586
-0.141869437628095 to -0.141549292798178
-0.342347037725751 to -0.342026892895834
-0.0554123043349435 to -0.0550921595050258
-0.294665110044782 to -0.294344965214864
-0.0631998895934274 to -0.0628797447635097
-0.067558419637983 to -0.0672382748080653
-0.529455941849058 to -0.52913579701914
0.0642401457497428 to 0.0645602905796605
-0.36806179278404 to -0.367741647954122
Changing layer 5's weights from 
0.219743180341418 to 0.220063325171335
-0.222330343656843 to -0.222010198826925
0.202858018941576 to 0.203178163771494
-0.732654162429875 to -0.732334017599957
-0.19538534873039 to -0.195065203900473
-0.593853291802709 to -0.593533146972791
-0.293887984686201 to -0.293567839856283
-0.221441757612531 to -0.221121612782614
0.168160247869189 to 0.168480392699106
0.161652135915453 to 0.161972280745371
Trying to learn from memory 27, 2, 0.16
sum 0.0466724534521924 distri 0.0400600832102801
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.574478107610752 to 0.601144773681372
0.852920073190739 to 0.879586739261359
0.765879708448459 to 0.79254637451908
0.111308003881981 to 0.137974669952601
Changing layer 0's weights from 
-0.025116149182455 to 0.00155051688816523
-0.397042605872289 to -0.370375939801669
-0.631391871566908 to -0.604725205496288
-0.596832621688978 to -0.570165955618358
0.151034947638376 to 0.177701613708996
-0.531607005591528 to -0.504940339520908
-0.377777281994955 to -0.351110615924335
-0.40523168801798 to -0.37856502194736
-0.642723549003736 to -0.616056882933116
-0.686473604138033 to -0.659806938067413
Changing layer 1's weights from 
-0.681376691754 to -0.65471002568338
-0.0802080070926112 to -0.053541341021991
-0.205910745854513 to -0.179244079783893
-0.215816084141867 to -0.189149418071246
-0.400868866439001 to -0.374202200368381
-0.539106700415747 to -0.512440034345127
-0.268387976880209 to -0.241721310809588
-0.654191005821363 to -0.627524339750743
-0.242528024907247 to -0.215861358836627
-0.605174887771742 to -0.578508221701122
Changing layer 2's weights from 
0.204013344054087 to 0.230680010124707
-0.350040141339437 to -0.323373475268817
-0.0654508507205409 to -0.0387841846499207
0.105749245886667 to 0.132415911957288
-0.0715034639789027 to -0.0448367979082825
-0.277513090367452 to -0.250846424296832
0.24129962563978 to 0.267966291710401
0.168877598051889 to 0.19554426412251
-0.563120696540014 to -0.536454030469394
-0.518163177962438 to -0.491496511891818
Changing layer 3's weights from 
0.170963045363291 to 0.197629711433911
-0.0925900375796718 to -0.0659233715090516
0.150019880537851 to 0.176686546608472
-0.757419351218292 to -0.730752685147672
0.0273439848469333 to 0.0540106509175536
-0.102904621358053 to -0.0762379552874329
-0.433089468474523 to -0.406422802403903
0.0254234039829807 to 0.0520900700536009
-0.306272569890157 to -0.279605903819537
0.166557308440073 to 0.193223974510693
Changing layer 4's weights from 
0.0672835075901586 to 0.0939501736607788
-0.141549292798178 to -0.114882626727557
-0.342026892895834 to -0.315360226825213
-0.0550921595050258 to -0.0284254934344056
-0.294344965214864 to -0.267678299144244
-0.0628797447635097 to -0.0362130786928894
-0.0672382748080653 to -0.0405716087374451
-0.52913579701914 to -0.50246913094852
0.0645602905796605 to 0.0912269566502808
-0.367741647954122 to -0.341074981883502
Changing layer 5's weights from 
0.220063325171335 to 0.246729991241956
-0.222010198826925 to -0.195343532756305
0.203178163771494 to 0.229844829842114
-0.732334017599957 to -0.705667351529337
-0.195065203900473 to -0.168398537829852
-0.593533146972791 to -0.566866480902171
-0.293567839856283 to -0.266901173785663
-0.221121612782614 to -0.194454946711993
0.168480392699106 to 0.195147058769726
0.161972280745371 to 0.188638946815991
Trying to learn from memory 28, 2, 0.16
sum 0.0466724534521924 distri 0.0400600832102801
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.601144773681372 to 0.627811439751992
0.879586739261359 to 0.906253405331979
0.79254637451908 to 0.8192130405897
0.137974669952601 to 0.164641336023221
Changing layer 0's weights from 
0.00155051688816523 to 0.0282171829587854
-0.370375939801669 to -0.343709273731049
-0.604725205496288 to -0.578058539425667
-0.570165955618358 to -0.543499289547738
0.177701613708996 to 0.204368279779617
-0.504940339520908 to -0.478273673450287
-0.351110615924335 to -0.324443949853714
-0.37856502194736 to -0.35189835587674
-0.616056882933116 to -0.589390216862496
-0.659806938067413 to -0.633140271996793
Changing layer 1's weights from 
-0.65471002568338 to -0.628043359612759
-0.053541341021991 to -0.0268746749513708
-0.179244079783893 to -0.152577413713273
-0.189149418071246 to -0.162482752000626
-0.374202200368381 to -0.34753553429776
-0.512440034345127 to -0.485773368274506
-0.241721310809588 to -0.215054644738968
-0.627524339750743 to -0.600857673680123
-0.215861358836627 to -0.189194692766007
-0.578508221701122 to -0.551841555630501
Changing layer 2's weights from 
0.230680010124707 to 0.257346676195327
-0.323373475268817 to -0.296706809198197
-0.0387841846499207 to -0.0121175185793005
0.132415911957288 to 0.159082578027908
-0.0448367979082825 to -0.0181701318376623
-0.250846424296832 to -0.224179758226212
0.267966291710401 to 0.294632957781021
0.19554426412251 to 0.22221093019313
-0.536454030469394 to -0.509787364398774
-0.491496511891818 to -0.464829845821198
Changing layer 3's weights from 
0.197629711433911 to 0.224296377504531
-0.0659233715090516 to -0.0392567054384313
0.176686546608472 to 0.203353212679092
-0.730752685147672 to -0.704086019077052
0.0540106509175536 to 0.0806773169881738
-0.0762379552874329 to -0.0495712892168126
-0.406422802403903 to -0.379756136333283
0.0520900700536009 to 0.0787567361242211
-0.279605903819537 to -0.252939237748917
0.193223974510693 to 0.219890640581313
Changing layer 4's weights from 
0.0939501736607788 to 0.120616839731399
-0.114882626727557 to -0.0882159606569373
-0.315360226825213 to -0.288693560754593
-0.0284254934344056 to -0.00175882736378534
-0.267678299144244 to -0.241011633073624
-0.0362130786928894 to -0.00954641262226923
-0.0405716087374451 to -0.0139049426668249
-0.50246913094852 to -0.4758024648779
0.0912269566502808 to 0.117893622720901
-0.341074981883502 to -0.314408315812882
Changing layer 5's weights from 
0.246729991241956 to 0.273396657312576
-0.195343532756305 to -0.168676866685685
0.229844829842114 to 0.256511495912735
-0.705667351529337 to -0.679000685458717
-0.168398537829852 to -0.141731871759232
-0.566866480902171 to -0.540199814831551
-0.266901173785663 to -0.240234507715042
-0.194454946711993 to -0.167788280641373
0.195147058769726 to 0.221813724840347
0.188638946815991 to 0.215305612886611
Trying to learn from memory 29, 2, 0.16
sum 0.0466724534521924 distri 0.0400600832102801
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.627811439751992 to 0.654478105822612
0.906253405331979 to 0.932920071402599
0.8192130405897 to 0.84587970666032
0.164641336023221 to 0.191308002093841
Changing layer 0's weights from 
0.0282171829587854 to 0.0548838490294057
-0.343709273731049 to -0.317042607660429
-0.578058539425667 to -0.551391873355047
-0.543499289547738 to -0.516832623477117
0.204368279779617 to 0.231034945850237
-0.478273673450287 to -0.451607007379667
-0.324443949853714 to -0.297777283783094
-0.35189835587674 to -0.32523168980612
-0.589390216862496 to -0.562723550791876
-0.633140271996793 to -0.606473605926172
Changing layer 1's weights from 
-0.628043359612759 to -0.601376693542139
-0.0268746749513708 to -0.000208008880750572
-0.152577413713273 to -0.125910747642652
-0.162482752000626 to -0.135816085930006
-0.34753553429776 to -0.32086886822714
-0.485773368274506 to -0.459106702203886
-0.215054644738968 to -0.188387978668348
-0.600857673680123 to -0.574191007609503
-0.189194692766007 to -0.162528026695387
-0.551841555630501 to -0.525174889559881
Changing layer 2's weights from 
0.257346676195327 to 0.284013342265948
-0.296706809198197 to -0.270040143127577
-0.0121175185793005 to 0.0145491474913197
0.159082578027908 to 0.185749244098528
-0.0181701318376623 to 0.00849653423295792
-0.224179758226212 to -0.197513092155592
0.294632957781021 to 0.321299623851641
0.22221093019313 to 0.24887759626375
-0.509787364398774 to -0.483120698328153
-0.464829845821198 to -0.438163179750578
Changing layer 3's weights from 
0.224296377504531 to 0.250963043575151
-0.0392567054384313 to -0.0125900393678111
0.203353212679092 to 0.230019878749712
-0.704086019077052 to -0.677419353006432
0.0806773169881738 to 0.107343983058794
-0.0495712892168126 to -0.0229046231461924
-0.379756136333283 to -0.353089470262662
0.0787567361242211 to 0.105423402194841
-0.252939237748917 to -0.226272571678297
0.219890640581313 to 0.246557306651934
Changing layer 4's weights from 
0.120616839731399 to 0.147283505802019
-0.0882159606569373 to -0.061549294586317
-0.288693560754593 to -0.262026894683973
-0.00175882736378534 to 0.0249078387068349
-0.241011633073624 to -0.214344967003004
-0.00954641262226923 to 0.017120253448351
-0.0139049426668249 to 0.0127617234037953
-0.4758024648779 to -0.44913579880728
0.117893622720901 to 0.144560288791521
-0.314408315812882 to -0.287741649742262
Changing layer 5's weights from 
0.273396657312576 to 0.300063323383196
-0.168676866685685 to -0.142010200615065
0.256511495912735 to 0.283178161983355
-0.679000685458717 to -0.652334019388097
-0.141731871759232 to -0.115065205688612
-0.540199814831551 to -0.513533148760931
-0.240234507715042 to -0.213567841644422
-0.167788280641373 to -0.141121614570753
0.221813724840347 to 0.248480390910967
0.215305612886611 to 0.241972278957232
Trying to learn from memory 30, 2, 0.16
sum 0.0466741899505833 distri 0.0400691060267252
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.654478105822612 to 0.681144771893233
0.932920071402599 to 0.959586737473219
0.84587970666032 to 0.87254637273094
0.191308002093841 to 0.217974668164462
Changing layer 0's weights from 
0.0548838490294057 to 0.0815505151000259
-0.317042607660429 to -0.290375941589808
-0.551391873355047 to -0.524725207284427
-0.516832623477117 to -0.490165957406497
0.231034945850237 to 0.257701611920857
-0.451607007379667 to -0.424940341309047
-0.297777283783094 to -0.271110617712474
-0.32523168980612 to -0.298565023735499
-0.562723550791876 to -0.536056884721256
-0.606473605926172 to -0.579806939855552
Changing layer 1's weights from 
-0.601376693542139 to -0.574710027471519
-0.000208008880750572 to 0.0264586571898696
-0.125910747642652 to -0.0992440815720322
-0.135816085930006 to -0.109149419859386
-0.32086886822714 to -0.29420220215652
-0.459106702203886 to -0.432440036133266
-0.188387978668348 to -0.161721312597728
-0.574191007609503 to -0.547524341538883
-0.162528026695387 to -0.135861360624766
-0.525174889559881 to -0.498508223489261
Changing layer 2's weights from 
0.284013342265948 to 0.310680008336568
-0.270040143127577 to -0.243373477056956
0.0145491474913197 to 0.04121581356194
0.185749244098528 to 0.212415910169148
0.00849653423295792 to 0.0351632003035781
-0.197513092155592 to -0.170846426084971
0.321299623851641 to 0.347966289922261
0.24887759626375 to 0.27554426233437
-0.483120698328153 to -0.456454032257533
-0.438163179750578 to -0.411496513679958
Changing layer 3's weights from 
0.250963043575151 to 0.277629709645772
-0.0125900393678111 to 0.0140766267028091
0.230019878749712 to 0.256686544820332
-0.677419353006432 to -0.650752686935811
0.107343983058794 to 0.134010649129414
-0.0229046231461924 to 0.0037620429244278
-0.353089470262662 to -0.326422804192042
0.105423402194841 to 0.132090068265462
-0.226272571678297 to -0.199605905607676
0.246557306651934 to 0.273223972722554
Changing layer 4's weights from 
0.147283505802019 to 0.173950171872639
-0.061549294586317 to -0.0348826285156968
-0.262026894683973 to -0.235360228613353
0.0249078387068349 to 0.0515745047774551
-0.214344967003004 to -0.187678300932384
0.017120253448351 to 0.0437869195189712
0.0127617234037953 to 0.0394283894744155
-0.44913579880728 to -0.422469132736659
0.144560288791521 to 0.171226954862141
-0.287741649742262 to -0.261074983671641
Changing layer 5's weights from 
0.300063323383196 to 0.326729989453816
-0.142010200615065 to -0.115343534544444
0.283178161983355 to 0.309844828053975
-0.652334019388097 to -0.625667353317476
-0.115065205688612 to -0.0883985396179917
-0.513533148760931 to -0.486866482690311
-0.213567841644422 to -0.186901175573802
-0.141121614570753 to -0.114454948500133
0.248480390910967 to 0.275147056981587
0.241972278957232 to 0.268638945027852
10/5/2016 1:23:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:23:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:19 PMStarting learning phase with deltaScore: 0.6666667
Modified index 0's learning in memoryPool to 0.1333333
Modified index 1's learning in memoryPool to 0.1333333
Modified index 2's learning in memoryPool to 0.1333333
Modified index 3's learning in memoryPool to 0.1333333
Modified index 4's learning in memoryPool to 0.1333333
Modified index 5's learning in memoryPool to 0.1333333
Modified index 6's learning in memoryPool to 0.1333333
Modified index 7's learning in memoryPool to 0.1333333
Modified index 8's learning in memoryPool to 0.1333333
Modified index 9's learning in memoryPool to 0.1333333
Modified index 10's learning in memoryPool to 0.1333333
Modified index 11's learning in memoryPool to 0.1333333
Modified index 12's learning in memoryPool to 0.1333333
Modified index 13's learning in memoryPool to 0.1333333
Modified index 14's learning in memoryPool to 0.1333333
Modified index 15's learning in memoryPool to 0.1333333
Modified index 16's learning in memoryPool to 0.1333333
Modified index 17's learning in memoryPool to 0.1333333
Modified index 18's learning in memoryPool to 0.1333333
Modified index 19's learning in memoryPool to 0.1333333
Modified index 20's learning in memoryPool to 0.1333333
Modified index 21's learning in memoryPool to 0.1333333
10/5/2016 1:24:20 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 31, 2, 0.1333333
sum 0.0881611867386626 distri 0.0786219525543842
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.681144771893233 to 0.703366995274434
0.959586737473219 to 0.981808960854421
0.87254637273094 to 0.894768596112142
0.217974668164462 to 0.240196891545663
Changing layer 0's weights from 
0.0815505151000259 to 0.103772738481227
-0.290375941589808 to -0.268153718208607
-0.524725207284427 to -0.502502983903226
-0.490165957406497 to -0.467943734025296
0.257701611920857 to 0.279923835302059
-0.424940341309047 to -0.402718117927846
-0.271110617712474 to -0.248888394331272
-0.298565023735499 to -0.276342800354298
-0.536056884721256 to -0.513834661340054
-0.579806939855552 to -0.557584716474351
Changing layer 1's weights from 
-0.574710027471519 to -0.552487804090318
0.0264586571898696 to 0.0486808805710711
-0.0992440815720322 to -0.0770218581908308
-0.109149419859386 to -0.0869271964781843
-0.29420220215652 to -0.271979978775318
-0.432440036133266 to -0.410217812752064
-0.161721312597728 to -0.139499089216526
-0.547524341538883 to -0.525302118157681
-0.135861360624766 to -0.113639137243565
-0.498508223489261 to -0.47628600010806
Changing layer 2's weights from 
0.310680008336568 to 0.332902231717769
-0.243373477056956 to -0.221151253675755
0.04121581356194 to 0.0634380369431414
0.212415910169148 to 0.23463813355035
0.0351632003035781 to 0.0573854236847796
-0.170846426084971 to -0.14862420270377
0.347966289922261 to 0.370188513303463
0.27554426233437 to 0.297766485715572
-0.456454032257533 to -0.434231808876332
-0.411496513679958 to -0.389274290298756
Changing layer 3's weights from 
0.277629709645772 to 0.299851933026973
0.0140766267028091 to 0.0362988500840105
0.256686544820332 to 0.278908768201534
-0.650752686935811 to -0.62853046355461
0.134010649129414 to 0.156232872510616
0.0037620429244278 to 0.0259842663056292
-0.326422804192042 to -0.304200580810841
0.132090068265462 to 0.154312291646663
-0.199605905607676 to -0.177383682226475
0.273223972722554 to 0.295446196103755
Changing layer 4's weights from 
0.173950171872639 to 0.196172395253841
-0.0348826285156968 to -0.0126604051344954
-0.235360228613353 to -0.213138005232151
0.0515745047774551 to 0.0737967281586565
-0.187678300932384 to -0.165456077551182
0.0437869195189712 to 0.0660091429001726
0.0394283894744155 to 0.061650612855617
-0.422469132736659 to -0.400246909355458
0.171226954862141 to 0.193449178243343
-0.261074983671641 to -0.23885276029044
Changing layer 5's weights from 
0.326729989453816 to 0.348952212835018
-0.115343534544444 to -0.0931213111632429
0.309844828053975 to 0.332067051435177
-0.625667353317476 to -0.603445129936275
-0.0883985396179917 to -0.0661763162367903
-0.486866482690311 to -0.464644259309109
-0.186901175573802 to -0.164678952192601
-0.114454948500133 to -0.0922327251189314
0.275147056981587 to 0.297369280362789
0.268638945027852 to 0.290861168409053
Trying to learn from memory 32, 2, 0.1333333
sum 0.0881611987229029 distri 0.0786219593766724
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.703366995274434 to 0.725589218655635
0.981808960854421 to 1.00403118423562
0.894768596112142 to 0.916990819493343
0.240196891545663 to 0.262419114926865
Changing layer 0's weights from 
0.103772738481227 to 0.125994961862429
-0.268153718208607 to -0.245931494827406
-0.502502983903226 to -0.480280760522024
-0.467943734025296 to -0.445721510644094
0.279923835302059 to 0.30214605868326
-0.402718117927846 to -0.380495894546644
-0.248888394331272 to -0.226666170950071
-0.276342800354298 to -0.254120576973096
-0.513834661340054 to -0.491612437958853
-0.557584716474351 to -0.535362493093149
Changing layer 1's weights from 
-0.552487804090318 to -0.530265580709116
0.0486808805710711 to 0.0709031039522725
-0.0770218581908308 to -0.0547996348096294
-0.0869271964781843 to -0.0647049730969829
-0.271979978775318 to -0.249757755394117
-0.410217812752064 to -0.387995589370863
-0.139499089216526 to -0.117276865835325
-0.525302118157681 to -0.50307989477648
-0.113639137243565 to -0.0914169138623636
-0.47628600010806 to -0.454063776726858
Changing layer 2's weights from 
0.332902231717769 to 0.355124455098971
-0.221151253675755 to -0.198929030294553
0.0634380369431414 to 0.0856602603243428
0.23463813355035 to 0.256860356931551
0.0573854236847796 to 0.079607647065981
-0.14862420270377 to -0.126401979322569
0.370188513303463 to 0.392410736684664
0.297766485715572 to 0.319988709096773
-0.434231808876332 to -0.41200958549513
-0.389274290298756 to -0.367052066917555
Changing layer 3's weights from 
0.299851933026973 to 0.322074156408175
0.0362988500840105 to 0.0585210734652119
0.278908768201534 to 0.301130991582735
-0.62853046355461 to -0.606308240173409
0.156232872510616 to 0.178455095891817
0.0259842663056292 to 0.0482064896868306
-0.304200580810841 to -0.281978357429639
0.154312291646663 to 0.176534515027864
-0.177383682226475 to -0.155161458845274
0.295446196103755 to 0.317668419484957
Changing layer 4's weights from 
0.196172395253841 to 0.218394618635042
-0.0126604051344954 to 0.00956181824670602
-0.213138005232151 to -0.19091578185095
0.0737967281586565 to 0.0960189515398579
-0.165456077551182 to -0.143233854169981
0.0660091429001726 to 0.088231366281374
0.061650612855617 to 0.0838728362368184
-0.400246909355458 to -0.378024685974256
0.193449178243343 to 0.215671401624544
-0.23885276029044 to -0.216630536909238
Changing layer 5's weights from 
0.348952212835018 to 0.371174436216219
-0.0931213111632429 to -0.0708990877820415
0.332067051435177 to 0.354289274816378
-0.603445129936275 to -0.581222906555074
-0.0661763162367903 to -0.0439540928555889
-0.464644259309109 to -0.442422035927908
-0.164678952192601 to -0.142456728811399
-0.0922327251189314 to -0.07001050173773
0.297369280362789 to 0.31959150374399
0.290861168409053 to 0.313083391790255
Trying to learn from memory 33, 2, 0.1333333
sum 0.0881614375220582 distri 0.0786221130090617
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.725589218655635 to 0.747811442036837
1.00403118423562 to 1.02625340761682
0.916990819493343 to 0.939213042874544
0.262419114926865 to 0.284641338308066
Changing layer 0's weights from 
0.125994961862429 to 0.14821718524363
-0.245931494827406 to -0.223709271446204
-0.480280760522024 to -0.458058537140823
-0.445721510644094 to -0.423499287262893
0.30214605868326 to 0.324368282064461
-0.380495894546644 to -0.358273671165443
-0.226666170950071 to -0.20444394756887
-0.254120576973096 to -0.231898353591895
-0.491612437958853 to -0.469390214577651
-0.535362493093149 to -0.513140269711948
Changing layer 1's weights from 
-0.530265580709116 to -0.508043357327915
0.0709031039522725 to 0.0931253273334739
-0.0547996348096294 to -0.0325774114284279
-0.0647049730969829 to -0.0424827497157815
-0.249757755394117 to -0.227535532012916
-0.387995589370863 to -0.365773365989661
-0.117276865835325 to -0.0950546424541236
-0.50307989477648 to -0.480857671395278
-0.0914169138623636 to -0.0691946904811622
-0.454063776726858 to -0.431841553345657
Changing layer 2's weights from 
0.355124455098971 to 0.377346678480172
-0.198929030294553 to -0.176706806913352
0.0856602603243428 to 0.107882483705544
0.256860356931551 to 0.279082580312752
0.079607647065981 to 0.101829870447182
-0.126401979322569 to -0.104179755941367
0.392410736684664 to 0.414632960065866
0.319988709096773 to 0.342210932477975
-0.41200958549513 to -0.389787362113929
-0.367052066917555 to -0.344829843536353
Changing layer 3's weights from 
0.322074156408175 to 0.344296379789376
0.0585210734652119 to 0.0807432968464134
0.301130991582735 to 0.323353214963937
-0.606308240173409 to -0.584086016792207
0.178455095891817 to 0.200677319273018
0.0482064896868306 to 0.0704287130680321
-0.281978357429639 to -0.259756134048438
0.176534515027864 to 0.198756738409066
-0.155161458845274 to -0.132939235464072
0.317668419484957 to 0.339890642866158
Changing layer 4's weights from 
0.218394618635042 to 0.240616842016244
0.00956181824670602 to 0.0317840416279074
-0.19091578185095 to -0.168693558469749
0.0960189515398579 to 0.118241174921059
-0.143233854169981 to -0.121011630788779
0.088231366281374 to 0.110453589662575
0.0838728362368184 to 0.10609505961802
-0.378024685974256 to -0.355802462593055
0.215671401624544 to 0.237893625005746
-0.216630536909238 to -0.194408313528037
Changing layer 5's weights from 
0.371174436216219 to 0.393396659597421
-0.0708990877820415 to -0.0486768644008401
0.354289274816378 to 0.376511498197579
-0.581222906555074 to -0.559000683173872
-0.0439540928555889 to -0.0217318694743874
-0.442422035927908 to -0.420199812546706
-0.142456728811399 to -0.120234505430198
-0.07001050173773 to -0.0477882783565285
0.31959150374399 to 0.341813727125191
0.313083391790255 to 0.335305615171456
Trying to learn from memory 34, 1, 0.1333333
sum 0.0881612784074764 distri 0.0328615266527133
Using diff 0.033259432152894 and condRate 0.166666666666667
Changed category 1 weights from 
0.321849575139585 to 0.322588673670418
0.555165162183347 to 0.55590426071418
0.130239924289289 to 0.130979022820122
0.134332736827436 to 0.135071835358269
Changing layer 0's weights from 
0.14821718524363 to 0.148956283774464
-0.223709271446204 to -0.222970172915371
-0.458058537140823 to -0.457319438609989
-0.423499287262893 to -0.422760188732059
0.324368282064461 to 0.325107380595295
-0.358273671165443 to -0.357534572634609
-0.20444394756887 to -0.203704849038036
-0.231898353591895 to -0.231159255061062
-0.469390214577651 to -0.468651116046818
-0.513140269711948 to -0.512401171181115
Changing layer 1's weights from 
-0.508043357327915 to -0.507304258797081
0.0931253273334739 to 0.0938644258643074
-0.0325774114284279 to -0.0318383128975944
-0.0424827497157815 to -0.0417436511849479
-0.227535532012916 to -0.226796433482082
-0.365773365989661 to -0.365034267458828
-0.0950546424541236 to -0.09431554392329
-0.480857671395278 to -0.480118572864445
-0.0691946904811622 to -0.0684555919503286
-0.431841553345657 to -0.431102454814823
Changing layer 2's weights from 
0.377346678480172 to 0.378085777011006
-0.176706806913352 to -0.175967708382519
0.107882483705544 to 0.108621582236378
0.279082580312752 to 0.279821678843586
0.101829870447182 to 0.102568968978016
-0.104179755941367 to -0.103440657410534
0.414632960065866 to 0.415372058596699
0.342210932477975 to 0.342950031008808
-0.389787362113929 to -0.389048263583095
-0.344829843536353 to -0.34409074500552
Changing layer 3's weights from 
0.344296379789376 to 0.34503547832021
0.0807432968464134 to 0.0814823953772469
0.323353214963937 to 0.32409231349477
-0.584086016792207 to -0.583346918261374
0.200677319273018 to 0.201416417803852
0.0704287130680321 to 0.0711678115988656
-0.259756134048438 to -0.259017035517604
0.198756738409066 to 0.199495836939899
-0.132939235464072 to -0.132200136933239
0.339890642866158 to 0.340629741396992
Changing layer 4's weights from 
0.240616842016244 to 0.241355940547077
0.0317840416279074 to 0.032523140158741
-0.168693558469749 to -0.167954459938915
0.118241174921059 to 0.118980273451893
-0.121011630788779 to -0.120272532257946
0.110453589662575 to 0.111192688193409
0.10609505961802 to 0.106834158148853
-0.355802462593055 to -0.355063364062221
0.237893625005746 to 0.238632723536579
-0.194408313528037 to -0.193669214997204
Changing layer 5's weights from 
0.393396659597421 to 0.394135758128254
-0.0486768644008401 to -0.0479377658700065
0.376511498197579 to 0.377250596728413
-0.559000683173872 to -0.558261584643039
-0.0217318694743874 to -0.0209927709435539
-0.420199812546706 to -0.419460714015873
-0.120234505430198 to -0.119495406899364
-0.0477882783565285 to -0.047049179825695
0.341813727125191 to 0.342552825656025
0.335305615171456 to 0.33604471370229
Trying to learn from memory 35, 2, 0.1333333
sum 0.0881606928287414 distri 0.0786214127702333
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.747811442036837 to 0.770033665418038
1.02625340761682 to 1.04847563099803
0.939213042874544 to 0.961435266255746
0.284641338308066 to 0.306863561689267
Changing layer 0's weights from 
0.148956283774464 to 0.171178507155665
-0.222970172915371 to -0.200747949534169
-0.457319438609989 to -0.435097215228788
-0.422760188732059 to -0.400537965350858
0.325107380595295 to 0.347329603976496
-0.357534572634609 to -0.335312349253408
-0.203704849038036 to -0.181482625656835
-0.231159255061062 to -0.20893703167986
-0.468651116046818 to -0.446428892665616
-0.512401171181115 to -0.490178947799913
Changing layer 1's weights from 
-0.507304258797081 to -0.48508203541588
0.0938644258643074 to 0.116086649245509
-0.0318383128975944 to -0.009616089516393
-0.0417436511849479 to -0.0195214278037465
-0.226796433482082 to -0.204574210100881
-0.365034267458828 to -0.342812044077627
-0.09431554392329 to -0.0720933205420886
-0.480118572864445 to -0.457896349483243
-0.0684555919503286 to -0.0462333685691272
-0.431102454814823 to -0.408880231433622
Changing layer 2's weights from 
0.378085777011006 to 0.400308000392207
-0.175967708382519 to -0.153745485001317
0.108621582236378 to 0.130843805617579
0.279821678843586 to 0.302043902224787
0.102568968978016 to 0.124791192359217
-0.103440657410534 to -0.0812184340293322
0.415372058596699 to 0.437594281977901
0.342950031008808 to 0.36517225439001
-0.389048263583095 to -0.366826040201894
-0.34409074500552 to -0.321868521624318
Changing layer 3's weights from 
0.34503547832021 to 0.367257701701411
0.0814823953772469 to 0.103704618758448
0.32409231349477 to 0.346314536875972
-0.583346918261374 to -0.561124694880172
0.201416417803852 to 0.223638641185053
0.0711678115988656 to 0.093390034980067
-0.259017035517604 to -0.236794812136403
0.199495836939899 to 0.221718060321101
-0.132200136933239 to -0.109977913552037
0.340629741396992 to 0.362851964778193
Changing layer 4's weights from 
0.241355940547077 to 0.263578163928279
0.032523140158741 to 0.0547453635399424
-0.167954459938915 to -0.145732236557714
0.118980273451893 to 0.141202496833094
-0.120272532257946 to -0.0980503088767443
0.111192688193409 to 0.13341491157461
0.106834158148853 to 0.129056381530055
-0.355063364062221 to -0.33284114068102
0.238632723536579 to 0.260854946917781
-0.193669214997204 to -0.171446991616002
Changing layer 5's weights from 
0.394135758128254 to 0.416357981509456
-0.0479377658700065 to -0.0257155424888051
0.377250596728413 to 0.399472820109614
-0.558261584643039 to -0.536039361261837
-0.0209927709435539 to 0.0012294524376475
-0.419460714015873 to -0.397238490634671
-0.119495406899364 to -0.0972731835181627
-0.047049179825695 to -0.0248269564444936
0.342552825656025 to 0.364775049037226
0.33604471370229 to 0.358266937083491
Trying to learn from memory 36, 1, 0.1333333
sum 0.0881617715588908 distri 0.032861708628925
Using diff 0.0332596200402431 and condRate 0.166666666666667
Changed category 1 weights from 
0.322588673670418 to 0.323327776376526
0.55590426071418 to 0.556643363420289
0.130979022820122 to 0.13171812552623
0.135071835358269 to 0.135810938064377
Changing layer 0's weights from 
0.171178507155665 to 0.171917609861773
-0.200747949534169 to -0.200008846828061
-0.435097215228788 to -0.43435811252268
-0.400537965350858 to -0.39979886264475
0.347329603976496 to 0.348068706682605
-0.335312349253408 to -0.334573246547299
-0.181482625656835 to -0.180743522950726
-0.20893703167986 to -0.208197928973752
-0.446428892665616 to -0.445689789959508
-0.490178947799913 to -0.489439845093805
Changing layer 1's weights from 
-0.48508203541588 to -0.484342932709772
0.116086649245509 to 0.116825751951617
-0.009616089516393 to -0.00887698681028483
-0.0195214278037465 to -0.0187823250976384
-0.204574210100881 to -0.203835107394772
-0.342812044077627 to -0.342072941371518
-0.0720933205420886 to -0.0713542178359804
-0.457896349483243 to -0.457157246777135
-0.0462333685691272 to -0.0454942658630191
-0.408880231433622 to -0.408141128727514
Changing layer 2's weights from 
0.400308000392207 to 0.401047103098315
-0.153745485001317 to -0.153006382295209
0.130843805617579 to 0.131582908323687
0.302043902224787 to 0.302783004930896
0.124791192359217 to 0.125530295065326
-0.0812184340293322 to -0.0804793313232241
0.437594281977901 to 0.438333384684009
0.36517225439001 to 0.365911357096118
-0.366826040201894 to -0.366086937495786
-0.321868521624318 to -0.32112941891821
Changing layer 3's weights from 
0.367257701701411 to 0.367996804407519
0.103704618758448 to 0.104443721464556
0.346314536875972 to 0.34705363958208
-0.561124694880172 to -0.560385592174064
0.223638641185053 to 0.224377743891162
0.093390034980067 to 0.0941291376861752
-0.236794812136403 to -0.236055709430295
0.221718060321101 to 0.222457163027209
-0.109977913552037 to -0.109238810845929
0.362851964778193 to 0.363591067484301
Changing layer 4's weights from 
0.263578163928279 to 0.264317266634387
0.0547453635399424 to 0.0554844662460506
-0.145732236557714 to -0.144993133851605
0.141202496833094 to 0.141941599539202
-0.0980503088767443 to -0.0973112061706361
0.13341491157461 to 0.134154014280719
0.129056381530055 to 0.129795484236163
-0.33284114068102 to -0.332102037974912
0.260854946917781 to 0.261594049623889
-0.171446991616002 to -0.170707888909894
Changing layer 5's weights from 
0.416357981509456 to 0.417097084215564
-0.0257155424888051 to -0.0249764397826969
0.399472820109614 to 0.400211922815723
-0.536039361261837 to -0.535300258555729
0.0012294524376475 to 0.00196855514375567
-0.397238490634671 to -0.396499387928563
-0.0972731835181627 to -0.0965340808120546
-0.0248269564444936 to -0.0240878537383854
0.364775049037226 to 0.365514151743335
0.358266937083491 to 0.359006039789599
Trying to learn from memory 37, 2, 0.1333333
sum 0.088162277487999 distri 0.0786226636528728
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.770033665418038 to 0.79225588879924
1.04847563099803 to 1.07069785437923
0.961435266255746 to 0.983657489636947
0.306863561689267 to 0.329085785070469
Changing layer 0's weights from 
0.171917609861773 to 0.194139833242975
-0.200008846828061 to -0.17778662344686
-0.43435811252268 to -0.412135889141478
-0.39979886264475 to -0.377576639263548
0.348068706682605 to 0.370290930063806
-0.334573246547299 to -0.312351023166098
-0.180743522950726 to -0.158521299569525
-0.208197928973752 to -0.185975705592551
-0.445689789959508 to -0.423467566578307
-0.489439845093805 to -0.467217621712603
Changing layer 1's weights from 
-0.484342932709772 to -0.46212070932857
0.116825751951617 to 0.139047975332818
-0.00887698681028483 to 0.0133452365709166
-0.0187823250976384 to 0.00343989828356307
-0.203835107394772 to -0.181612884013571
-0.342072941371518 to -0.319850717990317
-0.0713542178359804 to -0.049131994454779
-0.457157246777135 to -0.434935023395934
-0.0454942658630191 to -0.0232720424818176
-0.408141128727514 to -0.385918905346312
Changing layer 2's weights from 
0.401047103098315 to 0.423269326479517
-0.153006382295209 to -0.130784158914008
0.131582908323687 to 0.153805131704889
0.302783004930896 to 0.325005228312097
0.125530295065326 to 0.147752518446527
-0.0804793313232241 to -0.0582571079420227
0.438333384684009 to 0.46055560806521
0.365911357096118 to 0.388133580477319
-0.366086937495786 to -0.343864714114584
-0.32112941891821 to -0.298907195537009
Changing layer 3's weights from 
0.367996804407519 to 0.390219027788721
0.104443721464556 to 0.126665944845758
0.34705363958208 to 0.369275862963281
-0.560385592174064 to -0.538163368792863
0.224377743891162 to 0.246599967272363
0.0941291376861752 to 0.116351361067377
-0.236055709430295 to -0.213833486049093
0.222457163027209 to 0.24467938640841
-0.109238810845929 to -0.0870165874647277
0.363591067484301 to 0.385813290865503
Changing layer 4's weights from 
0.264317266634387 to 0.286539490015588
0.0554844662460506 to 0.077706689627252
-0.144993133851605 to -0.122770910470404
0.141941599539202 to 0.164163822920404
-0.0973112061706361 to -0.0750889827894347
0.134154014280719 to 0.15637623766192
0.129795484236163 to 0.152017707617364
-0.332102037974912 to -0.30987981459371
0.261594049623889 to 0.28381627300509
-0.170707888909894 to -0.148485665528693
Changing layer 5's weights from 
0.417097084215564 to 0.439319307596765
-0.0249764397826969 to -0.00275421640149552
0.400211922815723 to 0.422434146196924
-0.535300258555729 to -0.513078035174528
0.00196855514375567 to 0.0241907785249571
-0.396499387928563 to -0.374277164547362
-0.0965340808120546 to -0.0743118574308532
-0.0240878537383854 to -0.001865630357184
0.365514151743335 to 0.387736375124536
0.359006039789599 to 0.381228263170801
Trying to learn from memory 38, 2, 0.1333333
sum 0.0881613357358955 distri 0.0786220851152102
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.79225588879924 to 0.814478112180441
1.07069785437923 to 1.09292007776043
0.983657489636947 to 1.00587971301815
0.329085785070469 to 0.35130800845167
Changing layer 0's weights from 
0.194139833242975 to 0.216362056624176
-0.17778662344686 to -0.155564400065658
-0.412135889141478 to -0.389913665760277
-0.377576639263548 to -0.355354415882347
0.370290930063806 to 0.392513153445007
-0.312351023166098 to -0.290128799784897
-0.158521299569525 to -0.136299076188324
-0.185975705592551 to -0.163753482211349
-0.423467566578307 to -0.401245343197105
-0.467217621712603 to -0.444995398331402
Changing layer 1's weights from 
-0.46212070932857 to -0.439898485947369
0.139047975332818 to 0.16127019871402
0.0133452365709166 to 0.035567459952118
0.00343989828356307 to 0.0256621216647645
-0.181612884013571 to -0.15939066063237
-0.319850717990317 to -0.297628494609115
-0.049131994454779 to -0.0269097710735776
-0.434935023395934 to -0.412712800014732
-0.0232720424818176 to -0.00104981910061621
-0.385918905346312 to -0.363696681965111
Changing layer 2's weights from 
0.423269326479517 to 0.445491549860718
-0.130784158914008 to -0.108561935532806
0.153805131704889 to 0.17602735508609
0.325005228312097 to 0.347227451693298
0.147752518446527 to 0.169974741827728
-0.0582571079420227 to -0.0360348845608212
0.46055560806521 to 0.482777831446412
0.388133580477319 to 0.410355803858521
-0.343864714114584 to -0.321642490733383
-0.298907195537009 to -0.276684972155807
Changing layer 3's weights from 
0.390219027788721 to 0.412441251169922
0.126665944845758 to 0.148888168226959
0.369275862963281 to 0.391498086344483
-0.538163368792863 to -0.515941145411661
0.246599967272363 to 0.268822190653564
0.116351361067377 to 0.138573584448578
-0.213833486049093 to -0.191611262667892
0.24467938640841 to 0.266901609789612
-0.0870165874647277 to -0.0647943640835263
0.385813290865503 to 0.408035514246704
Changing layer 4's weights from 
0.286539490015588 to 0.30876171339679
0.077706689627252 to 0.0999289130084534
-0.122770910470404 to -0.100548687089203
0.164163822920404 to 0.186386046301605
-0.0750889827894347 to -0.0528667594082333
0.15637623766192 to 0.178598461043121
0.152017707617364 to 0.174239930998566
-0.30987981459371 to -0.287657591212509
0.28381627300509 to 0.306038496386292
-0.148485665528693 to -0.126263442147491
Changing layer 5's weights from 
0.439319307596765 to 0.461541530977967
-0.00275421640149552 to 0.0194680069797059
0.422434146196924 to 0.444656369578125
-0.513078035174528 to -0.490855811793326
0.0241907785249571 to 0.0464130019061585
-0.374277164547362 to -0.35205494116616
-0.0743118574308532 to -0.0520896340496517
-0.001865630357184 to 0.0203565930240174
0.387736375124536 to 0.409958598505737
0.381228263170801 to 0.403450486552002
Trying to learn from memory 39, 2, 0.1333333
sum 0.0881611637434022 distri 0.078621937478893
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.814478112180441 to 0.836700335561642
1.09292007776043 to 1.11514230114163
1.00587971301815 to 1.02810193639935
0.35130800845167 to 0.373530231832872
Changing layer 0's weights from 
0.216362056624176 to 0.238584280005377
-0.155564400065658 to -0.133342176684457
-0.389913665760277 to -0.367691442379075
-0.355354415882347 to -0.333132192501145
0.392513153445007 to 0.414735376826209
-0.290128799784897 to -0.267906576403695
-0.136299076188324 to -0.114076852807122
-0.163753482211349 to -0.141531258830148
-0.401245343197105 to -0.379023119815904
-0.444995398331402 to -0.422773174950201
Changing layer 1's weights from 
-0.439898485947369 to -0.417676262566167
0.16127019871402 to 0.183492422095221
0.035567459952118 to 0.0577896833333194
0.0256621216647645 to 0.0478843450459659
-0.15939066063237 to -0.137168437251168
-0.297628494609115 to -0.275406271227914
-0.0269097710735776 to -0.00468754769237618
-0.412712800014732 to -0.390490576633531
-0.00104981910061621 to 0.0211724042805852
-0.363696681965111 to -0.341474458583909
Changing layer 2's weights from 
0.445491549860718 to 0.46771377324192
-0.108561935532806 to -0.0863397121516047
0.17602735508609 to 0.198249578467292
0.347227451693298 to 0.3694496750745
0.169974741827728 to 0.19219696520893
-0.0360348845608212 to -0.0138126611796198
0.482777831446412 to 0.505000054827613
0.410355803858521 to 0.432578027239722
-0.321642490733383 to -0.299420267352181
-0.276684972155807 to -0.254462748774606
Changing layer 3's weights from 
0.412441251169922 to 0.434663474551124
0.148888168226959 to 0.171110391608161
0.391498086344483 to 0.413720309725684
-0.515941145411661 to -0.49371892203046
0.268822190653564 to 0.291044414034766
0.138573584448578 to 0.160795807829779
-0.191611262667892 to -0.169389039286691
0.266901609789612 to 0.289123833170813
-0.0647943640835263 to -0.0425721407023248
0.408035514246704 to 0.430257737627906
Changing layer 4's weights from 
0.30876171339679 to 0.330983936777991
0.0999289130084534 to 0.122151136389655
-0.100548687089203 to -0.0783264637080012
0.186386046301605 to 0.208608269682807
-0.0528667594082333 to -0.0306445360270319
0.178598461043121 to 0.200820684424323
0.174239930998566 to 0.196462154379767
-0.287657591212509 to -0.265435367831307
0.306038496386292 to 0.328260719767493
-0.126263442147491 to -0.10404121876629
Changing layer 5's weights from 
0.461541530977967 to 0.483763754359168
0.0194680069797059 to 0.0416902303609073
0.444656369578125 to 0.466878592959327
-0.490855811793326 to -0.468633588412125
0.0464130019061585 to 0.0686352252873599
-0.35205494116616 to -0.329832717784959
-0.0520896340496517 to -0.0298674106684503
0.0203565930240174 to 0.0425788164052189
0.409958598505737 to 0.432180821886939
0.403450486552002 to 0.425672709933204
Trying to learn from memory 40, 2, 0.1333333
sum 0.0881611997946588 distri 0.0786219577193335
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.836700335561642 to 0.858922558942844
1.11514230114163 to 1.13736452452283
1.02810193639935 to 1.05032415978055
0.373530231832872 to 0.395752455214073
Changing layer 0's weights from 
0.238584280005377 to 0.260806503386579
-0.133342176684457 to -0.111119953303255
-0.367691442379075 to -0.345469218997874
-0.333132192501145 to -0.310909969119944
0.414735376826209 to 0.43695760020741
-0.267906576403695 to -0.245684353022494
-0.114076852807122 to -0.0918546294259208
-0.141531258830148 to -0.119309035448946
-0.379023119815904 to -0.356800896434702
-0.422773174950201 to -0.400550951568999
Changing layer 1's weights from 
-0.417676262566167 to -0.395454039184966
0.183492422095221 to 0.205714645476423
0.0577896833333194 to 0.0800119067145209
0.0478843450459659 to 0.0701065684271673
-0.137168437251168 to -0.114946213869967
-0.275406271227914 to -0.253184047846713
-0.00468754769237618 to 0.0175346756888252
-0.390490576633531 to -0.368268353252329
0.0211724042805852 to 0.0433946276617866
-0.341474458583909 to -0.319252235202708
Changing layer 2's weights from 
0.46771377324192 to 0.489935996623121
-0.0863397121516047 to -0.0641174887704033
0.198249578467292 to 0.220471801848493
0.3694496750745 to 0.391671898455701
0.19219696520893 to 0.214419188590131
-0.0138126611796198 to 0.00840956220158162
0.505000054827613 to 0.527222278208814
0.432578027239722 to 0.454800250620924
-0.299420267352181 to -0.27719804397098
-0.254462748774606 to -0.232240525393404
Changing layer 3's weights from 
0.434663474551124 to 0.456885697932325
0.171110391608161 to 0.193332614989362
0.413720309725684 to 0.435942533106885
-0.49371892203046 to -0.471496698649258
0.291044414034766 to 0.313266637415967
0.160795807829779 to 0.183018031210981
-0.169389039286691 to -0.147166815905489
0.289123833170813 to 0.311346056552015
-0.0425721407023248 to -0.0203499173211234
0.430257737627906 to 0.452479961009107
Changing layer 4's weights from 
0.330983936777991 to 0.353206160159193
0.122151136389655 to 0.144373359770856
-0.0783264637080012 to -0.0561042403267997
0.208608269682807 to 0.230830493064008
-0.0306445360270319 to -0.00842231264583044
0.200820684424323 to 0.223042907805524
0.196462154379767 to 0.218684377760969
-0.265435367831307 to -0.243213144450106
0.328260719767493 to 0.350482943148695
-0.10404121876629 to -0.0818189953850883
Changing layer 5's weights from 
0.483763754359168 to 0.50598597774037
0.0416902303609073 to 0.0639124537421087
0.466878592959327 to 0.489100816340528
-0.468633588412125 to -0.446411365030923
0.0686352252873599 to 0.0908574486685614
-0.329832717784959 to -0.307610494403757
-0.0298674106684503 to -0.0076451872872489
0.0425788164052189 to 0.0648010397864203
0.432180821886939 to 0.45440304526814
0.425672709933204 to 0.447894933314405
10/5/2016 1:24:20 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 41, 2, 0.1333333
sum 0.0881612035663051 distri 0.0786219647652975
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.858922558942844 to 0.881144782324045
1.13736452452283 to 1.15958674790403
1.05032415978055 to 1.07254638316175
0.395752455214073 to 0.417974678595275
Changing layer 0's weights from 
0.260806503386579 to 0.28302872676778
-0.111119953303255 to -0.0888977299220539
-0.345469218997874 to -0.323246995616672
-0.310909969119944 to -0.288687745738743
0.43695760020741 to 0.459179823588612
-0.245684353022494 to -0.223462129641292
-0.0918546294259208 to -0.0696324060447194
-0.119309035448946 to -0.0970868120677448
-0.356800896434702 to -0.334578673053501
-0.400550951568999 to -0.378328728187798
Changing layer 1's weights from 
-0.395454039184966 to -0.373231815803764
0.205714645476423 to 0.227936868857624
0.0800119067145209 to 0.102234130095722
0.0701065684271673 to 0.0923287918083688
-0.114946213869967 to -0.0927239904887653
-0.253184047846713 to -0.230961824465511
0.0175346756888252 to 0.0397568990700267
-0.368268353252329 to -0.346046129871128
0.0433946276617866 to 0.0656168510429881
-0.319252235202708 to -0.297030011821506
Changing layer 2's weights from 
0.489935996623121 to 0.512158220004323
-0.0641174887704033 to -0.0418952653892018
0.220471801848493 to 0.242694025229694
0.391671898455701 to 0.413894121836903
0.214419188590131 to 0.236641411971333
0.00840956220158162 to 0.030631785582783
0.527222278208814 to 0.549444501590016
0.454800250620924 to 0.477022474002125
-0.27719804397098 to -0.254975820589779
-0.232240525393404 to -0.210018302012203
Changing layer 3's weights from 
0.456885697932325 to 0.479107921313526
0.193332614989362 to 0.215554838370564
0.435942533106885 to 0.458164756488087
-0.471496698649258 to -0.449274475268057
0.313266637415967 to 0.335488860797169
0.183018031210981 to 0.205240254592182
-0.147166815905489 to -0.124944592524288
0.311346056552015 to 0.333568279933216
-0.0203499173211234 to 0.00187230606007801
0.452479961009107 to 0.474702184390309
Changing layer 4's weights from 
0.353206160159193 to 0.375428383540394
0.144373359770856 to 0.166595583152058
-0.0561042403267997 to -0.0338820169455983
0.230830493064008 to 0.25305271644521
-0.00842231264583044 to 0.013799910735371
0.223042907805524 to 0.245265131186726
0.218684377760969 to 0.24090660114217
-0.243213144450106 to -0.220990921068905
0.350482943148695 to 0.372705166529896
-0.0818189953850883 to -0.0595967720038868
Changing layer 5's weights from 
0.50598597774037 to 0.528208201121571
0.0639124537421087 to 0.0861346771233102
0.489100816340528 to 0.51132303972173
-0.446411365030923 to -0.424189141649722
0.0908574486685614 to 0.113079672049763
-0.307610494403757 to -0.285388271022556
-0.0076451872872489 to 0.0145770360939525
0.0648010397864203 to 0.0870232631676217
0.45440304526814 to 0.476625268649342
0.447894933314405 to 0.470117156695606
Trying to learn from memory 42, 2, 0.1333333
sum 0.088161184218642 distri 0.078621936082158
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.881144782324045 to 0.903367005705247
1.15958674790403 to 1.18180897128523
1.07254638316175 to 1.09476860654295
0.417974678595275 to 0.440196901976476
Changing layer 0's weights from 
0.28302872676778 to 0.305250950148982
-0.0888977299220539 to -0.0666755065408525
-0.323246995616672 to -0.301024772235471
-0.288687745738743 to -0.266465522357541
0.459179823588612 to 0.481402046969813
-0.223462129641292 to -0.201239906260091
-0.0696324060447194 to -0.047410182663518
-0.0970868120677448 to -0.0748645886865434
-0.334578673053501 to -0.3123564496723
-0.378328728187798 to -0.356106504806596
Changing layer 1's weights from 
-0.373231815803764 to -0.351009592422563
0.227936868857624 to 0.250159092238826
0.102234130095722 to 0.124456353476924
0.0923287918083688 to 0.11455101518957
-0.0927239904887653 to -0.0705017671075639
-0.230961824465511 to -0.20873960108431
0.0397568990700267 to 0.0619791224512281
-0.346046129871128 to -0.323823906489927
0.0656168510429881 to 0.0878390744241895
-0.297030011821506 to -0.274807788440305
Changing layer 2's weights from 
0.512158220004323 to 0.534380443385524
-0.0418952653892018 to -0.0196730420080004
0.242694025229694 to 0.264916248610896
0.413894121836903 to 0.436116345218104
0.236641411971333 to 0.258863635352534
0.030631785582783 to 0.0528540089639845
0.549444501590016 to 0.571666724971217
0.477022474002125 to 0.499244697383326
-0.254975820589779 to -0.232753597208577
-0.210018302012203 to -0.187796078631001
Changing layer 3's weights from 
0.479107921313526 to 0.501330144694728
0.215554838370564 to 0.237777061751765
0.458164756488087 to 0.480386979869288
-0.449274475268057 to -0.427052251886855
0.335488860797169 to 0.35771108417837
0.205240254592182 to 0.227462477973384
-0.124944592524288 to -0.102722369143086
0.333568279933216 to 0.355790503314418
0.00187230606007801 to 0.0240945294412794
0.474702184390309 to 0.49692440777151
Changing layer 4's weights from 
0.375428383540394 to 0.397650606921595
0.166595583152058 to 0.188817806533259
-0.0338820169455983 to -0.0116597935643969
0.25305271644521 to 0.275274939826411
0.013799910735371 to 0.0360221341165724
0.245265131186726 to 0.267487354567927
0.24090660114217 to 0.263128824523371
-0.220990921068905 to -0.198768697687703
0.372705166529896 to 0.394927389911097
-0.0595967720038868 to -0.0373745486226854
Changing layer 5's weights from 
0.528208201121571 to 0.550430424502772
0.0861346771233102 to 0.108356900504512
0.51132303972173 to 0.533545263102931
-0.424189141649722 to -0.40196691826852
0.113079672049763 to 0.135301895430964
-0.285388271022556 to -0.263166047641355
0.0145770360939525 to 0.036799259475154
0.0870232631676217 to 0.109245486548823
0.476625268649342 to 0.498847492030543
0.470117156695606 to 0.492339380076808
Trying to learn from memory 42, 1, 0.1333333
sum 0.088161184218642 distri 0.0328615300893922
Using diff 0.0332593580745893 and condRate 0.166666666666667
Changed category 1 weights from 
0.323327776376526 to 0.324066873261175
0.556643363420289 to 0.557382460304937
0.13171812552623 to 0.132457222410879
0.135810938064377 to 0.136550034949026
Changing layer 0's weights from 
0.305250950148982 to 0.305990047033631
-0.0666755065408525 to -0.0659364096562036
-0.301024772235471 to -0.300285675350822
-0.266465522357541 to -0.265726425472892
0.481402046969813 to 0.482141143854462
-0.201239906260091 to -0.200500809375442
-0.047410182663518 to -0.0466710857788691
-0.0748645886865434 to -0.0741254918018945
-0.3123564496723 to -0.311617352787651
-0.356106504806596 to -0.355367407921947
Changing layer 1's weights from 
-0.351009592422563 to -0.350270495537914
0.250159092238826 to 0.250898189123474
0.124456353476924 to 0.125195450361573
0.11455101518957 to 0.115290112074219
-0.0705017671075639 to -0.069762670222915
-0.20873960108431 to -0.208000504199661
0.0619791224512281 to 0.062718219335877
-0.323823906489927 to -0.323084809605278
0.0878390744241895 to 0.0885781713088384
-0.274807788440305 to -0.274068691555656
Changing layer 2's weights from 
0.534380443385524 to 0.535119540270173
-0.0196730420080004 to -0.0189339451233515
0.264916248610896 to 0.265655345495545
0.436116345218104 to 0.436855442102753
0.258863635352534 to 0.259602732237183
0.0528540089639845 to 0.0535931058486334
0.571666724971217 to 0.572405821855866
0.499244697383326 to 0.499983794267975
-0.232753597208577 to -0.232014500323928
-0.187796078631001 to -0.187056981746353
Changing layer 3's weights from 
0.501330144694728 to 0.502069241579377
0.237777061751765 to 0.238516158636414
0.480386979869288 to 0.481126076753937
-0.427052251886855 to -0.426313155002207
0.35771108417837 to 0.358450181063019
0.227462477973384 to 0.228201574858033
-0.102722369143086 to -0.101983272258437
0.355790503314418 to 0.356529600199066
0.0240945294412794 to 0.0248336263259283
0.49692440777151 to 0.497663504656159
Changing layer 4's weights from 
0.397650606921595 to 0.398389703806244
0.188817806533259 to 0.189556903417908
-0.0116597935643969 to -0.010920696679748
0.275274939826411 to 0.27601403671106
0.0360221341165724 to 0.0367612310012213
0.267487354567927 to 0.268226451452576
0.263128824523371 to 0.26386792140802
-0.198768697687703 to -0.198029600803054
0.394927389911097 to 0.395666486795746
-0.0373745486226854 to -0.0366354517380365
Changing layer 5's weights from 
0.550430424502772 to 0.551169521387421
0.108356900504512 to 0.10909599738916
0.533545263102931 to 0.53428435998758
-0.40196691826852 to -0.401227821383872
0.135301895430964 to 0.136040992315613
-0.263166047641355 to -0.262426950756706
0.036799259475154 to 0.0375383563598028
0.109245486548823 to 0.109984583433472
0.498847492030543 to 0.499586588915192
0.492339380076808 to 0.493078476961457
Trying to learn from memory 42, 2, 0.1333333
sum 0.088161184218642 distri 0.078621936082158
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.903367005705247 to 0.925589229086448
1.18180897128523 to 1.20403119466644
1.09476860654295 to 1.11699082992416
0.440196901976476 to 0.462419125357678
Changing layer 0's weights from 
0.305990047033631 to 0.328212270414832
-0.0659364096562036 to -0.0437141862750022
-0.300285675350822 to -0.278063451969621
-0.265726425472892 to -0.243504202091691
0.482141143854462 to 0.504363367235664
-0.200500809375442 to -0.178278585994241
-0.0466710857788691 to -0.0244488623976676
-0.0741254918018945 to -0.0519032684206931
-0.311617352787651 to -0.289395129406449
-0.355367407921947 to -0.333145184540746
Changing layer 1's weights from 
-0.350270495537914 to -0.328048272156713
0.250898189123474 to 0.273120412504676
0.125195450361573 to 0.147417673742774
0.115290112074219 to 0.13751233545542
-0.069762670222915 to -0.0475404468417136
-0.208000504199661 to -0.185778280818459
0.062718219335877 to 0.0849404427170784
-0.323084809605278 to -0.300862586224076
0.0885781713088384 to 0.11080039469004
-0.274068691555656 to -0.251846468174455
Changing layer 2's weights from 
0.535119540270173 to 0.557341763651374
-0.0189339451233515 to 0.00328827825784989
0.265655345495545 to 0.287877568876746
0.436855442102753 to 0.459077665483955
0.259602732237183 to 0.281824955618384
0.0535931058486334 to 0.0758153292298348
0.572405821855866 to 0.594628045237067
0.499983794267975 to 0.522206017649177
-0.232014500323928 to -0.209792276942727
-0.187056981746353 to -0.164834758365151
Changing layer 3's weights from 
0.502069241579377 to 0.524291464960578
0.238516158636414 to 0.260738382017615
0.481126076753937 to 0.503348300135139
-0.426313155002207 to -0.404090931621005
0.358450181063019 to 0.380672404444221
0.228201574858033 to 0.250423798239234
-0.101983272258437 to -0.079761048877236
0.356529600199066 to 0.378751823580268
0.0248336263259283 to 0.0470558497071297
0.497663504656159 to 0.51988572803736
Changing layer 4's weights from 
0.398389703806244 to 0.420611927187446
0.189556903417908 to 0.211779126799109
-0.010920696679748 to 0.0113015267014534
0.27601403671106 to 0.298236260092261
0.0367612310012213 to 0.0589834543824227
0.268226451452576 to 0.290448674833777
0.26386792140802 to 0.286090144789222
-0.198029600803054 to -0.175807377421853
0.395666486795746 to 0.417888710176948
-0.0366354517380365 to -0.0144132283568351
Changing layer 5's weights from 
0.551169521387421 to 0.573391744768623
0.10909599738916 to 0.131318220770362
0.53428435998758 to 0.556506583368781
-0.401227821383872 to -0.37900559800267
0.136040992315613 to 0.158263215696814
-0.262426950756706 to -0.240204727375504
0.0375383563598028 to 0.0597605797410043
0.109984583433472 to 0.132206806814673
0.499586588915192 to 0.521808812296394
0.493078476961457 to 0.515300700342658
Trying to learn from memory 42, 1, 0.1333333
sum 0.088161184218642 distri 0.0328615300893922
Using diff 0.0332593580745893 and condRate 0.166666666666667
Changed category 1 weights from 
0.324066873261175 to 0.324805970145824
0.557382460304937 to 0.558121557189586
0.132457222410879 to 0.133196319295528
0.136550034949026 to 0.137289131833675
Changing layer 0's weights from 
0.328212270414832 to 0.328951367299481
-0.0437141862750022 to -0.0429750893903533
-0.278063451969621 to -0.277324355084972
-0.243504202091691 to -0.242765105207042
0.504363367235664 to 0.505102464120312
-0.178278585994241 to -0.177539489109592
-0.0244488623976676 to -0.0237097655130187
-0.0519032684206931 to -0.0511641715360442
-0.289395129406449 to -0.2886560325218
-0.333145184540746 to -0.332406087656097
Changing layer 1's weights from 
-0.328048272156713 to -0.327309175272064
0.273120412504676 to 0.273859509389325
0.147417673742774 to 0.148156770627423
0.13751233545542 to 0.138251432340069
-0.0475404468417136 to -0.0468013499570647
-0.185778280818459 to -0.185039183933811
0.0849404427170784 to 0.0856795396017273
-0.300862586224076 to -0.300123489339427
0.11080039469004 to 0.111539491574689
-0.251846468174455 to -0.251107371289806
Changing layer 2's weights from 
0.557341763651374 to 0.558080860536023
0.00328827825784989 to 0.00402737514249878
0.287877568876746 to 0.288616665761395
0.459077665483955 to 0.459816762368603
0.281824955618384 to 0.282564052503033
0.0758153292298348 to 0.0765544261144837
0.594628045237067 to 0.595367142121716
0.522206017649177 to 0.522945114533826
-0.209792276942727 to -0.209053180058078
-0.164834758365151 to -0.164095661480502
Changing layer 3's weights from 
0.524291464960578 to 0.525030561845227
0.260738382017615 to 0.261477478902264
0.503348300135139 to 0.504087397019788
-0.404090931621005 to -0.403351834736356
0.380672404444221 to 0.381411501328869
0.250423798239234 to 0.251162895123883
-0.079761048877236 to -0.0790219519925871
0.378751823580268 to 0.379490920464917
0.0470558497071297 to 0.0477949465917786
0.51988572803736 to 0.520624824922009
Changing layer 4's weights from 
0.420611927187446 to 0.421351024072095
0.211779126799109 to 0.212518223683758
0.0113015267014534 to 0.0120406235861023
0.298236260092261 to 0.29897535697691
0.0589834543824227 to 0.0597225512670716
0.290448674833777 to 0.291187771718426
0.286090144789222 to 0.286829241673871
-0.175807377421853 to -0.175068280537204
0.417888710176948 to 0.418627807061597
-0.0144132283568351 to -0.0136741314721862
Changing layer 5's weights from 
0.573391744768623 to 0.574130841653272
0.131318220770362 to 0.132057317655011
0.556506583368781 to 0.55724568025343
-0.37900559800267 to -0.378266501118021
0.158263215696814 to 0.159002312581463
-0.240204727375504 to -0.239465630490855
0.0597605797410043 to 0.0604996766256532
0.132206806814673 to 0.132945903699322
0.521808812296394 to 0.522547909181042
0.515300700342658 to 0.516039797227307
Trying to learn from memory 42, 1, 0.1333333
sum 0.088161184218642 distri 0.0328615300893922
Using diff 0.0332593580745893 and condRate 0.166666666666667
Changed category 1 weights from 
0.324805970145824 to 0.325545067030473
0.558121557189586 to 0.558860654074235
0.133196319295528 to 0.133935416180177
0.137289131833675 to 0.138028228718324
Changing layer 0's weights from 
0.328951367299481 to 0.32969046418413
-0.0429750893903533 to -0.0422359925057044
-0.277324355084972 to -0.276585258200323
-0.242765105207042 to -0.242026008322393
0.505102464120312 to 0.505841561004961
-0.177539489109592 to -0.176800392224943
-0.0237097655130187 to -0.0229706686283699
-0.0511641715360442 to -0.0504250746513953
-0.2886560325218 to -0.287916935637151
-0.332406087656097 to -0.331666990771448
Changing layer 1's weights from 
-0.327309175272064 to -0.326570078387415
0.273859509389325 to 0.274598606273974
0.148156770627423 to 0.148895867512072
0.138251432340069 to 0.138990529224718
-0.0468013499570647 to -0.0460622530724158
-0.185039183933811 to -0.184300087049162
0.0856795396017273 to 0.0864186364863762
-0.300123489339427 to -0.299384392454778
0.111539491574689 to 0.112278588459338
-0.251107371289806 to -0.250368274405157
Changing layer 2's weights from 
0.558080860536023 to 0.558819957420672
0.00402737514249878 to 0.00476647202714767
0.288616665761395 to 0.289355762646044
0.459816762368603 to 0.460555859253252
0.282564052503033 to 0.283303149387682
0.0765544261144837 to 0.0772935229991326
0.595367142121716 to 0.596106239006365
0.522945114533826 to 0.523684211418475
-0.209053180058078 to -0.208314083173429
-0.164095661480502 to -0.163356564595853
Changing layer 3's weights from 
0.525030561845227 to 0.525769658729876
0.261477478902264 to 0.262216575786913
0.504087397019788 to 0.504826493904436
-0.403351834736356 to -0.402612737851707
0.381411501328869 to 0.382150598213518
0.251162895123883 to 0.251901992008532
-0.0790219519925871 to -0.0782828551079382
0.379490920464917 to 0.380230017349566
0.0477949465917786 to 0.0485340434764275
0.520624824922009 to 0.521363921806658
Changing layer 4's weights from 
0.421351024072095 to 0.422090120956744
0.212518223683758 to 0.213257320568407
0.0120406235861023 to 0.0127797204707512
0.29897535697691 to 0.299714453861559
0.0597225512670716 to 0.0604616481517205
0.291187771718426 to 0.291926868603075
0.286829241673871 to 0.28756833855852
-0.175068280537204 to -0.174329183652555
0.418627807061597 to 0.419366903946246
-0.0136741314721862 to -0.0129350345875373
Changing layer 5's weights from 
0.574130841653272 to 0.57486993853792
0.132057317655011 to 0.13279641453966
0.55724568025343 to 0.557984777138079
-0.378266501118021 to -0.377527404233372
0.159002312581463 to 0.159741409466112
-0.239465630490855 to -0.238726533606206
0.0604996766256532 to 0.061238773510302
0.132945903699322 to 0.133685000583971
0.522547909181042 to 0.523287006065691
0.516039797227307 to 0.516778894111956
Trying to learn from memory 42, 2, 0.1333333
sum 0.088161184218642 distri 0.078621936082158
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.925589229086448 to 0.947811452467649
1.20403119466644 to 1.22625341804764
1.11699082992416 to 1.13921305330536
0.462419125357678 to 0.484641348738879
Changing layer 0's weights from 
0.32969046418413 to 0.351912687565331
-0.0422359925057044 to -0.020013769124503
-0.276585258200323 to -0.254363034819121
-0.242026008322393 to -0.219803784941192
0.505841561004961 to 0.528063784386163
-0.176800392224943 to -0.154578168843741
-0.0229706686283699 to -0.000748445247168432
-0.0504250746513953 to -0.0282028512701939
-0.287916935637151 to -0.26569471225595
-0.331666990771448 to -0.309444767390247
Changing layer 1's weights from 
-0.326570078387415 to -0.304347855006213
0.274598606273974 to 0.296820829655175
0.148895867512072 to 0.171118090893273
0.138990529224718 to 0.16121275260592
-0.0460622530724158 to -0.0238400296912144
-0.184300087049162 to -0.16207786366796
0.0864186364863762 to 0.108640859867578
-0.299384392454778 to -0.277162169073577
0.112278588459338 to 0.134500811840539
-0.250368274405157 to -0.228146051023955
Changing layer 2's weights from 
0.558819957420672 to 0.581042180801873
0.00476647202714767 to 0.0269886954083491
0.289355762646044 to 0.311577986027245
0.460555859253252 to 0.482778082634454
0.283303149387682 to 0.305525372768884
0.0772935229991326 to 0.099515746380334
0.596106239006365 to 0.618328462387567
0.523684211418475 to 0.545906434799676
-0.208314083173429 to -0.186091859792228
-0.163356564595853 to -0.141134341214652
Changing layer 3's weights from 
0.525769658729876 to 0.547991882111077
0.262216575786913 to 0.284438799168115
0.504826493904436 to 0.527048717285638
-0.402612737851707 to -0.380390514470506
0.382150598213518 to 0.40437282159472
0.251901992008532 to 0.274124215389733
-0.0782828551079382 to -0.0560606317267367
0.380230017349566 to 0.402452240730767
0.0485340434764275 to 0.070756266857629
0.521363921806658 to 0.54358614518786
Changing layer 4's weights from 
0.422090120956744 to 0.444312344337945
0.213257320568407 to 0.235479543949609
0.0127797204707512 to 0.0350019438519526
0.299714453861559 to 0.321936677242761
0.0604616481517205 to 0.0826838715329219
0.291926868603075 to 0.314149091984277
0.28756833855852 to 0.309790561939721
-0.174329183652555 to -0.152106960271354
0.419366903946246 to 0.441589127327447
-0.0129350345875373 to 0.00928718879366409
Changing layer 5's weights from 
0.57486993853792 to 0.597092161919122
0.13279641453966 to 0.155018637920861
0.557984777138079 to 0.580207000519281
-0.377527404233372 to -0.355305180852171
0.159741409466112 to 0.181963632847314
-0.238726533606206 to -0.216504310225005
0.061238773510302 to 0.0834609968915035
0.133685000583971 to 0.155907223965173
0.523287006065691 to 0.545509229446893
0.516778894111956 to 0.539001117493157
Trying to learn from memory 42, 2, 0.1333333
sum 0.088161184218642 distri 0.078621936082158
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.947811452467649 to 0.970033675848851
1.22625341804764 to 1.24847564142884
1.13921305330536 to 1.16143527668656
0.484641348738879 to 0.50686357212008
Changing layer 0's weights from 
0.351912687565331 to 0.374134910946533
-0.020013769124503 to 0.00220845425669845
-0.254363034819121 to -0.23214081143792
-0.219803784941192 to -0.19758156155999
0.528063784386163 to 0.550286007767364
-0.154578168843741 to -0.13235594546254
-0.000748445247168432 to 0.021473778134033
-0.0282028512701939 to -0.00598062788899247
-0.26569471225595 to -0.243472488874749
-0.309444767390247 to -0.287222544009045
Changing layer 1's weights from 
-0.304347855006213 to -0.282125631625012
0.296820829655175 to 0.319043053036377
0.171118090893273 to 0.193340314274475
0.16121275260592 to 0.183434975987121
-0.0238400296912144 to -0.00161780631001298
-0.16207786366796 to -0.139855640286759
0.108640859867578 to 0.130863083248779
-0.277162169073577 to -0.254939945692376
0.134500811840539 to 0.15672303522174
-0.228146051023955 to -0.205923827642754
Changing layer 2's weights from 
0.581042180801873 to 0.603264404183075
0.0269886954083491 to 0.0492109187895505
0.311577986027245 to 0.333800209408447
0.482778082634454 to 0.505000306015655
0.305525372768884 to 0.327747596150085
0.099515746380334 to 0.121737969761535
0.618328462387567 to 0.640550685768768
0.545906434799676 to 0.568128658180877
-0.186091859792228 to -0.163869636411026
-0.141134341214652 to -0.118912117833451
Changing layer 3's weights from 
0.547991882111077 to 0.570214105492279
0.284438799168115 to 0.306661022549316
0.527048717285638 to 0.549270940666839
-0.380390514470506 to -0.358168291089304
0.40437282159472 to 0.426595044975921
0.274124215389733 to 0.296346438770935
-0.0560606317267367 to -0.0338384083455353
0.402452240730767 to 0.424674464111969
0.070756266857629 to 0.0929784902388304
0.54358614518786 to 0.565808368569061
Changing layer 4's weights from 
0.444312344337945 to 0.466534567719147
0.235479543949609 to 0.25770176733081
0.0350019438519526 to 0.057224167233154
0.321936677242761 to 0.344158900623962
0.0826838715329219 to 0.104906094914123
0.314149091984277 to 0.336371315365478
0.309790561939721 to 0.332012785320922
-0.152106960271354 to -0.129884736890152
0.441589127327447 to 0.463811350708648
0.00928718879366409 to 0.0315094121748655
Changing layer 5's weights from 
0.597092161919122 to 0.619314385300323
0.155018637920861 to 0.177240861302062
0.580207000519281 to 0.602429223900482
-0.355305180852171 to -0.333082957470969
0.181963632847314 to 0.204185856228515
-0.216504310225005 to -0.194282086843804
0.0834609968915035 to 0.105683220272705
0.155907223965173 to 0.178129447346374
0.545509229446893 to 0.567731452828094
0.539001117493157 to 0.561223340874359
Trying to learn from memory 42, 2, 0.1333333
sum 0.088161184218642 distri 0.078621936082158
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.970033675848851 to 0.992255899230052
1.24847564142884 to 1.27069786481004
1.16143527668656 to 1.18365750006776
0.50686357212008 to 0.529085795501282
Changing layer 0's weights from 
0.374134910946533 to 0.396357134327734
0.00220845425669845 to 0.0244306776378999
-0.23214081143792 to -0.209918588056719
-0.19758156155999 to -0.175359338178789
0.550286007767364 to 0.572508231148566
-0.13235594546254 to -0.110133722081339
0.021473778134033 to 0.0436960015152344
-0.00598062788899247 to 0.016241595492209
-0.243472488874749 to -0.221250265493547
-0.287222544009045 to -0.265000320627844
Changing layer 1's weights from 
-0.282125631625012 to -0.259903408243811
0.319043053036377 to 0.341265276417578
0.193340314274475 to 0.215562537655676
0.183434975987121 to 0.205657199368323
-0.00161780631001298 to 0.0206044170711884
-0.139855640286759 to -0.117633416905557
0.130863083248779 to 0.15308530662998
-0.254939945692376 to -0.232717722311174
0.15672303522174 to 0.178945258602942
-0.205923827642754 to -0.183701604261552
Changing layer 2's weights from 
0.603264404183075 to 0.625486627564276
0.0492109187895505 to 0.0714331421707519
0.333800209408447 to 0.356022432789648
0.505000306015655 to 0.527222529396857
0.327747596150085 to 0.349969819531287
0.121737969761535 to 0.143960193142737
0.640550685768768 to 0.662772909149969
0.568128658180877 to 0.590350881562079
-0.163869636411026 to -0.141647413029825
-0.118912117833451 to -0.0966898944522492
Changing layer 3's weights from 
0.570214105492279 to 0.59243632887348
0.306661022549316 to 0.328883245930517
0.549270940666839 to 0.571493164048041
-0.358168291089304 to -0.335946067708103
0.426595044975921 to 0.448817268357123
0.296346438770935 to 0.318568662152136
-0.0338384083455353 to -0.0116161849643339
0.424674464111969 to 0.44689668749317
0.0929784902388304 to 0.115200713620032
0.565808368569061 to 0.588030591950262
Changing layer 4's weights from 
0.466534567719147 to 0.488756791100348
0.25770176733081 to 0.279923990712011
0.057224167233154 to 0.0794463906143555
0.344158900623962 to 0.366381124005163
0.104906094914123 to 0.127128318295325
0.336371315365478 to 0.35859353874668
0.332012785320922 to 0.354235008702124
-0.129884736890152 to -0.107662513508951
0.463811350708648 to 0.48603357408985
0.0315094121748655 to 0.0537316355560669
Changing layer 5's weights from 
0.619314385300323 to 0.641536608681525
0.177240861302062 to 0.199463084683264
0.602429223900482 to 0.624651447281683
-0.333082957470969 to -0.310860734089768
0.204185856228515 to 0.226408079609716
-0.194282086843804 to -0.172059863462602
0.105683220272705 to 0.127905443653906
0.178129447346374 to 0.200351670727575
0.567731452828094 to 0.589953676209296
0.561223340874359 to 0.58344556425556
Trying to learn from memory 43, 2, 0.1333333
sum 0.088161184218642 distri 0.078621936082158
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.992255899230052 to 1.01447812261125
1.27069786481004 to 1.29292008819124
1.18365750006776 to 1.20587972344896
0.529085795501282 to 0.551308018882483
Changing layer 0's weights from 
0.396357134327734 to 0.418579357708936
0.0244306776378999 to 0.0466529010191013
-0.209918588056719 to -0.187696364675517
-0.175359338178789 to -0.153137114797587
0.572508231148566 to 0.594730454529767
-0.110133722081339 to -0.0879114987001372
0.0436960015152344 to 0.0659182248964358
0.016241595492209 to 0.0384638188734104
-0.221250265493547 to -0.199028042112346
-0.265000320627844 to -0.242778097246642
Changing layer 1's weights from 
-0.259903408243811 to -0.237681184862609
0.341265276417578 to 0.363487499798779
0.215562537655676 to 0.237784761036877
0.205657199368323 to 0.227879422749524
0.0206044170711884 to 0.0428266404523899
-0.117633416905557 to -0.095411193524356
0.15308530662998 to 0.175307530011182
-0.232717722311174 to -0.210495498929973
0.178945258602942 to 0.201167481984143
-0.183701604261552 to -0.161479380880351
Changing layer 2's weights from 
0.625486627564276 to 0.647708850945478
0.0714331421707519 to 0.0936553655519534
0.356022432789648 to 0.37824465617085
0.527222529396857 to 0.549444752778058
0.349969819531287 to 0.372192042912488
0.143960193142737 to 0.166182416523938
0.662772909149969 to 0.684995132531171
0.590350881562079 to 0.61257310494328
-0.141647413029825 to -0.119425189648623
-0.0966898944522492 to -0.0744676710710478
Changing layer 3's weights from 
0.59243632887348 to 0.614658552254681
0.328883245930517 to 0.351105469311719
0.571493164048041 to 0.593715387429242
-0.335946067708103 to -0.313723844326902
0.448817268357123 to 0.471039491738324
0.318568662152136 to 0.340790885533338
-0.0116161849643339 to 0.0106060384168675
0.44689668749317 to 0.469118910874371
0.115200713620032 to 0.137422937001233
0.588030591950262 to 0.610252815331464
Changing layer 4's weights from 
0.488756791100348 to 0.510979014481549
0.279923990712011 to 0.302146214093213
0.0794463906143555 to 0.101668613995557
0.366381124005163 to 0.388603347386365
0.127128318295325 to 0.149350541676526
0.35859353874668 to 0.380815762127881
0.354235008702124 to 0.376457232083325
-0.107662513508951 to -0.0854402901277495
0.48603357408985 to 0.508255797471051
0.0537316355560669 to 0.0759538589372684
Changing layer 5's weights from 
0.641536608681525 to 0.663758832062726
0.199463084683264 to 0.221685308064465
0.624651447281683 to 0.646873670662885
-0.310860734089768 to -0.288638510708567
0.226408079609716 to 0.248630302990918
-0.172059863462602 to -0.149837640081401
0.127905443653906 to 0.150127667035108
0.200351670727575 to 0.222573894108777
0.589953676209296 to 0.612175899590497
0.58344556425556 to 0.605667787636762
10/5/2016 1:24:20 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 44, 2, 0.1333333
sum 0.088161184218642 distri 0.078621936082158
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
1.01447812261125 to 1.03670034599246
1.29292008819124 to 1.31514231157244
1.20587972344896 to 1.22810194683016
0.551308018882483 to 0.573530242263685
Changing layer 0's weights from 
0.418579357708936 to 0.440801581090137
0.0466529010191013 to 0.0688751244003027
-0.187696364675517 to -0.165474141294316
-0.153137114797587 to -0.130914891416386
0.594730454529767 to 0.616952677910968
-0.0879114987001372 to -0.0656892753189358
0.0659182248964358 to 0.0881404482776373
0.0384638188734104 to 0.0606860422546118
-0.199028042112346 to -0.176805818731144
-0.242778097246642 to -0.220555873865441
Changing layer 1's weights from 
-0.237681184862609 to -0.215458961481408
0.363487499798779 to 0.385709723179981
0.237784761036877 to 0.260006984418079
0.227879422749524 to 0.250101646130725
0.0428266404523899 to 0.0650488638335913
-0.095411193524356 to -0.0731889701431546
0.175307530011182 to 0.197529753392383
-0.210495498929973 to -0.188273275548771
0.201167481984143 to 0.223389705365345
-0.161479380880351 to -0.13925715749915
Changing layer 2's weights from 
0.647708850945478 to 0.669931074326679
0.0936553655519534 to 0.115877588933155
0.37824465617085 to 0.400466879552051
0.549444752778058 to 0.571666976159259
0.372192042912488 to 0.394414266293689
0.166182416523938 to 0.18840463990514
0.684995132531171 to 0.707217355912372
0.61257310494328 to 0.634795328324481
-0.119425189648623 to -0.097202966267422
-0.0744676710710478 to -0.0522454476898463
Changing layer 3's weights from 
0.614658552254681 to 0.636880775635883
0.351105469311719 to 0.37332769269292
0.593715387429242 to 0.615937610810443
-0.313723844326902 to -0.2915016209457
0.471039491738324 to 0.493261715119526
0.340790885533338 to 0.363013108914539
0.0106060384168675 to 0.0328282617980689
0.469118910874371 to 0.491341134255573
0.137422937001233 to 0.159645160382435
0.610252815331464 to 0.632475038712665
Changing layer 4's weights from 
0.510979014481549 to 0.533201237862751
0.302146214093213 to 0.324368437474414
0.101668613995557 to 0.123890837376758
0.388603347386365 to 0.410825570767566
0.149350541676526 to 0.171572765057728
0.380815762127881 to 0.403037985509082
0.376457232083325 to 0.398679455464527
-0.0854402901277495 to -0.0632180667465481
0.508255797471051 to 0.530478020852253
0.0759538589372684 to 0.0981760823184698
Changing layer 5's weights from 
0.663758832062726 to 0.685981055443927
0.221685308064465 to 0.243907531445667
0.646873670662885 to 0.669095894044086
-0.288638510708567 to -0.266416287327365
0.248630302990918 to 0.270852526372119
-0.149837640081401 to -0.127615416700199
0.150127667035108 to 0.172349890416309
0.222573894108777 to 0.244796117489978
0.612175899590497 to 0.634398122971698
0.605667787636762 to 0.627890011017963
Trying to learn from memory 44, 2, 0.1333333
sum 0.088161184218642 distri 0.078621936082158
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
1.03670034599246 to 1.05892256937366
1.31514231157244 to 1.33736453495364
1.22810194683016 to 1.25032417021137
0.573530242263685 to 0.595752465644886
Changing layer 0's weights from 
0.440801581090137 to 0.463023804471339
0.0688751244003027 to 0.0910973477815041
-0.165474141294316 to -0.143251917913114
-0.130914891416386 to -0.108692668035184
0.616952677910968 to 0.63917490129217
-0.0656892753189358 to -0.0434670519377343
0.0881404482776373 to 0.110362671658839
0.0606860422546118 to 0.0829082656358132
-0.176805818731144 to -0.154583595349943
-0.220555873865441 to -0.198333650484239
Changing layer 1's weights from 
-0.215458961481408 to -0.193236738100206
0.385709723179981 to 0.407931946561182
0.260006984418079 to 0.28222920779928
0.250101646130725 to 0.272323869511927
0.0650488638335913 to 0.0872710872147927
-0.0731889701431546 to -0.0509667467619532
0.197529753392383 to 0.219751976773585
-0.188273275548771 to -0.16605105216757
0.223389705365345 to 0.245611928746546
-0.13925715749915 to -0.117034934117948
Changing layer 2's weights from 
0.669931074326679 to 0.69215329770788
0.115877588933155 to 0.138099812314356
0.400466879552051 to 0.422689102933253
0.571666976159259 to 0.593889199540461
0.394414266293689 to 0.416636489674891
0.18840463990514 to 0.210626863286341
0.707217355912372 to 0.729439579293574
0.634795328324481 to 0.657017551705683
-0.097202966267422 to -0.0749807428862206
-0.0522454476898463 to -0.0300232243086449
Changing layer 3's weights from 
0.636880775635883 to 0.659102999017084
0.37332769269292 to 0.395549916074122
0.615937610810443 to 0.638159834191645
-0.2915016209457 to -0.269279397564499
0.493261715119526 to 0.515483938500727
0.363013108914539 to 0.38523533229574
0.0328282617980689 to 0.0550504851792704
0.491341134255573 to 0.513563357636774
0.159645160382435 to 0.181867383763636
0.632475038712665 to 0.654697262093866
Changing layer 4's weights from 
0.533201237862751 to 0.555423461243952
0.324368437474414 to 0.346590660855616
0.123890837376758 to 0.14611306075796
0.410825570767566 to 0.433047794148768
0.171572765057728 to 0.193794988438929
0.403037985509082 to 0.425260208890284
0.398679455464527 to 0.420901678845728
-0.0632180667465481 to -0.0409958433653466
0.530478020852253 to 0.552700244233454
0.0981760823184698 to 0.120398305699671
Changing layer 5's weights from 
0.685981055443927 to 0.708203278825129
0.243907531445667 to 0.266129754826868
0.669095894044086 to 0.691318117425288
-0.266416287327365 to -0.244194063946164
0.270852526372119 to 0.293074749753321
-0.127615416700199 to -0.105393193318998
0.172349890416309 to 0.194572113797511
0.244796117489978 to 0.26701834087118
0.634398122971698 to 0.6566203463529
0.627890011017963 to 0.650112234399164
10/5/2016 1:24:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:21 PMStarting learning phase with deltaScore: -0.03333334
Modified index 0's learning in memoryPool to -0.006666667
10/5/2016 1:24:21 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 45, 2, -0.006666667
sum 0.290540424290587 distri 0.267721917379197
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
1.05892256937366 to 1.05781145823564
1.33736453495364 to 1.33625342381563
1.25032417021137 to 1.24921305907335
0.595752465644886 to 0.59464135450687
Changing layer 0's weights from 
0.463023804471339 to 0.461912693333323
0.0910973477815041 to 0.0899862366434881
-0.143251917913114 to -0.14436302905113
-0.108692668035184 to -0.1098037791732
0.63917490129217 to 0.638063790154154
-0.0434670519377343 to -0.0445781630757503
0.110362671658839 to 0.109251560520823
0.0829082656358132 to 0.0817971544977972
-0.154583595349943 to -0.155694706487959
-0.198333650484239 to -0.199444761622255
Changing layer 1's weights from 
-0.193236738100206 to -0.194347849238222
0.407931946561182 to 0.406820835423166
0.28222920779928 to 0.281118096661264
0.272323869511927 to 0.271212758373911
0.0872710872147927 to 0.0861599760767767
-0.0509667467619532 to -0.0520778578999692
0.219751976773585 to 0.218640865635569
-0.16605105216757 to -0.167162163305586
0.245611928746546 to 0.24450081760853
-0.117034934117948 to -0.118146045255964
Changing layer 2's weights from 
0.69215329770788 to 0.691042186569864
0.138099812314356 to 0.13698870117634
0.422689102933253 to 0.421577991795237
0.593889199540461 to 0.592778088402445
0.416636489674891 to 0.415525378536875
0.210626863286341 to 0.209515752148325
0.729439579293574 to 0.728328468155558
0.657017551705683 to 0.655906440567667
-0.0749807428862206 to -0.0760918540242365
-0.0300232243086449 to -0.0311343354466609
Changing layer 3's weights from 
0.659102999017084 to 0.657991887879068
0.395549916074122 to 0.394438804936106
0.638159834191645 to 0.637048723053629
-0.269279397564499 to -0.270390508702515
0.515483938500727 to 0.514372827362711
0.38523533229574 to 0.384124221157724
0.0550504851792704 to 0.0539393740412544
0.513563357636774 to 0.512452246498758
0.181867383763636 to 0.18075627262562
0.654697262093866 to 0.65358615095585
Changing layer 4's weights from 
0.555423461243952 to 0.554312350105936
0.346590660855616 to 0.3454795497176
0.14611306075796 to 0.145001949619944
0.433047794148768 to 0.431936683010752
0.193794988438929 to 0.192683877300913
0.425260208890284 to 0.424149097752268
0.420901678845728 to 0.419790567707712
-0.0409958433653466 to -0.0421069545033626
0.552700244233454 to 0.551589133095438
0.120398305699671 to 0.119287194561655
Changing layer 5's weights from 
0.708203278825129 to 0.707092167687113
0.266129754826868 to 0.265018643688852
0.691318117425288 to 0.690207006287272
-0.244194063946164 to -0.24530517508418
0.293074749753321 to 0.291963638615305
-0.105393193318998 to -0.106504304457014
0.194572113797511 to 0.193461002659495
0.26701834087118 to 0.265907229733164
0.6566203463529 to 0.655509235214884
0.650112234399164 to 0.649001123261148
10/5/2016 1:24:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:24:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:25:25 PMStarting AI
Reading weights from 10/5/2016 1:25:25 PMStarting AI
Weights.txt
Layer 0's weights: 0.461912693333323 0.0899862366434881 -0.14436302905113 -0.1098037791732 0.638063790154154 -0.0445781630757503 0.109251560520823 0.0817971544977972 -0.155694706487959 -0.199444761622255 14 11 
Layer 1's weights: -0.194347849238222 0.406820835423166 0.281118096661264 0.271212758373911 0.0861599760767767 -0.0520778578999692 0.218640865635569 -0.167162163305586 0.24450081760853 -0.118146045255964 12 9 
Layer 2's weights: 0.691042186569864 0.13698870117634 0.421577991795237 0.592778088402445 0.415525378536875 0.209515752148325 0.728328468155558 0.655906440567667 -0.0760918540242365 -0.0311343354466609 10 7 
Layer 3's weights: 0.657991887879068 0.394438804936106 0.637048723053629 -0.270390508702515 0.514372827362711 0.384124221157724 0.0539393740412544 0.512452246498758 0.18075627262562 0.65358615095585 8 5 
Layer 4's weights: 0.554312350105936 0.3454795497176 0.145001949619944 0.431936683010752 0.192683877300913 0.424149097752268 0.419790567707712 -0.0421069545033626 0.551589133095438 0.119287194561655 6 3 
Layer 5's weights: 0.707092167687113 0.265018643688852 0.690207006287272 -0.24530517508418 0.291963638615305 -0.106504304457014 0.193461002659495 0.265907229733164 0.655509235214884 0.649001123261148 4 1 
Layer 6's weights: 0.200873899447671 -0.199161064636954 -0.48168176115776 -0.330176395190009 
Layer 7's weights: 0.325545067030473 0.558860654074235 0.133935416180177 0.138028228718324 
Layer 8's weights: 1.05781145823564 1.33625342381563 1.24921305907335 0.59464135450687 
10/5/2016 1:25:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:25:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:25:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:14 PMStarting AI
Reading weights from 10/5/2016 1:28:14 PMStarting AI
Weights.txt
Layer 0's weights: 0.461912693333323 0.0899862366434881 -0.14436302905113 -0.1098037791732 0.638063790154154 -0.0445781630757503 0.109251560520823 0.0817971544977972 -0.155694706487959 -0.199444761622255 14 11 
Layer 1's weights: -0.194347849238222 0.406820835423166 0.281118096661264 0.271212758373911 0.0861599760767767 -0.0520778578999692 0.218640865635569 -0.167162163305586 0.24450081760853 -0.118146045255964 12 9 
Layer 2's weights: 0.691042186569864 0.13698870117634 0.421577991795237 0.592778088402445 0.415525378536875 0.209515752148325 0.728328468155558 0.655906440567667 -0.0760918540242365 -0.0311343354466609 10 7 
Layer 3's weights: 0.657991887879068 0.394438804936106 0.637048723053629 -0.270390508702515 0.514372827362711 0.384124221157724 0.0539393740412544 0.512452246498758 0.18075627262562 0.65358615095585 8 5 
Layer 4's weights: 0.554312350105936 0.3454795497176 0.145001949619944 0.431936683010752 0.192683877300913 0.424149097752268 0.419790567707712 -0.0421069545033626 0.551589133095438 0.119287194561655 6 3 
Layer 5's weights: 0.707092167687113 0.265018643688852 0.690207006287272 -0.24530517508418 0.291963638615305 -0.106504304457014 0.193461002659495 0.265907229733164 0.655509235214884 0.649001123261148 4 1 
Layer 6's weights: 0.200873899447671 -0.199161064636954 -0.48168176115776 -0.330176395190009 
Layer 7's weights: 0.325545067030473 0.558860654074235 0.133935416180177 0.138028228718324 
Layer 8's weights: 1.05781145823564 1.33625342381563 1.24921305907335 0.59464135450687 
10/5/2016 1:28:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:28:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:12 PMStarting AI
Reading weights from 10/5/2016 1:29:12 PMStarting AI
Weights.txt
Layer 0's weights: 0.461912693333323 0.0899862366434881 -0.14436302905113 -0.1098037791732 0.638063790154154 -0.0445781630757503 0.109251560520823 0.0817971544977972 -0.155694706487959 -0.199444761622255 14 11 
Layer 1's weights: -0.194347849238222 0.406820835423166 0.281118096661264 0.271212758373911 0.0861599760767767 -0.0520778578999692 0.218640865635569 -0.167162163305586 0.24450081760853 -0.118146045255964 12 9 
Layer 2's weights: 0.691042186569864 0.13698870117634 0.421577991795237 0.592778088402445 0.415525378536875 0.209515752148325 0.728328468155558 0.655906440567667 -0.0760918540242365 -0.0311343354466609 10 7 
Layer 3's weights: 0.657991887879068 0.394438804936106 0.637048723053629 -0.270390508702515 0.514372827362711 0.384124221157724 0.0539393740412544 0.512452246498758 0.18075627262562 0.65358615095585 8 5 
Layer 4's weights: 0.554312350105936 0.3454795497176 0.145001949619944 0.431936683010752 0.192683877300913 0.424149097752268 0.419790567707712 -0.0421069545033626 0.551589133095438 0.119287194561655 6 3 
Layer 5's weights: 0.707092167687113 0.265018643688852 0.690207006287272 -0.24530517508418 0.291963638615305 -0.106504304457014 0.193461002659495 0.265907229733164 0.655509235214884 0.649001123261148 4 1 
Layer 6's weights: 0.200873899447671 -0.199161064636954 -0.48168176115776 -0.330176395190009 
Layer 7's weights: 0.325545067030473 0.558860654074235 0.133935416180177 0.138028228718324 
Layer 8's weights: 1.05781145823564 1.33625342381563 1.24921305907335 0.59464135450687 
10/5/2016 1:29:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:41 PMStarting learning phase with deltaScore: -0.8
Modified index 0's learning in memoryPool to -0.16
Modified index 1's learning in memoryPool to -0.16
Modified index 2's learning in memoryPool to -0.16
Modified index 3's learning in memoryPool to -0.16
Modified index 4's learning in memoryPool to -0.16
Modified index 5's learning in memoryPool to -0.16
Modified index 6's learning in memoryPool to -0.16
Modified index 7's learning in memoryPool to -0.16
Modified index 8's learning in memoryPool to -0.16
Modified index 9's learning in memoryPool to -0.16
Modified index 10's learning in memoryPool to -0.16
Modified index 11's learning in memoryPool to -0.16
Modified index 12's learning in memoryPool to -0.16
Modified index 13's learning in memoryPool to -0.16
Modified index 14's learning in memoryPool to -0.16
Modified index 15's learning in memoryPool to -0.16
Modified index 16's learning in memoryPool to -0.16
Modified index 17's learning in memoryPool to -0.16
Modified index 18's learning in memoryPool to -0.16
Modified index 19's learning in memoryPool to -0.16
Modified index 20's learning in memoryPool to -0.16
Modified index 21's learning in memoryPool to -0.16
Modified index 22's learning in memoryPool to -0.16
Modified index 23's learning in memoryPool to -0.16
Modified index 24's learning in memoryPool to -0.16
Modified index 25's learning in memoryPool to -0.16
10/5/2016 1:29:41 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 0, 2, -0.16
sum 0.336405846314805 distri 0.3109663739807
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
1.05781145823564 to 1.03114479216502
1.33625342381563 to 1.30958675774501
1.24921305907335 to 1.22254639300273
0.59464135450687 to 0.56797468843625
Changing layer 0's weights from 
0.461912693333323 to 0.435246027262703
0.0899862366434881 to 0.0633195705728679
-0.14436302905113 to -0.17102969512175
-0.1098037791732 to -0.13647044524382
0.638063790154154 to 0.611397124083534
-0.0445781630757503 to -0.0712448291463705
0.109251560520823 to 0.0825848944502028
0.0817971544977972 to 0.055130488427177
-0.155694706487959 to -0.182361372558579
-0.199444761622255 to -0.226111427692875
Changing layer 1's weights from 
-0.194347849238222 to -0.221014515308842
0.406820835423166 to 0.380154169352546
0.281118096661264 to 0.254451430590644
0.271212758373911 to 0.244546092303291
0.0861599760767767 to 0.0594933100061565
-0.0520778578999692 to -0.0787445239705894
0.218640865635569 to 0.191974199564949
-0.167162163305586 to -0.193828829376206
0.24450081760853 to 0.21783415153791
-0.118146045255964 to -0.144812711326584
Changing layer 2's weights from 
0.691042186569864 to 0.664375520499244
0.13698870117634 to 0.11032203510572
0.421577991795237 to 0.394911325724617
0.592778088402445 to 0.566111422331825
0.415525378536875 to 0.388858712466255
0.209515752148325 to 0.182849086077705
0.728328468155558 to 0.701661802084938
0.655906440567667 to 0.629239774497047
-0.0760918540242365 to -0.102758520094857
-0.0311343354466609 to -0.0578010015172811
Changing layer 3's weights from 
0.657991887879068 to 0.631325221808448
0.394438804936106 to 0.367772138865486
0.637048723053629 to 0.610382056983009
-0.270390508702515 to -0.297057174773135
0.514372827362711 to 0.487706161292091
0.384124221157724 to 0.357457555087104
0.0539393740412544 to 0.0272727079706342
0.512452246498758 to 0.485785580428138
0.18075627262562 to 0.154089606555
0.65358615095585 to 0.62691948488523
Changing layer 4's weights from 
0.554312350105936 to 0.527645684035316
0.3454795497176 to 0.31881288364698
0.145001949619944 to 0.118335283549324
0.431936683010752 to 0.405270016940132
0.192683877300913 to 0.166017211230293
0.424149097752268 to 0.397482431681648
0.419790567707712 to 0.393123901637092
-0.0421069545033626 to -0.0687736205739828
0.551589133095438 to 0.524922467024818
0.119287194561655 to 0.0926205284910348
Changing layer 5's weights from 
0.707092167687113 to 0.680425501616493
0.265018643688852 to 0.238351977618232
0.690207006287272 to 0.663540340216652
-0.24530517508418 to -0.2719718411548
0.291963638615305 to 0.265296972544685
-0.106504304457014 to -0.133170970527634
0.193461002659495 to 0.166794336588875
0.265907229733164 to 0.239240563662544
0.655509235214884 to 0.628842569144264
0.649001123261148 to 0.622334457190528
Trying to learn from memory 1, 2, -0.16
sum 0.336405834039077 distri 0.310966355844603
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
1.03114479216502 to 1.0044781260944
1.30958675774501 to 1.28292009167439
1.22254639300273 to 1.19587972693211
0.56797468843625 to 0.54130802236563
Changing layer 0's weights from 
0.435246027262703 to 0.408579361192083
0.0633195705728679 to 0.0366529045022477
-0.17102969512175 to -0.19769636119237
-0.13647044524382 to -0.16313711131444
0.611397124083534 to 0.584730458012914
-0.0712448291463705 to -0.0979114952169907
0.0825848944502028 to 0.0559182283795826
0.055130488427177 to 0.0284638223565568
-0.182361372558579 to -0.209028038629199
-0.226111427692875 to -0.252778093763495
Changing layer 1's weights from 
-0.221014515308842 to -0.247681181379462
0.380154169352546 to 0.353487503281926
0.254451430590644 to 0.227784764520024
0.244546092303291 to 0.217879426232671
0.0594933100061565 to 0.0328266439355363
-0.0787445239705894 to -0.10541119004121
0.191974199564949 to 0.165307533494329
-0.193828829376206 to -0.220495495446826
0.21783415153791 to 0.19116748546729
-0.144812711326584 to -0.171479377397204
Changing layer 2's weights from 
0.664375520499244 to 0.637708854428624
0.11032203510572 to 0.0836553690350996
0.394911325724617 to 0.368244659653997
0.566111422331825 to 0.539444756261205
0.388858712466255 to 0.362192046395635
0.182849086077705 to 0.156182420007085
0.701661802084938 to 0.674995136014318
0.629239774497047 to 0.602573108426427
-0.102758520094857 to -0.129425186165477
-0.0578010015172811 to -0.0844676675879013
Changing layer 3's weights from 
0.631325221808448 to 0.604658555737828
0.367772138865486 to 0.341105472794866
0.610382056983009 to 0.583715390912389
-0.297057174773135 to -0.323723840843755
0.487706161292091 to 0.46103949522147
0.357457555087104 to 0.330790889016484
0.0272727079706342 to 0.000606041900013968
0.485785580428138 to 0.459118914357518
0.154089606555 to 0.12742294048438
0.62691948488523 to 0.60025281881461
Changing layer 4's weights from 
0.527645684035316 to 0.500979017964696
0.31881288364698 to 0.29214621757636
0.118335283549324 to 0.0916686174787036
0.405270016940132 to 0.378603350869512
0.166017211230293 to 0.139350545159673
0.397482431681648 to 0.370815765611028
0.393123901637092 to 0.366457235566472
-0.0687736205739828 to -0.095440286644603
0.524922467024818 to 0.498255800954198
0.0926205284910348 to 0.0659538624204146
Changing layer 5's weights from 
0.680425501616493 to 0.653758835545873
0.238351977618232 to 0.211685311547612
0.663540340216652 to 0.636873674146032
-0.2719718411548 to -0.29863850722542
0.265296972544685 to 0.238630306474065
-0.133170970527634 to -0.159837636598254
0.166794336588875 to 0.140127670518255
0.239240563662544 to 0.212573897591924
0.628842569144264 to 0.602175903073644
0.622334457190528 to 0.595667791119908
Trying to learn from memory 2, 1, -0.16
sum 0.336430523013157 distri 0.0848639336125367
Using diff 0.167458958647331 and condRate 0.166666666666667
Changed category 1 weights from 
0.325545067030473 to 0.321079494899691
0.558860654074235 to 0.554395081943453
0.133935416180177 to 0.129469844049395
0.138028228718324 to 0.133562656587542
Changing layer 0's weights from 
0.408579361192083 to 0.4041137890613
0.0366529045022477 to 0.0321873323714655
-0.19769636119237 to -0.202161933323153
-0.16313711131444 to -0.167602683445223
0.584730458012914 to 0.580264885882132
-0.0979114952169907 to -0.102377067347773
0.0559182283795826 to 0.0514526562488004
0.0284638223565568 to 0.0239982502257746
-0.209028038629199 to -0.213493610759982
-0.252778093763495 to -0.257243665894278
Changing layer 1's weights from 
-0.247681181379462 to -0.252146753510245
0.353487503281926 to 0.349021931151143
0.227784764520024 to 0.223319192389241
0.217879426232671 to 0.213413854101888
0.0328266439355363 to 0.0283610718047541
-0.10541119004121 to -0.109876762171992
0.165307533494329 to 0.160841961363546
-0.220495495446826 to -0.224961067577609
0.19116748546729 to 0.186701913336507
-0.171479377397204 to -0.175944949527987
Changing layer 2's weights from 
0.637708854428624 to 0.633243282297842
0.0836553690350996 to 0.0791897969043174
0.368244659653997 to 0.363779087523214
0.539444756261205 to 0.534979184130422
0.362192046395635 to 0.357726474264852
0.156182420007085 to 0.151716847876302
0.674995136014318 to 0.670529563883535
0.602573108426427 to 0.598107536295645
-0.129425186165477 to -0.133890758296259
-0.0844676675879013 to -0.0889332397186835
Changing layer 3's weights from 
0.604658555737828 to 0.600192983607046
0.341105472794866 to 0.336639900664083
0.583715390912389 to 0.579249818781607
-0.323723840843755 to -0.328189412974538
0.46103949522147 to 0.456573923090688
0.330790889016484 to 0.326325316885701
0.000606041900013968 to -0.00385953023076821
0.459118914357518 to 0.454653342226735
0.12742294048438 to 0.122957368353597
0.60025281881461 to 0.595787246683828
Changing layer 4's weights from 
0.500979017964696 to 0.496513445833913
0.29214621757636 to 0.287680645445577
0.0916686174787036 to 0.0872030453479214
0.378603350869512 to 0.374137778738729
0.139350545159673 to 0.13488497302889
0.370815765611028 to 0.366350193480245
0.366457235566472 to 0.361991663435689
-0.095440286644603 to -0.0999058587753852
0.498255800954198 to 0.493790228823415
0.0659538624204146 to 0.0614882902896324
Changing layer 5's weights from 
0.653758835545873 to 0.649293263415091
0.211685311547612 to 0.207219739416829
0.636873674146032 to 0.63240810201525
-0.29863850722542 to -0.303104079356203
0.238630306474065 to 0.234164734343282
-0.159837636598254 to -0.164303208729037
0.140127670518255 to 0.135662098387472
0.212573897591924 to 0.208108325461141
0.602175903073644 to 0.597710330942862
0.595667791119908 to 0.591202218989126
Trying to learn from memory 3, 1, -0.16
sum 0.336430538362186 distri 0.0848639388762265
Using diff 0.167458964895413 and condRate 0.166666666666667
Changed category 1 weights from 
0.321079494899691 to 0.316613922602293
0.554395081943453 to 0.549929509646055
0.129469844049395 to 0.125004271751997
0.133562656587542 to 0.129097084290144
Changing layer 0's weights from 
0.4041137890613 to 0.399648216763903
0.0321873323714655 to 0.0277217600740678
-0.202161933323153 to -0.20662750562055
-0.167602683445223 to -0.17206825574262
0.580264885882132 to 0.575799313584734
-0.102377067347773 to -0.106842639645171
0.0514526562488004 to 0.0469870839514027
0.0239982502257746 to 0.0195326779283769
-0.213493610759982 to -0.217959183057379
-0.257243665894278 to -0.261709238191675
Changing layer 1's weights from 
-0.252146753510245 to -0.256612325807642
0.349021931151143 to 0.344556358853746
0.223319192389241 to 0.218853620091844
0.213413854101888 to 0.208948281804491
0.0283610718047541 to 0.0238954995073564
-0.109876762171992 to -0.11434233446939
0.160841961363546 to 0.156376389066149
-0.224961067577609 to -0.229426639875006
0.186701913336507 to 0.18223634103911
-0.175944949527987 to -0.180410521825384
Changing layer 2's weights from 
0.633243282297842 to 0.628777710000444
0.0791897969043174 to 0.0747242246069197
0.363779087523214 to 0.359313515225817
0.534979184130422 to 0.530513611833025
0.357726474264852 to 0.353260901967455
0.151716847876302 to 0.147251275578905
0.670529563883535 to 0.666063991586138
0.598107536295645 to 0.593641963998247
-0.133890758296259 to -0.138356330593657
-0.0889332397186835 to -0.0933988120160812
Changing layer 3's weights from 
0.600192983607046 to 0.595727411309648
0.336639900664083 to 0.332174328366686
0.579249818781607 to 0.574784246484209
-0.328189412974538 to -0.332654985271935
0.456573923090688 to 0.452108350793291
0.326325316885701 to 0.321859744588304
-0.00385953023076821 to -0.00832510252816591
0.454653342226735 to 0.450187769929338
0.122957368353597 to 0.1184917960562
0.595787246683828 to 0.59132167438643
Changing layer 4's weights from 
0.496513445833913 to 0.492047873536516
0.287680645445577 to 0.28321507314818
0.0872030453479214 to 0.0827374730505237
0.374137778738729 to 0.369672206441332
0.13488497302889 to 0.130419400731493
0.366350193480245 to 0.361884621182848
0.361991663435689 to 0.357526091138292
-0.0999058587753852 to -0.104371431072783
0.493790228823415 to 0.489324656526018
0.0614882902896324 to 0.0570227179922347
Changing layer 5's weights from 
0.649293263415091 to 0.644827691117693
0.207219739416829 to 0.202754167119432
0.63240810201525 to 0.627942529717852
-0.303104079356203 to -0.3075696516536
0.234164734343282 to 0.229699162045885
-0.164303208729037 to -0.168768781026434
0.135662098387472 to 0.131196526090075
0.208108325461141 to 0.203642753163744
0.597710330942862 to 0.593244758645464
0.591202218989126 to 0.586736646691728
Trying to learn from memory 4, 1, -0.16
sum 0.336430538362186 distri 0.0848639388762265
Using diff 0.167458964895413 and condRate 0.166666666666667
Changed category 1 weights from 
0.316613922602293 to 0.312148350304895
0.549929509646055 to 0.545463937348657
0.125004271751997 to 0.120538699454599
0.129097084290144 to 0.124631511992746
Changing layer 0's weights from 
0.399648216763903 to 0.395182644466505
0.0277217600740678 to 0.0232561877766701
-0.20662750562055 to -0.211093077917948
-0.17206825574262 to -0.176533828040018
0.575799313584734 to 0.571333741287336
-0.106842639645171 to -0.111308211942568
0.0469870839514027 to 0.042521511654005
0.0195326779283769 to 0.0150671056309792
-0.217959183057379 to -0.222424755354777
-0.261709238191675 to -0.266174810489073
Changing layer 1's weights from 
-0.256612325807642 to -0.26107789810504
0.344556358853746 to 0.340090786556348
0.218853620091844 to 0.214388047794446
0.208948281804491 to 0.204482709507093
0.0238954995073564 to 0.0194299272099587
-0.11434233446939 to -0.118807906766787
0.156376389066149 to 0.151910816768751
-0.229426639875006 to -0.233892212172404
0.18223634103911 to 0.177770768741712
-0.180410521825384 to -0.184876094122782
Changing layer 2's weights from 
0.628777710000444 to 0.624312137703046
0.0747242246069197 to 0.070258652309522
0.359313515225817 to 0.354847942928419
0.530513611833025 to 0.526048039535627
0.353260901967455 to 0.348795329670057
0.147251275578905 to 0.142785703281507
0.666063991586138 to 0.66159841928874
0.593641963998247 to 0.589176391700849
-0.138356330593657 to -0.142821902891055
-0.0933988120160812 to -0.0978643843134789
Changing layer 3's weights from 
0.595727411309648 to 0.59126183901225
0.332174328366686 to 0.327708756069288
0.574784246484209 to 0.570318674186811
-0.332654985271935 to -0.337120557569333
0.452108350793291 to 0.447642778495893
0.321859744588304 to 0.317394172290906
-0.00832510252816591 to -0.0127906748255636
0.450187769929338 to 0.44572219763194
0.1184917960562 to 0.114026223758802
0.59132167438643 to 0.586856102089032
Changing layer 4's weights from 
0.492047873536516 to 0.487582301239118
0.28321507314818 to 0.278749500850782
0.0827374730505237 to 0.078271900753126
0.369672206441332 to 0.365206634143934
0.130419400731493 to 0.125953828434095
0.361884621182848 to 0.35741904888545
0.357526091138292 to 0.353060518840894
-0.104371431072783 to -0.108837003370181
0.489324656526018 to 0.48485908422862
0.0570227179922347 to 0.052557145694837
Changing layer 5's weights from 
0.644827691117693 to 0.640362118820295
0.202754167119432 to 0.198288594822034
0.627942529717852 to 0.623476957420454
-0.3075696516536 to -0.312035223950998
0.229699162045885 to 0.225233589748487
-0.168768781026434 to -0.173234353323832
0.131196526090075 to 0.126730953792677
0.203642753163744 to 0.199177180866346
0.593244758645464 to 0.588779186348066
0.586736646691728 to 0.58227107439433
Trying to learn from memory 4, 1, -0.16
sum 0.336430538362186 distri 0.0848639388762265
Using diff 0.167458964895413 and condRate 0.166666666666667
Changed category 1 weights from 
0.312148350304895 to 0.307682778007498
0.545463937348657 to 0.54099836505126
0.120538699454599 to 0.116073127157202
0.124631511992746 to 0.120165939695349
Changing layer 0's weights from 
0.395182644466505 to 0.390717072169107
0.0232561877766701 to 0.0187906154792724
-0.211093077917948 to -0.215558650215346
-0.176533828040018 to -0.180999400337416
0.571333741287336 to 0.566868168989938
-0.111308211942568 to -0.115773784239966
0.042521511654005 to 0.0380559393566073
0.0150671056309792 to 0.0106015333335815
-0.222424755354777 to -0.226890327652175
-0.266174810489073 to -0.270640382786471
Changing layer 1's weights from 
-0.26107789810504 to -0.265543470402438
0.340090786556348 to 0.33562521425895
0.214388047794446 to 0.209922475497048
0.204482709507093 to 0.200017137209695
0.0194299272099587 to 0.014964354912561
-0.118807906766787 to -0.123273479064185
0.151910816768751 to 0.147445244471353
-0.233892212172404 to -0.238357784469802
0.177770768741712 to 0.173305196444314
-0.184876094122782 to -0.18934166642018
Changing layer 2's weights from 
0.624312137703046 to 0.619846565405648
0.070258652309522 to 0.0657930800121243
0.354847942928419 to 0.350382370631021
0.526048039535627 to 0.521582467238229
0.348795329670057 to 0.344329757372659
0.142785703281507 to 0.138320130984109
0.66159841928874 to 0.657132846991342
0.589176391700849 to 0.584710819403451
-0.142821902891055 to -0.147287475188452
-0.0978643843134789 to -0.102329956610877
Changing layer 3's weights from 
0.59126183901225 to 0.586796266714852
0.327708756069288 to 0.32324318377189
0.570318674186811 to 0.565853101889413
-0.337120557569333 to -0.341586129866731
0.447642778495893 to 0.443177206198495
0.317394172290906 to 0.312928599993508
-0.0127906748255636 to -0.0172562471229613
0.44572219763194 to 0.441256625334542
0.114026223758802 to 0.109560651461404
0.586856102089032 to 0.582390529791634
Changing layer 4's weights from 
0.487582301239118 to 0.48311672894172
0.278749500850782 to 0.274283928553384
0.078271900753126 to 0.0738063284557283
0.365206634143934 to 0.360741061846536
0.125953828434095 to 0.121488256136697
0.35741904888545 to 0.352953476588052
0.353060518840894 to 0.348594946543496
-0.108837003370181 to -0.113302575667578
0.48485908422862 to 0.480393511931222
0.052557145694837 to 0.0480915733974393
Changing layer 5's weights from 
0.640362118820295 to 0.635896546522897
0.198288594822034 to 0.193823022524636
0.623476957420454 to 0.619011385123056
-0.312035223950998 to -0.316500796248396
0.225233589748487 to 0.220768017451089
-0.173234353323832 to -0.17769992562123
0.126730953792677 to 0.122265381495279
0.199177180866346 to 0.194711608568948
0.588779186348066 to 0.584313614050668
0.58227107439433 to 0.577805502096932
Trying to learn from memory 4, 1, -0.16
sum 0.336430538362186 distri 0.0848639388762265
Using diff 0.167458964895413 and condRate 0.166666666666667
Changed category 1 weights from 
0.307682778007498 to 0.3032172057101
0.54099836505126 to 0.536532792753862
0.116073127157202 to 0.111607554859804
0.120165939695349 to 0.115700367397951
Changing layer 0's weights from 
0.390717072169107 to 0.38625149987171
0.0187906154792724 to 0.0143250431818747
-0.215558650215346 to -0.220024222512743
-0.180999400337416 to -0.185464972634813
0.566868168989938 to 0.562402596692541
-0.115773784239966 to -0.120239356537364
0.0380559393566073 to 0.0335903670592096
0.0106015333335815 to 0.00613596103618381
-0.226890327652175 to -0.231355899949572
-0.270640382786471 to -0.275105955083868
Changing layer 1's weights from 
-0.265543470402438 to -0.270009042699835
0.33562521425895 to 0.331159641961553
0.209922475497048 to 0.205456903199651
0.200017137209695 to 0.195551564912298
0.014964354912561 to 0.0104987826151633
-0.123273479064185 to -0.127739051361583
0.147445244471353 to 0.142979672173956
-0.238357784469802 to -0.242823356767199
0.173305196444314 to 0.168839624146917
-0.18934166642018 to -0.193807238717577
Changing layer 2's weights from 
0.619846565405648 to 0.615380993108251
0.0657930800121243 to 0.0613275077147266
0.350382370631021 to 0.345916798333624
0.521582467238229 to 0.517116894940831
0.344329757372659 to 0.339864185075262
0.138320130984109 to 0.133854558686712
0.657132846991342 to 0.652667274693944
0.584710819403451 to 0.580245247106054
-0.147287475188452 to -0.15175304748585
-0.102329956610877 to -0.106795528908274
Changing layer 3's weights from 
0.586796266714852 to 0.582330694417455
0.32324318377189 to 0.318777611474493
0.565853101889413 to 0.561387529592016
-0.341586129866731 to -0.346051702164128
0.443177206198495 to 0.438711633901098
0.312928599993508 to 0.308463027696111
-0.0172562471229613 to -0.021721819420359
0.441256625334542 to 0.436791053037145
0.109560651461404 to 0.105095079164007
0.582390529791634 to 0.577924957494237
Changing layer 4's weights from 
0.48311672894172 to 0.478651156644323
0.274283928553384 to 0.269818356255987
0.0738063284557283 to 0.0693407561583306
0.360741061846536 to 0.356275489549139
0.121488256136697 to 0.1170226838393
0.352953476588052 to 0.348487904290655
0.348594946543496 to 0.344129374246099
-0.113302575667578 to -0.117768147964976
0.480393511931222 to 0.475927939633825
0.0480915733974393 to 0.0436260011000416
Changing layer 5's weights from 
0.635896546522897 to 0.6314309742255
0.193823022524636 to 0.189357450227239
0.619011385123056 to 0.614545812825659
-0.316500796248396 to -0.320966368545793
0.220768017451089 to 0.216302445153692
-0.17769992562123 to -0.182165497918627
0.122265381495279 to 0.117799809197882
0.194711608568948 to 0.190246036271551
0.584313614050668 to 0.579848041753271
0.577805502096932 to 0.573339929799535
Trying to learn from memory 5, 0, -0.16
sum 0.336430538362186 distri -0.0594193732926627
Using diff 0.311742277064302 and condRate 0.166666666666667
Changed category 0 weights from 
0.200873899447671 to 0.192560772245102
-0.199161064636954 to -0.207474191839523
-0.48168176115776 to -0.489994888360329
-0.330176395190009 to -0.338489522392578
Changing layer 0's weights from 
0.38625149987171 to 0.377938372669141
0.0143250431818747 to 0.00601191597930619
-0.220024222512743 to -0.228337349715312
-0.185464972634813 to -0.193778099837382
0.562402596692541 to 0.554089469489972
-0.120239356537364 to -0.128552483739932
0.0335903670592096 to 0.0252772398566411
0.00613596103618381 to -0.0021771661663847
-0.231355899949572 to -0.239669027152141
-0.275105955083868 to -0.283419082286437
Changing layer 1's weights from 
-0.270009042699835 to -0.278322169902404
0.331159641961553 to 0.322846514758984
0.205456903199651 to 0.197143775997082
0.195551564912298 to 0.187238437709729
0.0104987826151633 to 0.00218565541259479
-0.127739051361583 to -0.136052178564151
0.142979672173956 to 0.134666544971387
-0.242823356767199 to -0.251136483969768
0.168839624146917 to 0.160526496944348
-0.193807238717577 to -0.202120365920146
Changing layer 2's weights from 
0.615380993108251 to 0.607067865905682
0.0613275077147266 to 0.0530143805121581
0.345916798333624 to 0.337603671131055
0.517116894940831 to 0.508803767738263
0.339864185075262 to 0.331551057872693
0.133854558686712 to 0.125541431484143
0.652667274693944 to 0.644354147491376
0.580245247106054 to 0.571932119903485
-0.15175304748585 to -0.160066174688418
-0.106795528908274 to -0.115108656110843
Changing layer 3's weights from 
0.582330694417455 to 0.574017567214886
0.318777611474493 to 0.310464484271924
0.561387529592016 to 0.553074402389447
-0.346051702164128 to -0.354364829366697
0.438711633901098 to 0.430398506698529
0.308463027696111 to 0.300149900493542
-0.021721819420359 to -0.0300349466229275
0.436791053037145 to 0.428477925834576
0.105095079164007 to 0.0967819519614381
0.577924957494237 to 0.569611830291668
Changing layer 4's weights from 
0.478651156644323 to 0.470338029441754
0.269818356255987 to 0.261505229053418
0.0693407561583306 to 0.0610276289557621
0.356275489549139 to 0.34796236234657
0.1170226838393 to 0.108709556636731
0.348487904290655 to 0.340174777088086
0.344129374246099 to 0.33581624704353
-0.117768147964976 to -0.126081275167544
0.475927939633825 to 0.467614812431256
0.0436260011000416 to 0.0353128738974731
Changing layer 5's weights from 
0.6314309742255 to 0.623117847022931
0.189357450227239 to 0.18104432302467
0.614545812825659 to 0.60623268562309
-0.320966368545793 to -0.329279495748362
0.216302445153692 to 0.207989317951123
-0.182165497918627 to -0.190478625121196
0.117799809197882 to 0.109486681995313
0.190246036271551 to 0.181932909068982
0.579848041753271 to 0.571534914550702
0.573339929799535 to 0.565026802596966
Trying to learn from memory 6, 2, -0.16
sum 0.336430538362186 distri 0.310985972778622
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
1.0044781260944 to 0.977811460023779
1.28292009167439 to 1.25625342560377
1.19587972693211 to 1.16921306086149
0.54130802236563 to 0.514641356295009
Changing layer 0's weights from 
0.377938372669141 to 0.351271706598521
0.00601191597930619 to -0.020654750091314
-0.228337349715312 to -0.255004015785932
-0.193778099837382 to -0.220444765908002
0.554089469489972 to 0.527422803419352
-0.128552483739932 to -0.155219149810552
0.0252772398566411 to -0.00138942621397914
-0.0021771661663847 to -0.0288438322370049
-0.239669027152141 to -0.266335693222761
-0.283419082286437 to -0.310085748357057
Changing layer 1's weights from 
-0.278322169902404 to -0.304988835973024
0.322846514758984 to 0.296179848688364
0.197143775997082 to 0.170477109926462
0.187238437709729 to 0.160571771639109
0.00218565541259479 to -0.0244810106580254
-0.136052178564151 to -0.162718844634771
0.134666544971387 to 0.107999878900767
-0.251136483969768 to -0.277803150040388
0.160526496944348 to 0.133859830873728
-0.202120365920146 to -0.228787031990766
Changing layer 2's weights from 
0.607067865905682 to 0.580401199835062
0.0530143805121581 to 0.0263477144415379
0.337603671131055 to 0.310937005060435
0.508803767738263 to 0.482137101667643
0.331551057872693 to 0.304884391802073
0.125541431484143 to 0.0988747654135229
0.644354147491376 to 0.617687481420756
0.571932119903485 to 0.545265453832865
-0.160066174688418 to -0.186732840759039
-0.115108656110843 to -0.141775322181463
Changing layer 3's weights from 
0.574017567214886 to 0.547350901144266
0.310464484271924 to 0.283797818201304
0.553074402389447 to 0.526407736318827
-0.354364829366697 to -0.381031495437317
0.430398506698529 to 0.403731840627909
0.300149900493542 to 0.273483234422922
-0.0300349466229275 to -0.0567016126935477
0.428477925834576 to 0.401811259763956
0.0967819519614381 to 0.0701152858908179
0.569611830291668 to 0.542945164221048
Changing layer 4's weights from 
0.470338029441754 to 0.443671363371134
0.261505229053418 to 0.234838562982798
0.0610276289557621 to 0.0343609628851419
0.34796236234657 to 0.32129569627595
0.108709556636731 to 0.0820428905661109
0.340174777088086 to 0.313508111017466
0.33581624704353 to 0.30914958097291
-0.126081275167544 to -0.152747941238165
0.467614812431256 to 0.440948146360636
0.0353128738974731 to 0.00864620782685286
Changing layer 5's weights from 
0.623117847022931 to 0.596451180952311
0.18104432302467 to 0.15437765695405
0.60623268562309 to 0.57956601955247
-0.329279495748362 to -0.355946161818982
0.207989317951123 to 0.181322651880503
-0.190478625121196 to -0.217145291191816
0.109486681995313 to 0.0828200159246929
0.181932909068982 to 0.155266242998362
0.571534914550702 to 0.544868248480082
0.565026802596966 to 0.538360136526346
Trying to learn from memory 6, 2, -0.16
sum 0.336430538362186 distri 0.310985972778622
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.977811460023779 to 0.951144793953159
1.25625342560377 to 1.22958675953315
1.16921306086149 to 1.14254639479087
0.514641356295009 to 0.487974690224389
Changing layer 0's weights from 
0.351271706598521 to 0.324605040527901
-0.020654750091314 to -0.0473214161619342
-0.255004015785932 to -0.281670681856552
-0.220444765908002 to -0.247111431978622
0.527422803419352 to 0.500756137348732
-0.155219149810552 to -0.181885815881173
-0.00138942621397914 to -0.0280560922845994
-0.0288438322370049 to -0.0555104983076251
-0.266335693222761 to -0.293002359293381
-0.310085748357057 to -0.336752414427677
Changing layer 1's weights from 
-0.304988835973024 to -0.331655502043644
0.296179848688364 to 0.269513182617744
0.170477109926462 to 0.143810443855842
0.160571771639109 to 0.133905105568489
-0.0244810106580254 to -0.0511476767286456
-0.162718844634771 to -0.189385510705392
0.107999878900767 to 0.0813332128301467
-0.277803150040388 to -0.304469816111008
0.133859830873728 to 0.107193164803108
-0.228787031990766 to -0.255453698061386
Changing layer 2's weights from 
0.580401199835062 to 0.553734533764442
0.0263477144415379 to -0.000318951629082326
0.310937005060435 to 0.284270338989815
0.482137101667643 to 0.455470435597022
0.304884391802073 to 0.278217725731453
0.0988747654135229 to 0.0722080993429027
0.617687481420756 to 0.591020815350136
0.545265453832865 to 0.518598787762245
-0.186732840759039 to -0.213399506829659
-0.141775322181463 to -0.168441988252083
Changing layer 3's weights from 
0.547350901144266 to 0.520684235073646
0.283797818201304 to 0.257131152130684
0.526407736318827 to 0.499741070248207
-0.381031495437317 to -0.407698161507937
0.403731840627909 to 0.377065174557289
0.273483234422922 to 0.246816568352302
-0.0567016126935477 to -0.0833682787641679
0.401811259763956 to 0.375144593693336
0.0701152858908179 to 0.0434486198201977
0.542945164221048 to 0.516278498150428
Changing layer 4's weights from 
0.443671363371134 to 0.417004697300514
0.234838562982798 to 0.208171896912178
0.0343609628851419 to 0.00769429681452169
0.32129569627595 to 0.29462903020533
0.0820428905661109 to 0.0553762244954907
0.313508111017466 to 0.286841444946846
0.30914958097291 to 0.28248291490229
-0.152747941238165 to -0.179414607308785
0.440948146360636 to 0.414281480290016
0.00864620782685286 to -0.0180204582437674
Changing layer 5's weights from 
0.596451180952311 to 0.569784514881691
0.15437765695405 to 0.12771099088343
0.57956601955247 to 0.55289935348185
-0.355946161818982 to -0.382612827889602
0.181322651880503 to 0.154655985809883
-0.217145291191816 to -0.243811957262436
0.0828200159246929 to 0.0561533498540727
0.155266242998362 to 0.128599576927742
0.544868248480082 to 0.518201582409462
0.538360136526346 to 0.511693470455726
10/5/2016 1:29:41 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 6, 1, -0.16
sum 0.336430538362186 distri 0.0848639388762265
Using diff 0.167458964895413 and condRate 0.166666666666667
Changed category 1 weights from 
0.3032172057101 to 0.298751633412702
0.536532792753862 to 0.532067220456464
0.111607554859804 to 0.107141982562406
0.115700367397951 to 0.111234795100553
Changing layer 0's weights from 
0.324605040527901 to 0.320139468230503
-0.0473214161619342 to -0.0517869884593319
-0.281670681856552 to -0.28613625415395
-0.247111431978622 to -0.25157700427602
0.500756137348732 to 0.496290565051334
-0.181885815881173 to -0.18635138817857
-0.0280560922845994 to -0.032521664581997
-0.0555104983076251 to -0.0599760706050228
-0.293002359293381 to -0.297467931590779
-0.336752414427677 to -0.341217986725075
Changing layer 1's weights from 
-0.331655502043644 to -0.336121074341042
0.269513182617744 to 0.265047610320346
0.143810443855842 to 0.139344871558444
0.133905105568489 to 0.129439533271091
-0.0511476767286456 to -0.0556132490260433
-0.189385510705392 to -0.193851083002789
0.0813332128301467 to 0.076867640532749
-0.304469816111008 to -0.308935388408406
0.107193164803108 to 0.10272759250571
-0.255453698061386 to -0.259919270358784
Changing layer 2's weights from 
0.553734533764442 to 0.549268961467044
-0.000318951629082326 to -0.00478452392648002
0.284270338989815 to 0.279804766692417
0.455470435597022 to 0.451004863299625
0.278217725731453 to 0.273752153434055
0.0722080993429027 to 0.067742527045505
0.591020815350136 to 0.586555243052738
0.518598787762245 to 0.514133215464847
-0.213399506829659 to -0.217865079127057
-0.168441988252083 to -0.172907560549481
Changing layer 3's weights from 
0.520684235073646 to 0.516218662776248
0.257131152130684 to 0.252665579833286
0.499741070248207 to 0.495275497950809
-0.407698161507937 to -0.412163733805335
0.377065174557289 to 0.372599602259891
0.246816568352302 to 0.242350996054904
-0.0833682787641679 to -0.0878338510615656
0.375144593693336 to 0.370679021395938
0.0434486198201977 to 0.0389830475228
0.516278498150428 to 0.51181292585303
Changing layer 4's weights from 
0.417004697300514 to 0.412539125003116
0.208171896912178 to 0.20370632461478
0.00769429681452169 to 0.00322872451712399
0.29462903020533 to 0.290163457907932
0.0553762244954907 to 0.050910652198093
0.286841444946846 to 0.282375872649448
0.28248291490229 to 0.278017342604892
-0.179414607308785 to -0.183880179606183
0.414281480290016 to 0.409815907992618
-0.0180204582437674 to -0.0224860305411651
Changing layer 5's weights from 
0.569784514881691 to 0.565318942584293
0.12771099088343 to 0.123245418586032
0.55289935348185 to 0.548433781184452
-0.382612827889602 to -0.387078400187
0.154655985809883 to 0.150190413512485
-0.243811957262436 to -0.248277529559834
0.0561533498540727 to 0.051687777556675
0.128599576927742 to 0.124134004630344
0.518201582409462 to 0.513736010112064
0.511693470455726 to 0.507227898158328
Trying to learn from memory 6, 2, -0.16
sum 0.336430538362186 distri 0.310985972778622
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.951144793953159 to 0.924478127882539
1.22958675953315 to 1.20292009346253
1.14254639479087 to 1.11587972872025
0.487974690224389 to 0.461308024153769
Changing layer 0's weights from 
0.320139468230503 to 0.293472802159883
-0.0517869884593319 to -0.0784536545299521
-0.28613625415395 to -0.31280292022457
-0.25157700427602 to -0.27824367034664
0.496290565051334 to 0.469623898980714
-0.18635138817857 to -0.21301805424919
-0.032521664581997 to -0.0591883306526173
-0.0599760706050228 to -0.0866427366756431
-0.297467931590779 to -0.324134597661399
-0.341217986725075 to -0.367884652795695
Changing layer 1's weights from 
-0.336121074341042 to -0.362787740411662
0.265047610320346 to 0.238380944249726
0.139344871558444 to 0.112678205487824
0.129439533271091 to 0.102772867200471
-0.0556132490260433 to -0.0822799150966636
-0.193851083002789 to -0.220517749073409
0.076867640532749 to 0.0502009744621288
-0.308935388408406 to -0.335602054479026
0.10272759250571 to 0.0760609264350898
-0.259919270358784 to -0.286585936429404
Changing layer 2's weights from 
0.549268961467044 to 0.522602295396424
-0.00478452392648002 to -0.0314511899971002
0.279804766692417 to 0.253138100621797
0.451004863299625 to 0.424338197229005
0.273752153434055 to 0.247085487363435
0.067742527045505 to 0.0410758609748848
0.586555243052738 to 0.559888576982118
0.514133215464847 to 0.487466549394227
-0.217865079127057 to -0.244531745197677
-0.172907560549481 to -0.199574226620101
Changing layer 3's weights from 
0.516218662776248 to 0.489551996705628
0.252665579833286 to 0.225998913762666
0.495275497950809 to 0.468608831880189
-0.412163733805335 to -0.438830399875955
0.372599602259891 to 0.345932936189271
0.242350996054904 to 0.215684329984284
-0.0878338510615656 to -0.114500517132186
0.370679021395938 to 0.344012355325318
0.0389830475228 to 0.0123163814521798
0.51181292585303 to 0.48514625978241
Changing layer 4's weights from 
0.412539125003116 to 0.385872458932496
0.20370632461478 to 0.17703965854416
0.00322872451712399 to -0.0234379415534962
0.290163457907932 to 0.263496791837312
0.050910652198093 to 0.0242439861274728
0.282375872649448 to 0.255709206578828
0.278017342604892 to 0.251350676534272
-0.183880179606183 to -0.210546845676803
0.409815907992618 to 0.383149241921998
-0.0224860305411651 to -0.0491526966117853
Changing layer 5's weights from 
0.565318942584293 to 0.538652276513673
0.123245418586032 to 0.0965787525154118
0.548433781184452 to 0.521767115113832
-0.387078400187 to -0.41374506625762
0.150190413512485 to 0.123523747441865
-0.248277529559834 to -0.274944195630454
0.051687777556675 to 0.0250211114860548
0.124134004630344 to 0.0974673385597238
0.513736010112064 to 0.487069344041444
0.507227898158328 to 0.480561232087708
Trying to learn from memory 6, 2, -0.16
sum 0.336430538362186 distri 0.310985972778622
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.924478127882539 to 0.897811461811919
1.20292009346253 to 1.17625342739191
1.11587972872025 to 1.08921306264963
0.461308024153769 to 0.434641358083149
Changing layer 0's weights from 
0.293472802159883 to 0.266806136089262
-0.0784536545299521 to -0.105120320600572
-0.31280292022457 to -0.339469586295191
-0.27824367034664 to -0.304910336417261
0.469623898980714 to 0.442957232910094
-0.21301805424919 to -0.239684720319811
-0.0591883306526173 to -0.0858549967232375
-0.0866427366756431 to -0.113309402746263
-0.324134597661399 to -0.35080126373202
-0.367884652795695 to -0.394551318866316
Changing layer 1's weights from 
-0.362787740411662 to -0.389454406482282
0.238380944249726 to 0.211714278179105
0.112678205487824 to 0.0860115394172036
0.102772867200471 to 0.0761062011298506
-0.0822799150966636 to -0.108946581167284
-0.220517749073409 to -0.24718441514403
0.0502009744621288 to 0.0235343083915086
-0.335602054479026 to -0.362268720549647
0.0760609264350898 to 0.0493942603644696
-0.286585936429404 to -0.313252602500025
Changing layer 2's weights from 
0.522602295396424 to 0.495935629325804
-0.0314511899971002 to -0.0581178560677205
0.253138100621797 to 0.226471434551176
0.424338197229005 to 0.397671531158384
0.247085487363435 to 0.220418821292814
0.0410758609748848 to 0.0144091949042646
0.559888576982118 to 0.533221910911498
0.487466549394227 to 0.460799883323606
-0.244531745197677 to -0.271198411268297
-0.199574226620101 to -0.226240892690721
Changing layer 3's weights from 
0.489551996705628 to 0.462885330635007
0.225998913762666 to 0.199332247692045
0.468608831880189 to 0.441942165809568
-0.438830399875955 to -0.465497065946576
0.345932936189271 to 0.31926627011865
0.215684329984284 to 0.189017663913664
-0.114500517132186 to -0.141167183202806
0.344012355325318 to 0.317345689254697
0.0123163814521798 to -0.0143502846184405
0.48514625978241 to 0.458479593711789
Changing layer 4's weights from 
0.385872458932496 to 0.359205792861876
0.17703965854416 to 0.15037299247354
-0.0234379415534962 to -0.0501046076241164
0.263496791837312 to 0.236830125766691
0.0242439861274728 to -0.00242267994314746
0.255709206578828 to 0.229042540508207
0.251350676534272 to 0.224684010463651
-0.210546845676803 to -0.237213511747423
0.383149241921998 to 0.356482575851377
-0.0491526966117853 to -0.0758193626824055
Changing layer 5's weights from 
0.538652276513673 to 0.511985610443053
0.0965787525154118 to 0.0699120864447915
0.521767115113832 to 0.495100449043212
-0.41374506625762 to -0.440411732328241
0.123523747441865 to 0.0968570813712445
-0.274944195630454 to -0.301610861701074
0.0250211114860548 to -0.00164555458456545
0.0974673385597238 to 0.0708006724891036
0.487069344041444 to 0.460402677970823
0.480561232087708 to 0.453894566017087
Trying to learn from memory 7, 0, -0.16
sum 0.336430538362186 distri -0.0594193732926627
Using diff 0.311742277064302 and condRate 0.166666666666667
Changed category 0 weights from 
0.192560772245102 to 0.184247645042534
-0.207474191839523 to -0.215787319042091
-0.489994888360329 to -0.498308015562897
-0.338489522392578 to -0.346802649595146
Changing layer 0's weights from 
0.266806136089262 to 0.258493008886694
-0.105120320600572 to -0.113433447803141
-0.339469586295191 to -0.347782713497759
-0.304910336417261 to -0.313223463619829
0.442957232910094 to 0.434644105707525
-0.239684720319811 to -0.247997847522379
-0.0858549967232375 to -0.094168123925806
-0.113309402746263 to -0.121622529948832
-0.35080126373202 to -0.359114390934588
-0.394551318866316 to -0.402864446068884
Changing layer 1's weights from 
-0.389454406482282 to -0.397767533684851
0.211714278179105 to 0.203401150976537
0.0860115394172036 to 0.0776984122146351
0.0761062011298506 to 0.0677930739272821
-0.108946581167284 to -0.117259708369852
-0.24718441514403 to -0.255497542346598
0.0235343083915086 to 0.01522118118894
-0.362268720549647 to -0.370581847752215
0.0493942603644696 to 0.0410811331619011
-0.313252602500025 to -0.321565729702593
Changing layer 2's weights from 
0.495935629325804 to 0.487622502123235
-0.0581178560677205 to -0.066430983270289
0.226471434551176 to 0.218158307348608
0.397671531158384 to 0.389358403955816
0.220418821292814 to 0.212105694090246
0.0144091949042646 to 0.00609606770169603
0.533221910911498 to 0.524908783708929
0.460799883323606 to 0.452486756121038
-0.271198411268297 to -0.279511538470865
-0.226240892690721 to -0.23455401989329
Changing layer 3's weights from 
0.462885330635007 to 0.454572203432439
0.199332247692045 to 0.191019120489477
0.441942165809568 to 0.433629038607
-0.465497065946576 to -0.473810193149144
0.31926627011865 to 0.310953142916082
0.189017663913664 to 0.180704536711095
-0.141167183202806 to -0.149480310405375
0.317345689254697 to 0.309032562052129
-0.0143502846184405 to -0.022663411821009
0.458479593711789 to 0.450166466509221
Changing layer 4's weights from 
0.359205792861876 to 0.350892665659307
0.15037299247354 to 0.142059865270971
-0.0501046076241164 to -0.058417734826685
0.236830125766691 to 0.228516998564123
-0.00242267994314746 to -0.010735807145716
0.229042540508207 to 0.220729413305639
0.224684010463651 to 0.216370883261083
-0.237213511747423 to -0.245526638949992
0.356482575851377 to 0.348169448648809
-0.0758193626824055 to -0.084132489884974
Changing layer 5's weights from 
0.511985610443053 to 0.503672483240484
0.0699120864447915 to 0.061598959242223
0.495100449043212 to 0.486787321840643
-0.440411732328241 to -0.448724859530809
0.0968570813712445 to 0.088543954168676
-0.301610861701074 to -0.309923988903643
-0.00164555458456545 to -0.00995868178713396
0.0708006724891036 to 0.0624875452865351
0.460402677970823 to 0.452089550768255
0.453894566017087 to 0.445581438814519
Trying to learn from memory 7, 2, -0.16
sum 0.336430538362186 distri 0.310985972778622
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.897811461811919 to 0.871144795741298
1.17625342739191 to 1.14958676132129
1.08921306264963 to 1.06254639657901
0.434641358083149 to 0.407974692012528
Changing layer 0's weights from 
0.258493008886694 to 0.231826342816074
-0.113433447803141 to -0.140100113873761
-0.347782713497759 to -0.374449379568379
-0.313223463619829 to -0.339890129690449
0.434644105707525 to 0.407977439636905
-0.247997847522379 to -0.274664513592999
-0.094168123925806 to -0.120834789996426
-0.121622529948832 to -0.148289196019452
-0.359114390934588 to -0.385781057005208
-0.402864446068884 to -0.429531112139504
Changing layer 1's weights from 
-0.397767533684851 to -0.424434199755471
0.203401150976537 to 0.176734484905917
0.0776984122146351 to 0.0510317461440148
0.0677930739272821 to 0.0411264078566618
-0.117259708369852 to -0.143926374440473
-0.255497542346598 to -0.282164208417218
0.01522118118894 to -0.0114454848816802
-0.370581847752215 to -0.397248513822835
0.0410811331619011 to 0.0144144670912809
-0.321565729702593 to -0.348232395773213
Changing layer 2's weights from 
0.487622502123235 to 0.460955836052615
-0.066430983270289 to -0.0930976493409092
0.218158307348608 to 0.191491641277988
0.389358403955816 to 0.362691737885196
0.212105694090246 to 0.185439028019626
0.00609606770169603 to -0.0205705983689242
0.524908783708929 to 0.498242117638309
0.452486756121038 to 0.425820090050418
-0.279511538470865 to -0.306178204541486
-0.23455401989329 to -0.26122068596391
Changing layer 3's weights from 
0.454572203432439 to 0.427905537361819
0.191019120489477 to 0.164352454418857
0.433629038607 to 0.40696237253638
-0.473810193149144 to -0.500476859219764
0.310953142916082 to 0.284286476845462
0.180704536711095 to 0.154037870640475
-0.149480310405375 to -0.176146976475995
0.309032562052129 to 0.282365895981509
-0.022663411821009 to -0.0493300778916292
0.450166466509221 to 0.423499800438601
Changing layer 4's weights from 
0.350892665659307 to 0.324225999588687
0.142059865270971 to 0.115393199200351
-0.058417734826685 to -0.0850844008973052
0.228516998564123 to 0.201850332493503
-0.010735807145716 to -0.0374024732163362
0.220729413305639 to 0.194062747235019
0.216370883261083 to 0.189704217190463
-0.245526638949992 to -0.272193305020612
0.348169448648809 to 0.321502782578189
-0.084132489884974 to -0.110799155955594
Changing layer 5's weights from 
0.503672483240484 to 0.477005817169864
0.061598959242223 to 0.0349322931716028
0.486787321840643 to 0.460120655770023
-0.448724859530809 to -0.475391525601429
0.088543954168676 to 0.0618772880980558
-0.309923988903643 to -0.336590654974263
-0.00995868178713396 to -0.0366253478577542
0.0624875452865351 to 0.0358208792159148
0.452089550768255 to 0.425422884697635
0.445581438814519 to 0.418914772743899
Trying to learn from memory 8, 2, -0.16
sum 0.336430538362186 distri 0.310985972778622
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.871144795741298 to 0.844478129670678
1.14958676132129 to 1.12292009525067
1.06254639657901 to 1.03587973050839
0.407974692012528 to 0.381308025941908
Changing layer 0's weights from 
0.231826342816074 to 0.205159676745454
-0.140100113873761 to -0.166766779944381
-0.374449379568379 to -0.401116045639
-0.339890129690449 to -0.366556795761069
0.407977439636905 to 0.381310773566285
-0.274664513592999 to -0.30133117966362
-0.120834789996426 to -0.147501456067046
-0.148289196019452 to -0.174955862090072
-0.385781057005208 to -0.412447723075829
-0.429531112139504 to -0.456197778210124
Changing layer 1's weights from 
-0.424434199755471 to -0.451100865826091
0.176734484905917 to 0.150067818835297
0.0510317461440148 to 0.0243650800733946
0.0411264078566618 to 0.0144597417860416
-0.143926374440473 to -0.170593040511093
-0.282164208417218 to -0.308830874487839
-0.0114454848816802 to -0.0381121509523004
-0.397248513822835 to -0.423915179893456
0.0144144670912809 to -0.0122521989793394
-0.348232395773213 to -0.374899061843833
Changing layer 2's weights from 
0.460955836052615 to 0.434289169981995
-0.0930976493409092 to -0.119764315411529
0.191491641277988 to 0.164824975207368
0.362691737885196 to 0.336025071814575
0.185439028019626 to 0.158772361949006
-0.0205705983689242 to -0.0472372644395444
0.498242117638309 to 0.471575451567689
0.425820090050418 to 0.399153423979797
-0.306178204541486 to -0.332844870612106
-0.26122068596391 to -0.28788735203453
Changing layer 3's weights from 
0.427905537361819 to 0.401238871291199
0.164352454418857 to 0.137685788348237
0.40696237253638 to 0.38029570646576
-0.500476859219764 to -0.527143525290384
0.284286476845462 to 0.257619810774841
0.154037870640475 to 0.127371204569855
-0.176146976475995 to -0.202813642546615
0.282365895981509 to 0.255699229910888
-0.0493300778916292 to -0.0759967439622494
0.423499800438601 to 0.396833134367981
Changing layer 4's weights from 
0.324225999588687 to 0.297559333518067
0.115393199200351 to 0.0887265331297306
-0.0850844008973052 to -0.111751066967925
0.201850332493503 to 0.175183666422883
-0.0374024732163362 to -0.0640691392869564
0.194062747235019 to 0.167396081164399
0.189704217190463 to 0.163037551119843
-0.272193305020612 to -0.298859971091232
0.321502782578189 to 0.294836116507569
-0.110799155955594 to -0.137465822026214
Changing layer 5's weights from 
0.477005817169864 to 0.450339151099244
0.0349322931716028 to 0.00826562710098259
0.460120655770023 to 0.433453989699403
-0.475391525601429 to -0.502058191672049
0.0618772880980558 to 0.0352106220274356
-0.336590654974263 to -0.363257321044883
-0.0366253478577542 to -0.0632920139283744
0.0358208792159148 to 0.00915421314529462
0.425422884697635 to 0.398756218627014
0.418914772743899 to 0.392248106673278
Trying to learn from memory 8, 2, -0.16
sum 0.336430538362186 distri 0.310985972778622
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.844478129670678 to 0.817811463600058
1.12292009525067 to 1.09625342918005
1.03587973050839 to 1.00921306443777
0.381308025941908 to 0.354641359871288
Changing layer 0's weights from 
0.205159676745454 to 0.178493010674833
-0.166766779944381 to -0.193433446015002
-0.401116045639 to -0.42778271170962
-0.366556795761069 to -0.39322346183169
0.381310773566285 to 0.354644107495664
-0.30133117966362 to -0.32799784573424
-0.147501456067046 to -0.174168122137667
-0.174955862090072 to -0.201622528160692
-0.412447723075829 to -0.439114389146449
-0.456197778210124 to -0.482864444280745
Changing layer 1's weights from 
-0.451100865826091 to -0.477767531896712
0.150067818835297 to 0.123401152764676
0.0243650800733946 to -0.0023015859972256
0.0144597417860416 to -0.0122069242845786
-0.170593040511093 to -0.197259706581713
-0.308830874487839 to -0.335497540558459
-0.0381121509523004 to -0.0647788170229206
-0.423915179893456 to -0.450581845964076
-0.0122521989793394 to -0.0389188650499596
-0.374899061843833 to -0.401565727914454
Changing layer 2's weights from 
0.434289169981995 to 0.407622503911374
-0.119764315411529 to -0.14643098148215
0.164824975207368 to 0.138158309136747
0.336025071814575 to 0.309358405743955
0.158772361949006 to 0.132105695878385
-0.0472372644395444 to -0.0739039305101646
0.471575451567689 to 0.444908785497068
0.399153423979797 to 0.372486757909177
-0.332844870612106 to -0.359511536682726
-0.28788735203453 to -0.314554018105151
Changing layer 3's weights from 
0.401238871291199 to 0.374572205220578
0.137685788348237 to 0.111019122277616
0.38029570646576 to 0.353629040395139
-0.527143525290384 to -0.553810191361005
0.257619810774841 to 0.230953144704221
0.127371204569855 to 0.100704538499234
-0.202813642546615 to -0.229480308617235
0.255699229910888 to 0.229032563840268
-0.0759967439622494 to -0.10266341003287
0.396833134367981 to 0.37016646829736
Changing layer 4's weights from 
0.297559333518067 to 0.270892667447446
0.0887265331297306 to 0.0620598670591104
-0.111751066967925 to -0.138417733038546
0.175183666422883 to 0.148517000352262
-0.0640691392869564 to -0.0907358053575766
0.167396081164399 to 0.140729415093778
0.163037551119843 to 0.136370885049222
-0.298859971091232 to -0.325526637161852
0.294836116507569 to 0.268169450436948
-0.137465822026214 to -0.164132488096835
Changing layer 5's weights from 
0.450339151099244 to 0.423672485028623
0.00826562710098259 to -0.0184010389696376
0.433453989699403 to 0.406787323628782
-0.502058191672049 to -0.52872485774267
0.0352106220274356 to 0.00854395595681538
-0.363257321044883 to -0.389923987115504
-0.0632920139283744 to -0.0899586799989946
0.00915421314529462 to -0.0175124529253256
0.398756218627014 to 0.372089552556394
0.392248106673278 to 0.365581440602658
Trying to learn from memory 8, 2, -0.16
sum 0.336430538362186 distri 0.310985972778622
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.817811463600058 to 0.791144797529438
1.09625342918005 to 1.06958676310943
1.00921306443777 to 0.982546398367147
0.354641359871288 to 0.327974693800668
Changing layer 0's weights from 
0.178493010674833 to 0.151826344604213
-0.193433446015002 to -0.220100112085622
-0.42778271170962 to -0.45444937778024
-0.39322346183169 to -0.41989012790231
0.354644107495664 to 0.327977441425044
-0.32799784573424 to -0.35466451180486
-0.174168122137667 to -0.200834788208287
-0.201622528160692 to -0.228289194231313
-0.439114389146449 to -0.465781055217069
-0.482864444280745 to -0.509531110351365
Changing layer 1's weights from 
-0.477767531896712 to -0.504434197967332
0.123401152764676 to 0.0967344866940561
-0.0023015859972256 to -0.0289682520678458
-0.0122069242845786 to -0.0388735903551988
-0.197259706581713 to -0.223926372652333
-0.335497540558459 to -0.362164206629079
-0.0647788170229206 to -0.0914454830935408
-0.450581845964076 to -0.477248512034696
-0.0389188650499596 to -0.0655855311205798
-0.401565727914454 to -0.428232393985074
Changing layer 2's weights from 
0.407622503911374 to 0.380955837840754
-0.14643098148215 to -0.17309764755277
0.138158309136747 to 0.111491643066127
0.309358405743955 to 0.282691739673335
0.132105695878385 to 0.105439029807765
-0.0739039305101646 to -0.100570596580785
0.444908785497068 to 0.418242119426448
0.372486757909177 to 0.345820091838557
-0.359511536682726 to -0.386178202753346
-0.314554018105151 to -0.341220684175771
Changing layer 3's weights from 
0.374572205220578 to 0.347905539149958
0.111019122277616 to 0.0843524562069961
0.353629040395139 to 0.326962374324519
-0.553810191361005 to -0.580476857431625
0.230953144704221 to 0.204286478633601
0.100704538499234 to 0.0740378724286141
-0.229480308617235 to -0.256146974687855
0.229032563840268 to 0.202365897769648
-0.10266341003287 to -0.12933007610349
0.37016646829736 to 0.34349980222674
Changing layer 4's weights from 
0.270892667447446 to 0.244226001376826
0.0620598670591104 to 0.0353932009884902
-0.138417733038546 to -0.165084399109166
0.148517000352262 to 0.121850334281642
-0.0907358053575766 to -0.117402471428197
0.140729415093778 to 0.114062749023158
0.136370885049222 to 0.109704218978602
-0.325526637161852 to -0.352193303232472
0.268169450436948 to 0.241502784366328
-0.164132488096835 to -0.190799154167455
Changing layer 5's weights from 
0.423672485028623 to 0.397005818958003
-0.0184010389696376 to -0.0450677050402578
0.406787323628782 to 0.380120657558162
-0.52872485774267 to -0.55539152381329
0.00854395595681538 to -0.0181227101138048
-0.389923987115504 to -0.416590653186124
-0.0899586799989946 to -0.116625346069615
-0.0175124529253256 to -0.0441791189959458
0.372089552556394 to 0.345422886485774
0.365581440602658 to 0.338914774532038
Trying to learn from memory 9, 1, -0.16
sum 0.336430538362186 distri 0.0848639388762265
Using diff 0.167458964895413 and condRate 0.166666666666667
Changed category 1 weights from 
0.298751633412702 to 0.294286061115305
0.532067220456464 to 0.527601648159066
0.107141982562406 to 0.102676410265009
0.111234795100553 to 0.106769222803156
Changing layer 0's weights from 
0.151826344604213 to 0.147360772306815
-0.220100112085622 to -0.224565684383019
-0.45444937778024 to -0.458914950077638
-0.41989012790231 to -0.424355700199708
0.327977441425044 to 0.323511869127646
-0.35466451180486 to -0.359130084102258
-0.200834788208287 to -0.205300360505685
-0.228289194231313 to -0.23275476652871
-0.465781055217069 to -0.470246627514467
-0.509531110351365 to -0.513996682648763
Changing layer 1's weights from 
-0.504434197967332 to -0.50889977026473
0.0967344866940561 to 0.0922689143966584
-0.0289682520678458 to -0.0334338243652435
-0.0388735903551988 to -0.0433391626525965
-0.223926372652333 to -0.228391944949731
-0.362164206629079 to -0.366629778926477
-0.0914454830935408 to -0.0959110553909385
-0.477248512034696 to -0.481714084332094
-0.0655855311205798 to -0.0700511034179775
-0.428232393985074 to -0.432697966282472
Changing layer 2's weights from 
0.380955837840754 to 0.376490265543356
-0.17309764755277 to -0.177563219850168
0.111491643066127 to 0.107026070768729
0.282691739673335 to 0.278226167375937
0.105439029807765 to 0.100973457510367
-0.100570596580785 to -0.105036168878183
0.418242119426448 to 0.41377654712905
0.345820091838557 to 0.341354519541159
-0.386178202753346 to -0.390643775050744
-0.341220684175771 to -0.345686256473168
Changing layer 3's weights from 
0.347905539149958 to 0.34343996685256
0.0843524562069961 to 0.0798868839095984
0.326962374324519 to 0.322496802027121
-0.580476857431625 to -0.584942429729023
0.204286478633601 to 0.199820906336203
0.0740378724286141 to 0.0695723001312165
-0.256146974687855 to -0.260612546985253
0.202365897769648 to 0.19790032547225
-0.12933007610349 to -0.133795648400888
0.34349980222674 to 0.339034229929342
Changing layer 4's weights from 
0.244226001376826 to 0.239760429079428
0.0353932009884902 to 0.0309276286910925
-0.165084399109166 to -0.169549971406564
0.121850334281642 to 0.117384761984244
-0.117402471428197 to -0.121868043725595
0.114062749023158 to 0.10959717672576
0.109704218978602 to 0.105238646681204
-0.352193303232472 to -0.35665887552987
0.241502784366328 to 0.23703721206893
-0.190799154167455 to -0.195264726464853
Changing layer 5's weights from 
0.397005818958003 to 0.392540246660605
-0.0450677050402578 to -0.0495332773376555
0.380120657558162 to 0.375655085260764
-0.55539152381329 to -0.559857096110688
-0.0181227101138048 to -0.0225882824112025
-0.416590653186124 to -0.421056225483522
-0.116625346069615 to -0.121090918367013
-0.0441791189959458 to -0.0486446912933435
0.345422886485774 to 0.340957314188376
0.338914774532038 to 0.33444920223464
Trying to learn from memory 9, 2, -0.16
sum 0.336430538362186 distri 0.310985972778622
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.791144797529438 to 0.764478131458818
1.06958676310943 to 1.04292009703881
0.982546398367147 to 0.955879732296527
0.327974693800668 to 0.301308027730048
Changing layer 0's weights from 
0.147360772306815 to 0.120694106236195
-0.224565684383019 to -0.25123235045364
-0.458914950077638 to -0.485581616148258
-0.424355700199708 to -0.451022366270328
0.323511869127646 to 0.296845203057026
-0.359130084102258 to -0.385796750172878
-0.205300360505685 to -0.231967026576305
-0.23275476652871 to -0.259421432599331
-0.470246627514467 to -0.496913293585087
-0.513996682648763 to -0.540663348719383
Changing layer 1's weights from 
-0.50889977026473 to -0.53556643633535
0.0922689143966584 to 0.0656022483260382
-0.0334338243652435 to -0.0601004904358637
-0.0433391626525965 to -0.0700058287232167
-0.228391944949731 to -0.255058611020351
-0.366629778926477 to -0.393296444997097
-0.0959110553909385 to -0.122577721461559
-0.481714084332094 to -0.508380750402714
-0.0700511034179775 to -0.0967177694885977
-0.432697966282472 to -0.459364632353092
Changing layer 2's weights from 
0.376490265543356 to 0.349823599472736
-0.177563219850168 to -0.204229885920788
0.107026070768729 to 0.0803594046981092
0.278226167375937 to 0.251559501305317
0.100973457510367 to 0.0743067914397472
-0.105036168878183 to -0.131702834948803
0.41377654712905 to 0.38710988105843
0.341354519541159 to 0.314687853470539
-0.390643775050744 to -0.417310441121364
-0.345686256473168 to -0.372352922543789
Changing layer 3's weights from 
0.34343996685256 to 0.31677330078194
0.0798868839095984 to 0.0532202178389782
0.322496802027121 to 0.295830135956501
-0.584942429729023 to -0.611609095799643
0.199820906336203 to 0.173154240265583
0.0695723001312165 to 0.0429056340605962
-0.260612546985253 to -0.287279213055873
0.19790032547225 to 0.17123365940163
-0.133795648400888 to -0.160462314471508
0.339034229929342 to 0.312367563858722
Changing layer 4's weights from 
0.239760429079428 to 0.213093763008808
0.0309276286910925 to 0.00426096262047224
-0.169549971406564 to -0.196216637477184
0.117384761984244 to 0.0907180959136242
-0.121868043725595 to -0.148534709796215
0.10959717672576 to 0.0829305106551402
0.105238646681204 to 0.0785719806105842
-0.35665887552987 to -0.38332554160049
0.23703721206893 to 0.21037054599831
-0.195264726464853 to -0.221931392535473
Changing layer 5's weights from 
0.392540246660605 to 0.365873580589985
-0.0495332773376555 to -0.0761999434082758
0.375655085260764 to 0.348988419190144
-0.559857096110688 to -0.586523762181308
-0.0225882824112025 to -0.0492549484818228
-0.421056225483522 to -0.447722891554142
-0.121090918367013 to -0.147757584437633
-0.0486446912933435 to -0.0753113573639637
0.340957314188376 to 0.314290648117756
0.33444920223464 to 0.30778253616402
10/5/2016 1:29:42 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 10, 1, -0.16
sum 0.336430538362186 distri 0.0848639388762265
Using diff 0.167458964895413 and condRate 0.166666666666667
Changed category 1 weights from 
0.294286061115305 to 0.289820488817907
0.527601648159066 to 0.523136075861669
0.102676410265009 to 0.098210837967611
0.106769222803156 to 0.102303650505758
Changing layer 0's weights from 
0.120694106236195 to 0.116228533938798
-0.25123235045364 to -0.255697922751037
-0.485581616148258 to -0.490047188445656
-0.451022366270328 to -0.455487938567726
0.296845203057026 to 0.292379630759628
-0.385796750172878 to -0.390262322470276
-0.231967026576305 to -0.236432598873702
-0.259421432599331 to -0.263887004896728
-0.496913293585087 to -0.501378865882485
-0.540663348719383 to -0.545128921016781
Changing layer 1's weights from 
-0.53556643633535 to -0.540032008632748
0.0656022483260382 to 0.0611366760286405
-0.0601004904358637 to -0.0645660627332614
-0.0700058287232167 to -0.0744714010206144
-0.255058611020351 to -0.259524183317749
-0.393296444997097 to -0.397762017294495
-0.122577721461559 to -0.127043293758956
-0.508380750402714 to -0.512846322700112
-0.0967177694885977 to -0.101183341785995
-0.459364632353092 to -0.46383020465049
Changing layer 2's weights from 
0.349823599472736 to 0.345358027175338
-0.204229885920788 to -0.208695458218185
0.0803594046981092 to 0.0758938324007115
0.251559501305317 to 0.247093929007919
0.0743067914397472 to 0.0698412191423495
-0.131702834948803 to -0.1361684072462
0.38710988105843 to 0.382644308761032
0.314687853470539 to 0.310222281173141
-0.417310441121364 to -0.421776013418762
-0.372352922543789 to -0.376818494841186
Changing layer 3's weights from 
0.31677330078194 to 0.312307728484542
0.0532202178389782 to 0.0487546455415805
0.295830135956501 to 0.291364563659103
-0.611609095799643 to -0.61607466809704
0.173154240265583 to 0.168688667968185
0.0429056340605962 to 0.0384400617631985
-0.287279213055873 to -0.291744785353271
0.17123365940163 to 0.166768087104232
-0.160462314471508 to -0.164927886768905
0.312367563858722 to 0.307901991561324
Changing layer 4's weights from 
0.213093763008808 to 0.208628190711411
0.00426096262047224 to -0.000204609676925455
-0.196216637477184 to -0.200682209774581
0.0907180959136242 to 0.0862525236162265
-0.148534709796215 to -0.153000282093612
0.0829305106551402 to 0.0784649383577425
0.0785719806105842 to 0.0741064083131865
-0.38332554160049 to -0.387791113897888
0.21037054599831 to 0.205904973700912
-0.221931392535473 to -0.22639696483287
Changing layer 5's weights from 
0.365873580589985 to 0.361408008292588
-0.0761999434082758 to -0.0806655157056734
0.348988419190144 to 0.344522846892746
-0.586523762181308 to -0.590989334478706
-0.0492549484818228 to -0.0537205207792205
-0.447722891554142 to -0.45218846385154
-0.147757584437633 to -0.15222315673503
-0.0753113573639637 to -0.0797769296613614
0.314290648117756 to 0.309825075820358
0.30778253616402 to 0.303316963866622
Trying to learn from memory 11, 2, -0.16
sum 0.336430570277397 distri 0.310986062836712
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.764478131458818 to 0.737811465388198
1.04292009703881 to 1.01625343096819
0.955879732296527 to 0.929213066225907
0.301308027730048 to 0.274641361659427
Changing layer 0's weights from 
0.116228533938798 to 0.0895618678681773
-0.255697922751037 to -0.282364588821658
-0.490047188445656 to -0.516713854516276
-0.455487938567726 to -0.482154604638346
0.292379630759628 to 0.265712964689008
-0.390262322470276 to -0.416928988540896
-0.236432598873702 to -0.263099264944323
-0.263887004896728 to -0.290553670967348
-0.501378865882485 to -0.528045531953105
-0.545128921016781 to -0.571795587087401
Changing layer 1's weights from 
-0.540032008632748 to -0.566698674703368
0.0611366760286405 to 0.0344700099580203
-0.0645660627332614 to -0.0912327288038816
-0.0744714010206144 to -0.101138067091235
-0.259524183317749 to -0.286190849388369
-0.397762017294495 to -0.424428683365115
-0.127043293758956 to -0.153709959829577
-0.512846322700112 to -0.539512988770732
-0.101183341785995 to -0.127850007856616
-0.46383020465049 to -0.49049687072111
Changing layer 2's weights from 
0.345358027175338 to 0.318691361104718
-0.208695458218185 to -0.235362124288806
0.0758938324007115 to 0.0492271663300913
0.247093929007919 to 0.220427262937299
0.0698412191423495 to 0.0431745530717293
-0.1361684072462 to -0.162835073316821
0.382644308761032 to 0.355977642690412
0.310222281173141 to 0.283555615102521
-0.421776013418762 to -0.448442679489382
-0.376818494841186 to -0.403485160911807
Changing layer 3's weights from 
0.312307728484542 to 0.285641062413922
0.0487546455415805 to 0.0220879794709603
0.291364563659103 to 0.264697897588483
-0.61607466809704 to -0.642741334167661
0.168688667968185 to 0.142022001897565
0.0384400617631985 to 0.0117733956925783
-0.291744785353271 to -0.318411451423891
0.166768087104232 to 0.140101421033612
-0.164927886768905 to -0.191594552839526
0.307901991561324 to 0.281235325490704
Changing layer 4's weights from 
0.208628190711411 to 0.18196152464079
-0.000204609676925455 to -0.0268712757475457
-0.200682209774581 to -0.227348875845202
0.0862525236162265 to 0.0595858575456063
-0.153000282093612 to -0.179666948164233
0.0784649383577425 to 0.0517982722871223
0.0741064083131865 to 0.0474397422425663
-0.387791113897888 to -0.414457779968508
0.205904973700912 to 0.179238307630292
-0.22639696483287 to -0.253063630903491
Changing layer 5's weights from 
0.361408008292588 to 0.334741342221967
-0.0806655157056734 to -0.107332181776294
0.344522846892746 to 0.317856180822126
-0.590989334478706 to -0.617656000549326
-0.0537205207792205 to -0.0803871868498407
-0.45218846385154 to -0.47885512992216
-0.15222315673503 to -0.178889822805651
-0.0797769296613614 to -0.106443595731982
0.309825075820358 to 0.283158409749738
0.303316963866622 to 0.276650297796002
Trying to learn from memory 12, 2, -0.16
sum 0.336434264196613 distri 0.310989722099794
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.737811465388198 to 0.711144799317577
1.01625343096819 to 0.989586764897566
0.929213066225907 to 0.902546400155287
0.274641361659427 to 0.247974695588807
Changing layer 0's weights from 
0.0895618678681773 to 0.0628952017975571
-0.282364588821658 to -0.309031254892278
-0.516713854516276 to -0.543380520586896
-0.482154604638346 to -0.508821270708966
0.265712964689008 to 0.239046298618388
-0.416928988540896 to -0.443595654611516
-0.263099264944323 to -0.289765931014943
-0.290553670967348 to -0.317220337037969
-0.528045531953105 to -0.554712198023725
-0.571795587087401 to -0.598462253158021
Changing layer 1's weights from 
-0.566698674703368 to -0.593365340773988
0.0344700099580203 to 0.0078033438874001
-0.0912327288038816 to -0.117899394874502
-0.101138067091235 to -0.127804733161855
-0.286190849388369 to -0.312857515458989
-0.424428683365115 to -0.451095349435735
-0.153709959829577 to -0.180376625900197
-0.539512988770732 to -0.566179654841352
-0.127850007856616 to -0.154516673927236
-0.49049687072111 to -0.51716353679173
Changing layer 2's weights from 
0.318691361104718 to 0.292024695034098
-0.235362124288806 to -0.262028790359426
0.0492271663300913 to 0.0225605002594711
0.220427262937299 to 0.193760596866679
0.0431745530717293 to 0.0165078870011091
-0.162835073316821 to -0.189501739387441
0.355977642690412 to 0.329310976619792
0.283555615102521 to 0.256888949031901
-0.448442679489382 to -0.475109345560003
-0.403485160911807 to -0.430151826982427
Changing layer 3's weights from 
0.285641062413922 to 0.258974396343302
0.0220879794709603 to -0.0045786865996599
0.264697897588483 to 0.238031231517863
-0.642741334167661 to -0.669408000238281
0.142022001897565 to 0.115355335826945
0.0117733956925783 to -0.0148932703780419
-0.318411451423891 to -0.345078117494512
0.140101421033612 to 0.113434754962992
-0.191594552839526 to -0.218261218910146
0.281235325490704 to 0.254568659420084
Changing layer 4's weights from 
0.18196152464079 to 0.15529485857017
-0.0268712757475457 to -0.0535379418181659
-0.227348875845202 to -0.254015541915822
0.0595858575456063 to 0.0329191914749861
-0.179666948164233 to -0.206333614234853
0.0517982722871223 to 0.0251316062165021
0.0474397422425663 to 0.0207730761719461
-0.414457779968508 to -0.441124446039129
0.179238307630292 to 0.152571641559672
-0.253063630903491 to -0.279730296974111
Changing layer 5's weights from 
0.334741342221967 to 0.308074676151347
-0.107332181776294 to -0.133998847846914
0.317856180822126 to 0.291189514751506
-0.617656000549326 to -0.644322666619946
-0.0803871868498407 to -0.107053852920461
-0.47885512992216 to -0.50552179599278
-0.178889822805651 to -0.205556488876271
-0.106443595731982 to -0.133110261802602
0.283158409749738 to 0.256491743679118
0.276650297796002 to 0.249983631725382
Trying to learn from memory 13, 2, -0.16
sum 0.336433134752586 distri 0.310988378224722
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.711144799317577 to 0.684478133246957
0.989586764897566 to 0.962920098826946
0.902546400155287 to 0.875879734084666
0.247974695588807 to 0.221308029518187
Changing layer 0's weights from 
0.0628952017975571 to 0.0362285357269369
-0.309031254892278 to -0.335697920962898
-0.543380520586896 to -0.570047186657516
-0.508821270708966 to -0.535487936779586
0.239046298618388 to 0.212379632547768
-0.443595654611516 to -0.470262320682136
-0.289765931014943 to -0.316432597085563
-0.317220337037969 to -0.343887003108589
-0.554712198023725 to -0.581378864094345
-0.598462253158021 to -0.625128919228641
Changing layer 1's weights from 
-0.593365340773988 to -0.620032006844608
0.0078033438874001 to -0.0188633221832201
-0.117899394874502 to -0.144566060945122
-0.127804733161855 to -0.154471399232475
-0.312857515458989 to -0.339524181529609
-0.451095349435735 to -0.477762015506355
-0.180376625900197 to -0.207043291970817
-0.566179654841352 to -0.592846320911972
-0.154516673927236 to -0.181183339997856
-0.51716353679173 to -0.54383020286235
Changing layer 2's weights from 
0.292024695034098 to 0.265358028963478
-0.262028790359426 to -0.288695456430046
0.0225605002594711 to -0.00410616581114916
0.193760596866679 to 0.167093930796059
0.0165078870011091 to -0.0101587790695111
-0.189501739387441 to -0.216168405458061
0.329310976619792 to 0.302644310549172
0.256888949031901 to 0.230222282961281
-0.475109345560003 to -0.501776011630623
-0.430151826982427 to -0.456818493053047
Changing layer 3's weights from 
0.258974396343302 to 0.232307730272682
-0.0045786865996599 to -0.0312453526702801
0.238031231517863 to 0.211364565447243
-0.669408000238281 to -0.696074666308901
0.115355335826945 to 0.0886886697563248
-0.0148932703780419 to -0.0415599364486621
-0.345078117494512 to -0.371744783565132
0.113434754962992 to 0.0867680888923718
-0.218261218910146 to -0.244927884980766
0.254568659420084 to 0.227901993349464
Changing layer 4's weights from 
0.15529485857017 to 0.12862819249955
-0.0535379418181659 to -0.0802046078887861
-0.254015541915822 to -0.280682207986442
0.0329191914749861 to 0.00625252540436586
-0.206333614234853 to -0.233000280305473
0.0251316062165021 to -0.00153505985411814
0.0207730761719461 to -0.00589358989867413
-0.441124446039129 to -0.467791112109749
0.152571641559672 to 0.125904975489052
-0.279730296974111 to -0.306396963044731
Changing layer 5's weights from 
0.308074676151347 to 0.281408010080727
-0.133998847846914 to -0.160665513917534
0.291189514751506 to 0.264522848680886
-0.644322666619946 to -0.670989332690566
-0.107053852920461 to -0.133720518991081
-0.50552179599278 to -0.5321884620634
-0.205556488876271 to -0.232223154946891
-0.133110261802602 to -0.159776927873222
0.256491743679118 to 0.229825077608498
0.249983631725382 to 0.223316965654762
Trying to learn from memory 14, 2, -0.16
sum 0.336430538362186 distri 0.310985972778622
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.684478133246957 to 0.657811467176337
0.962920098826946 to 0.936253432756326
0.875879734084666 to 0.849213068014046
0.221308029518187 to 0.194641363447567
Changing layer 0's weights from 
0.0362285357269369 to 0.00956186965631668
-0.335697920962898 to -0.362364587033518
-0.570047186657516 to -0.596713852728136
-0.535487936779586 to -0.562154602850206
0.212379632547768 to 0.185712966477148
-0.470262320682136 to -0.496928986752757
-0.316432597085563 to -0.343099263156183
-0.343887003108589 to -0.370553669179209
-0.581378864094345 to -0.608045530164965
-0.625128919228641 to -0.651795585299261
Changing layer 1's weights from 
-0.620032006844608 to -0.646698672915228
-0.0188633221832201 to -0.0455299882538403
-0.144566060945122 to -0.171232727015742
-0.154471399232475 to -0.181138065303095
-0.339524181529609 to -0.36619084760023
-0.477762015506355 to -0.504428681576976
-0.207043291970817 to -0.233709958041437
-0.592846320911972 to -0.619512986982592
-0.181183339997856 to -0.207850006068476
-0.54383020286235 to -0.57049686893297
Changing layer 2's weights from 
0.265358028963478 to 0.238691362892858
-0.288695456430046 to -0.315362122500666
-0.00410616581114916 to -0.0307728318817694
0.167093930796059 to 0.140427264725438
-0.0101587790695111 to -0.0368254451401313
-0.216168405458061 to -0.242835071528681
0.302644310549172 to 0.275977644478552
0.230222282961281 to 0.20355561689066
-0.501776011630623 to -0.528442677701243
-0.456818493053047 to -0.483485159123667
Changing layer 3's weights from 
0.232307730272682 to 0.205641064202062
-0.0312453526702801 to -0.0579120187409003
0.211364565447243 to 0.184697899376623
-0.696074666308901 to -0.722741332379521
0.0886886697563248 to 0.0620220036857046
-0.0415599364486621 to -0.0682266025192823
-0.371744783565132 to -0.398411449635752
0.0867680888923718 to 0.0601014228217516
-0.244927884980766 to -0.271594551051386
0.227901993349464 to 0.201235327278844
Changing layer 4's weights from 
0.12862819249955 to 0.10196152642893
-0.0802046078887861 to -0.106871273959406
-0.280682207986442 to -0.307348874057062
0.00625252540436586 to -0.0204141406662544
-0.233000280305473 to -0.259666946376093
-0.00153505985411814 to -0.0282017259247384
-0.00589358989867413 to -0.0325602559692943
-0.467791112109749 to -0.494457778180369
0.125904975489052 to 0.0992383094184316
-0.306396963044731 to -0.333063629115351
Changing layer 5's weights from 
0.281408010080727 to 0.254741344010107
-0.160665513917534 to -0.187332179988154
0.264522848680886 to 0.237856182610266
-0.670989332690566 to -0.697655998761186
-0.133720518991081 to -0.160387185061701
-0.5321884620634 to -0.55885512813402
-0.232223154946891 to -0.258889821017511
-0.159776927873222 to -0.186443593943842
0.229825077608498 to 0.203158411537878
0.223316965654762 to 0.196650299584142
Trying to learn from memory 15, 0, -0.16
sum 0.336430570277397 distri -0.059419461558805
Using diff 0.311742389266853 and condRate 0.166666666666667
Changed category 0 weights from 
0.184247645042534 to 0.175934514847897
-0.215787319042091 to -0.224100449236728
-0.498308015562897 to -0.506621145757533
-0.346802649595146 to -0.355115779789782
Changing layer 0's weights from 
0.00956186965631668 to 0.00124873946168022
-0.362364587033518 to -0.370677717228155
-0.596713852728136 to -0.605026982922773
-0.562154602850206 to -0.570467733044843
0.185712966477148 to 0.177399836282511
-0.496928986752757 to -0.505242116947393
-0.343099263156183 to -0.35141239335082
-0.370553669179209 to -0.378866799373846
-0.608045530164965 to -0.616358660359602
-0.651795585299261 to -0.660108715493898
Changing layer 1's weights from 
-0.646698672915228 to -0.655011803109865
-0.0455299882538403 to -0.0538431184484768
-0.171232727015742 to -0.179545857210379
-0.181138065303095 to -0.189451195497732
-0.36619084760023 to -0.374503977794866
-0.504428681576976 to -0.512741811771612
-0.233709958041437 to -0.242023088236074
-0.619512986982592 to -0.627826117177229
-0.207850006068476 to -0.216163136263113
-0.57049686893297 to -0.578809999127607
Changing layer 2's weights from 
0.238691362892858 to 0.230378232698221
-0.315362122500666 to -0.323675252695303
-0.0307728318817694 to -0.0390859620764058
0.140427264725438 to 0.132114134530802
-0.0368254451401313 to -0.0451385753347678
-0.242835071528681 to -0.251148201723318
0.275977644478552 to 0.267664514283915
0.20355561689066 to 0.195242486696024
-0.528442677701243 to -0.536755807895879
-0.483485159123667 to -0.491798289318304
Changing layer 3's weights from 
0.205641064202062 to 0.197327934007425
-0.0579120187409003 to -0.0662251489355368
0.184697899376623 to 0.176384769181986
-0.722741332379521 to -0.731054462574158
0.0620220036857046 to 0.0537088734910681
-0.0682266025192823 to -0.0765397327139188
-0.398411449635752 to -0.406724579830388
0.0601014228217516 to 0.0517882926271151
-0.271594551051386 to -0.279907681246023
0.201235327278844 to 0.192922197084207
Changing layer 4's weights from 
0.10196152642893 to 0.0936483962342933
-0.106871273959406 to -0.115184404154043
-0.307348874057062 to -0.315662004251699
-0.0204141406662544 to -0.0287272708608908
-0.259666946376093 to -0.26798007657073
-0.0282017259247384 to -0.0365148561193748
-0.0325602559692943 to -0.0408733861639308
-0.494457778180369 to -0.502770908375005
0.0992383094184316 to 0.0909251792237952
-0.333063629115351 to -0.341376759309988
Changing layer 5's weights from 
0.254741344010107 to 0.24642821381547
-0.187332179988154 to -0.195645310182791
0.237856182610266 to 0.229543052415629
-0.697655998761186 to -0.705969128955823
-0.160387185061701 to -0.168700315256338
-0.55885512813402 to -0.567168258328657
-0.258889821017511 to -0.267202951212148
-0.186443593943842 to -0.194756724138479
0.203158411537878 to 0.194845281343241
0.196650299584142 to 0.188337169389505
10/5/2016 1:29:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:45 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
10/5/2016 1:29:45 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 16, 2, -0.2
sum 0.101624979970972 distri 0.0967426962148973
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.657811467176337 to 0.624478133346298
0.936253432756326 to 0.902920098926287
0.849213068014046 to 0.815879734184008
0.194641363447567 to 0.161308029617528
Changing layer 0's weights from 
0.00124873946168022 to -0.0320845943683585
-0.370677717228155 to -0.404011051058193
-0.605026982922773 to -0.638360316752811
-0.570467733044843 to -0.603801066874881
0.177399836282511 to 0.144066502452472
-0.505242116947393 to -0.538575450777432
-0.35141239335082 to -0.384745727180859
-0.378866799373846 to -0.412200133203884
-0.616358660359602 to -0.64969199418964
-0.660108715493898 to -0.693442049323936
Changing layer 1's weights from 
-0.655011803109865 to -0.688345136939903
-0.0538431184484768 to -0.0871764522785155
-0.179545857210379 to -0.212879191040417
-0.189451195497732 to -0.22278452932777
-0.374503977794866 to -0.407837311624905
-0.512741811771612 to -0.546075145601651
-0.242023088236074 to -0.275356422066112
-0.627826117177229 to -0.661159451007267
-0.216163136263113 to -0.249496470093151
-0.578809999127607 to -0.612143332957645
Changing layer 2's weights from 
0.230378232698221 to 0.197044898868182
-0.323675252695303 to -0.357008586525342
-0.0390859620764058 to -0.0724192959064446
0.132114134530802 to 0.0987808007007633
-0.0451385753347678 to -0.0784719091648065
-0.251148201723318 to -0.284481535553356
0.267664514283915 to 0.234331180453876
0.195242486696024 to 0.161909152865985
-0.536755807895879 to -0.570089141725918
-0.491798289318304 to -0.525131623148343
Changing layer 3's weights from 
0.197327934007425 to 0.163994600177386
-0.0662251489355368 to -0.0995584827655755
0.176384769181986 to 0.143051435351947
-0.731054462574158 to -0.764387796404196
0.0537088734910681 to 0.0203755396610294
-0.0765397327139188 to -0.109873066543957
-0.406724579830388 to -0.440057913660427
0.0517882926271151 to 0.0184549587970764
-0.279907681246023 to -0.313241015076061
0.192922197084207 to 0.159588863254168
Changing layer 4's weights from 
0.0936483962342933 to 0.0603150624042546
-0.115184404154043 to -0.148517737984081
-0.315662004251699 to -0.348995338081737
-0.0287272708608908 to -0.0620606046909295
-0.26798007657073 to -0.301313410400768
-0.0365148561193748 to -0.0698481899494135
-0.0408733861639308 to -0.0742067199939695
-0.502770908375005 to -0.536104242205044
0.0909251792237952 to 0.0575918453937565
-0.341376759309988 to -0.374710093140027
Changing layer 5's weights from 
0.24642821381547 to 0.213094879985431
-0.195645310182791 to -0.228978644012829
0.229543052415629 to 0.19620971858559
-0.705969128955823 to -0.739302462785861
-0.168700315256338 to -0.202033649086376
-0.567168258328657 to -0.600501592158695
-0.267202951212148 to -0.300536285042186
-0.194756724138479 to -0.228090057968517
0.194845281343241 to 0.161511947513202
0.188337169389505 to 0.155003835559466
10/5/2016 1:29:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:29:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:18 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
Modified index 9's learning in memoryPool to -0.2
Modified index 10's learning in memoryPool to -0.2
Modified index 11's learning in memoryPool to -0.2
Modified index 12's learning in memoryPool to -0.2
Modified index 13's learning in memoryPool to -0.2
Modified index 14's learning in memoryPool to -0.2
Modified index 15's learning in memoryPool to -0.2
Modified index 16's learning in memoryPool to -0.2
Modified index 17's learning in memoryPool to -0.2
Modified index 18's learning in memoryPool to -0.2
Modified index 19's learning in memoryPool to -0.2
Modified index 20's learning in memoryPool to -0.2
Modified index 21's learning in memoryPool to -0.2
Modified index 22's learning in memoryPool to -0.2
Modified index 23's learning in memoryPool to -0.2
Modified index 24's learning in memoryPool to -0.2
Modified index 25's learning in memoryPool to -0.2
Modified index 26's learning in memoryPool to -0.2
10/5/2016 1:30:18 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 17, 1, -0.2
sum 0.0588621414080785 distri 0.0228762385434347
Using diff 0.0212703675126242 and condRate 0.166666666666667
Changed category 1 weights from 
0.289820488817907 to 0.289111476556921
0.523136075861669 to 0.522427063600683
0.098210837967611 to 0.0975018257066251
0.102303650505758 to 0.101594638244772
Changing layer 0's weights from 
-0.0320845943683585 to -0.0327936066293444
-0.404011051058193 to -0.404720063319179
-0.638360316752811 to -0.639069329013797
-0.603801066874881 to -0.604510079135867
0.144066502452472 to 0.143357490191487
-0.538575450777432 to -0.539284463038418
-0.384745727180859 to -0.385454739441845
-0.412200133203884 to -0.41290914546487
-0.64969199418964 to -0.650401006450626
-0.693442049323936 to -0.694151061584922
Changing layer 1's weights from 
-0.688345136939903 to -0.689054149200889
-0.0871764522785155 to -0.0878854645395014
-0.212879191040417 to -0.213588203301403
-0.22278452932777 to -0.223493541588756
-0.407837311624905 to -0.408546323885891
-0.546075145601651 to -0.546784157862637
-0.275356422066112 to -0.276065434327098
-0.661159451007267 to -0.661868463268253
-0.249496470093151 to -0.250205482354137
-0.612143332957645 to -0.612852345218631
Changing layer 2's weights from 
0.197044898868182 to 0.196335886607196
-0.357008586525342 to -0.357717598786327
-0.0724192959064446 to -0.0731283081674305
0.0987808007007633 to 0.0980717884397773
-0.0784719091648065 to -0.0791809214257924
-0.284481535553356 to -0.285190547814342
0.234331180453876 to 0.23362216819289
0.161909152865985 to 0.161200140604999
-0.570089141725918 to -0.570798153986904
-0.525131623148343 to -0.525840635409328
Changing layer 3's weights from 
0.163994600177386 to 0.1632855879164
-0.0995584827655755 to -0.100267495026561
0.143051435351947 to 0.142342423090961
-0.764387796404196 to -0.765096808665182
0.0203755396610294 to 0.0196665274000435
-0.109873066543957 to -0.110582078804943
-0.440057913660427 to -0.440766925921413
0.0184549587970764 to 0.0177459465360905
-0.313241015076061 to -0.313950027337047
0.159588863254168 to 0.158879850993182
Changing layer 4's weights from 
0.0603150624042546 to 0.0596060501432687
-0.148517737984081 to -0.149226750245067
-0.348995338081737 to -0.349704350342723
-0.0620606046909295 to -0.0627696169519154
-0.301313410400768 to -0.302022422661754
-0.0698481899494135 to -0.0705572022103994
-0.0742067199939695 to -0.0749157322549554
-0.536104242205044 to -0.53681325446603
0.0575918453937565 to 0.0568828331327706
-0.374710093140027 to -0.375419105401012
Changing layer 5's weights from 
0.213094879985431 to 0.212385867724445
-0.228978644012829 to -0.229687656273815
0.19620971858559 to 0.195500706324604
-0.739302462785861 to -0.740011475046847
-0.202033649086376 to -0.202742661347362
-0.600501592158695 to -0.601210604419681
-0.300536285042186 to -0.301245297303172
-0.228090057968517 to -0.228799070229503
0.161511947513202 to 0.160802935252216
0.155003835559466 to 0.15429482329848
Trying to learn from memory 18, 0, -0.2
sum 0.0588621414080785 distri -0.0205551931663522
Using diff 0.064701799222411 and condRate 0.166666666666667
Changed category 0 weights from 
0.175934514847897 to 0.173777788175013
-0.224100449236728 to -0.226257175909612
-0.506621145757533 to -0.508777872430418
-0.355115779789782 to -0.357272506462667
Changing layer 0's weights from 
-0.0327936066293444 to -0.0349503333022292
-0.404720063319179 to -0.406876789992064
-0.639069329013797 to -0.641226055686682
-0.604510079135867 to -0.606666805808752
0.143357490191487 to 0.141200763518602
-0.539284463038418 to -0.541441189711303
-0.385454739441845 to -0.387611466114729
-0.41290914546487 to -0.415065872137755
-0.650401006450626 to -0.652557733123511
-0.694151061584922 to -0.696307788257807
Changing layer 1's weights from 
-0.689054149200889 to -0.691210875873774
-0.0878854645395014 to -0.0900421912123862
-0.213588203301403 to -0.215744929974288
-0.223493541588756 to -0.225650268261641
-0.408546323885891 to -0.410703050558776
-0.546784157862637 to -0.548940884535521
-0.276065434327098 to -0.278222160999983
-0.661868463268253 to -0.664025189941138
-0.250205482354137 to -0.252362209027022
-0.612852345218631 to -0.615009071891516
Changing layer 2's weights from 
0.196335886607196 to 0.194179159934312
-0.357717598786327 to -0.359874325459212
-0.0731283081674305 to -0.0752850348403152
0.0980717884397773 to 0.0959150617668926
-0.0791809214257924 to -0.0813376480986772
-0.285190547814342 to -0.287347274487227
0.23362216819289 to 0.231465441520006
0.161200140604999 to 0.159043413932115
-0.570798153986904 to -0.572954880659789
-0.525840635409328 to -0.527997362082213
Changing layer 3's weights from 
0.1632855879164 to 0.161128861243516
-0.100267495026561 to -0.102424221699446
0.142342423090961 to 0.140185696418077
-0.765096808665182 to -0.767253535338067
0.0196665274000435 to 0.0175098007271587
-0.110582078804943 to -0.112738805477828
-0.440766925921413 to -0.442923652594298
0.0177459465360905 to 0.0155892198632057
-0.313950027337047 to -0.316106754009932
0.158879850993182 to 0.156723124320298
Changing layer 4's weights from 
0.0596060501432687 to 0.0574493234703839
-0.149226750245067 to -0.151383476917952
-0.349704350342723 to -0.351861077015608
-0.0627696169519154 to -0.0649263436248002
-0.302022422661754 to -0.304179149334639
-0.0705572022103994 to -0.0727139288832842
-0.0749157322549554 to -0.0770724589278402
-0.53681325446603 to -0.538969981138915
0.0568828331327706 to 0.0547261064598858
-0.375419105401012 to -0.377575832073897
Changing layer 5's weights from 
0.212385867724445 to 0.210229141051561
-0.229687656273815 to -0.2318443829467
0.195500706324604 to 0.19334397965172
-0.740011475046847 to -0.742168201719732
-0.202742661347362 to -0.204899388020247
-0.601210604419681 to -0.603367331092566
-0.301245297303172 to -0.303402023976057
-0.228799070229503 to -0.230955796902388
0.160802935252216 to 0.158646208579332
0.15429482329848 to 0.152138096625596
Trying to learn from memory 19, 0, -0.2
sum 0.0588621526736813 distri -0.0205551039664935
Using diff 0.0647017184717544 and condRate 0.166666666666667
Changed category 0 weights from 
0.173777788175013 to 0.171621064193817
-0.226257175909612 to -0.228413899890808
-0.508777872430418 to -0.510934596411614
-0.357272506462667 to -0.359429230443863
Changing layer 0's weights from 
-0.0349503333022292 to -0.0371070572834253
-0.406876789992064 to -0.40903351397326
-0.641226055686682 to -0.643382779667878
-0.606666805808752 to -0.608823529789948
0.141200763518602 to 0.139044039537406
-0.541441189711303 to -0.543597913692499
-0.387611466114729 to -0.389768190095925
-0.415065872137755 to -0.417222596118951
-0.652557733123511 to -0.654714457104707
-0.696307788257807 to -0.698464512239003
Changing layer 1's weights from 
-0.691210875873774 to -0.69336759985497
-0.0900421912123862 to -0.0921989151935823
-0.215744929974288 to -0.217901653955484
-0.225650268261641 to -0.227806992242837
-0.410703050558776 to -0.412859774539972
-0.548940884535521 to -0.551097608516718
-0.278222160999983 to -0.280378884981179
-0.664025189941138 to -0.666181913922334
-0.252362209027022 to -0.254518933008218
-0.615009071891516 to -0.617165795872712
Changing layer 2's weights from 
0.194179159934312 to 0.192022435953116
-0.359874325459212 to -0.362031049440408
-0.0752850348403152 to -0.0774417588215114
0.0959150617668926 to 0.0937583377856964
-0.0813376480986772 to -0.0834943720798733
-0.287347274487227 to -0.289503998468423
0.231465441520006 to 0.229308717538809
0.159043413932115 to 0.156886689950918
-0.572954880659789 to -0.575111604640985
-0.527997362082213 to -0.530154086063409
Changing layer 3's weights from 
0.161128861243516 to 0.158972137262319
-0.102424221699446 to -0.104580945680642
0.140185696418077 to 0.138028972436881
-0.767253535338067 to -0.769410259319263
0.0175098007271587 to 0.0153530767459625
-0.112738805477828 to -0.114895529459024
-0.442923652594298 to -0.445080376575494
0.0155892198632057 to 0.0134324958820096
-0.316106754009932 to -0.318263477991128
0.156723124320298 to 0.154566400339101
Changing layer 4's weights from 
0.0574493234703839 to 0.0552925994891877
-0.151383476917952 to -0.153540200899148
-0.351861077015608 to -0.354017800996804
-0.0649263436248002 to -0.0670830676059964
-0.304179149334639 to -0.306335873315835
-0.0727139288832842 to -0.0748706528644804
-0.0770724589278402 to -0.0792291829090364
-0.538969981138915 to -0.541126705120111
0.0547261064598858 to 0.0525693824786896
-0.377575832073897 to -0.379732556055093
Changing layer 5's weights from 
0.210229141051561 to 0.208072417070365
-0.2318443829467 to -0.234001106927896
0.19334397965172 to 0.191187255670524
-0.742168201719732 to -0.744324925700928
-0.204899388020247 to -0.207056112001443
-0.603367331092566 to -0.605524055073762
-0.303402023976057 to -0.305558747957253
-0.230955796902388 to -0.233112520883584
0.158646208579332 to 0.156489484598135
0.152138096625596 to 0.149981372644399
Trying to learn from memory 20, 0, -0.2
sum 0.0588602933603027 distri -0.0205558470099923
Using diff 0.0647010670302194 and condRate 0.166666666666667
Changed category 0 weights from 
0.171621064193817 to 0.169464361927339
-0.228413899890808 to -0.230570602157286
-0.510934596411614 to -0.513091298678092
-0.359429230443863 to -0.361585932710341
Changing layer 0's weights from 
-0.0371070572834253 to -0.0392637595499034
-0.40903351397326 to -0.411190216239738
-0.643382779667878 to -0.645539481934356
-0.608823529789948 to -0.610980232056426
0.139044039537406 to 0.136887337270928
-0.543597913692499 to -0.545754615958977
-0.389768190095925 to -0.391924892362403
-0.417222596118951 to -0.419379298385429
-0.654714457104707 to -0.656871159371185
-0.698464512239003 to -0.700621214505481
Changing layer 1's weights from 
-0.69336759985497 to -0.695524302121448
-0.0921989151935823 to -0.0943556174600604
-0.217901653955484 to -0.220058356221962
-0.227806992242837 to -0.229963694509315
-0.412859774539972 to -0.41501647680645
-0.551097608516718 to -0.553254310783196
-0.280378884981179 to -0.282535587247657
-0.666181913922334 to -0.668338616188812
-0.254518933008218 to -0.256675635274696
-0.617165795872712 to -0.61932249813919
Changing layer 2's weights from 
0.192022435953116 to 0.189865733686638
-0.362031049440408 to -0.364187751706886
-0.0774417588215114 to -0.0795984610879894
0.0937583377856964 to 0.0916016355192184
-0.0834943720798733 to -0.0856510743463514
-0.289503998468423 to -0.291660700734901
0.229308717538809 to 0.227152015272331
0.156886689950918 to 0.15472998768444
-0.575111604640985 to -0.577268306907463
-0.530154086063409 to -0.532310788329887
Changing layer 3's weights from 
0.158972137262319 to 0.156815434995841
-0.104580945680642 to -0.10673764794712
0.138028972436881 to 0.135872270170402
-0.769410259319263 to -0.771566961585741
0.0153530767459625 to 0.0131963744794845
-0.114895529459024 to -0.117052231725502
-0.445080376575494 to -0.447237078841972
0.0134324958820096 to 0.0112757936155316
-0.318263477991128 to -0.320420180257606
0.154566400339101 to 0.152409698072623
Changing layer 4's weights from 
0.0552925994891877 to 0.0531358972227097
-0.153540200899148 to -0.155696903165626
-0.354017800996804 to -0.356174503263282
-0.0670830676059964 to -0.0692397698724744
-0.306335873315835 to -0.308492575582313
-0.0748706528644804 to -0.0770273551309584
-0.0792291829090364 to -0.0813858851755144
-0.541126705120111 to -0.543283407386589
0.0525693824786896 to 0.0504126802122116
-0.379732556055093 to -0.381889258321571
Changing layer 5's weights from 
0.208072417070365 to 0.205915714803887
-0.234001106927896 to -0.236157809194374
0.191187255670524 to 0.189030553404046
-0.744324925700928 to -0.746481627967406
-0.207056112001443 to -0.209212814267921
-0.605524055073762 to -0.60768075734024
-0.305558747957253 to -0.307715450223731
-0.233112520883584 to -0.235269223150062
0.156489484598135 to 0.154332782331657
0.149981372644399 to 0.147824670377921
Trying to learn from memory 21, 0, -0.2
sum 0.0588609867431918 distri -0.0205558234859022
Using diff 0.0647015635432961 and condRate 0.166666666666667
Changed category 0 weights from 
0.169464361927339 to 0.167307643110424
-0.230570602157286 to -0.232727320974201
-0.513091298678092 to -0.515248017495007
-0.361585932710341 to -0.363742651527256
Changing layer 0's weights from 
-0.0392637595499034 to -0.0414204783668175
-0.411190216239738 to -0.413346935056652
-0.645539481934356 to -0.647696200751271
-0.610980232056426 to -0.613136950873341
0.136887337270928 to 0.134730618454013
-0.545754615958977 to -0.547911334775891
-0.391924892362403 to -0.394081611179318
-0.419379298385429 to -0.421536017202343
-0.656871159371185 to -0.659027878188099
-0.700621214505481 to -0.702777933322395
Changing layer 1's weights from 
-0.695524302121448 to -0.697681020938362
-0.0943556174600604 to -0.0965123362769745
-0.220058356221962 to -0.222215075038876
-0.229963694509315 to -0.232120413326229
-0.41501647680645 to -0.417173195623364
-0.553254310783196 to -0.55541102960011
-0.282535587247657 to -0.284692306064571
-0.668338616188812 to -0.670495335005727
-0.256675635274696 to -0.25883235409161
-0.61932249813919 to -0.621479216956105
Changing layer 2's weights from 
0.189865733686638 to 0.187709014869723
-0.364187751706886 to -0.3663444705238
-0.0795984610879894 to -0.0817551799049036
0.0916016355192184 to 0.0894449167023043
-0.0856510743463514 to -0.0878077931632655
-0.291660700734901 to -0.293817419551815
0.227152015272331 to 0.224995296455417
0.15472998768444 to 0.152573268867526
-0.577268306907463 to -0.579425025724377
-0.532310788329887 to -0.534467507146802
Changing layer 3's weights from 
0.156815434995841 to 0.154658716178927
-0.10673764794712 to -0.108894366764034
0.135872270170402 to 0.133715551353488
-0.771566961585741 to -0.773723680402655
0.0131963744794845 to 0.0110396556625704
-0.117052231725502 to -0.119208950542416
-0.447237078841972 to -0.449393797658886
0.0112757936155316 to 0.00911907479861741
-0.320420180257606 to -0.32257689907452
0.152409698072623 to 0.150252979255709
Changing layer 4's weights from 
0.0531358972227097 to 0.0509791784057956
-0.155696903165626 to -0.157853621982541
-0.356174503263282 to -0.358331222080196
-0.0692397698724744 to -0.0713964886893885
-0.308492575582313 to -0.310649294399227
-0.0770273551309584 to -0.0791840739478725
-0.0813858851755144 to -0.0835426039924285
-0.543283407386589 to -0.545440126203503
0.0504126802122116 to 0.0482559613952975
-0.381889258321571 to -0.384045977138486
Changing layer 5's weights from 
0.205915714803887 to 0.203758995986972
-0.236157809194374 to -0.238314528011289
0.189030553404046 to 0.186873834587131
-0.746481627967406 to -0.74863834678432
-0.209212814267921 to -0.211369533084836
-0.60768075734024 to -0.609837476157155
-0.307715450223731 to -0.309872169040645
-0.235269223150062 to -0.237425941966976
0.154332782331657 to 0.152176063514743
0.147824670377921 to 0.145667951561007
Trying to learn from memory 22, 2, -0.2
sum 0.0588673727293089 distri 0.0565488771741856
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.624478133346298 to 0.59114479951626
0.902920098926287 to 0.869586765096249
0.815879734184008 to 0.782546400353969
0.161308029617528 to 0.127974695787489
Changing layer 0's weights from 
-0.0414204783668175 to -0.0747538121968562
-0.413346935056652 to -0.446680268886691
-0.647696200751271 to -0.681029534581309
-0.613136950873341 to -0.646470284703379
0.134730618454013 to 0.101397284623975
-0.547911334775891 to -0.58124466860593
-0.394081611179318 to -0.427414945009356
-0.421536017202343 to -0.454869351032382
-0.659027878188099 to -0.692361212018138
-0.702777933322395 to -0.736111267152434
Changing layer 1's weights from 
-0.697681020938362 to -0.731014354768401
-0.0965123362769745 to -0.129845670107013
-0.222215075038876 to -0.255548408868915
-0.232120413326229 to -0.265453747156268
-0.417173195623364 to -0.450506529453403
-0.55541102960011 to -0.588744363430148
-0.284692306064571 to -0.31802563989461
-0.670495335005727 to -0.703828668835765
-0.25883235409161 to -0.292165687921649
-0.621479216956105 to -0.654812550786143
Changing layer 2's weights from 
0.187709014869723 to 0.154375681039685
-0.3663444705238 to -0.399677804353839
-0.0817551799049036 to -0.115088513734942
0.0894449167023043 to 0.0561115828722656
-0.0878077931632655 to -0.121141126993304
-0.293817419551815 to -0.327150753381854
0.224995296455417 to 0.191661962625379
0.152573268867526 to 0.119239935037488
-0.579425025724377 to -0.612758359554416
-0.534467507146802 to -0.56780084097684
Changing layer 3's weights from 
0.154658716178927 to 0.121325382348889
-0.108894366764034 to -0.142227700594073
0.133715551353488 to 0.10038221752345
-0.773723680402655 to -0.807057014232694
0.0110396556625704 to -0.0222936781674683
-0.119208950542416 to -0.152542284372455
-0.449393797658886 to -0.482727131488925
0.00911907479861741 to -0.0242142590314213
-0.32257689907452 to -0.355910232904559
0.150252979255709 to 0.116919645425671
Changing layer 4's weights from 
0.0509791784057956 to 0.0176458445757569
-0.157853621982541 to -0.191186955812579
-0.358331222080196 to -0.391664555910235
-0.0713964886893885 to -0.104729822519427
-0.310649294399227 to -0.343982628229266
-0.0791840739478725 to -0.112517407777911
-0.0835426039924285 to -0.116875937822467
-0.545440126203503 to -0.578773460033542
0.0482559613952975 to 0.0149226275652588
-0.384045977138486 to -0.417379310968524
Changing layer 5's weights from 
0.203758995986972 to 0.170425662156934
-0.238314528011289 to -0.271647861841327
0.186873834587131 to 0.153540500757093
-0.74863834678432 to -0.781971680614359
-0.211369533084836 to -0.244702866914874
-0.609837476157155 to -0.643170809987193
-0.309872169040645 to -0.343205502870684
-0.237425941966976 to -0.270759275797015
0.152176063514743 to 0.118842729684705
0.145667951561007 to 0.112334617730969
Trying to learn from memory 23, 0, -0.2
sum 0.0588673747541356 distri -0.0205603650913449
Using diff 0.0647108961569465 and condRate 0.166666666666667
Changed category 0 weights from 
0.167307643110424 to 0.165150613206384
-0.232727320974201 to -0.234884350878241
-0.515248017495007 to -0.517405047399047
-0.363742651527256 to -0.365899681431296
Changing layer 0's weights from 
-0.0747538121968562 to -0.0769108421008967
-0.446680268886691 to -0.448837298790732
-0.681029534581309 to -0.68318656448535
-0.646470284703379 to -0.64862731460742
0.101397284623975 to 0.0992402547199342
-0.58124466860593 to -0.58340169850997
-0.427414945009356 to -0.429571974913397
-0.454869351032382 to -0.457026380936423
-0.692361212018138 to -0.694518241922179
-0.736111267152434 to -0.738268297056475
Changing layer 1's weights from 
-0.731014354768401 to -0.733171384672442
-0.129845670107013 to -0.132002700011054
-0.255548408868915 to -0.257705438772956
-0.265453747156268 to -0.267610777060309
-0.450506529453403 to -0.452663559357443
-0.588744363430148 to -0.590901393334189
-0.31802563989461 to -0.320182669798651
-0.703828668835765 to -0.705985698739806
-0.292165687921649 to -0.29432271782569
-0.654812550786143 to -0.656969580690184
Changing layer 2's weights from 
0.154375681039685 to 0.152218651135644
-0.399677804353839 to -0.40183483425788
-0.115088513734942 to -0.117245543638983
0.0561115828722656 to 0.0539545529682251
-0.121141126993304 to -0.123298156897345
-0.327150753381854 to -0.329307783285895
0.191661962625379 to 0.189504932721338
0.119239935037488 to 0.117082905133447
-0.612758359554416 to -0.614915389458456
-0.56780084097684 to -0.569957870880881
Changing layer 3's weights from 
0.121325382348889 to 0.119168352444848
-0.142227700594073 to -0.144384730498114
0.10038221752345 to 0.0982251876194092
-0.807057014232694 to -0.809214044136734
-0.0222936781674683 to -0.0244507080715088
-0.152542284372455 to -0.154699314276496
-0.482727131488925 to -0.484884161392965
-0.0242142590314213 to -0.0263712889354618
-0.355910232904559 to -0.3580672628086
0.116919645425671 to 0.11476261552163
Changing layer 4's weights from 
0.0176458445757569 to 0.0154888146717164
-0.191186955812579 to -0.19334398571662
-0.391664555910235 to -0.393821585814276
-0.104729822519427 to -0.106886852423468
-0.343982628229266 to -0.346139658133307
-0.112517407777911 to -0.114674437681952
-0.116875937822467 to -0.119032967726508
-0.578773460033542 to -0.580930489937582
0.0149226275652588 to 0.0127655976612183
-0.417379310968524 to -0.419536340872565
Changing layer 5's weights from 
0.170425662156934 to 0.168268632252893
-0.271647861841327 to -0.273804891745368
0.153540500757093 to 0.151383470853052
-0.781971680614359 to -0.7841287105184
-0.244702866914874 to -0.246859896818915
-0.643170809987193 to -0.645327839891234
-0.343205502870684 to -0.345362532774725
-0.270759275797015 to -0.272916305701056
0.118842729684705 to 0.116685699780664
0.112334617730969 to 0.110177587826928
Trying to learn from memory 24, 0, -0.2
sum 0.0588673975039243 distri -0.0205603777368317
Using diff 0.0647109258647749 and condRate 0.166666666666667
Changed category 0 weights from 
0.165150613206384 to 0.162993582312082
-0.234884350878241 to -0.237041381772543
-0.517405047399047 to -0.519562078293348
-0.365899681431296 to -0.368056712325597
Changing layer 0's weights from 
-0.0769108421008967 to -0.0790678729951981
-0.448837298790732 to -0.450994329685033
-0.68318656448535 to -0.685343595379651
-0.64862731460742 to -0.650784345501721
0.0992402547199342 to 0.0970832238256328
-0.58340169850997 to -0.585558729404271
-0.429571974913397 to -0.431729005807698
-0.457026380936423 to -0.459183411830724
-0.694518241922179 to -0.69667527281648
-0.738268297056475 to -0.740425327950776
Changing layer 1's weights from 
-0.733171384672442 to -0.735328415566743
-0.132002700011054 to -0.134159730905355
-0.257705438772956 to -0.259862469667257
-0.267610777060309 to -0.26976780795461
-0.452663559357443 to -0.454820590251744
-0.590901393334189 to -0.59305842422849
-0.320182669798651 to -0.322339700692952
-0.705985698739806 to -0.708142729634107
-0.29432271782569 to -0.296479748719991
-0.656969580690184 to -0.659126611584485
Changing layer 2's weights from 
0.152218651135644 to 0.150061620241343
-0.40183483425788 to -0.403991865152181
-0.117245543638983 to -0.119402574533284
0.0539545529682251 to 0.0517975220739237
-0.123298156897345 to -0.125455187791646
-0.329307783285895 to -0.331464814180196
0.189504932721338 to 0.187347901827037
0.117082905133447 to 0.114925874239146
-0.614915389458456 to -0.617072420352758
-0.569957870880881 to -0.572114901775182
Changing layer 3's weights from 
0.119168352444848 to 0.117011321550547
-0.144384730498114 to -0.146541761392415
0.0982251876194092 to 0.0960681567251077
-0.809214044136734 to -0.811371075031036
-0.0244507080715088 to -0.0266077389658102
-0.154699314276496 to -0.156856345170797
-0.484884161392965 to -0.487041192287267
-0.0263712889354618 to -0.0285283198297632
-0.3580672628086 to -0.360224293702901
0.11476261552163 to 0.112605584627329
Changing layer 4's weights from 
0.0154888146717164 to 0.013331783777415
-0.19334398571662 to -0.195501016610921
-0.393821585814276 to -0.395978616708577
-0.106886852423468 to -0.109043883317769
-0.346139658133307 to -0.348296689027608
-0.114674437681952 to -0.116831468576253
-0.119032967726508 to -0.121189998620809
-0.580930489937582 to -0.583087520831884
0.0127655976612183 to 0.0106085667669169
-0.419536340872565 to -0.421693371766866
Changing layer 5's weights from 
0.168268632252893 to 0.166111601358592
-0.273804891745368 to -0.275961922639669
0.151383470853052 to 0.149226439958751
-0.7841287105184 to -0.786285741412701
-0.246859896818915 to -0.249016927713216
-0.645327839891234 to -0.647484870785535
-0.345362532774725 to -0.347519563669026
-0.272916305701056 to -0.275073336595357
0.116685699780664 to 0.114528668886363
0.110177587826928 to 0.108020556932627
Trying to learn from memory 25, 1, -0.2
sum 0.0588674373567447 distri 0.0228788835459744
Using diff 0.0212716944715842 and condRate 0.166666666666667
Changed category 1 weights from 
0.289111476556921 to 0.288402420063969
0.522427063600683 to 0.521718007107731
0.0975018257066251 to 0.0967927692136732
0.101594638244772 to 0.10088558175182
Changing layer 0's weights from 
-0.0790678729951981 to -0.07977692948815
-0.450994329685033 to -0.451703386177985
-0.685343595379651 to -0.686052651872603
-0.650784345501721 to -0.651493401994673
0.0970832238256328 to 0.0963741673326809
-0.585558729404271 to -0.586267785897223
-0.431729005807698 to -0.43243806230065
-0.459183411830724 to -0.459892468323676
-0.69667527281648 to -0.697384329309432
-0.740425327950776 to -0.741134384443728
Changing layer 1's weights from 
-0.735328415566743 to -0.736037472059695
-0.134159730905355 to -0.134868787398307
-0.259862469667257 to -0.260571526160209
-0.26976780795461 to -0.270476864447562
-0.454820590251744 to -0.455529646744696
-0.59305842422849 to -0.593767480721442
-0.322339700692952 to -0.323048757185904
-0.708142729634107 to -0.708851786127059
-0.296479748719991 to -0.297188805212943
-0.659126611584485 to -0.659835668077437
Changing layer 2's weights from 
0.150061620241343 to 0.149352563748391
-0.403991865152181 to -0.404700921645133
-0.119402574533284 to -0.120111631026236
0.0517975220739237 to 0.0510884655809718
-0.125455187791646 to -0.126164244284598
-0.331464814180196 to -0.332173870673148
0.187347901827037 to 0.186638845334085
0.114925874239146 to 0.114216817746194
-0.617072420352758 to -0.61778147684571
-0.572114901775182 to -0.572823958268134
Changing layer 3's weights from 
0.117011321550547 to 0.116302265057595
-0.146541761392415 to -0.147250817885367
0.0960681567251077 to 0.0953591002321558
-0.811371075031036 to -0.812080131523988
-0.0266077389658102 to -0.0273167954587621
-0.156856345170797 to -0.157565401663749
-0.487041192287267 to -0.487750248780219
-0.0285283198297632 to -0.0292373763227151
-0.360224293702901 to -0.360933350195853
0.112605584627329 to 0.111896528134377
Changing layer 4's weights from 
0.013331783777415 to 0.0126227272844631
-0.195501016610921 to -0.196210073103873
-0.395978616708577 to -0.396687673201529
-0.109043883317769 to -0.109752939810721
-0.348296689027608 to -0.34900574552056
-0.116831468576253 to -0.117540525069205
-0.121189998620809 to -0.121899055113761
-0.583087520831884 to -0.583796577324836
0.0106085667669169 to 0.00989951027396496
-0.421693371766866 to -0.422402428259818
Changing layer 5's weights from 
0.166111601358592 to 0.16540254486564
-0.275961922639669 to -0.276670979132621
0.149226439958751 to 0.148517383465799
-0.786285741412701 to -0.786994797905653
-0.249016927713216 to -0.249725984206168
-0.647484870785535 to -0.648193927278487
-0.347519563669026 to -0.348228620161978
-0.275073336595357 to -0.275782393088309
0.114528668886363 to 0.113819612393411
0.108020556932627 to 0.107311500439675
Trying to learn from memory 25, 1, -0.2
sum 0.0588674373567447 distri 0.0228788835459744
Using diff 0.0212716944715842 and condRate 0.166666666666667
Changed category 1 weights from 
0.288402420063969 to 0.287693363571017
0.521718007107731 to 0.521008950614779
0.0967927692136732 to 0.0960837127207213
0.10088558175182 to 0.100176525258868
Changing layer 0's weights from 
-0.07977692948815 to -0.0804859859811019
-0.451703386177985 to -0.452412442670937
-0.686052651872603 to -0.686761708365555
-0.651493401994673 to -0.652202458487625
0.0963741673326809 to 0.095665110839729
-0.586267785897223 to -0.586976842390175
-0.43243806230065 to -0.433147118793602
-0.459892468323676 to -0.460601524816628
-0.697384329309432 to -0.698093385802384
-0.741134384443728 to -0.74184344093668
Changing layer 1's weights from 
-0.736037472059695 to -0.736746528552647
-0.134868787398307 to -0.135577843891259
-0.260571526160209 to -0.261280582653161
-0.270476864447562 to -0.271185920940514
-0.455529646744696 to -0.456238703237648
-0.593767480721442 to -0.594476537214394
-0.323048757185904 to -0.323757813678856
-0.708851786127059 to -0.709560842620011
-0.297188805212943 to -0.297897861705895
-0.659835668077437 to -0.660544724570389
Changing layer 2's weights from 
0.149352563748391 to 0.148643507255439
-0.404700921645133 to -0.405409978138085
-0.120111631026236 to -0.120820687519188
0.0510884655809718 to 0.0503794090880199
-0.126164244284598 to -0.12687330077755
-0.332173870673148 to -0.3328829271661
0.186638845334085 to 0.185929788841133
0.114216817746194 to 0.113507761253242
-0.61778147684571 to -0.618490533338661
-0.572823958268134 to -0.573533014761086
Changing layer 3's weights from 
0.116302265057595 to 0.115593208564643
-0.147250817885367 to -0.147959874378319
0.0953591002321558 to 0.0946500437392039
-0.812080131523988 to -0.81278918801694
-0.0273167954587621 to -0.028025851951714
-0.157565401663749 to -0.158274458156701
-0.487750248780219 to -0.488459305273171
-0.0292373763227151 to -0.029946432815667
-0.360933350195853 to -0.361642406688805
0.111896528134377 to 0.111187471641425
Changing layer 4's weights from 
0.0126227272844631 to 0.0119136707915112
-0.196210073103873 to -0.196919129596825
-0.396687673201529 to -0.397396729694481
-0.109752939810721 to -0.110461996303673
-0.34900574552056 to -0.349714802013512
-0.117540525069205 to -0.118249581562157
-0.121899055113761 to -0.122608111606713
-0.583796577324836 to -0.584505633817788
0.00989951027396496 to 0.00919045378101305
-0.422402428259818 to -0.42311148475277
Changing layer 5's weights from 
0.16540254486564 to 0.164693488372688
-0.276670979132621 to -0.277380035625573
0.148517383465799 to 0.147808326972847
-0.786994797905653 to -0.787703854398605
-0.249725984206168 to -0.25043504069912
-0.648193927278487 to -0.648902983771439
-0.348228620161978 to -0.34893767665493
-0.275782393088309 to -0.276491449581261
0.113819612393411 to 0.113110555900459
0.107311500439675 to 0.106602443946723
10/5/2016 1:30:18 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 25, 0, -0.2
sum 0.0588674373567447 distri -0.0205603829401262
Using diff 0.0647109609576848 and condRate 0.166666666666667
Changed category 0 weights from 
0.162993582312082 to 0.160836550248017
-0.237041381772543 to -0.239198413836608
-0.519562078293348 to -0.521719110357414
-0.368056712325597 to -0.370213744389663
Changing layer 0's weights from 
-0.0804859859811019 to -0.082643018045167
-0.452412442670937 to -0.454569474735002
-0.686761708365555 to -0.68891874042962
-0.652202458487625 to -0.65435949055169
0.095665110839729 to 0.0935080787756639
-0.586976842390175 to -0.58913387445424
-0.433147118793602 to -0.435304150857667
-0.460601524816628 to -0.462758556880693
-0.698093385802384 to -0.700250417866449
-0.74184344093668 to -0.744000473000745
Changing layer 1's weights from 
-0.736746528552647 to -0.738903560616712
-0.135577843891259 to -0.137734875955324
-0.261280582653161 to -0.263437614717226
-0.271185920940514 to -0.273342953004579
-0.456238703237648 to -0.458395735301713
-0.594476537214394 to -0.596633569278459
-0.323757813678856 to -0.325914845742921
-0.709560842620011 to -0.711717874684076
-0.297897861705895 to -0.30005489376996
-0.660544724570389 to -0.662701756634454
Changing layer 2's weights from 
0.148643507255439 to 0.146486475191374
-0.405409978138085 to -0.40756701020215
-0.120820687519188 to -0.122977719583253
0.0503794090880199 to 0.0482223770239548
-0.12687330077755 to -0.129030332841615
-0.3328829271661 to -0.335039959230165
0.185929788841133 to 0.183772756777068
0.113507761253242 to 0.111350729189177
-0.618490533338661 to -0.620647565402727
-0.573533014761086 to -0.575690046825151
Changing layer 3's weights from 
0.115593208564643 to 0.113436176500578
-0.147959874378319 to -0.150116906442384
0.0946500437392039 to 0.0924930116751388
-0.81278918801694 to -0.814946220081005
-0.028025851951714 to -0.0301828840157791
-0.158274458156701 to -0.160431490220766
-0.488459305273171 to -0.490616337337236
-0.029946432815667 to -0.0321034648797321
-0.361642406688805 to -0.36379943875287
0.111187471641425 to 0.10903043957736
Changing layer 4's weights from 
0.0119136707915112 to 0.00975663872744605
-0.196919129596825 to -0.19907616166089
-0.397396729694481 to -0.399553761758546
-0.110461996303673 to -0.112619028367738
-0.349714802013512 to -0.351871834077577
-0.118249581562157 to -0.120406613626222
-0.122608111606713 to -0.124765143670778
-0.584505633817788 to -0.586662665881853
0.00919045378101305 to 0.00703342171694795
-0.42311148475277 to -0.425268516816835
Changing layer 5's weights from 
0.164693488372688 to 0.162536456308623
-0.277380035625573 to -0.279537067689638
0.147808326972847 to 0.145651294908782
-0.787703854398605 to -0.78986088646267
-0.25043504069912 to -0.252592072763185
-0.648902983771439 to -0.651060015835504
-0.34893767665493 to -0.351094708718995
-0.276491449581261 to -0.278648481645326
0.113110555900459 to 0.110953523836394
0.106602443946723 to 0.104445411882658
Trying to learn from memory 25, 0, -0.2
sum 0.0588674373567447 distri -0.0205603829401262
Using diff 0.0647109609576848 and condRate 0.166666666666667
Changed category 0 weights from 
0.160836550248017 to 0.158679518183952
-0.239198413836608 to -0.241355445900673
-0.521719110357414 to -0.523876142421479
-0.370213744389663 to -0.372370776453728
Changing layer 0's weights from 
-0.082643018045167 to -0.0848000501092321
-0.454569474735002 to -0.456726506799067
-0.68891874042962 to -0.691075772493685
-0.65435949055169 to -0.656516522615755
0.0935080787756639 to 0.0913510467115988
-0.58913387445424 to -0.591290906518306
-0.435304150857667 to -0.437461182921732
-0.462758556880693 to -0.464915588944758
-0.700250417866449 to -0.702407449930514
-0.744000473000745 to -0.74615750506481
Changing layer 1's weights from 
-0.738903560616712 to -0.741060592680777
-0.137734875955324 to -0.139891908019389
-0.263437614717226 to -0.265594646781291
-0.273342953004579 to -0.275499985068644
-0.458395735301713 to -0.460552767365778
-0.596633569278459 to -0.598790601342524
-0.325914845742921 to -0.328071877806986
-0.711717874684076 to -0.713874906748141
-0.30005489376996 to -0.302211925834025
-0.662701756634454 to -0.664858788698519
Changing layer 2's weights from 
0.146486475191374 to 0.144329443127309
-0.40756701020215 to -0.409724042266215
-0.122977719583253 to -0.125134751647318
0.0482223770239548 to 0.0460653449598896
-0.129030332841615 to -0.13118736490568
-0.335039959230165 to -0.33719699129423
0.183772756777068 to 0.181615724713003
0.111350729189177 to 0.109193697125112
-0.620647565402727 to -0.622804597466792
-0.575690046825151 to -0.577847078889216
Changing layer 3's weights from 
0.113436176500578 to 0.111279144436513
-0.150116906442384 to -0.152273938506449
0.0924930116751388 to 0.0903359796110737
-0.814946220081005 to -0.81710325214507
-0.0301828840157791 to -0.0323399160798442
-0.160431490220766 to -0.162588522284831
-0.490616337337236 to -0.492773369401301
-0.0321034648797321 to -0.0342604969437972
-0.36379943875287 to -0.365956470816935
0.10903043957736 to 0.106873407513295
Changing layer 4's weights from 
0.00975663872744605 to 0.00759960666338094
-0.19907616166089 to -0.201233193724955
-0.399553761758546 to -0.401710793822611
-0.112619028367738 to -0.114776060431803
-0.351871834077577 to -0.354028866141642
-0.120406613626222 to -0.122563645690287
-0.124765143670778 to -0.126922175734843
-0.586662665881853 to -0.588819697945918
0.00703342171694795 to 0.00487638965288284
-0.425268516816835 to -0.4274255488809
Changing layer 5's weights from 
0.162536456308623 to 0.160379424244558
-0.279537067689638 to -0.281694099753703
0.145651294908782 to 0.143494262844717
-0.78986088646267 to -0.792017918526735
-0.252592072763185 to -0.25474910482725
-0.651060015835504 to -0.653217047899569
-0.351094708718995 to -0.35325174078306
-0.278648481645326 to -0.280805513709391
0.110953523836394 to 0.108796491772329
0.104445411882658 to 0.102288379818593
Trying to learn from memory 25, 1, -0.2
sum 0.0588674373567447 distri 0.0228788835459744
Using diff 0.0212716944715842 and condRate 0.166666666666667
Changed category 1 weights from 
0.287693363571017 to 0.286984307078065
0.521008950614779 to 0.520299894121827
0.0960837127207213 to 0.0953746562277694
0.100176525258868 to 0.0994674687659163
Changing layer 0's weights from 
-0.0848000501092321 to -0.085509106602184
-0.456726506799067 to -0.457435563292019
-0.691075772493685 to -0.691784828986637
-0.656516522615755 to -0.657225579108707
0.0913510467115988 to 0.0906419902186469
-0.591290906518306 to -0.591999963011258
-0.437461182921732 to -0.438170239414684
-0.464915588944758 to -0.46562464543771
-0.702407449930514 to -0.703116506423466
-0.74615750506481 to -0.746866561557762
Changing layer 1's weights from 
-0.741060592680777 to -0.741769649173729
-0.139891908019389 to -0.140600964512341
-0.265594646781291 to -0.266303703274243
-0.275499985068644 to -0.276209041561596
-0.460552767365778 to -0.46126182385873
-0.598790601342524 to -0.599499657835476
-0.328071877806986 to -0.328780934299938
-0.713874906748141 to -0.714583963241093
-0.302211925834025 to -0.302920982326977
-0.664858788698519 to -0.665567845191471
Changing layer 2's weights from 
0.144329443127309 to 0.143620386634357
-0.409724042266215 to -0.410433098759167
-0.125134751647318 to -0.12584380814027
0.0460653449598896 to 0.0453562884669377
-0.13118736490568 to -0.131896421398632
-0.33719699129423 to -0.337906047787182
0.181615724713003 to 0.180906668220051
0.109193697125112 to 0.10848464063216
-0.622804597466792 to -0.623513653959744
-0.577847078889216 to -0.578556135382168
Changing layer 3's weights from 
0.111279144436513 to 0.110570087943561
-0.152273938506449 to -0.152982994999401
0.0903359796110737 to 0.0896269231181218
-0.81710325214507 to -0.817812308638022
-0.0323399160798442 to -0.0330489725727961
-0.162588522284831 to -0.163297578777783
-0.492773369401301 to -0.493482425894253
-0.0342604969437972 to -0.0349695534367491
-0.365956470816935 to -0.366665527309887
0.106873407513295 to 0.106164351020343
Changing layer 4's weights from 
0.00759960666338094 to 0.00689055017042904
-0.201233193724955 to -0.201942250217907
-0.401710793822611 to -0.402419850315563
-0.114776060431803 to -0.115485116924755
-0.354028866141642 to -0.354737922634594
-0.122563645690287 to -0.123272702183239
-0.126922175734843 to -0.127631232227795
-0.588819697945918 to -0.58952875443887
0.00487638965288284 to 0.00416733315993094
-0.4274255488809 to -0.428134605373852
Changing layer 5's weights from 
0.160379424244558 to 0.159670367751606
-0.281694099753703 to -0.282403156246655
0.143494262844717 to 0.142785206351765
-0.792017918526735 to -0.792726975019687
-0.25474910482725 to -0.255458161320202
-0.653217047899569 to -0.653926104392521
-0.35325174078306 to -0.353960797276012
-0.280805513709391 to -0.281514570202343
0.108796491772329 to 0.108087435279377
0.102288379818593 to 0.101579323325641
Trying to learn from memory 26, 0, -0.2
sum 0.0588688527124989 distri -0.0205605085271807
Using diff 0.0647121480615549 and condRate 0.166666666666667
Changed category 0 weights from 
0.158679518183952 to 0.156522446549758
-0.241355445900673 to -0.243512517534867
-0.523876142421479 to -0.526033214055673
-0.372370776453728 to -0.374527848087922
Changing layer 0's weights from 
-0.085509106602184 to -0.0876661782363787
-0.457435563292019 to -0.459592634926214
-0.691784828986637 to -0.693941900620832
-0.657225579108707 to -0.659382650742902
0.0906419902186469 to 0.0884849185844522
-0.591999963011258 to -0.594157034645452
-0.438170239414684 to -0.440327311048879
-0.46562464543771 to -0.467781717071905
-0.703116506423466 to -0.705273578057661
-0.746866561557762 to -0.749023633191957
Changing layer 1's weights from 
-0.741769649173729 to -0.743926720807924
-0.140600964512341 to -0.142758036146536
-0.266303703274243 to -0.268460774908438
-0.276209041561596 to -0.278366113195791
-0.46126182385873 to -0.463418895492925
-0.599499657835476 to -0.601656729469671
-0.328780934299938 to -0.330938005934133
-0.714583963241093 to -0.716741034875288
-0.302920982326977 to -0.305078053961172
-0.665567845191471 to -0.667724916825666
Changing layer 2's weights from 
0.143620386634357 to 0.141463315000162
-0.410433098759167 to -0.412590170393362
-0.12584380814027 to -0.128000879774465
0.0453562884669377 to 0.043199216832743
-0.131896421398632 to -0.134053493032827
-0.337906047787182 to -0.340063119421377
0.180906668220051 to 0.178749596585856
0.10848464063216 to 0.106327568997965
-0.623513653959744 to -0.625670725593938
-0.578556135382168 to -0.580713207016363
Changing layer 3's weights from 
0.110570087943561 to 0.108413016309366
-0.152982994999401 to -0.155140066633596
0.0896269231181218 to 0.0874698514839271
-0.817812308638022 to -0.819969380272217
-0.0330489725727961 to -0.0352060442069908
-0.163297578777783 to -0.165454650411978
-0.493482425894253 to -0.495639497528447
-0.0349695534367491 to -0.0371266250709438
-0.366665527309887 to -0.368822598944082
0.106164351020343 to 0.104007279386148
Changing layer 4's weights from 
0.00689055017042904 to 0.00473347853623434
-0.201942250217907 to -0.204099321852102
-0.402419850315563 to -0.404576921949758
-0.115485116924755 to -0.11764218855895
-0.354737922634594 to -0.356894994268789
-0.123272702183239 to -0.125429773817434
-0.127631232227795 to -0.12978830386199
-0.58952875443887 to -0.591685826073065
0.00416733315993094 to 0.00201026152573624
-0.428134605373852 to -0.430291677008047
Changing layer 5's weights from 
0.159670367751606 to 0.157513296117411
-0.282403156246655 to -0.28456022788085
0.142785206351765 to 0.14062813471757
-0.792726975019687 to -0.794884046653882
-0.255458161320202 to -0.257615232954397
-0.653926104392521 to -0.656083176026716
-0.353960797276012 to -0.356117868910207
-0.281514570202343 to -0.283671641836538
0.108087435279377 to 0.105930363645182
0.101579323325641 to 0.0994222516914461
Trying to learn from memory 27, 0, -0.2
sum 0.0589135319606817 distri -0.0205668228818412
Using diff 0.0647519718523525 and condRate 0.166666666666667
Changed category 0 weights from 
0.156522446549758 to 0.15436404745585
-0.243512517534867 to -0.245670916628775
-0.526033214055673 to -0.528191613149581
-0.374527848087922 to -0.37668624718183
Changing layer 0's weights from 
-0.0876661782363787 to -0.0898245773302865
-0.459592634926214 to -0.461751034020121
-0.693941900620832 to -0.69610029971474
-0.659382650742902 to -0.66154104983681
0.0884849185844522 to 0.0863265194905445
-0.594157034645452 to -0.59631543373936
-0.440327311048879 to -0.442485710142786
-0.467781717071905 to -0.469940116165812
-0.705273578057661 to -0.707431977151568
-0.749023633191957 to -0.751182032285864
Changing layer 1's weights from 
-0.743926720807924 to -0.746085119901831
-0.142758036146536 to -0.144916435240444
-0.268460774908438 to -0.270619174002345
-0.278366113195791 to -0.280524512289698
-0.463418895492925 to -0.465577294586833
-0.601656729469671 to -0.603815128563579
-0.330938005934133 to -0.33309640502804
-0.716741034875288 to -0.718899433969196
-0.305078053961172 to -0.307236453055079
-0.667724916825666 to -0.669883315919574
Changing layer 2's weights from 
0.141463315000162 to 0.139304915906254
-0.412590170393362 to -0.414748569487269
-0.128000879774465 to -0.130159278868373
0.043199216832743 to 0.0410408177388353
-0.134053493032827 to -0.136211892126735
-0.340063119421377 to -0.342221518515284
0.178749596585856 to 0.176591197491948
0.106327568997965 to 0.104169169904057
-0.625670725593938 to -0.627829124687846
-0.580713207016363 to -0.582871606110271
Changing layer 3's weights from 
0.108413016309366 to 0.106254617215458
-0.155140066633596 to -0.157298465727504
0.0874698514839271 to 0.0853114523900194
-0.819969380272217 to -0.822127779366124
-0.0352060442069908 to -0.0373644433008986
-0.165454650411978 to -0.167613049505886
-0.495639497528447 to -0.497797896622355
-0.0371266250709438 to -0.0392850241648515
-0.368822598944082 to -0.370980998037989
0.104007279386148 to 0.10184888029224
Changing layer 4's weights from 
0.00473347853623434 to 0.0025750794423266
-0.204099321852102 to -0.20625772094601
-0.404576921949758 to -0.406735321043665
-0.11764218855895 to -0.119800587652857
-0.356894994268789 to -0.359053393362696
-0.125429773817434 to -0.127588172911341
-0.12978830386199 to -0.131946702955898
-0.591685826073065 to -0.593844225166972
0.00201026152573624 to -0.000148137568171498
-0.430291677008047 to -0.432450076101954
Changing layer 5's weights from 
0.157513296117411 to 0.155354897023503
-0.28456022788085 to -0.286718626974757
0.14062813471757 to 0.138469735623662
-0.794884046653882 to -0.797042445747789
-0.257615232954397 to -0.259773632048304
-0.656083176026716 to -0.658241575120624
-0.356117868910207 to -0.358276268004114
-0.283671641836538 to -0.285830040930445
0.105930363645182 to 0.103771964551274
0.0994222516914461 to 0.0972638525975384
Trying to learn from memory 28, 1, -0.2
sum 0.058989310598666 distri 0.0229207573718658
Using diff 0.0213212255771337 and condRate 0.166666666666667
Changed category 1 weights from 
0.286984307078065 to 0.286273599548237
0.520299894121827 to 0.519589186591999
0.0953746562277694 to 0.0946639486979412
0.0994674687659163 to 0.0987567612360882
Changing layer 0's weights from 
-0.0898245773302865 to -0.0905352848601146
-0.461751034020121 to -0.46246174154995
-0.69610029971474 to -0.696811007244568
-0.66154104983681 to -0.662251757366638
0.0863265194905445 to 0.0856158119607163
-0.59631543373936 to -0.597026141269188
-0.442485710142786 to -0.443196417672615
-0.469940116165812 to -0.47065082369564
-0.707431977151568 to -0.708142684681397
-0.751182032285864 to -0.751892739815693
Changing layer 1's weights from 
-0.746085119901831 to -0.74679582743166
-0.144916435240444 to -0.145627142770272
-0.270619174002345 to -0.271329881532174
-0.280524512289698 to -0.281235219819527
-0.465577294586833 to -0.466288002116661
-0.603815128563579 to -0.604525836093407
-0.33309640502804 to -0.333807112557868
-0.718899433969196 to -0.719610141499024
-0.307236453055079 to -0.307947160584907
-0.669883315919574 to -0.670594023449402
Changing layer 2's weights from 
0.139304915906254 to 0.138594208376426
-0.414748569487269 to -0.415459277017098
-0.130159278868373 to -0.130869986398201
0.0410408177388353 to 0.0403301102090072
-0.136211892126735 to -0.136922599656563
-0.342221518515284 to -0.342932226045112
0.176591197491948 to 0.17588048996212
0.104169169904057 to 0.103458462374229
-0.627829124687846 to -0.628539832217674
-0.582871606110271 to -0.583582313640099
Changing layer 3's weights from 
0.106254617215458 to 0.10554390968563
-0.157298465727504 to -0.158009173257332
0.0853114523900194 to 0.0846007448601912
-0.822127779366124 to -0.822838486895953
-0.0373644433008986 to -0.0380751508307267
-0.167613049505886 to -0.168323757035714
-0.497797896622355 to -0.498508604152183
-0.0392850241648515 to -0.0399957316946797
-0.370980998037989 to -0.371691705567818
0.10184888029224 to 0.101138172762412
Changing layer 4's weights from 
0.0025750794423266 to 0.00186437191249845
-0.20625772094601 to -0.206968428475838
-0.406735321043665 to -0.407446028573494
-0.119800587652857 to -0.120511295182686
-0.359053393362696 to -0.359764100892525
-0.127588172911341 to -0.12829888044117
-0.131946702955898 to -0.132657410485726
-0.593844225166972 to -0.5945549326968
-0.000148137568171498 to -0.000858845097999656
-0.432450076101954 to -0.433160783631783
Changing layer 5's weights from 
0.155354897023503 to 0.154644189493675
-0.286718626974757 to -0.287429334504586
0.138469735623662 to 0.137759028093834
-0.797042445747789 to -0.797753153277618
-0.259773632048304 to -0.260484339578133
-0.658241575120624 to -0.658952282650452
-0.358276268004114 to -0.358986975533942
-0.285830040930445 to -0.286540748460274
0.103771964551274 to 0.103061257021446
0.0972638525975384 to 0.0965531450677102
Trying to learn from memory 29, 1, -0.2
sum 0.0589847761354859 distri 0.0229188857430639
Using diff 0.0213196963585505 and condRate 0.166666666666667
Changed category 1 weights from 
0.286273599548237 to 0.285562942992363
0.519589186591999 to 0.518878530036124
0.0946639486979412 to 0.0939532921420666
0.0987567612360882 to 0.0980461046802136
Changing layer 0's weights from 
-0.0905352848601146 to -0.0912459414159893
-0.46246174154995 to -0.463172398105824
-0.696811007244568 to -0.697521663800442
-0.662251757366638 to -0.662962413922512
0.0856158119607163 to 0.0849051554048417
-0.597026141269188 to -0.597736797825063
-0.443196417672615 to -0.443907074228489
-0.47065082369564 to -0.471361480251515
-0.708142684681397 to -0.708853341237271
-0.751892739815693 to -0.752603396371567
Changing layer 1's weights from 
-0.74679582743166 to -0.747506483987534
-0.145627142770272 to -0.146337799326146
-0.271329881532174 to -0.272040538088048
-0.281235219819527 to -0.281945876375401
-0.466288002116661 to -0.466998658672536
-0.604525836093407 to -0.605236492649282
-0.333807112557868 to -0.334517769113743
-0.719610141499024 to -0.720320798054898
-0.307947160584907 to -0.308657817140782
-0.670594023449402 to -0.671304680005276
Changing layer 2's weights from 
0.138594208376426 to 0.137883551820552
-0.415459277017098 to -0.416169933572972
-0.130869986398201 to -0.131580642954075
0.0403301102090072 to 0.0396194536531325
-0.136922599656563 to -0.137633256212437
-0.342932226045112 to -0.343642882600987
0.17588048996212 to 0.175169833406245
0.103458462374229 to 0.102747805818355
-0.628539832217674 to -0.629250488773549
-0.583582313640099 to -0.584292970195973
Changing layer 3's weights from 
0.10554390968563 to 0.104833253129756
-0.158009173257332 to -0.158719829813206
0.0846007448601912 to 0.0838900883043166
-0.822838486895953 to -0.823549143451827
-0.0380751508307267 to -0.0387858073866014
-0.168323757035714 to -0.169034413591588
-0.498508604152183 to -0.499219260708058
-0.0399957316946797 to -0.0407063882505543
-0.371691705567818 to -0.372402362123692
0.101138172762412 to 0.100427516206538
Changing layer 4's weights from 
0.00186437191249845 to 0.00115371535662382
-0.206968428475838 to -0.207679085031712
-0.407446028573494 to -0.408156685129368
-0.120511295182686 to -0.12122195173856
-0.359764100892525 to -0.360474757448399
-0.12829888044117 to -0.129009536997044
-0.132657410485726 to -0.1333680670416
-0.5945549326968 to -0.595265589252675
-0.000858845097999656 to -0.00156950165387428
-0.433160783631783 to -0.433871440187657
Changing layer 5's weights from 
0.154644189493675 to 0.153933532937801
-0.287429334504586 to -0.28813999106046
0.137759028093834 to 0.13704837153796
-0.797753153277618 to -0.798463809833492
-0.260484339578133 to -0.261194996134007
-0.658952282650452 to -0.659662939206326
-0.358986975533942 to -0.359697632089817
-0.286540748460274 to -0.287251405016148
0.103061257021446 to 0.102350600465572
0.0965531450677102 to 0.0958424885118356
Trying to learn from memory 30, 0, -0.2
sum 0.0589555516590492 distri -0.0205591086994448
Using diff 0.0647757724437317 and condRate 0.166666666666667
Changed category 0 weights from 
0.15436404745585 to 0.152204855008884
-0.245670916628775 to -0.247830109075741
-0.528191613149581 to -0.530350805596547
-0.37668624718183 to -0.378845439628796
Changing layer 0's weights from 
-0.0912459414159893 to -0.0934051338629548
-0.463172398105824 to -0.46533159055279
-0.697521663800442 to -0.699680856247408
-0.662962413922512 to -0.665121606369478
0.0849051554048417 to 0.0827459629578762
-0.597736797825063 to -0.599895990272028
-0.443907074228489 to -0.446066266675455
-0.471361480251515 to -0.473520672698481
-0.708853341237271 to -0.711012533684237
-0.752603396371567 to -0.754762588818533
Changing layer 1's weights from 
-0.747506483987534 to -0.7496656764345
-0.146337799326146 to -0.148496991773112
-0.272040538088048 to -0.274199730535014
-0.281945876375401 to -0.284105068822367
-0.466998658672536 to -0.469157851119501
-0.605236492649282 to -0.607395685096247
-0.334517769113743 to -0.336676961560709
-0.720320798054898 to -0.722479990501864
-0.308657817140782 to -0.310817009587748
-0.671304680005276 to -0.673463872452242
Changing layer 2's weights from 
0.137883551820552 to 0.135724359373586
-0.416169933572972 to -0.418329126019938
-0.131580642954075 to -0.133739835401041
0.0396194536531325 to 0.037460261206167
-0.137633256212437 to -0.139792448659403
-0.343642882600987 to -0.345802075047953
0.175169833406245 to 0.17301064095928
0.102747805818355 to 0.100588613371389
-0.629250488773549 to -0.631409681220514
-0.584292970195973 to -0.586452162642939
Changing layer 3's weights from 
0.104833253129756 to 0.10267406068279
-0.158719829813206 to -0.160879022260172
0.0838900883043166 to 0.0817308958573511
-0.823549143451827 to -0.825708335898793
-0.0387858073866014 to -0.0409449998335669
-0.169034413591588 to -0.171193606038554
-0.499219260708058 to -0.501378453155023
-0.0407063882505543 to -0.0428655806975199
-0.372402362123692 to -0.374561554570658
0.100427516206538 to 0.0982683237595721
Changing layer 4's weights from 
0.00115371535662382 to -0.00100547709034171
-0.207679085031712 to -0.209838277478678
-0.408156685129368 to -0.410315877576334
-0.12122195173856 to -0.123381144185526
-0.360474757448399 to -0.362633949895365
-0.129009536997044 to -0.13116872944401
-0.1333680670416 to -0.135527259488566
-0.595265589252675 to -0.597424781699641
-0.00156950165387428 to -0.00372869410083981
-0.433871440187657 to -0.436030632634623
Changing layer 5's weights from 
0.153933532937801 to 0.151774340490835
-0.28813999106046 to -0.290299183507426
0.13704837153796 to 0.134889179090994
-0.798463809833492 to -0.800623002280458
-0.261194996134007 to -0.263354188580973
-0.659662939206326 to -0.661822131653292
-0.359697632089817 to -0.361856824536783
-0.287251405016148 to -0.289410597463114
0.102350600465572 to 0.100191408018606
0.0958424885118356 to 0.0936832960648701
Trying to learn from memory 31, 1, -0.2
sum 0.0589516229517581 distri 0.0229055172205062
Using diff 0.0213081999933123 and condRate 0.166666666666667
Changed category 1 weights from 
0.285562942992363 to 0.284852669648668
0.518878530036124 to 0.51816825669243
0.0939532921420666 to 0.0932430187983723
0.0980461046802136 to 0.0973358313365193
Changing layer 0's weights from 
-0.0934051338629548 to -0.0941154072066491
-0.46533159055279 to -0.466041863896484
-0.699680856247408 to -0.700391129591102
-0.665121606369478 to -0.665831879713172
0.0827459629578762 to 0.0820356896141819
-0.599895990272028 to -0.600606263615723
-0.446066266675455 to -0.446776540019149
-0.473520672698481 to -0.474230946042175
-0.711012533684237 to -0.711722807027931
-0.754762588818533 to -0.755472862162227
Changing layer 1's weights from 
-0.7496656764345 to -0.750375949778194
-0.148496991773112 to -0.149207265116806
-0.274199730535014 to -0.274910003878708
-0.284105068822367 to -0.284815342166061
-0.469157851119501 to -0.469868124463195
-0.607395685096247 to -0.608105958439941
-0.336676961560709 to -0.337387234904403
-0.722479990501864 to -0.723190263845558
-0.310817009587748 to -0.311527282931442
-0.673463872452242 to -0.674174145795936
Changing layer 2's weights from 
0.135724359373586 to 0.135014086029892
-0.418329126019938 to -0.419039399363632
-0.133739835401041 to -0.134450108744735
0.037460261206167 to 0.0367499878624727
-0.139792448659403 to -0.140502722003097
-0.345802075047953 to -0.346512348391647
0.17301064095928 to 0.172300367615586
0.100588613371389 to 0.0998783400276947
-0.631409681220514 to -0.632119954564209
-0.586452162642939 to -0.587162435986633
Changing layer 3's weights from 
0.10267406068279 to 0.101963787339096
-0.160879022260172 to -0.161589295603866
0.0817308958573511 to 0.0810206225136568
-0.825708335898793 to -0.826418609242487
-0.0409449998335669 to -0.0416552731772612
-0.171193606038554 to -0.171903879382248
-0.501378453155023 to -0.502088726498718
-0.0428655806975199 to -0.0435758540412142
-0.374561554570658 to -0.375271827914352
0.0982683237595721 to 0.0975580504158778
Changing layer 4's weights from 
-0.00100547709034171 to -0.00171575043403602
-0.209838277478678 to -0.210548550822372
-0.410315877576334 to -0.411026150920028
-0.123381144185526 to -0.12409141752922
-0.362633949895365 to -0.363344223239059
-0.13116872944401 to -0.131879002787704
-0.135527259488566 to -0.13623753283226
-0.597424781699641 to -0.598135055043335
-0.00372869410083981 to -0.00443896744453412
-0.436030632634623 to -0.436740905978317
Changing layer 5's weights from 
0.151774340490835 to 0.151064067147141
-0.290299183507426 to -0.29100945685112
0.134889179090994 to 0.1341789057473
-0.800623002280458 to -0.801333275624152
-0.263354188580973 to -0.264064461924667
-0.661822131653292 to -0.662532404996986
-0.361856824536783 to -0.362567097880477
-0.289410597463114 to -0.290120870806808
0.100191408018606 to 0.0994811346749118
0.0936832960648701 to 0.0929730227211758
Trying to learn from memory 32, 1, -0.2
sum 0.0589147554649495 distri 0.0228946945498405
Using diff 0.0212913720488716 and condRate 0.166666666666667
Changed category 1 weights from 
0.284852669648668 to 0.284142957236464
0.51816825669243 to 0.517458544280225
0.0932430187983723 to 0.0925333063861677
0.0973358313365193 to 0.0966261189243147
Changing layer 0's weights from 
-0.0941154072066491 to -0.0948251196188537
-0.466041863896484 to -0.466751576308689
-0.700391129591102 to -0.701100842003307
-0.665831879713172 to -0.666541592125377
0.0820356896141819 to 0.0813259772019773
-0.600606263615723 to -0.601315976027927
-0.446776540019149 to -0.447486252431354
-0.474230946042175 to -0.474940658454379
-0.711722807027931 to -0.712432519440136
-0.755472862162227 to -0.756182574574432
Changing layer 1's weights from 
-0.750375949778194 to -0.751085662190399
-0.149207265116806 to -0.149916977529011
-0.274910003878708 to -0.275619716290913
-0.284815342166061 to -0.285525054578266
-0.469868124463195 to -0.4705778368754
-0.608105958439941 to -0.608815670852146
-0.337387234904403 to -0.338096947316608
-0.723190263845558 to -0.723899976257763
-0.311527282931442 to -0.312236995343647
-0.674174145795936 to -0.674883858208141
Changing layer 2's weights from 
0.135014086029892 to 0.134304373617687
-0.419039399363632 to -0.419749111775837
-0.134450108744735 to -0.13515982115694
0.0367499878624727 to 0.0360402754502681
-0.140502722003097 to -0.141212434415302
-0.346512348391647 to -0.347222060803852
0.172300367615586 to 0.171590655203381
0.0998783400276947 to 0.0991686276154901
-0.632119954564209 to -0.632829666976413
-0.587162435986633 to -0.587872148398838
Changing layer 3's weights from 
0.101963787339096 to 0.101254074926891
-0.161589295603866 to -0.162299008016071
0.0810206225136568 to 0.0803109101014522
-0.826418609242487 to -0.827128321654692
-0.0416552731772612 to -0.0423649855894658
-0.171903879382248 to -0.172613591794453
-0.502088726498718 to -0.502798438910922
-0.0435758540412142 to -0.0442855664534188
-0.375271827914352 to -0.375981540326557
0.0975580504158778 to 0.0968483380036732
Changing layer 4's weights from 
-0.00171575043403602 to -0.00242546284624061
-0.210548550822372 to -0.211258263234577
-0.411026150920028 to -0.411735863332233
-0.12409141752922 to -0.124801129941425
-0.363344223239059 to -0.364053935651264
-0.131879002787704 to -0.132588715199909
-0.13623753283226 to -0.136947245244465
-0.598135055043335 to -0.59884476745554
-0.00443896744453412 to -0.00514867985673871
-0.436740905978317 to -0.437450618390522
Changing layer 5's weights from 
0.151064067147141 to 0.150354354734936
-0.29100945685112 to -0.291719169263325
0.1341789057473 to 0.133469193335095
-0.801333275624152 to -0.802042988036357
-0.264064461924667 to -0.264774174336872
-0.662532404996986 to -0.663242117409191
-0.362567097880477 to -0.363276810292682
-0.290120870806808 to -0.290830583219013
0.0994811346749118 to 0.0987714222627072
0.0929730227211758 to 0.0922633103089712
10/5/2016 1:30:19 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 33, 0, -0.2
sum 0.0589130351452952 distri -0.020580444384118
Using diff 0.0647652207430894 and condRate 0.166666666666667
Changed category 0 weights from 
0.152204855008884 to 0.150046014285279
-0.247830109075741 to -0.249988949799346
-0.530350805596547 to -0.532509646320152
-0.378845439628796 to -0.381004280352401
Changing layer 0's weights from 
-0.0948251196188537 to -0.0969839603424592
-0.466751576308689 to -0.468910417032294
-0.701100842003307 to -0.703259682726912
-0.666541592125377 to -0.668700432848982
0.0813259772019773 to 0.0791671364783717
-0.601315976027927 to -0.603474816751533
-0.447486252431354 to -0.449645093154959
-0.474940658454379 to -0.477099499177985
-0.712432519440136 to -0.714591360163741
-0.756182574574432 to -0.758341415298037
Changing layer 1's weights from 
-0.751085662190399 to -0.753244502914004
-0.149916977529011 to -0.152075818252616
-0.275619716290913 to -0.277778557014518
-0.285525054578266 to -0.287683895301871
-0.4705778368754 to -0.472736677599006
-0.608815670852146 to -0.610974511575752
-0.338096947316608 to -0.340255788040213
-0.723899976257763 to -0.726058816981368
-0.312236995343647 to -0.314395836067252
-0.674883858208141 to -0.677042698931746
Changing layer 2's weights from 
0.134304373617687 to 0.132145532894082
-0.419749111775837 to -0.421907952499442
-0.13515982115694 to -0.137318661880545
0.0360402754502681 to 0.0338814347266626
-0.141212434415302 to -0.143371275138907
-0.347222060803852 to -0.349380901527457
0.171590655203381 to 0.169431814479776
0.0991686276154901 to 0.0970097868918846
-0.632829666976413 to -0.634988507700019
-0.587872148398838 to -0.590030989122443
Changing layer 3's weights from 
0.101254074926891 to 0.0990952342032856
-0.162299008016071 to -0.164457848739676
0.0803109101014522 to 0.0781520693778466
-0.827128321654692 to -0.829287162378297
-0.0423649855894658 to -0.0445238263130713
-0.172613591794453 to -0.174772432518058
-0.502798438910922 to -0.504957279634528
-0.0442855664534188 to -0.0464444071770243
-0.375981540326557 to -0.378140381050162
0.0968483380036732 to 0.0946894972800676
Changing layer 4's weights from 
-0.00242546284624061 to -0.00458430356984616
-0.211258263234577 to -0.213417103958182
-0.411735863332233 to -0.413894704055838
-0.124801129941425 to -0.12695997066503
-0.364053935651264 to -0.366212776374869
-0.132588715199909 to -0.134747555923514
-0.136947245244465 to -0.13910608596807
-0.59884476745554 to -0.601003608179145
-0.00514867985673871 to -0.00730752058034426
-0.437450618390522 to -0.439609459114127
Changing layer 5's weights from 
0.150354354734936 to 0.148195514011331
-0.291719169263325 to -0.29387800998693
0.133469193335095 to 0.13131035261149
-0.802042988036357 to -0.804201828759962
-0.264774174336872 to -0.266933015060477
-0.663242117409191 to -0.665400958132796
-0.363276810292682 to -0.365435651016287
-0.290830583219013 to -0.292989423942618
0.0987714222627072 to 0.0966125815391016
0.0922633103089712 to 0.0901044695853656
Trying to learn from memory 33, 2, -0.2
sum 0.0589130351452952 distri 0.0565960810148359
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.59114479951626 to 0.557811465686221
0.869586765096249 to 0.83625343126621
0.782546400353969 to 0.74921306652393
0.127974695787489 to 0.0946413619574505
Changing layer 0's weights from 
-0.0969839603424592 to -0.130317294172498
-0.468910417032294 to -0.502243750862333
-0.703259682726912 to -0.736593016556951
-0.668700432848982 to -0.702033766679021
0.0791671364783717 to 0.045833802648333
-0.603474816751533 to -0.636808150581571
-0.449645093154959 to -0.482978426984998
-0.477099499177985 to -0.510432833008024
-0.714591360163741 to -0.74792469399378
-0.758341415298037 to -0.791674749128076
Changing layer 1's weights from 
-0.753244502914004 to -0.786577836744043
-0.152075818252616 to -0.185409152082655
-0.277778557014518 to -0.311111890844557
-0.287683895301871 to -0.32101722913191
-0.472736677599006 to -0.506070011429044
-0.610974511575752 to -0.64430784540579
-0.340255788040213 to -0.373589121870252
-0.726058816981368 to -0.759392150811407
-0.314395836067252 to -0.347729169897291
-0.677042698931746 to -0.710376032761785
Changing layer 2's weights from 
0.132145532894082 to 0.0988121990640429
-0.421907952499442 to -0.455241286329481
-0.137318661880545 to -0.170651995710584
0.0338814347266626 to 0.000548100896623847
-0.143371275138907 to -0.176704608968946
-0.349380901527457 to -0.382714235357496
0.169431814479776 to 0.136098480649737
0.0970097868918846 to 0.0636764530618459
-0.634988507700019 to -0.668321841530057
-0.590030989122443 to -0.623364322952482
Changing layer 3's weights from 
0.0990952342032856 to 0.0657619003732469
-0.164457848739676 to -0.197791182569715
0.0781520693778466 to 0.0448187355478079
-0.829287162378297 to -0.862620496208336
-0.0445238263130713 to -0.07785716014311
-0.174772432518058 to -0.208105766348097
-0.504957279634528 to -0.538290613464567
-0.0464444071770243 to -0.079777741007063
-0.378140381050162 to -0.411473714880201
0.0946894972800676 to 0.0613561634500289
Changing layer 4's weights from 
-0.00458430356984616 to -0.0379176373998849
-0.213417103958182 to -0.246750437788221
-0.413894704055838 to -0.447228037885877
-0.12695997066503 to -0.160293304495069
-0.366212776374869 to -0.399546110204908
-0.134747555923514 to -0.168080889753553
-0.13910608596807 to -0.172439419798109
-0.601003608179145 to -0.634336942009184
-0.00730752058034426 to -0.040640854410383
-0.439609459114127 to -0.472942792944166
Changing layer 5's weights from 
0.148195514011331 to 0.114862180181292
-0.29387800998693 to -0.327211343816969
0.13131035261149 to 0.0979770187814509
-0.804201828759962 to -0.837535162590001
-0.266933015060477 to -0.300266348890516
-0.665400958132796 to -0.698734291962835
-0.365435651016287 to -0.398768984846326
-0.292989423942618 to -0.326322757772657
0.0966125815391016 to 0.0632792477090629
0.0901044695853656 to 0.0567711357553269
Trying to learn from memory 34, 2, -0.2
sum 0.0589145976139171 distri 0.0566070603644252
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.557811465686221 to 0.524478131856182
0.83625343126621 to 0.802920097436171
0.74921306652393 to 0.715879732693891
0.0946413619574505 to 0.0613080281274118
Changing layer 0's weights from 
-0.130317294172498 to -0.163650628002537
-0.502243750862333 to -0.535577084692372
-0.736593016556951 to -0.76992635038699
-0.702033766679021 to -0.73536710050906
0.045833802648333 to 0.0125004688182943
-0.636808150581571 to -0.67014148441161
-0.482978426984998 to -0.516311760815037
-0.510432833008024 to -0.543766166838062
-0.74792469399378 to -0.781258027823819
-0.791674749128076 to -0.825008082958115
Changing layer 1's weights from 
-0.786577836744043 to -0.819911170574082
-0.185409152082655 to -0.218742485912694
-0.311111890844557 to -0.344445224674596
-0.32101722913191 to -0.354350562961949
-0.506070011429044 to -0.539403345259083
-0.64430784540579 to -0.677641179235829
-0.373589121870252 to -0.406922455700291
-0.759392150811407 to -0.792725484641446
-0.347729169897291 to -0.38106250372733
-0.710376032761785 to -0.743709366591824
Changing layer 2's weights from 
0.0988121990640429 to 0.0654788652340042
-0.455241286329481 to -0.48857462015952
-0.170651995710584 to -0.203985329540623
0.000548100896623847 to -0.0327852329334149
-0.176704608968946 to -0.210037942798985
-0.382714235357496 to -0.416047569187535
0.136098480649737 to 0.102765146819698
0.0636764530618459 to 0.0303431192318072
-0.668321841530057 to -0.701655175360096
-0.623364322952482 to -0.656697656782521
Changing layer 3's weights from 
0.0657619003732469 to 0.0324285665432082
-0.197791182569715 to -0.231124516399754
0.0448187355478079 to 0.0114854017177692
-0.862620496208336 to -0.895953830038374
-0.07785716014311 to -0.111190493973149
-0.208105766348097 to -0.241439100178136
-0.538290613464567 to -0.571623947294605
-0.079777741007063 to -0.113111074837102
-0.411473714880201 to -0.44480704871024
0.0613561634500289 to 0.0280228296199902
Changing layer 4's weights from 
-0.0379176373998849 to -0.0712509712299236
-0.246750437788221 to -0.28008377161826
-0.447228037885877 to -0.480561371715916
-0.160293304495069 to -0.193626638325108
-0.399546110204908 to -0.432879444034947
-0.168080889753553 to -0.201414223583592
-0.172439419798109 to -0.205772753628148
-0.634336942009184 to -0.667670275839222
-0.040640854410383 to -0.0739741882404217
-0.472942792944166 to -0.506276126774205
Changing layer 5's weights from 
0.114862180181292 to 0.0815288463512532
-0.327211343816969 to -0.360544677647008
0.0979770187814509 to 0.0646436849514122
-0.837535162590001 to -0.87086849642004
-0.300266348890516 to -0.333599682720555
-0.698734291962835 to -0.732067625792874
-0.398768984846326 to -0.432102318676365
-0.326322757772657 to -0.359656091602696
0.0632792477090629 to 0.0299459138790242
0.0567711357553269 to 0.0234378019252882
Trying to learn from memory 35, 0, -0.2
sum 0.0588972850741627 distri -0.0205864350601327
Using diff 0.0647593988657548 and condRate 0.166666666666667
Changed category 0 weights from 
0.150046014285279 to 0.147887367624254
-0.249988949799346 to -0.252147596460371
-0.532509646320152 to -0.534668292981177
-0.381004280352401 to -0.383162927013426
Changing layer 0's weights from 
-0.163650628002537 to -0.165809274663561
-0.535577084692372 to -0.537735731353396
-0.76992635038699 to -0.772084997048014
-0.73536710050906 to -0.737525747170084
0.0125004688182943 to 0.0103418221572695
-0.67014148441161 to -0.672300131072635
-0.516311760815037 to -0.518470407476062
-0.543766166838062 to -0.545924813499087
-0.781258027823819 to -0.783416674484843
-0.825008082958115 to -0.827166729619139
Changing layer 1's weights from 
-0.819911170574082 to -0.822069817235106
-0.218742485912694 to -0.220901132573719
-0.344445224674596 to -0.34660387133562
-0.354350562961949 to -0.356509209622973
-0.539403345259083 to -0.541561991920108
-0.677641179235829 to -0.679799825896854
-0.406922455700291 to -0.409081102361315
-0.792725484641446 to -0.79488413130247
-0.38106250372733 to -0.383221150388354
-0.743709366591824 to -0.745868013252848
Changing layer 2's weights from 
0.0654788652340042 to 0.0633202185729793
-0.48857462015952 to -0.490733266820544
-0.203985329540623 to -0.206143976201648
-0.0327852329334149 to -0.0349438795944397
-0.210037942798985 to -0.21219658946001
-0.416047569187535 to -0.418206215848559
0.102765146819698 to 0.100606500158673
0.0303431192318072 to 0.0281844725707823
-0.701655175360096 to -0.703813822021121
-0.656697656782521 to -0.658856303443546
Changing layer 3's weights from 
0.0324285665432082 to 0.0302699198821834
-0.231124516399754 to -0.233283163060779
0.0114854017177692 to 0.00932675505674441
-0.895953830038374 to -0.898112476699399
-0.111190493973149 to -0.113349140634174
-0.241439100178136 to -0.24359774683916
-0.571623947294605 to -0.57378259395563
-0.113111074837102 to -0.115269721498127
-0.44480704871024 to -0.446965695371264
0.0280228296199902 to 0.0258641829589654
Changing layer 4's weights from 
-0.0712509712299236 to -0.0734096178909484
-0.28008377161826 to -0.282242418279285
-0.480561371715916 to -0.48272001837694
-0.193626638325108 to -0.195785284986132
-0.432879444034947 to -0.435038090695971
-0.201414223583592 to -0.203572870244616
-0.205772753628148 to -0.207931400289172
-0.667670275839222 to -0.669828922500247
-0.0739741882404217 to -0.0761328349014465
-0.506276126774205 to -0.508434773435229
Changing layer 5's weights from 
0.0815288463512532 to 0.0793701996902283
-0.360544677647008 to -0.362703324308032
0.0646436849514122 to 0.0624850382903873
-0.87086849642004 to -0.873027143081064
-0.333599682720555 to -0.33575832938158
-0.732067625792874 to -0.734226272453898
-0.432102318676365 to -0.434260965337389
-0.359656091602696 to -0.36181473826372
0.0299459138790242 to 0.0277872672179994
0.0234378019252882 to 0.0212791552642634
Trying to learn from memory 36, 0, -0.2
sum 0.0588684497587778 distri -0.0205638804481441
Using diff 0.0647152177672274 and condRate 0.166666666666667
Changed category 0 weights from 
0.147887367624254 to 0.145730193666535
-0.252147596460371 to -0.25430477041809
-0.534668292981177 to -0.536825466938896
-0.383162927013426 to -0.385320100971145
Changing layer 0's weights from 
-0.165809274663561 to -0.16796644862128
-0.537735731353396 to -0.539892905311115
-0.772084997048014 to -0.774242171005733
-0.737525747170084 to -0.739682921127803
0.0103418221572695 to 0.00818464819955083
-0.672300131072635 to -0.674457305030354
-0.518470407476062 to -0.52062758143378
-0.545924813499087 to -0.548081987456806
-0.783416674484843 to -0.785573848442562
-0.827166729619139 to -0.829323903576858
Changing layer 1's weights from 
-0.822069817235106 to -0.824226991192825
-0.220901132573719 to -0.223058306531437
-0.34660387133562 to -0.348761045293339
-0.356509209622973 to -0.358666383580692
-0.541561991920108 to -0.543719165877826
-0.679799825896854 to -0.681956999854572
-0.409081102361315 to -0.411238276319034
-0.79488413130247 to -0.797041305260189
-0.383221150388354 to -0.385378324346073
-0.745868013252848 to -0.748025187210567
Changing layer 2's weights from 
0.0633202185729793 to 0.0611630446152607
-0.490733266820544 to -0.492890440778263
-0.206143976201648 to -0.208301150159366
-0.0349438795944397 to -0.0371010535521583
-0.21219658946001 to -0.214353763417728
-0.418206215848559 to -0.420363389806278
0.100606500158673 to 0.0984493262009546
0.0281844725707823 to 0.0260272986130637
-0.703813822021121 to -0.70597099597884
-0.658856303443546 to -0.661013477401264
Changing layer 3's weights from 
0.0302699198821834 to 0.0281127459244647
-0.233283163060779 to -0.235440337018497
0.00932675505674441 to 0.00716958109902576
-0.898112476699399 to -0.900269650657118
-0.113349140634174 to -0.115506314591892
-0.24359774683916 to -0.245754920796879
-0.57378259395563 to -0.575939767913349
-0.115269721498127 to -0.117426895455845
-0.446965695371264 to -0.449122869328983
0.0258641829589654 to 0.0237070090012467
Changing layer 4's weights from 
-0.0734096178909484 to -0.075566791848667
-0.282242418279285 to -0.284399592237003
-0.48272001837694 to -0.484877192334659
-0.195785284986132 to -0.197942458943851
-0.435038090695971 to -0.43719526465369
-0.203572870244616 to -0.205730044202335
-0.207931400289172 to -0.210088574246891
-0.669828922500247 to -0.671986096457966
-0.0761328349014465 to -0.0782900088591651
-0.508434773435229 to -0.510591947392948
Changing layer 5's weights from 
0.0793701996902283 to 0.0772130257325097
-0.362703324308032 to -0.364860498265751
0.0624850382903873 to 0.0603278643326687
-0.873027143081064 to -0.875184317038783
-0.33575832938158 to -0.337915503339298
-0.734226272453898 to -0.736383446411617
-0.434260965337389 to -0.436418139295108
-0.36181473826372 to -0.363971912221439
0.0277872672179994 to 0.0256300932602807
0.0212791552642634 to 0.0191219813065447
Trying to learn from memory 36, 0, -0.2
sum 0.0588684497587778 distri -0.0205638804481441
Using diff 0.0647152177672274 and condRate 0.166666666666667
Changed category 0 weights from 
0.145730193666535 to 0.143573019708817
-0.25430477041809 to -0.256461944375808
-0.536825466938896 to -0.538982640896614
-0.385320100971145 to -0.387477274928863
Changing layer 0's weights from 
-0.16796644862128 to -0.170123622578999
-0.539892905311115 to -0.542050079268834
-0.774242171005733 to -0.776399344963452
-0.739682921127803 to -0.741840095085522
0.00818464819955083 to 0.00602747424183219
-0.674457305030354 to -0.676614478988072
-0.52062758143378 to -0.522784755391499
-0.548081987456806 to -0.550239161414525
-0.785573848442562 to -0.787731022400281
-0.829323903576858 to -0.831481077534577
Changing layer 1's weights from 
-0.824226991192825 to -0.826384165150544
-0.223058306531437 to -0.225215480489156
-0.348761045293339 to -0.350918219251058
-0.358666383580692 to -0.360823557538411
-0.543719165877826 to -0.545876339835545
-0.681956999854572 to -0.684114173812291
-0.411238276319034 to -0.413395450276753
-0.797041305260189 to -0.799198479217908
-0.385378324346073 to -0.387535498303792
-0.748025187210567 to -0.750182361168286
Changing layer 2's weights from 
0.0611630446152607 to 0.0590058706575421
-0.492890440778263 to -0.495047614735982
-0.208301150159366 to -0.210458324117085
-0.0371010535521583 to -0.039258227509877
-0.214353763417728 to -0.216510937375447
-0.420363389806278 to -0.422520563763997
0.0984493262009546 to 0.096292152243236
0.0260272986130637 to 0.0238701246553451
-0.70597099597884 to -0.708128169936558
-0.661013477401264 to -0.663170651358983
Changing layer 3's weights from 
0.0281127459244647 to 0.0259555719667461
-0.235440337018497 to -0.237597510976216
0.00716958109902576 to 0.00501240714130712
-0.900269650657118 to -0.902426824614837
-0.115506314591892 to -0.117663488549611
-0.245754920796879 to -0.247912094754598
-0.575939767913349 to -0.578096941871067
-0.117426895455845 to -0.119584069413564
-0.449122869328983 to -0.451280043286702
0.0237070090012467 to 0.0215498350435281
Changing layer 4's weights from 
-0.075566791848667 to -0.0777239658063857
-0.284399592237003 to -0.286556766194722
-0.484877192334659 to -0.487034366292378
-0.197942458943851 to -0.20009963290157
-0.43719526465369 to -0.439352438611409
-0.205730044202335 to -0.207887218160054
-0.210088574246891 to -0.21224574820461
-0.671986096457966 to -0.674143270415684
-0.0782900088591651 to -0.0804471828168838
-0.510591947392948 to -0.512749121350667
Changing layer 5's weights from 
0.0772130257325097 to 0.0750558517747911
-0.364860498265751 to -0.36701767222347
0.0603278643326687 to 0.0581706903749501
-0.875184317038783 to -0.877341490996502
-0.337915503339298 to -0.340072677297017
-0.736383446411617 to -0.738540620369336
-0.436418139295108 to -0.438575313252827
-0.363971912221439 to -0.366129086179158
0.0256300932602807 to 0.0234729193025621
0.0191219813065447 to 0.0169648073488261
Trying to learn from memory 36, 2, -0.2
sum 0.0588684497587778 distri 0.0565523774581309
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.524478131856182 to 0.491144798026144
0.802920097436171 to 0.769586763606133
0.715879732693891 to 0.682546398863853
0.0613080281274118 to 0.0279746942973731
Changing layer 0's weights from 
-0.170123622578999 to -0.203456956409037
-0.542050079268834 to -0.575383413098872
-0.776399344963452 to -0.80973267879349
-0.741840095085522 to -0.77517342891556
0.00602747424183219 to -0.0273058595882065
-0.676614478988072 to -0.709947812818111
-0.522784755391499 to -0.556118089221537
-0.550239161414525 to -0.583572495244563
-0.787731022400281 to -0.821064356230319
-0.831481077534577 to -0.864814411364615
Changing layer 1's weights from 
-0.826384165150544 to -0.859717498980582
-0.225215480489156 to -0.258548814319195
-0.350918219251058 to -0.384251553081096
-0.360823557538411 to -0.394156891368449
-0.545876339835545 to -0.579209673665584
-0.684114173812291 to -0.71744750764233
-0.413395450276753 to -0.446728784106791
-0.799198479217908 to -0.832531813047946
-0.387535498303792 to -0.42086883213383
-0.750182361168286 to -0.783515694998324
Changing layer 2's weights from 
0.0590058706575421 to 0.0256725368275034
-0.495047614735982 to -0.52838094856602
-0.210458324117085 to -0.243791657947123
-0.039258227509877 to -0.0725915613399157
-0.216510937375447 to -0.249844271205486
-0.422520563763997 to -0.455853897594035
0.096292152243236 to 0.0629588184131973
0.0238701246553451 to -0.00946320917469365
-0.708128169936558 to -0.741461503766597
-0.663170651358983 to -0.696503985189021
Changing layer 3's weights from 
0.0259555719667461 to -0.00737776186329261
-0.237597510976216 to -0.270930844806255
0.00501240714130712 to -0.0283209266887316
-0.902426824614837 to -0.935760158444875
-0.117663488549611 to -0.15099682237965
-0.247912094754598 to -0.281245428584636
-0.578096941871067 to -0.611430275701106
-0.119584069413564 to -0.152917403243603
-0.451280043286702 to -0.48461337711674
0.0215498350435281 to -0.0117834987865106
Changing layer 4's weights from 
-0.0777239658063857 to -0.111057299636424
-0.286556766194722 to -0.319890100024761
-0.487034366292378 to -0.520367700122416
-0.20009963290157 to -0.233432966731608
-0.439352438611409 to -0.472685772441447
-0.207887218160054 to -0.241220551990092
-0.21224574820461 to -0.245579082034648
-0.674143270415684 to -0.707476604245723
-0.0804471828168838 to -0.113780516646922
-0.512749121350667 to -0.546082455180705
Changing layer 5's weights from 
0.0750558517747911 to 0.0417225179447524
-0.36701767222347 to -0.400351006053508
0.0581706903749501 to 0.0248373565449114
-0.877341490996502 to -0.91067482482654
-0.340072677297017 to -0.373406011127056
-0.738540620369336 to -0.771873954199374
-0.438575313252827 to -0.471908647082865
-0.366129086179158 to -0.399462420009196
0.0234729193025621 to -0.00986041452747663
0.0169648073488261 to -0.0163685264812126
10/5/2016 1:30:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:30 PMStarting learning phase with deltaScore: 3.466667
Modified index 0's learning in memoryPool to 0.6933333
Modified index 1's learning in memoryPool to 0.6933333
Modified index 2's learning in memoryPool to 0.6933333
Modified index 3's learning in memoryPool to 0.6933333
10/5/2016 1:30:31 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 37, 0, 0.6933333
sum 0.0481196656558689 distri -0.0205724170574118
Using diff 0.0566621662993135 and condRate 0.166666666666667
Changed category 0 weights from 
0.143573019708817 to 0.150120647761979
-0.256461944375808 to -0.249914316322646
-0.538982640896614 to -0.532435012843452
-0.387477274928863 to -0.380929646875701
Changing layer 0's weights from 
-0.203456956409037 to -0.196909328355875
-0.575383413098872 to -0.56883578504571
-0.80973267879349 to -0.803185050740328
-0.77517342891556 to -0.768625800862398
-0.0273058595882065 to -0.0207582315350443
-0.709947812818111 to -0.703400184764949
-0.556118089221537 to -0.549570461168375
-0.583572495244563 to -0.577024867191401
-0.821064356230319 to -0.814516728177157
-0.864814411364615 to -0.858266783311453
Changing layer 1's weights from 
-0.859717498980582 to -0.85316987092742
-0.258548814319195 to -0.252001186266032
-0.384251553081096 to -0.377703925027934
-0.394156891368449 to -0.387609263315287
-0.579209673665584 to -0.572662045612422
-0.71744750764233 to -0.710899879589167
-0.446728784106791 to -0.440181156053629
-0.832531813047946 to -0.825984184994784
-0.42086883213383 to -0.414321204080668
-0.783515694998324 to -0.776968066945162
Changing layer 2's weights from 
0.0256725368275034 to 0.0322201648806656
-0.52838094856602 to -0.521833320512858
-0.243791657947123 to -0.237244029893961
-0.0725915613399157 to -0.0660439332867535
-0.249844271205486 to -0.243296643152323
-0.455853897594035 to -0.449306269540873
0.0629588184131973 to 0.0695064464663595
-0.00946320917469365 to -0.00291558112153142
-0.741461503766597 to -0.734913875713435
-0.696503985189021 to -0.689956357135859
Changing layer 3's weights from 
-0.00737776186329261 to -0.000830133810130381
-0.270930844806255 to -0.264383216753092
-0.0283209266887316 to -0.0217732986355694
-0.935760158444875 to -0.929212530391713
-0.15099682237965 to -0.144449194326487
-0.281245428584636 to -0.274697800531474
-0.611430275701106 to -0.604882647647944
-0.152917403243603 to -0.14636977519044
-0.48461337711674 to -0.478065749063578
-0.0117834987865106 to -0.00523587073334837
Changing layer 4's weights from 
-0.111057299636424 to -0.104509671583262
-0.319890100024761 to -0.313342471971598
-0.520367700122416 to -0.513820072069254
-0.233432966731608 to -0.226885338678446
-0.472685772441447 to -0.466138144388285
-0.241220551990092 to -0.23467292393693
-0.245579082034648 to -0.239031453981486
-0.707476604245723 to -0.700928976192561
-0.113780516646922 to -0.10723288859376
-0.546082455180705 to -0.539534827127543
Changing layer 5's weights from 
0.0417225179447524 to 0.0482701459979146
-0.400351006053508 to -0.393803378000346
0.0248373565449114 to 0.0313849845980736
-0.91067482482654 to -0.904127196773378
-0.373406011127056 to -0.366858383073893
-0.771873954199374 to -0.765326326146212
-0.471908647082865 to -0.465361019029703
-0.399462420009196 to -0.392914791956034
-0.00986041452747663 to -0.0033127864743144
-0.0163685264812126 to -0.00982089842805038
Trying to learn from memory 38, 1, 0.6933333
sum 0.0384821919497375 distri 0.0199146350206651
Using diff 0.00894700894163803 and condRate 0.166666666666667
Changed category 1 weights from 
0.284142957236464 to 0.28517683381698
0.517458544280225 to 0.518492420860741
0.0925333063861677 to 0.0935671829666837
0.0966261189243147 to 0.0976599955048306
Changing layer 0's weights from 
-0.196909328355875 to -0.195875451775359
-0.56883578504571 to -0.567801908465194
-0.803185050740328 to -0.802151174159812
-0.768625800862398 to -0.767591924281882
-0.0207582315350443 to -0.0197243549545283
-0.703400184764949 to -0.702366308184433
-0.549570461168375 to -0.548536584587859
-0.577024867191401 to -0.575990990610885
-0.814516728177157 to -0.813482851596641
-0.858266783311453 to -0.857232906730937
Changing layer 1's weights from 
-0.85316987092742 to -0.852135994346904
-0.252001186266032 to -0.250967309685516
-0.377703925027934 to -0.376670048447418
-0.387609263315287 to -0.386575386734771
-0.572662045612422 to -0.571628169031906
-0.710899879589167 to -0.709866003008651
-0.440181156053629 to -0.439147279473113
-0.825984184994784 to -0.824950308414268
-0.414321204080668 to -0.413287327500152
-0.776968066945162 to -0.775934190364646
Changing layer 2's weights from 
0.0322201648806656 to 0.0332540414611816
-0.521833320512858 to -0.520799443932342
-0.237244029893961 to -0.236210153313445
-0.0660439332867535 to -0.0650100567062375
-0.243296643152323 to -0.242262766571807
-0.449306269540873 to -0.448272392960357
0.0695064464663595 to 0.0705403230468755
-0.00291558112153142 to -0.00188170454101543
-0.734913875713435 to -0.733879999132919
-0.689956357135859 to -0.688922480555343
Changing layer 3's weights from 
-0.000830133810130381 to 0.000203742770385606
-0.264383216753092 to -0.263349340172576
-0.0217732986355694 to -0.0207394220550534
-0.929212530391713 to -0.928178653811197
-0.144449194326487 to -0.143415317745971
-0.274697800531474 to -0.273663923950958
-0.604882647647944 to -0.603848771067428
-0.14636977519044 to -0.145335898609924
-0.478065749063578 to -0.477031872483062
-0.00523587073334837 to -0.00420199415283238
Changing layer 4's weights from 
-0.104509671583262 to -0.103475795002746
-0.313342471971598 to -0.312308595391082
-0.513820072069254 to -0.512786195488738
-0.226885338678446 to -0.22585146209793
-0.466138144388285 to -0.465104267807769
-0.23467292393693 to -0.233639047356414
-0.239031453981486 to -0.23799757740097
-0.700928976192561 to -0.699895099612045
-0.10723288859376 to -0.106199012013244
-0.539534827127543 to -0.538500950547027
Changing layer 5's weights from 
0.0482701459979146 to 0.0493040225784306
-0.393803378000346 to -0.39276950141983
0.0313849845980736 to 0.0324188611785896
-0.904127196773378 to -0.903093320192862
-0.366858383073893 to -0.365824506493377
-0.765326326146212 to -0.764292449565696
-0.465361019029703 to -0.464327142449187
-0.392914791956034 to -0.391880915375518
-0.0033127864743144 to -0.00227890989379841
-0.00982089842805038 to -0.00878702184753439
Trying to learn from memory 39, 1, 0.6933333
sum 0.0384821919497375 distri 0.0199146350206651
Using diff 0.00894700894163803 and condRate 0.166666666666667
Changed category 1 weights from 
0.28517683381698 to 0.286210710397496
0.518492420860741 to 0.519526297441257
0.0935671829666837 to 0.0946010595471997
0.0976599955048306 to 0.0986938720853466
Changing layer 0's weights from 
-0.195875451775359 to -0.194841575194843
-0.567801908465194 to -0.566768031884678
-0.802151174159812 to -0.801117297579296
-0.767591924281882 to -0.766558047701366
-0.0197243549545283 to -0.0186904783740123
-0.702366308184433 to -0.701332431603917
-0.548536584587859 to -0.547502708007343
-0.575990990610885 to -0.574957114030369
-0.813482851596641 to -0.812448975016125
-0.857232906730937 to -0.856199030150421
Changing layer 1's weights from 
-0.852135994346904 to -0.851102117766388
-0.250967309685516 to -0.249933433105
-0.376670048447418 to -0.375636171866902
-0.386575386734771 to -0.385541510154255
-0.571628169031906 to -0.570594292451389
-0.709866003008651 to -0.708832126428135
-0.439147279473113 to -0.438113402892597
-0.824950308414268 to -0.823916431833752
-0.413287327500152 to -0.412253450919636
-0.775934190364646 to -0.77490031378413
Changing layer 2's weights from 
0.0332540414611816 to 0.0342879180416976
-0.520799443932342 to -0.519765567351826
-0.236210153313445 to -0.235176276732929
-0.0650100567062375 to -0.0639761801257215
-0.242262766571807 to -0.241228889991291
-0.448272392960357 to -0.447238516379841
0.0705403230468755 to 0.0715741996273915
-0.00188170454101543 to -0.00084782796049944
-0.733879999132919 to -0.732846122552403
-0.688922480555343 to -0.687888603974827
Changing layer 3's weights from 
0.000203742770385606 to 0.00123761935090159
-0.263349340172576 to -0.26231546359206
-0.0207394220550534 to -0.0197055454745374
-0.928178653811197 to -0.927144777230681
-0.143415317745971 to -0.142381441165455
-0.273663923950958 to -0.272630047370442
-0.603848771067428 to -0.602814894486912
-0.145335898609924 to -0.144302022029408
-0.477031872483062 to -0.475997995902546
-0.00420199415283238 to -0.00316811757231639
Changing layer 4's weights from 
-0.103475795002746 to -0.10244191842223
-0.312308595391082 to -0.311274718810566
-0.512786195488738 to -0.511752318908222
-0.22585146209793 to -0.224817585517414
-0.465104267807769 to -0.464070391227253
-0.233639047356414 to -0.232605170775898
-0.23799757740097 to -0.236963700820454
-0.699895099612045 to -0.698861223031529
-0.106199012013244 to -0.105165135432728
-0.538500950547027 to -0.537467073966511
Changing layer 5's weights from 
0.0493040225784306 to 0.0503378991589466
-0.39276950141983 to -0.391735624839314
0.0324188611785896 to 0.0334527377591056
-0.903093320192862 to -0.902059443612346
-0.365824506493377 to -0.364790629912861
-0.764292449565696 to -0.76325857298518
-0.464327142449187 to -0.463293265868671
-0.391880915375518 to -0.390847038795002
-0.00227890989379841 to -0.00124503331328242
-0.00878702184753439 to -0.0077531452670184
Trying to learn from memory 40, 1, 0.6933333
sum 0.0384821919497375 distri 0.0199146350206651
Using diff 0.00894700894163803 and condRate 0.166666666666667
Changed category 1 weights from 
0.286210710397496 to 0.287244586978012
0.519526297441257 to 0.520560174021773
0.0946010595471997 to 0.0956349361277156
0.0986938720853466 to 0.0997277486658626
Changing layer 0's weights from 
-0.194841575194843 to -0.193807698614327
-0.566768031884678 to -0.565734155304162
-0.801117297579296 to -0.80008342099878
-0.766558047701366 to -0.76552417112085
-0.0186904783740123 to -0.0176566017934963
-0.701332431603917 to -0.700298555023401
-0.547502708007343 to -0.546468831426827
-0.574957114030369 to -0.573923237449853
-0.812448975016125 to -0.811415098435609
-0.856199030150421 to -0.855165153569905
Changing layer 1's weights from 
-0.851102117766388 to -0.850068241185872
-0.249933433105 to -0.248899556524484
-0.375636171866902 to -0.374602295286386
-0.385541510154255 to -0.384507633573739
-0.570594292451389 to -0.569560415870873
-0.708832126428135 to -0.707798249847619
-0.438113402892597 to -0.437079526312081
-0.823916431833752 to -0.822882555253236
-0.412253450919636 to -0.41121957433912
-0.77490031378413 to -0.773866437203614
Changing layer 2's weights from 
0.0342879180416976 to 0.0353217946222135
-0.519765567351826 to -0.51873169077131
-0.235176276732929 to -0.234142400152413
-0.0639761801257215 to -0.0629423035452055
-0.241228889991291 to -0.240195013410775
-0.447238516379841 to -0.446204639799325
0.0715741996273915 to 0.0726080762079075
-0.00084782796049944 to 0.000186048620016548
-0.732846122552403 to -0.731812245971887
-0.687888603974827 to -0.686854727394311
Changing layer 3's weights from 
0.00123761935090159 to 0.00227149593141758
-0.26231546359206 to -0.261281587011544
-0.0197055454745374 to -0.0186716688940214
-0.927144777230681 to -0.926110900650165
-0.142381441165455 to -0.141347564584939
-0.272630047370442 to -0.271596170789926
-0.602814894486912 to -0.601781017906396
-0.144302022029408 to -0.143268145448892
-0.475997995902546 to -0.47496411932203
-0.00316811757231639 to -0.00213424099180041
Changing layer 4's weights from 
-0.10244191842223 to -0.101408041841714
-0.311274718810566 to -0.31024084223005
-0.511752318908222 to -0.510718442327706
-0.224817585517414 to -0.223783708936898
-0.464070391227253 to -0.463036514646737
-0.232605170775898 to -0.231571294195382
-0.236963700820454 to -0.235929824239938
-0.698861223031529 to -0.697827346451013
-0.105165135432728 to -0.104131258852212
-0.537467073966511 to -0.536433197385995
Changing layer 5's weights from 
0.0503378991589466 to 0.0513717757394626
-0.391735624839314 to -0.390701748258798
0.0334527377591056 to 0.0344866143396216
-0.902059443612346 to -0.90102556703183
-0.364790629912861 to -0.363756753332345
-0.76325857298518 to -0.762224696404664
-0.463293265868671 to -0.462259389288155
-0.390847038795002 to -0.389813162214486
-0.00124503331328242 to -0.000211156732766433
-0.0077531452670184 to -0.00671926868650242
10/5/2016 1:30:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:30:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:31:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:32 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
Modified index 9's learning in memoryPool to -0.2
Modified index 10's learning in memoryPool to -0.2
Modified index 11's learning in memoryPool to -0.2
Modified index 12's learning in memoryPool to -0.2
Modified index 13's learning in memoryPool to -0.2
Modified index 14's learning in memoryPool to -0.2
Modified index 15's learning in memoryPool to -0.2
Modified index 16's learning in memoryPool to -0.2
Modified index 17's learning in memoryPool to -0.2
Modified index 18's learning in memoryPool to -0.2
Modified index 19's learning in memoryPool to -0.2
Modified index 20's learning in memoryPool to -0.2
Modified index 21's learning in memoryPool to -0.2
Modified index 22's learning in memoryPool to -0.2
Modified index 23's learning in memoryPool to -0.2
Modified index 24's learning in memoryPool to -0.2
Modified index 25's learning in memoryPool to -0.2
10/5/2016 1:32:32 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 41, 0, -0.2
sum 0.0388461499043899 distri -0.0202854827590359
Using diff 0.0494200951873283 and condRate 0.166666666666667
Changed category 0 weights from 
0.150120647761979 to 0.148473311231187
-0.249914316322646 to -0.251561652853438
-0.532435012843452 to -0.534082349374243
-0.380929646875701 to -0.382576983406493
Changing layer 0's weights from 
-0.193807698614327 to -0.195455035145119
-0.565734155304162 to -0.567381491834954
-0.80008342099878 to -0.801730757529572
-0.76552417112085 to -0.767171507651642
-0.0176566017934963 to -0.0193039383242878
-0.700298555023401 to -0.701945891554192
-0.546468831426827 to -0.548116167957619
-0.573923237449853 to -0.575570573980644
-0.811415098435609 to -0.813062434966401
-0.855165153569905 to -0.856812490100697
Changing layer 1's weights from 
-0.850068241185872 to -0.851715577716664
-0.248899556524484 to -0.250546893055276
-0.374602295286386 to -0.376249631817178
-0.384507633573739 to -0.386154970104531
-0.569560415870873 to -0.571207752401665
-0.707798249847619 to -0.709445586378411
-0.437079526312081 to -0.438726862842873
-0.822882555253236 to -0.824529891784028
-0.41121957433912 to -0.412866910869912
-0.773866437203614 to -0.775513773734406
Changing layer 2's weights from 
0.0353217946222135 to 0.033674458091422
-0.51873169077131 to -0.520379027302102
-0.234142400152413 to -0.235789736683205
-0.0629423035452055 to -0.064589640075997
-0.240195013410775 to -0.241842349941567
-0.446204639799325 to -0.447851976330117
0.0726080762079075 to 0.070960739677116
0.000186048620016548 to -0.00146128791077496
-0.731812245971887 to -0.733459582502678
-0.686854727394311 to -0.688502063925103
Changing layer 3's weights from 
0.00227149593141758 to 0.000624159400626079
-0.261281587011544 to -0.262928923542336
-0.0186716688940214 to -0.0203190054248129
-0.926110900650165 to -0.927758237180957
-0.141347564584939 to -0.142994901115731
-0.271596170789926 to -0.273243507320718
-0.601781017906396 to -0.603428354437187
-0.143268145448892 to -0.144915481979684
-0.47496411932203 to -0.476611455852822
-0.00213424099180041 to -0.00378157752259191
Changing layer 4's weights from 
-0.101408041841714 to -0.103055378372506
-0.31024084223005 to -0.311888178760842
-0.510718442327706 to -0.512365778858498
-0.223783708936898 to -0.22543104546769
-0.463036514646737 to -0.464683851177529
-0.231571294195382 to -0.233218630726174
-0.235929824239938 to -0.23757716077073
-0.697827346451013 to -0.699474682981804
-0.104131258852212 to -0.105778595383004
-0.536433197385995 to -0.538080533916787
Changing layer 5's weights from 
0.0513717757394626 to 0.0497244392086711
-0.390701748258798 to -0.39234908478959
0.0344866143396216 to 0.0328392778088301
-0.90102556703183 to -0.902672903562622
-0.363756753332345 to -0.365404089863137
-0.762224696404664 to -0.763872032935456
-0.462259389288155 to -0.463906725818947
-0.389813162214486 to -0.391460498745278
-0.000211156732766433 to -0.00185849326355794
-0.00671926868650242 to -0.00836660521729392
Trying to learn from memory 42, 1, -0.2
sum 0.038846144111029 distri 0.0199489364447839
Using diff 0.00918567163848786 and condRate 0.166666666666667
Changed category 1 weights from 
0.287244586978012 to 0.286938397918833
0.520560174021773 to 0.520253984962594
0.0956349361277156 to 0.0953287470685368
0.0997277486658626 to 0.0994215596066838
Changing layer 0's weights from 
-0.195455035145119 to -0.195761224204298
-0.567381491834954 to -0.567687680894132
-0.801730757529572 to -0.80203694658875
-0.767171507651642 to -0.76747769671082
-0.0193039383242878 to -0.0196101273834667
-0.701945891554192 to -0.702252080613371
-0.548116167957619 to -0.548422357016798
-0.575570573980644 to -0.575876763039823
-0.813062434966401 to -0.813368624025579
-0.856812490100697 to -0.857118679159875
Changing layer 1's weights from 
-0.851715577716664 to -0.852021766775842
-0.250546893055276 to -0.250853082114455
-0.376249631817178 to -0.376555820876357
-0.386154970104531 to -0.38646115916371
-0.571207752401665 to -0.571513941460844
-0.709445586378411 to -0.70975177543759
-0.438726862842873 to -0.439033051902052
-0.824529891784028 to -0.824836080843206
-0.412866910869912 to -0.41317309992909
-0.775513773734406 to -0.775819962793584
Changing layer 2's weights from 
0.033674458091422 to 0.0333682690322432
-0.520379027302102 to -0.520685216361281
-0.235789736683205 to -0.236095925742384
-0.064589640075997 to -0.0648958291351758
-0.241842349941567 to -0.242148539000746
-0.447851976330117 to -0.448158165389295
0.070960739677116 to 0.0706545506179371
-0.00146128791077496 to -0.00176747696995379
-0.733459582502678 to -0.733765771561857
-0.688502063925103 to -0.688808252984282
Changing layer 3's weights from 
0.000624159400626079 to 0.000317970341447244
-0.262928923542336 to -0.263235112601515
-0.0203190054248129 to -0.0206251944839917
-0.927758237180957 to -0.928064426240135
-0.142994901115731 to -0.14330109017491
-0.273243507320718 to -0.273549696379897
-0.603428354437187 to -0.603734543496366
-0.144915481979684 to -0.145221671038863
-0.476611455852822 to -0.476917644912001
-0.00378157752259191 to -0.00408776658177074
Changing layer 4's weights from 
-0.103055378372506 to -0.103361567431685
-0.311888178760842 to -0.312194367820021
-0.512365778858498 to -0.512671967917676
-0.22543104546769 to -0.225737234526869
-0.464683851177529 to -0.464990040236708
-0.233218630726174 to -0.233524819785353
-0.23757716077073 to -0.237883349829909
-0.699474682981804 to -0.699780872040983
-0.105778595383004 to -0.106084784442183
-0.538080533916787 to -0.538386722975965
Changing layer 5's weights from 
0.0497244392086711 to 0.0494182501494922
-0.39234908478959 to -0.392655273848769
0.0328392778088301 to 0.0325330887496512
-0.902672903562622 to -0.9029790926218
-0.365404089863137 to -0.365710278922316
-0.763872032935456 to -0.764178221994634
-0.463906725818947 to -0.464212914878125
-0.391460498745278 to -0.391766687804457
-0.00185849326355794 to -0.00216468232273677
-0.00836660521729392 to -0.00867279427647275
Trying to learn from memory 43, 0, -0.2
sum 0.038846144111029 distri -0.0202854900355273
Using diff 0.049420098118799 and condRate 0.166666666666667
Changed category 0 weights from 
0.148473311231187 to 0.14682597460268
-0.251561652853438 to -0.253208989481945
-0.534082349374243 to -0.535729686002751
-0.382576983406493 to -0.384224320035
Changing layer 0's weights from 
-0.195761224204298 to -0.197408560832805
-0.567687680894132 to -0.56933501752264
-0.80203694658875 to -0.803684283217258
-0.76747769671082 to -0.769125033339328
-0.0196101273834667 to -0.0212574640119738
-0.702252080613371 to -0.703899417241878
-0.548422357016798 to -0.550069693645305
-0.575876763039823 to -0.577524099668331
-0.813368624025579 to -0.815015960654087
-0.857118679159875 to -0.858766015788383
Changing layer 1's weights from 
-0.852021766775842 to -0.85366910340435
-0.250853082114455 to -0.252500418742962
-0.376555820876357 to -0.378203157504864
-0.38646115916371 to -0.388108495792217
-0.571513941460844 to -0.573161278089351
-0.70975177543759 to -0.711399112066097
-0.439033051902052 to -0.440680388530559
-0.824836080843206 to -0.826483417471714
-0.41317309992909 to -0.414820436557598
-0.775819962793584 to -0.777467299422092
Changing layer 2's weights from 
0.0333682690322432 to 0.031720932403736
-0.520685216361281 to -0.522332552989788
-0.236095925742384 to -0.237743262370891
-0.0648958291351758 to -0.066543165763683
-0.242148539000746 to -0.243795875629253
-0.448158165389295 to -0.449805502017803
0.0706545506179371 to 0.06900721398943
-0.00176747696995379 to -0.00341481359846099
-0.733765771561857 to -0.735413108190364
-0.688808252984282 to -0.690455589612789
Changing layer 3's weights from 
0.000317970341447244 to -0.00132936628705995
-0.263235112601515 to -0.264882449230022
-0.0206251944839917 to -0.0222725311124989
-0.928064426240135 to -0.929711762868643
-0.14330109017491 to -0.144948426803417
-0.273549696379897 to -0.275197033008404
-0.603734543496366 to -0.605381880124873
-0.145221671038863 to -0.14686900766737
-0.476917644912001 to -0.478564981540508
-0.00408776658177074 to -0.00573510321027794
Changing layer 4's weights from 
-0.103361567431685 to -0.105008904060192
-0.312194367820021 to -0.313841704448528
-0.512671967917676 to -0.514319304546184
-0.225737234526869 to -0.227384571155376
-0.464990040236708 to -0.466637376865215
-0.233524819785353 to -0.23517215641386
-0.237883349829909 to -0.239530686458416
-0.699780872040983 to -0.70142820866949
-0.106084784442183 to -0.10773212107069
-0.538386722975965 to -0.540034059604473
Changing layer 5's weights from 
0.0494182501494922 to 0.047770913520985
-0.392655273848769 to -0.394302610477276
0.0325330887496512 to 0.030885752121144
-0.9029790926218 to -0.904626429250308
-0.365710278922316 to -0.367357615550823
-0.764178221994634 to -0.765825558623142
-0.464212914878125 to -0.465860251506633
-0.391766687804457 to -0.393414024432964
-0.00216468232273677 to -0.00381201895124397
-0.00867279427647275 to -0.0103201309049799
Trying to learn from memory 44, 1, -0.2
sum 0.038846144111029 distri 0.0199489364447839
Using diff 0.00918567163848786 and condRate 0.166666666666667
Changed category 1 weights from 
0.286938397918833 to 0.286632208859654
0.520253984962594 to 0.519947795903416
0.0953287470685368 to 0.095022558009358
0.0994215596066838 to 0.0991153705475049
Changing layer 0's weights from 
-0.197408560832805 to -0.197714749891984
-0.56933501752264 to -0.569641206581818
-0.803684283217258 to -0.803990472276437
-0.769125033339328 to -0.769431222398506
-0.0212574640119738 to -0.0215636530711527
-0.703899417241878 to -0.704205606301057
-0.550069693645305 to -0.550375882704484
-0.577524099668331 to -0.577830288727509
-0.815015960654087 to -0.815322149713265
-0.858766015788383 to -0.859072204847561
Changing layer 1's weights from 
-0.85366910340435 to -0.853975292463528
-0.252500418742962 to -0.252806607802141
-0.378203157504864 to -0.378509346564043
-0.388108495792217 to -0.388414684851396
-0.573161278089351 to -0.57346746714853
-0.711399112066097 to -0.711705301125276
-0.440680388530559 to -0.440986577589738
-0.826483417471714 to -0.826789606530893
-0.414820436557598 to -0.415126625616776
-0.777467299422092 to -0.77777348848127
Changing layer 2's weights from 
0.031720932403736 to 0.0314147433445572
-0.522332552989788 to -0.522638742048967
-0.237743262370891 to -0.23804945143007
-0.066543165763683 to -0.0668493548228619
-0.243795875629253 to -0.244102064688432
-0.449805502017803 to -0.450111691076982
0.06900721398943 to 0.0687010249302511
-0.00341481359846099 to -0.00372100265763982
-0.735413108190364 to -0.735719297249543
-0.690455589612789 to -0.690761778671968
Changing layer 3's weights from 
-0.00132936628705995 to -0.00163555534623879
-0.264882449230022 to -0.265188638289201
-0.0222725311124989 to -0.0225787201716778
-0.929711762868643 to -0.930017951927821
-0.144948426803417 to -0.145254615862596
-0.275197033008404 to -0.275503222067583
-0.605381880124873 to -0.605688069184052
-0.14686900766737 to -0.147175196726549
-0.478564981540508 to -0.478871170599687
-0.00573510321027794 to -0.00604129226945677
Changing layer 4's weights from 
-0.105008904060192 to -0.105315093119371
-0.313841704448528 to -0.314147893507707
-0.514319304546184 to -0.514625493605362
-0.227384571155376 to -0.227690760214555
-0.466637376865215 to -0.466943565924394
-0.23517215641386 to -0.235478345473039
-0.239530686458416 to -0.239836875517595
-0.70142820866949 to -0.701734397728669
-0.10773212107069 to -0.108038310129869
-0.540034059604473 to -0.540340248663651
Changing layer 5's weights from 
0.047770913520985 to 0.0474647244618062
-0.394302610477276 to -0.394608799536455
0.030885752121144 to 0.0305795630619652
-0.904626429250308 to -0.904932618309486
-0.367357615550823 to -0.367663804610002
-0.765825558623142 to -0.766131747682321
-0.465860251506633 to -0.466166440565812
-0.393414024432964 to -0.393720213492143
-0.00381201895124397 to -0.0041182080104228
-0.0103201309049799 to -0.0106263199641588
Trying to learn from memory 45, 1, -0.2
sum 0.038846144111029 distri 0.0199489364447839
Using diff 0.00918567163848786 and condRate 0.166666666666667
Changed category 1 weights from 
0.286632208859654 to 0.286326019800475
0.519947795903416 to 0.519641606844237
0.095022558009358 to 0.0947163689501791
0.0991153705475049 to 0.0988091814883261
Changing layer 0's weights from 
-0.197714749891984 to -0.198020938951162
-0.569641206581818 to -0.569947395640997
-0.803990472276437 to -0.804296661335615
-0.769431222398506 to -0.769737411457685
-0.0215636530711527 to -0.0218698421303315
-0.704205606301057 to -0.704511795360236
-0.550375882704484 to -0.550682071763662
-0.577830288727509 to -0.578136477786688
-0.815322149713265 to -0.815628338772444
-0.859072204847561 to -0.85937839390674
Changing layer 1's weights from 
-0.853975292463528 to -0.854281481522707
-0.252806607802141 to -0.25311279686132
-0.378509346564043 to -0.378815535623221
-0.388414684851396 to -0.388720873910574
-0.57346746714853 to -0.573773656207709
-0.711705301125276 to -0.712011490184455
-0.440986577589738 to -0.441292766648916
-0.826789606530893 to -0.827095795590071
-0.415126625616776 to -0.415432814675955
-0.77777348848127 to -0.778079677540449
Changing layer 2's weights from 
0.0314147433445572 to 0.0311085542853783
-0.522638742048967 to -0.522944931108145
-0.23804945143007 to -0.238355640489249
-0.0668493548228619 to -0.0671555438820407
-0.244102064688432 to -0.244408253747611
-0.450111691076982 to -0.45041788013616
0.0687010249302511 to 0.0683948358710723
-0.00372100265763982 to -0.00402719171681865
-0.735719297249543 to -0.736025486308722
-0.690761778671968 to -0.691067967731146
Changing layer 3's weights from 
-0.00163555534623879 to -0.00194174440541762
-0.265188638289201 to -0.265494827348379
-0.0225787201716778 to -0.0228849092308566
-0.930017951927821 to -0.930324140987
-0.145254615862596 to -0.145560804921775
-0.275503222067583 to -0.275809411126761
-0.605688069184052 to -0.605994258243231
-0.147175196726549 to -0.147481385785728
-0.478871170599687 to -0.479177359658865
-0.00604129226945677 to -0.00634748132863561
Changing layer 4's weights from 
-0.105315093119371 to -0.105621282178549
-0.314147893507707 to -0.314454082566886
-0.514625493605362 to -0.514931682664541
-0.227690760214555 to -0.227996949273733
-0.466943565924394 to -0.467249754983572
-0.235478345473039 to -0.235784534532217
-0.239836875517595 to -0.240143064576773
-0.701734397728669 to -0.702040586787848
-0.108038310129869 to -0.108344499189048
-0.540340248663651 to -0.54064643772283
Changing layer 5's weights from 
0.0474647244618062 to 0.0471585354026274
-0.394608799536455 to -0.394914988595633
0.0305795630619652 to 0.0302733740027864
-0.904932618309486 to -0.905238807368665
-0.367663804610002 to -0.36796999366918
-0.766131747682321 to -0.766437936741499
-0.466166440565812 to -0.46647262962499
-0.393720213492143 to -0.394026402551321
-0.0041182080104228 to -0.00442439706960164
-0.0106263199641588 to -0.0109325090233376
Trying to learn from memory 46, 1, -0.2
sum 0.0388460419749235 distri 0.0199489918459666
Using diff 0.00918553963522598 and condRate 0.166666666666667
Changed category 1 weights from 
0.286326019800475 to 0.286019835141405
0.519641606844237 to 0.519335422185167
0.0947163689501791 to 0.0944101842911091
0.0988091814883261 to 0.0985029968292561
Changing layer 0's weights from 
-0.198020938951162 to -0.198327123610233
-0.569947395640997 to -0.570253580300067
-0.804296661335615 to -0.804602845994685
-0.769737411457685 to -0.770043596116755
-0.0218698421303315 to -0.0221760267894016
-0.704511795360236 to -0.704817980019306
-0.550682071763662 to -0.550988256422732
-0.578136477786688 to -0.578442662445758
-0.815628338772444 to -0.815934523431514
-0.85937839390674 to -0.85968457856581
Changing layer 1's weights from 
-0.854281481522707 to -0.854587666181777
-0.25311279686132 to -0.25341898152039
-0.378815535623221 to -0.379121720282291
-0.388720873910574 to -0.389027058569644
-0.573773656207709 to -0.574079840866779
-0.712011490184455 to -0.712317674843525
-0.441292766648916 to -0.441598951307986
-0.827095795590071 to -0.827401980249141
-0.415432814675955 to -0.415738999335025
-0.778079677540449 to -0.778385862199519
Changing layer 2's weights from 
0.0311085542853783 to 0.0308023696263083
-0.522944931108145 to -0.523251115767215
-0.238355640489249 to -0.238661825148319
-0.0671555438820407 to -0.0674617285411108
-0.244408253747611 to -0.244714438406681
-0.45041788013616 to -0.45072406479523
0.0683948358710723 to 0.0680886512120022
-0.00402719171681865 to -0.00433337637588869
-0.736025486308722 to -0.736331670967792
-0.691067967731146 to -0.691374152390216
Changing layer 3's weights from 
-0.00194174440541762 to -0.00224792906448766
-0.265494827348379 to -0.26580101200745
-0.0228849092308566 to -0.0231910938899266
-0.930324140987 to -0.93063032564607
-0.145560804921775 to -0.145866989580845
-0.275809411126761 to -0.276115595785832
-0.605994258243231 to -0.606300442902301
-0.147481385785728 to -0.147787570444798
-0.479177359658865 to -0.479483544317935
-0.00634748132863561 to -0.00665366598770565
Changing layer 4's weights from 
-0.105621282178549 to -0.105927466837619
-0.314454082566886 to -0.314760267225956
-0.514931682664541 to -0.515237867323611
-0.227996949273733 to -0.228303133932803
-0.467249754983572 to -0.467555939642642
-0.235784534532217 to -0.236090719191287
-0.240143064576773 to -0.240449249235844
-0.702040586787848 to -0.702346771446918
-0.108344499189048 to -0.108650683848118
-0.54064643772283 to -0.5409526223819
Changing layer 5's weights from 
0.0471585354026274 to 0.0468523507435573
-0.394914988595633 to -0.395221173254704
0.0302733740027864 to 0.0299671893437163
-0.905238807368665 to -0.905544992027735
-0.36796999366918 to -0.368276178328251
-0.766437936741499 to -0.766744121400569
-0.46647262962499 to -0.46677881428406
-0.394026402551321 to -0.394332587210391
-0.00442439706960164 to -0.00473058172867168
-0.0109325090233376 to -0.0112386936824077
Trying to learn from memory 47, 0, -0.2
sum 0.0388477900181229 distri -0.0202912792058014
Using diff 0.0494271217193936 and condRate 0.166666666666667
Changed category 0 weights from 
0.14682597460268 to 0.14517840385415
-0.253208989481945 to -0.254856560230475
-0.535729686002751 to -0.537377256751281
-0.384224320035 to -0.38587189078353
Changing layer 0's weights from 
-0.198327123610233 to -0.199974694358763
-0.570253580300067 to -0.571901151048598
-0.804602845994685 to -0.806250416743216
-0.770043596116755 to -0.771691166865286
-0.0221760267894016 to -0.0238235975379321
-0.704817980019306 to -0.706465550767836
-0.550988256422732 to -0.552635827171263
-0.578442662445758 to -0.580090233194289
-0.815934523431514 to -0.817582094180045
-0.85968457856581 to -0.861332149314341
Changing layer 1's weights from 
-0.854587666181777 to -0.856235236930308
-0.25341898152039 to -0.25506655226892
-0.379121720282291 to -0.380769291030822
-0.389027058569644 to -0.390674629318175
-0.574079840866779 to -0.575727411615309
-0.712317674843525 to -0.713965245592055
-0.441598951307986 to -0.443246522056517
-0.827401980249141 to -0.829049550997672
-0.415738999335025 to -0.417386570083556
-0.778385862199519 to -0.78003343294805
Changing layer 2's weights from 
0.0308023696263083 to 0.0291547988777778
-0.523251115767215 to -0.524898686515746
-0.238661825148319 to -0.240309395896849
-0.0674617285411108 to -0.0691092992896413
-0.244714438406681 to -0.246362009155211
-0.45072406479523 to -0.452371635543761
0.0680886512120022 to 0.0664410804634717
-0.00433337637588869 to -0.0059809471244192
-0.736331670967792 to -0.737979241716322
-0.691374152390216 to -0.693021723138747
Changing layer 3's weights from 
-0.00224792906448766 to -0.00389549981301816
-0.26580101200745 to -0.26744858275598
-0.0231910938899266 to -0.0248386646384571
-0.93063032564607 to -0.932277896394601
-0.145866989580845 to -0.147514560329375
-0.276115595785832 to -0.277763166534362
-0.606300442902301 to -0.607948013650831
-0.147787570444798 to -0.149435141193328
-0.479483544317935 to -0.481131115066466
-0.00665366598770565 to -0.00830123673623615
Changing layer 4's weights from 
-0.105927466837619 to -0.10757503758615
-0.314760267225956 to -0.316407837974486
-0.515237867323611 to -0.516885438072142
-0.228303133932803 to -0.229950704681334
-0.467555939642642 to -0.469203510391173
-0.236090719191287 to -0.237738289939818
-0.240449249235844 to -0.242096819984374
-0.702346771446918 to -0.703994342195449
-0.108650683848118 to -0.110298254596648
-0.5409526223819 to -0.542600193130431
Changing layer 5's weights from 
0.0468523507435573 to 0.0452047799950268
-0.395221173254704 to -0.396868744003234
0.0299671893437163 to 0.0283196185951858
-0.905544992027735 to -0.907192562776266
-0.368276178328251 to -0.369923749076781
-0.766744121400569 to -0.7683916921491
-0.46677881428406 to -0.468426385032591
-0.394332587210391 to -0.395980157958922
-0.00473058172867168 to -0.00637815247720218
-0.0112386936824077 to -0.0128862644309382
Trying to learn from memory 48, 0, -0.2
sum 0.0388648310459114 distri -0.0202991128257295
Using diff 0.049447736110163 and condRate 0.166666666666667
Changed category 0 weights from 
0.14517840385415 to 0.14353014595925
-0.254856560230475 to -0.256504818125375
-0.537377256751281 to -0.539025514646181
-0.38587189078353 to -0.38752014867843
Changing layer 0's weights from 
-0.199974694358763 to -0.201622952253663
-0.571901151048598 to -0.573549408943497
-0.806250416743216 to -0.807898674638116
-0.771691166865286 to -0.773339424760186
-0.0238235975379321 to -0.0254718554328318
-0.706465550767836 to -0.708113808662736
-0.552635827171263 to -0.554284085066163
-0.580090233194289 to -0.581738491089188
-0.817582094180045 to -0.819230352074945
-0.861332149314341 to -0.862980407209241
Changing layer 1's weights from 
-0.856235236930308 to -0.857883494825208
-0.25506655226892 to -0.25671481016382
-0.380769291030822 to -0.382417548925722
-0.390674629318175 to -0.392322887213075
-0.575727411615309 to -0.577375669510209
-0.713965245592055 to -0.715613503486955
-0.443246522056517 to -0.444894779951417
-0.829049550997672 to -0.830697808892572
-0.417386570083556 to -0.419034827978456
-0.78003343294805 to -0.78168169084295
Changing layer 2's weights from 
0.0291547988777778 to 0.0275065409828781
-0.524898686515746 to -0.526546944410646
-0.240309395896849 to -0.241957653791749
-0.0691092992896413 to -0.070757557184541
-0.246362009155211 to -0.248010267050111
-0.452371635543761 to -0.454019893438661
0.0664410804634717 to 0.064792822568572
-0.0059809471244192 to -0.00762920501931892
-0.737979241716322 to -0.739627499611222
-0.693021723138747 to -0.694669981033647
Changing layer 3's weights from 
-0.00389549981301816 to -0.00554375770791789
-0.26744858275598 to -0.26909684065088
-0.0248386646384571 to -0.0264869225333569
-0.932277896394601 to -0.9339261542895
-0.147514560329375 to -0.149162818224275
-0.277763166534362 to -0.279411424429262
-0.607948013650831 to -0.609596271545731
-0.149435141193328 to -0.151083399088228
-0.481131115066466 to -0.482779372961366
-0.00830123673623615 to -0.00994949463113588
Changing layer 4's weights from 
-0.10757503758615 to -0.10922329548105
-0.316407837974486 to -0.318056095869386
-0.516885438072142 to -0.518533695967042
-0.229950704681334 to -0.231598962576234
-0.469203510391173 to -0.470851768286073
-0.237738289939818 to -0.239386547834718
-0.242096819984374 to -0.243745077879274
-0.703994342195449 to -0.705642600090348
-0.110298254596648 to -0.111946512491548
-0.542600193130431 to -0.544248451025331
Changing layer 5's weights from 
0.0452047799950268 to 0.0435565221001271
-0.396868744003234 to -0.398517001898134
0.0283196185951858 to 0.0266713607002861
-0.907192562776266 to -0.908840820671165
-0.369923749076781 to -0.371572006971681
-0.7683916921491 to -0.770039950044
-0.468426385032591 to -0.470074642927491
-0.395980157958922 to -0.397628415853822
-0.00637815247720218 to -0.0080264103721019
-0.0128862644309382 to -0.0145345223258379
Trying to learn from memory 49, 1, -0.2
sum 0.0388680842912957 distri 0.0199582591903613
Using diff 0.00919280402811044 and condRate 0.166666666666667
Changed category 1 weights from 
0.286019835141405 to 0.285713408335902
0.519335422185167 to 0.519028995379664
0.0944101842911091 to 0.094103757485606
0.0985029968292561 to 0.0981965700237529
Changing layer 0's weights from 
-0.201622952253663 to -0.201929379059166
-0.573549408943497 to -0.573855835749001
-0.807898674638116 to -0.808205101443619
-0.773339424760186 to -0.773645851565689
-0.0254718554328318 to -0.0257782822383349
-0.708113808662736 to -0.708420235468239
-0.554284085066163 to -0.554590511871666
-0.581738491089188 to -0.582044917894691
-0.819230352074945 to -0.819536778880448
-0.862980407209241 to -0.863286834014744
Changing layer 1's weights from 
-0.857883494825208 to -0.858189921630711
-0.25671481016382 to -0.257021236969323
-0.382417548925722 to -0.382723975731225
-0.392322887213075 to -0.392629314018578
-0.577375669510209 to -0.577682096315712
-0.715613503486955 to -0.715919930292458
-0.444894779951417 to -0.44520120675692
-0.830697808892572 to -0.831004235698075
-0.419034827978456 to -0.419341254783959
-0.78168169084295 to -0.781988117648453
Changing layer 2's weights from 
0.0275065409828781 to 0.027200114177375
-0.526546944410646 to -0.526853371216149
-0.241957653791749 to -0.242264080597252
-0.070757557184541 to -0.0710639839900441
-0.248010267050111 to -0.248316693855614
-0.454019893438661 to -0.454326320244164
0.064792822568572 to 0.0644863957630689
-0.00762920501931892 to -0.00793563182482205
-0.739627499611222 to -0.739933926416725
-0.694669981033647 to -0.69497640783915
Changing layer 3's weights from 
-0.00554375770791789 to -0.00585018451342102
-0.26909684065088 to -0.269403267456383
-0.0264869225333569 to -0.02679334933886
-0.9339261542895 to -0.934232581095004
-0.149162818224275 to -0.149469245029778
-0.279411424429262 to -0.279717851234765
-0.609596271545731 to -0.609902698351234
-0.151083399088228 to -0.151389825893731
-0.482779372961366 to -0.483085799766869
-0.00994949463113588 to -0.010255921436639
Changing layer 4's weights from 
-0.10922329548105 to -0.109529722286553
-0.318056095869386 to -0.318362522674889
-0.518533695967042 to -0.518840122772545
-0.231598962576234 to -0.231905389381737
-0.470851768286073 to -0.471158195091576
-0.239386547834718 to -0.239692974640221
-0.243745077879274 to -0.244051504684777
-0.705642600090348 to -0.705949026895851
-0.111946512491548 to -0.112252939297051
-0.544248451025331 to -0.544554877830834
Changing layer 5's weights from 
0.0435565221001271 to 0.043250095294624
-0.398517001898134 to -0.398823428703637
0.0266713607002861 to 0.026364933894783
-0.908840820671165 to -0.909147247476669
-0.371572006971681 to -0.371878433777184
-0.770039950044 to -0.770346376849503
-0.470074642927491 to -0.470381069732994
-0.397628415853822 to -0.397934842659325
-0.0080264103721019 to -0.00833283717760503
-0.0145345223258379 to -0.014840949131341
Trying to learn from memory 50, 0, -0.2
sum 0.0388618949306888 distri -0.0202875232573012
Using diff 0.0494339444553178 and condRate 0.166666666666667
Changed category 0 weights from 
0.14353014595925 to 0.141882347786185
-0.256504818125375 to -0.25815261629844
-0.539025514646181 to -0.540673312819246
-0.38752014867843 to -0.389167946851495
Changing layer 0's weights from 
-0.201929379059166 to -0.203577177232231
-0.573855835749001 to -0.575503633922065
-0.808205101443619 to -0.809852899616683
-0.773645851565689 to -0.775293649738753
-0.0257782822383349 to -0.0274260804113996
-0.708420235468239 to -0.710068033641304
-0.554590511871666 to -0.55623831004473
-0.582044917894691 to -0.583692716067756
-0.819536778880448 to -0.821184577053512
-0.863286834014744 to -0.864934632187808
Changing layer 1's weights from 
-0.858189921630711 to -0.859837719803775
-0.257021236969323 to -0.258669035142388
-0.382723975731225 to -0.38437177390429
-0.392629314018578 to -0.394277112191643
-0.577682096315712 to -0.579329894488777
-0.715919930292458 to -0.717567728465523
-0.44520120675692 to -0.446849004929984
-0.831004235698075 to -0.832652033871139
-0.419341254783959 to -0.420989052957023
-0.781988117648453 to -0.783635915821517
Changing layer 2's weights from 
0.027200114177375 to 0.0255523160043103
-0.526853371216149 to -0.528501169389213
-0.242264080597252 to -0.243911878770317
-0.0710639839900441 to -0.0727117821631088
-0.248316693855614 to -0.249964492028679
-0.454326320244164 to -0.455974118417228
0.0644863957630689 to 0.0628385975900042
-0.00793563182482205 to -0.00958342999788675
-0.739933926416725 to -0.74158172458979
-0.69497640783915 to -0.696624206012214
Changing layer 3's weights from 
-0.00585018451342102 to -0.00749798268648572
-0.269403267456383 to -0.271051065629448
-0.02679334933886 to -0.0284411475119247
-0.934232581095004 to -0.935880379268068
-0.149469245029778 to -0.151117043202843
-0.279717851234765 to -0.28136564940783
-0.609902698351234 to -0.611550496524299
-0.151389825893731 to -0.153037624066796
-0.483085799766869 to -0.484733597939933
-0.010255921436639 to -0.0119037196097037
Changing layer 4's weights from 
-0.109529722286553 to -0.111177520459618
-0.318362522674889 to -0.320010320847954
-0.518840122772545 to -0.520487920945609
-0.231905389381737 to -0.233553187554802
-0.471158195091576 to -0.472805993264641
-0.239692974640221 to -0.241340772813286
-0.244051504684777 to -0.245699302857842
-0.705949026895851 to -0.707596825068916
-0.112252939297051 to -0.113900737470116
-0.544554877830834 to -0.546202676003898
Changing layer 5's weights from 
0.043250095294624 to 0.0416022971215593
-0.398823428703637 to -0.400471226876702
0.026364933894783 to 0.0247171357217183
-0.909147247476669 to -0.910795045649733
-0.371878433777184 to -0.373526231950249
-0.770346376849503 to -0.771994175022567
-0.470381069732994 to -0.472028867906058
-0.397934842659325 to -0.39958264083239
-0.00833283717760503 to -0.00998063535066973
-0.014840949131341 to -0.0164887473044057
10/5/2016 1:32:33 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 50, 1, -0.2
sum 0.0388618949306888 distri 0.0199552523163501
Using diff 0.00919116888166652 and condRate 0.166666666666667
Changed category 1 weights from 
0.285713408335902 to 0.285407036035281
0.519028995379664 to 0.518722623079043
0.094103757485606 to 0.0937973851849851
0.0981965700237529 to 0.0978901977231321
Changing layer 0's weights from 
-0.203577177232231 to -0.203883549532851
-0.575503633922065 to -0.575810006222686
-0.809852899616683 to -0.810159271917304
-0.775293649738753 to -0.775600022039374
-0.0274260804113996 to -0.0277324527120205
-0.710068033641304 to -0.710374405941925
-0.55623831004473 to -0.556544682345351
-0.583692716067756 to -0.583999088368377
-0.821184577053512 to -0.821490949354133
-0.864934632187808 to -0.865241004488429
Changing layer 1's weights from 
-0.859837719803775 to -0.860144092104396
-0.258669035142388 to -0.258975407443009
-0.38437177390429 to -0.38467814620491
-0.394277112191643 to -0.394583484492263
-0.579329894488777 to -0.579636266789398
-0.717567728465523 to -0.717874100766143
-0.446849004929984 to -0.447155377230605
-0.832652033871139 to -0.83295840617176
-0.420989052957023 to -0.421295425257644
-0.783635915821517 to -0.783942288122138
Changing layer 2's weights from 
0.0255523160043103 to 0.0252459437036894
-0.528501169389213 to -0.528807541689834
-0.243911878770317 to -0.244218251070937
-0.0727117821631088 to -0.0730181544637297
-0.249964492028679 to -0.250270864329299
-0.455974118417228 to -0.456280490717849
0.0628385975900042 to 0.0625322252893833
-0.00958342999788675 to -0.00988980229850761
-0.74158172458979 to -0.741888096890411
-0.696624206012214 to -0.696930578312835
Changing layer 3's weights from 
-0.00749798268648572 to -0.00780435498710657
-0.271051065629448 to -0.271357437930068
-0.0284411475119247 to -0.0287475198125455
-0.935880379268068 to -0.936186751568689
-0.151117043202843 to -0.151423415503464
-0.28136564940783 to -0.28167202170845
-0.611550496524299 to -0.61185686882492
-0.153037624066796 to -0.153343996367417
-0.484733597939933 to -0.485039970240554
-0.0119037196097037 to -0.0122100919103246
Changing layer 4's weights from 
-0.111177520459618 to -0.111483892760238
-0.320010320847954 to -0.320316693148574
-0.520487920945609 to -0.52079429324623
-0.233553187554802 to -0.233859559855422
-0.472805993264641 to -0.473112365565261
-0.241340772813286 to -0.241647145113906
-0.245699302857842 to -0.246005675158462
-0.707596825068916 to -0.707903197369537
-0.113900737470116 to -0.114207109770736
-0.546202676003898 to -0.546509048304519
Changing layer 5's weights from 
0.0416022971215593 to 0.0412959248209384
-0.400471226876702 to -0.400777599177322
0.0247171357217183 to 0.0244107634210974
-0.910795045649733 to -0.911101417950354
-0.373526231950249 to -0.373832604250869
-0.771994175022567 to -0.772300547323188
-0.472028867906058 to -0.472335240206679
-0.39958264083239 to -0.39988901313301
-0.00998063535066973 to -0.0102870076512906
-0.0164887473044057 to -0.0167951196050266
Trying to learn from memory 51, 1, -0.2
sum 0.0388475511763254 distri 0.0199494150686747
Using diff 0.00918624831356934 and condRate 0.166666666666667
Changed category 1 weights from 
0.285407036035281 to 0.285100827753599
0.518722623079043 to 0.518416414797361
0.0937973851849851 to 0.0934911769033033
0.0978901977231321 to 0.0975839894414502
Changing layer 0's weights from 
-0.203883549532851 to -0.204189757814533
-0.575810006222686 to -0.576116214504368
-0.810159271917304 to -0.810465480198986
-0.775600022039374 to -0.775906230321056
-0.0277324527120205 to -0.0280386609937023
-0.710374405941925 to -0.710680614223606
-0.556544682345351 to -0.556850890627033
-0.583999088368377 to -0.584305296650059
-0.821490949354133 to -0.821797157635815
-0.865241004488429 to -0.865547212770111
Changing layer 1's weights from 
-0.860144092104396 to -0.860450300386078
-0.258975407443009 to -0.25928161572469
-0.38467814620491 to -0.384984354486592
-0.394583484492263 to -0.394889692773945
-0.579636266789398 to -0.579942475071079
-0.717874100766143 to -0.718180309047825
-0.447155377230605 to -0.447461585512287
-0.83295840617176 to -0.833264614453442
-0.421295425257644 to -0.421601633539326
-0.783942288122138 to -0.78424849640382
Changing layer 2's weights from 
0.0252459437036894 to 0.0249397354220076
-0.528807541689834 to -0.529113749971516
-0.244218251070937 to -0.244524459352619
-0.0730181544637297 to -0.0733243627454115
-0.250270864329299 to -0.250577072610981
-0.456280490717849 to -0.456586698999531
0.0625322252893833 to 0.0622260170077015
-0.00988980229850761 to -0.0101960105801894
-0.741888096890411 to -0.742194305172093
-0.696930578312835 to -0.697236786594517
Changing layer 3's weights from 
-0.00780435498710657 to -0.00811056326878841
-0.271357437930068 to -0.27166364621175
-0.0287475198125455 to -0.0290537280942274
-0.936186751568689 to -0.936492959850371
-0.151423415503464 to -0.151729623785145
-0.28167202170845 to -0.281978229990132
-0.61185686882492 to -0.612163077106602
-0.153343996367417 to -0.153650204649098
-0.485039970240554 to -0.485346178522236
-0.0122100919103246 to -0.0125163001920064
Changing layer 4's weights from 
-0.111483892760238 to -0.11179010104192
-0.320316693148574 to -0.320622901430256
-0.52079429324623 to -0.521100501527912
-0.233859559855422 to -0.234165768137104
-0.473112365565261 to -0.473418573846943
-0.241647145113906 to -0.241953353395588
-0.246005675158462 to -0.246311883440144
-0.707903197369537 to -0.708209405651219
-0.114207109770736 to -0.114513318052418
-0.546509048304519 to -0.546815256586201
Changing layer 5's weights from 
0.0412959248209384 to 0.0409897165392566
-0.400777599177322 to -0.401083807459004
0.0244107634210974 to 0.0241045551394156
-0.911101417950354 to -0.911407626232036
-0.373832604250869 to -0.374138812532551
-0.772300547323188 to -0.77260675560487
-0.472335240206679 to -0.472641448488361
-0.39988901313301 to -0.400195221414692
-0.0102870076512906 to -0.0105932159329724
-0.0167951196050266 to -0.0171013278867084
Trying to learn from memory 52, 0, -0.2
sum 0.0389999574982601 distri -0.0203124995858538
Using diff 0.0495624677095489 and condRate 0.166666666666667
Changed category 0 weights from 
0.141882347786185 to 0.140230265504582
-0.25815261629844 to -0.259804698580043
-0.540673312819246 to -0.542325395100849
-0.389167946851495 to -0.390820029133098
Changing layer 0's weights from 
-0.204189757814533 to -0.205841840096136
-0.576116214504368 to -0.577768296785971
-0.810465480198986 to -0.812117562480589
-0.775906230321056 to -0.777558312602659
-0.0280386609937023 to -0.0296907432753052
-0.710680614223606 to -0.712332696505209
-0.556850890627033 to -0.558502972908636
-0.584305296650059 to -0.585957378931662
-0.821797157635815 to -0.823449239917418
-0.865547212770111 to -0.867199295051714
Changing layer 1's weights from 
-0.860450300386078 to -0.862102382667681
-0.25928161572469 to -0.260933698006293
-0.384984354486592 to -0.386636436768195
-0.394889692773945 to -0.396541775055548
-0.579942475071079 to -0.581594557352682
-0.718180309047825 to -0.719832391329428
-0.447461585512287 to -0.44911366779389
-0.833264614453442 to -0.834916696735045
-0.421601633539326 to -0.423253715820929
-0.78424849640382 to -0.785900578685423
Changing layer 2's weights from 
0.0249397354220076 to 0.0232876531404047
-0.529113749971516 to -0.530765832253119
-0.244524459352619 to -0.246176541634222
-0.0733243627454115 to -0.0749764450270144
-0.250577072610981 to -0.252229154892584
-0.456586698999531 to -0.458238781281134
0.0622260170077015 to 0.0605739347260986
-0.0101960105801894 to -0.0118480928617923
-0.742194305172093 to -0.743846387453695
-0.697236786594517 to -0.69888886887612
Changing layer 3's weights from 
-0.00811056326878841 to -0.00976264555039131
-0.27166364621175 to -0.273315728493353
-0.0290537280942274 to -0.0307058103758303
-0.936492959850371 to -0.938145042131974
-0.151729623785145 to -0.153381706066748
-0.281978229990132 to -0.283630312271735
-0.612163077106602 to -0.613815159388204
-0.153650204649098 to -0.155302286930701
-0.485346178522236 to -0.486998260803839
-0.0125163001920064 to -0.0141683824736093
Changing layer 4's weights from 
-0.11179010104192 to -0.113442183323523
-0.320622901430256 to -0.322274983711859
-0.521100501527912 to -0.522752583809515
-0.234165768137104 to -0.235817850418707
-0.473418573846943 to -0.475070656128546
-0.241953353395588 to -0.243605435677191
-0.246311883440144 to -0.247963965721747
-0.708209405651219 to -0.709861487932822
-0.114513318052418 to -0.116165400334021
-0.546815256586201 to -0.548467338867804
Changing layer 5's weights from 
0.0409897165392566 to 0.0393376342576537
-0.401083807459004 to -0.402735889740607
0.0241045551394156 to 0.0224524728578127
-0.911407626232036 to -0.913059708513639
-0.374138812532551 to -0.375790894814154
-0.77260675560487 to -0.774258837886473
-0.472641448488361 to -0.474293530769964
-0.400195221414692 to -0.401847303696295
-0.0105932159329724 to -0.0122452982145753
-0.0171013278867084 to -0.0187534101683113
Trying to learn from memory 53, 0, -0.2
sum 0.0404435109397489 distri -0.0206127954574211
Using diff 0.0509454286622328 and condRate 0.166666666666667
Changed category 0 weights from 
0.140230265504582 to 0.13853208452387
-0.259804698580043 to -0.261502879560755
-0.542325395100849 to -0.544023576081561
-0.390820029133098 to -0.39251821011381
Changing layer 0's weights from 
-0.205841840096136 to -0.207540021076849
-0.577768296785971 to -0.579466477766683
-0.812117562480589 to -0.813815743461301
-0.777558312602659 to -0.779256493583371
-0.0296907432753052 to -0.0313889242560178
-0.712332696505209 to -0.714030877485922
-0.558502972908636 to -0.560201153889349
-0.585957378931662 to -0.587655559912374
-0.823449239917418 to -0.82514742089813
-0.867199295051714 to -0.868897476032426
Changing layer 1's weights from 
-0.862102382667681 to -0.863800563648393
-0.260933698006293 to -0.262631878987006
-0.386636436768195 to -0.388334617748908
-0.396541775055548 to -0.398239956036261
-0.581594557352682 to -0.583292738333395
-0.719832391329428 to -0.721530572310141
-0.44911366779389 to -0.450811848774603
-0.834916696735045 to -0.836614877715757
-0.423253715820929 to -0.424951896801642
-0.785900578685423 to -0.787598759666135
Changing layer 2's weights from 
0.0232876531404047 to 0.021589472159692
-0.530765832253119 to -0.532464013233832
-0.246176541634222 to -0.247874722614935
-0.0749764450270144 to -0.076674626007727
-0.252229154892584 to -0.253927335873297
-0.458238781281134 to -0.459936962261847
0.0605739347260986 to 0.058875753745386
-0.0118480928617923 to -0.013546273842505
-0.743846387453695 to -0.745544568434408
-0.69888886887612 to -0.700587049856833
Changing layer 3's weights from 
-0.00976264555039131 to -0.0114608265311039
-0.273315728493353 to -0.275013909474066
-0.0307058103758303 to -0.0324039913565429
-0.938145042131974 to -0.939843223112686
-0.153381706066748 to -0.155079887047461
-0.283630312271735 to -0.285328493252448
-0.613815159388204 to -0.615513340368917
-0.155302286930701 to -0.157000467911414
-0.486998260803839 to -0.488696441784552
-0.0141683824736093 to -0.0158665634543219
Changing layer 4's weights from 
-0.113442183323523 to -0.115140364304236
-0.322274983711859 to -0.323973164692572
-0.522752583809515 to -0.524450764790227
-0.235817850418707 to -0.23751603139942
-0.475070656128546 to -0.476768837109259
-0.243605435677191 to -0.245303616657904
-0.247963965721747 to -0.24966214670246
-0.709861487932822 to -0.711559668913534
-0.116165400334021 to -0.117863581314734
-0.548467338867804 to -0.550165519848516
Changing layer 5's weights from 
0.0393376342576537 to 0.037639453276941
-0.402735889740607 to -0.40443407072132
0.0224524728578127 to 0.0207542918771
-0.913059708513639 to -0.914757889494351
-0.375790894814154 to -0.377489075794867
-0.774258837886473 to -0.775957018867185
-0.474293530769964 to -0.475991711750677
-0.401847303696295 to -0.403545484677008
-0.0122452982145753 to -0.013943479195288
-0.0187534101683113 to -0.0204515911490239
Trying to learn from memory 54, 1, -0.2
sum 0.041914652641835 distri 0.0212398541447802
Using diff 0.0101961353365961 and condRate 0.166666666666667
Changed category 1 weights from 
0.285100827753599 to 0.284760956570648
0.518416414797361 to 0.51807654361441
0.0934911769033033 to 0.0931513057203523
0.0975839894414502 to 0.0972441182584992
Changing layer 0's weights from 
-0.207540021076849 to -0.2078798922598
-0.579466477766683 to -0.579806348949634
-0.813815743461301 to -0.814155614644252
-0.779256493583371 to -0.779596364766322
-0.0313889242560178 to -0.0317287954389689
-0.714030877485922 to -0.714370748668873
-0.560201153889349 to -0.5605410250723
-0.587655559912374 to -0.587995431095325
-0.82514742089813 to -0.825487292081081
-0.868897476032426 to -0.869237347215377
Changing layer 1's weights from 
-0.863800563648393 to -0.864140434831344
-0.262631878987006 to -0.262971750169957
-0.388334617748908 to -0.388674488931859
-0.398239956036261 to -0.398579827219212
-0.583292738333395 to -0.583632609516346
-0.721530572310141 to -0.721870443493092
-0.450811848774603 to -0.451151719957554
-0.836614877715757 to -0.836954748898708
-0.424951896801642 to -0.425291767984593
-0.787598759666135 to -0.787938630849086
Changing layer 2's weights from 
0.021589472159692 to 0.021249600976741
-0.532464013233832 to -0.532803884416783
-0.247874722614935 to -0.248214593797886
-0.076674626007727 to -0.077014497190678
-0.253927335873297 to -0.254267207056248
-0.459936962261847 to -0.460276833444798
0.058875753745386 to 0.0585358825624349
-0.013546273842505 to -0.013886145025456
-0.745544568434408 to -0.745884439617359
-0.700587049856833 to -0.700926921039783
Changing layer 3's weights from 
-0.0114608265311039 to -0.011800697714055
-0.275013909474066 to -0.275353780657017
-0.0324039913565429 to -0.0327438625394939
-0.939843223112686 to -0.940183094295637
-0.155079887047461 to -0.155419758230412
-0.285328493252448 to -0.285668364435399
-0.615513340368917 to -0.615853211551868
-0.157000467911414 to -0.157340339094365
-0.488696441784552 to -0.489036312967503
-0.0158665634543219 to -0.0162064346372729
Changing layer 4's weights from 
-0.115140364304236 to -0.115480235487187
-0.323973164692572 to -0.324313035875523
-0.524450764790227 to -0.524790635973178
-0.23751603139942 to -0.237855902582371
-0.476768837109259 to -0.47710870829221
-0.245303616657904 to -0.245643487840855
-0.24966214670246 to -0.250002017885411
-0.711559668913534 to -0.711899540096485
-0.117863581314734 to -0.118203452497685
-0.550165519848516 to -0.550505391031467
Changing layer 5's weights from 
0.037639453276941 to 0.03729958209399
-0.40443407072132 to -0.404773941904271
0.0207542918771 to 0.020414420694149
-0.914757889494351 to -0.915097760677302
-0.377489075794867 to -0.377828946977818
-0.775957018867185 to -0.776296890050136
-0.475991711750677 to -0.476331582933628
-0.403545484677008 to -0.403885355859959
-0.013943479195288 to -0.014283350378239
-0.0204515911490239 to -0.020791462331975
Trying to learn from memory 54, 1, -0.2
sum 0.041914652641835 distri 0.0212398541447802
Using diff 0.0101961353365961 and condRate 0.166666666666667
Changed category 1 weights from 
0.284760956570648 to 0.284421085387697
0.51807654361441 to 0.517736672431459
0.0931513057203523 to 0.0928114345374013
0.0972441182584992 to 0.0969042470755482
Changing layer 0's weights from 
-0.2078798922598 to -0.208219763442751
-0.579806348949634 to -0.580146220132585
-0.814155614644252 to -0.814495485827203
-0.779596364766322 to -0.779936235949273
-0.0317287954389689 to -0.0320686666219199
-0.714370748668873 to -0.714710619851824
-0.5605410250723 to -0.560880896255251
-0.587995431095325 to -0.588335302278276
-0.825487292081081 to -0.825827163264032
-0.869237347215377 to -0.869577218398328
Changing layer 1's weights from 
-0.864140434831344 to -0.864480306014295
-0.262971750169957 to -0.263311621352908
-0.388674488931859 to -0.38901436011481
-0.398579827219212 to -0.398919698402163
-0.583632609516346 to -0.583972480699297
-0.721870443493092 to -0.722210314676043
-0.451151719957554 to -0.451491591140505
-0.836954748898708 to -0.837294620081659
-0.425291767984593 to -0.425631639167544
-0.787938630849086 to -0.788278502032037
Changing layer 2's weights from 
0.021249600976741 to 0.02090972979379
-0.532803884416783 to -0.533143755599733
-0.248214593797886 to -0.248554464980837
-0.077014497190678 to -0.077354368373629
-0.254267207056248 to -0.254607078239199
-0.460276833444798 to -0.460616704627749
0.0585358825624349 to 0.0581960113794839
-0.013886145025456 to -0.014226016208407
-0.745884439617359 to -0.74622431080031
-0.700926921039783 to -0.701266792222734
Changing layer 3's weights from 
-0.011800697714055 to -0.012140568897006
-0.275353780657017 to -0.275693651839968
-0.0327438625394939 to -0.0330837337224449
-0.940183094295637 to -0.940522965478588
-0.155419758230412 to -0.155759629413363
-0.285668364435399 to -0.28600823561835
-0.615853211551868 to -0.616193082734819
-0.157340339094365 to -0.157680210277316
-0.489036312967503 to -0.489376184150454
-0.0162064346372729 to -0.016546305820224
Changing layer 4's weights from 
-0.115480235487187 to -0.115820106670138
-0.324313035875523 to -0.324652907058474
-0.524790635973178 to -0.525130507156129
-0.237855902582371 to -0.238195773765322
-0.47710870829221 to -0.477448579475161
-0.245643487840855 to -0.245983359023806
-0.250002017885411 to -0.250341889068362
-0.711899540096485 to -0.712239411279436
-0.118203452497685 to -0.118543323680636
-0.550505391031467 to -0.550845262214418
Changing layer 5's weights from 
0.03729958209399 to 0.036959710911039
-0.404773941904271 to -0.405113813087222
0.020414420694149 to 0.020074549511198
-0.915097760677302 to -0.915437631860253
-0.377828946977818 to -0.378168818160769
-0.776296890050136 to -0.776636761233087
-0.476331582933628 to -0.476671454116579
-0.403885355859959 to -0.40422522704291
-0.014283350378239 to -0.01462322156119
-0.020791462331975 to -0.021131333514926
Trying to learn from memory 54, 1, -0.2
sum 0.041914652641835 distri 0.0212398541447802
Using diff 0.0101961353365961 and condRate 0.166666666666667
Changed category 1 weights from 
0.284421085387697 to 0.284081214204746
0.517736672431459 to 0.517396801248508
0.0928114345374013 to 0.0924715633544503
0.0969042470755482 to 0.0965643758925972
Changing layer 0's weights from 
-0.208219763442751 to -0.208559634625702
-0.580146220132585 to -0.580486091315536
-0.814495485827203 to -0.814835357010154
-0.779936235949273 to -0.780276107132224
-0.0320686666219199 to -0.0324085378048709
-0.714710619851824 to -0.715050491034775
-0.560880896255251 to -0.561220767438201
-0.588335302278276 to -0.588675173461227
-0.825827163264032 to -0.826167034446983
-0.869577218398328 to -0.869917089581279
Changing layer 1's weights from 
-0.864480306014295 to -0.864820177197246
-0.263311621352908 to -0.263651492535859
-0.38901436011481 to -0.389354231297761
-0.398919698402163 to -0.399259569585114
-0.583972480699297 to -0.584312351882248
-0.722210314676043 to -0.722550185858994
-0.451491591140505 to -0.451831462323456
-0.837294620081659 to -0.83763449126461
-0.425631639167544 to -0.425971510350495
-0.788278502032037 to -0.788618373214988
Changing layer 2's weights from 
0.02090972979379 to 0.020569858610839
-0.533143755599733 to -0.533483626782684
-0.248554464980837 to -0.248894336163788
-0.077354368373629 to -0.07769423955658
-0.254607078239199 to -0.25494694942215
-0.460616704627749 to -0.4609565758107
0.0581960113794839 to 0.0578561401965329
-0.014226016208407 to -0.014565887391358
-0.74622431080031 to -0.746564181983261
-0.701266792222734 to -0.701606663405685
Changing layer 3's weights from 
-0.012140568897006 to -0.012480440079957
-0.275693651839968 to -0.276033523022919
-0.0330837337224449 to -0.0334236049053959
-0.940522965478588 to -0.940862836661539
-0.155759629413363 to -0.156099500596314
-0.28600823561835 to -0.286348106801301
-0.616193082734819 to -0.61653295391777
-0.157680210277316 to -0.158020081460267
-0.489376184150454 to -0.489716055333405
-0.016546305820224 to -0.016886177003175
Changing layer 4's weights from 
-0.115820106670138 to -0.116159977853089
-0.324652907058474 to -0.324992778241425
-0.525130507156129 to -0.52547037833908
-0.238195773765322 to -0.238535644948273
-0.477448579475161 to -0.477788450658112
-0.245983359023806 to -0.246323230206757
-0.250341889068362 to -0.250681760251313
-0.712239411279436 to -0.712579282462387
-0.118543323680636 to -0.118883194863587
-0.550845262214418 to -0.551185133397369
Changing layer 5's weights from 
0.036959710911039 to 0.036619839728088
-0.405113813087222 to -0.405453684270173
0.020074549511198 to 0.019734678328247
-0.915437631860253 to -0.915777503043204
-0.378168818160769 to -0.37850868934372
-0.776636761233087 to -0.776976632416038
-0.476671454116579 to -0.47701132529953
-0.40422522704291 to -0.404565098225861
-0.01462322156119 to -0.014963092744141
-0.021131333514926 to -0.021471204697877
Trying to learn from memory 55, 0, -0.2
sum 0.0420297330847072 distri -0.020996165388995
Using diff 0.0525184652025254 and condRate 0.166666666666667
Changed category 0 weights from 
0.13853208452387 to 0.136781468991033
-0.261502879560755 to -0.263253495093592
-0.544023576081561 to -0.545774191614398
-0.39251821011381 to -0.394268825646647
Changing layer 0's weights from 
-0.208559634625702 to -0.210310250158539
-0.580486091315536 to -0.582236706848373
-0.814835357010154 to -0.816585972542991
-0.780276107132224 to -0.782026722665061
-0.0324085378048709 to -0.0341591533377079
-0.715050491034775 to -0.716801106567612
-0.561220767438201 to -0.562971382971039
-0.588675173461227 to -0.590425788994064
-0.826167034446983 to -0.82791764997982
-0.869917089581279 to -0.871667705114116
Changing layer 1's weights from 
-0.864820177197246 to -0.866570792730083
-0.263651492535859 to -0.265402108068696
-0.389354231297761 to -0.391104846830598
-0.399259569585114 to -0.401010185117951
-0.584312351882248 to -0.586062967415085
-0.722550185858994 to -0.724300801391831
-0.451831462323456 to -0.453582077856293
-0.83763449126461 to -0.839385106797447
-0.425971510350495 to -0.427722125883332
-0.788618373214988 to -0.790368988747825
Changing layer 2's weights from 
0.020569858610839 to 0.0188192430780019
-0.533483626782684 to -0.535234242315522
-0.248894336163788 to -0.250644951696625
-0.07769423955658 to -0.0794448550894171
-0.25494694942215 to -0.256697564954987
-0.4609565758107 to -0.462707191343537
0.0578561401965329 to 0.0561055246636959
-0.014565887391358 to -0.0163165029241951
-0.746564181983261 to -0.748314797516098
-0.701606663405685 to -0.703357278938523
Changing layer 3's weights from 
-0.012480440079957 to -0.014231055612794
-0.276033523022919 to -0.277784138555756
-0.0334236049053959 to -0.035174220438233
-0.940862836661539 to -0.942613452194376
-0.156099500596314 to -0.157850116129151
-0.286348106801301 to -0.288098722334138
-0.61653295391777 to -0.618283569450607
-0.158020081460267 to -0.159770696993104
-0.489716055333405 to -0.491466670866242
-0.016886177003175 to -0.018636792536012
Changing layer 4's weights from 
-0.116159977853089 to -0.117910593385926
-0.324992778241425 to -0.326743393774262
-0.52547037833908 to -0.527220993871917
-0.238535644948273 to -0.24028626048111
-0.477788450658112 to -0.479539066190949
-0.246323230206757 to -0.248073845739594
-0.250681760251313 to -0.25243237578415
-0.712579282462387 to -0.714329897995224
-0.118883194863587 to -0.120633810396424
-0.551185133397369 to -0.552935748930206
Changing layer 5's weights from 
0.036619839728088 to 0.0348692241952509
-0.405453684270173 to -0.40720429980301
0.019734678328247 to 0.0179840627954099
-0.915777503043204 to -0.917528118576041
-0.37850868934372 to -0.380259304876557
-0.776976632416038 to -0.778727247948875
-0.47701132529953 to -0.478761940832367
-0.404565098225861 to -0.406315713758698
-0.014963092744141 to -0.016713708276978
-0.021471204697877 to -0.023221820230714
Trying to learn from memory 55, 0, -0.2
sum 0.0420297330847072 distri -0.020996165388995
Using diff 0.0525184652025254 and condRate 0.166666666666667
Changed category 0 weights from 
0.136781468991033 to 0.135030853458196
-0.263253495093592 to -0.265004110626429
-0.545774191614398 to -0.547524807147235
-0.394268825646647 to -0.396019441179484
Changing layer 0's weights from 
-0.210310250158539 to -0.212060865691376
-0.582236706848373 to -0.58398732238121
-0.816585972542991 to -0.818336588075829
-0.782026722665061 to -0.783777338197899
-0.0341591533377079 to -0.035909768870545
-0.716801106567612 to -0.718551722100449
-0.562971382971039 to -0.564721998503876
-0.590425788994064 to -0.592176404526901
-0.82791764997982 to -0.829668265512657
-0.871667705114116 to -0.873418320646953
Changing layer 1's weights from 
-0.866570792730083 to -0.86832140826292
-0.265402108068696 to -0.267152723601533
-0.391104846830598 to -0.392855462363435
-0.401010185117951 to -0.402760800650788
-0.586062967415085 to -0.587813582947922
-0.724300801391831 to -0.726051416924668
-0.453582077856293 to -0.45533269338913
-0.839385106797447 to -0.841135722330285
-0.427722125883332 to -0.429472741416169
-0.790368988747825 to -0.792119604280663
Changing layer 2's weights from 
0.0188192430780019 to 0.0170686275451649
-0.535234242315522 to -0.536984857848359
-0.250644951696625 to -0.252395567229462
-0.0794448550894171 to -0.0811954706222541
-0.256697564954987 to -0.258448180487824
-0.462707191343537 to -0.464457806876374
0.0561055246636959 to 0.0543549091308588
-0.0163165029241951 to -0.0180671184570321
-0.748314797516098 to -0.750065413048935
-0.703357278938523 to -0.70510789447136
Changing layer 3's weights from 
-0.014231055612794 to -0.0159816711456311
-0.277784138555756 to -0.279534754088593
-0.035174220438233 to -0.03692483597107
-0.942613452194376 to -0.944364067727213
-0.157850116129151 to -0.159600731661988
-0.288098722334138 to -0.289849337866975
-0.618283569450607 to -0.620034184983444
-0.159770696993104 to -0.161521312525941
-0.491466670866242 to -0.493217286399079
-0.018636792536012 to -0.0203874080688491
Changing layer 4's weights from 
-0.117910593385926 to -0.119661208918763
-0.326743393774262 to -0.328494009307099
-0.527220993871917 to -0.528971609404755
-0.24028626048111 to -0.242036876013947
-0.479539066190949 to -0.481289681723786
-0.248073845739594 to -0.249824461272431
-0.25243237578415 to -0.254182991316987
-0.714329897995224 to -0.716080513528061
-0.120633810396424 to -0.122384425929261
-0.552935748930206 to -0.554686364463044
Changing layer 5's weights from 
0.0348692241952509 to 0.0331186086624139
-0.40720429980301 to -0.408954915335847
0.0179840627954099 to 0.0162334472625729
-0.917528118576041 to -0.919278734108878
-0.380259304876557 to -0.382009920409394
-0.778727247948875 to -0.780477863481713
-0.478761940832367 to -0.480512556365204
-0.406315713758698 to -0.408066329291535
-0.016713708276978 to -0.0184643238098151
-0.023221820230714 to -0.0249724357635511
Trying to learn from memory 55, 0, -0.2
sum 0.0420297330847072 distri -0.020996165388995
Using diff 0.0525184652025254 and condRate 0.166666666666667
Changed category 0 weights from 
0.135030853458196 to 0.133280237925359
-0.265004110626429 to -0.266754726159266
-0.547524807147235 to -0.549275422680072
-0.396019441179484 to -0.397770056712321
Changing layer 0's weights from 
-0.212060865691376 to -0.213811481224213
-0.58398732238121 to -0.585737937914047
-0.818336588075829 to -0.820087203608666
-0.783777338197899 to -0.785527953730736
-0.035909768870545 to -0.037660384403382
-0.718551722100449 to -0.720302337633286
-0.564721998503876 to -0.566472614036713
-0.592176404526901 to -0.593927020059738
-0.829668265512657 to -0.831418881045495
-0.873418320646953 to -0.875168936179791
Changing layer 1's weights from 
-0.86832140826292 to -0.870072023795758
-0.267152723601533 to -0.26890333913437
-0.392855462363435 to -0.394606077896272
-0.402760800650788 to -0.404511416183625
-0.587813582947922 to -0.589564198480759
-0.726051416924668 to -0.727802032457505
-0.45533269338913 to -0.457083308921967
-0.841135722330285 to -0.842886337863122
-0.429472741416169 to -0.431223356949006
-0.792119604280663 to -0.7938702198135
Changing layer 2's weights from 
0.0170686275451649 to 0.0153180120123278
-0.536984857848359 to -0.538735473381196
-0.252395567229462 to -0.254146182762299
-0.0811954706222541 to -0.0829460861550912
-0.258448180487824 to -0.260198796020661
-0.464457806876374 to -0.466208422409211
0.0543549091308588 to 0.0526042935980218
-0.0180671184570321 to -0.0198177339898692
-0.750065413048935 to -0.751816028581772
-0.70510789447136 to -0.706858510004197
Changing layer 3's weights from 
-0.0159816711456311 to -0.0177322866784681
-0.279534754088593 to -0.28128536962143
-0.03692483597107 to -0.0386754515039071
-0.944364067727213 to -0.94611468326005
-0.159600731661988 to -0.161351347194825
-0.289849337866975 to -0.291599953399812
-0.620034184983444 to -0.621784800516281
-0.161521312525941 to -0.163271928058778
-0.493217286399079 to -0.494967901931916
-0.0203874080688491 to -0.0221380236016861
Changing layer 4's weights from 
-0.119661208918763 to -0.1214118244516
-0.328494009307099 to -0.330244624839936
-0.528971609404755 to -0.530722224937592
-0.242036876013947 to -0.243787491546784
-0.481289681723786 to -0.483040297256623
-0.249824461272431 to -0.251575076805268
-0.254182991316987 to -0.255933606849824
-0.716080513528061 to -0.717831129060898
-0.122384425929261 to -0.124135041462098
-0.554686364463044 to -0.556436979995881
Changing layer 5's weights from 
0.0331186086624139 to 0.0313679931295768
-0.408954915335847 to -0.410705530868684
0.0162334472625729 to 0.0144828317297359
-0.919278734108878 to -0.921029349641715
-0.382009920409394 to -0.383760535942231
-0.780477863481713 to -0.78222847901455
-0.480512556365204 to -0.482263171898041
-0.408066329291535 to -0.409816944824372
-0.0184643238098151 to -0.0202149393426521
-0.0249724357635511 to -0.0267230512963881
10/5/2016 1:32:33 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 55, 0, -0.2
sum 0.0420297330847072 distri -0.020996165388995
Using diff 0.0525184652025254 and condRate 0.166666666666667
Changed category 0 weights from 
0.133280237925359 to 0.131529622392521
-0.266754726159266 to -0.268505341692104
-0.549275422680072 to -0.551026038212909
-0.397770056712321 to -0.399520672245158
Changing layer 0's weights from 
-0.213811481224213 to -0.21556209675705
-0.585737937914047 to -0.587488553446885
-0.820087203608666 to -0.821837819141503
-0.785527953730736 to -0.787278569263573
-0.037660384403382 to -0.0394109999362191
-0.720302337633286 to -0.722052953166123
-0.566472614036713 to -0.56822322956955
-0.593927020059738 to -0.595677635592575
-0.831418881045495 to -0.833169496578332
-0.875168936179791 to -0.876919551712628
Changing layer 1's weights from 
-0.870072023795758 to -0.871822639328595
-0.26890333913437 to -0.270653954667207
-0.394606077896272 to -0.396356693429109
-0.404511416183625 to -0.406262031716462
-0.589564198480759 to -0.591314814013596
-0.727802032457505 to -0.729552647990342
-0.457083308921967 to -0.458833924454804
-0.842886337863122 to -0.844636953395959
-0.431223356949006 to -0.432973972481843
-0.7938702198135 to -0.795620835346337
Changing layer 2's weights from 
0.0153180120123278 to 0.0135673964794908
-0.538735473381196 to -0.540486088914033
-0.254146182762299 to -0.255896798295136
-0.0829460861550912 to -0.0846967016879282
-0.260198796020661 to -0.261949411553498
-0.466208422409211 to -0.467959037942048
0.0526042935980218 to 0.0508536780651847
-0.0198177339898692 to -0.0215683495227062
-0.751816028581772 to -0.753566644114609
-0.706858510004197 to -0.708609125537034
Changing layer 3's weights from 
-0.0177322866784681 to -0.0194829022113052
-0.28128536962143 to -0.283035985154267
-0.0386754515039071 to -0.0404260670367441
-0.94611468326005 to -0.947865298792888
-0.161351347194825 to -0.163101962727662
-0.291599953399812 to -0.293350568932649
-0.621784800516281 to -0.623535416049118
-0.163271928058778 to -0.165022543591615
-0.494967901931916 to -0.496718517464753
-0.0221380236016861 to -0.0238886391345232
Changing layer 4's weights from 
-0.1214118244516 to -0.123162439984437
-0.330244624839936 to -0.331995240372773
-0.530722224937592 to -0.532472840470429
-0.243787491546784 to -0.245538107079621
-0.483040297256623 to -0.48479091278946
-0.251575076805268 to -0.253325692338105
-0.255933606849824 to -0.257684222382661
-0.717831129060898 to -0.719581744593735
-0.124135041462098 to -0.125885656994935
-0.556436979995881 to -0.558187595528718
Changing layer 5's weights from 
0.0313679931295768 to 0.0296173775967398
-0.410705530868684 to -0.412456146401521
0.0144828317297359 to 0.0127322161968988
-0.921029349641715 to -0.922779965174553
-0.383760535942231 to -0.385511151475068
-0.78222847901455 to -0.783979094547387
-0.482263171898041 to -0.484013787430878
-0.409816944824372 to -0.411567560357209
-0.0202149393426521 to -0.0219655548754892
-0.0267230512963881 to -0.0284736668292252
Trying to learn from memory 55, 1, -0.2
sum 0.0420297330847072 distri 0.0213015843880592
Using diff 0.0102207154254713 and condRate 0.166666666666667
Changed category 1 weights from 
0.284081214204746 to 0.283740523685487
0.517396801248508 to 0.517056110729249
0.0924715633544503 to 0.0921308728351912
0.0965643758925972 to 0.0962236853733382
Changing layer 0's weights from 
-0.21556209675705 to -0.215902787276309
-0.587488553446885 to -0.587829243966144
-0.821837819141503 to -0.822178509660762
-0.787278569263573 to -0.787619259782832
-0.0394109999362191 to -0.0397516904554781
-0.722052953166123 to -0.722393643685382
-0.56822322956955 to -0.568563920088809
-0.595677635592575 to -0.596018326111835
-0.833169496578332 to -0.833510187097591
-0.876919551712628 to -0.877260242231887
Changing layer 1's weights from 
-0.871822639328595 to -0.872163329847854
-0.270653954667207 to -0.270994645186466
-0.396356693429109 to -0.396697383948368
-0.406262031716462 to -0.406602722235721
-0.591314814013596 to -0.591655504532855
-0.729552647990342 to -0.729893338509601
-0.458833924454804 to -0.459174614974063
-0.844636953395959 to -0.844977643915218
-0.432973972481843 to -0.433314663001102
-0.795620835346337 to -0.795961525865596
Changing layer 2's weights from 
0.0135673964794908 to 0.0132267059602317
-0.540486088914033 to -0.540826779433292
-0.255896798295136 to -0.256237488814395
-0.0846967016879282 to -0.0850373922071873
-0.261949411553498 to -0.262290102072757
-0.467959037942048 to -0.468299728461307
0.0508536780651847 to 0.0505129875459257
-0.0215683495227062 to -0.0219090400419653
-0.753566644114609 to -0.753907334633868
-0.708609125537034 to -0.708949816056293
Changing layer 3's weights from 
-0.0194829022113052 to -0.0198235927305642
-0.283035985154267 to -0.283376675673526
-0.0404260670367441 to -0.0407667575560032
-0.947865298792888 to -0.948205989312147
-0.163101962727662 to -0.163442653246921
-0.293350568932649 to -0.293691259451908
-0.623535416049118 to -0.623876106568377
-0.165022543591615 to -0.165363234110874
-0.496718517464753 to -0.497059207984012
-0.0238886391345232 to -0.0242293296537822
Changing layer 4's weights from 
-0.123162439984437 to -0.123503130503696
-0.331995240372773 to -0.332335930892032
-0.532472840470429 to -0.532813530989688
-0.245538107079621 to -0.24587879759888
-0.48479091278946 to -0.485131603308719
-0.253325692338105 to -0.253666382857364
-0.257684222382661 to -0.25802491290192
-0.719581744593735 to -0.719922435112994
-0.125885656994935 to -0.126226347514194
-0.558187595528718 to -0.558528286047977
Changing layer 5's weights from 
0.0296173775967398 to 0.0292766870774807
-0.412456146401521 to -0.41279683692078
0.0127322161968988 to 0.0123915256776397
-0.922779965174553 to -0.923120655693812
-0.385511151475068 to -0.385851841994327
-0.783979094547387 to -0.784319785066646
-0.484013787430878 to -0.484354477950137
-0.411567560357209 to -0.411908250876468
-0.0219655548754892 to -0.0223062453947482
-0.0284736668292252 to -0.0288143573484842
Trying to learn from memory 56, 1, -0.2
sum 0.0410947012079111 distri 0.0209142988317765
Using diff 0.00990672707415683 and condRate 0.166666666666667
Changed category 1 weights from 
0.283740523685487 to 0.283410299444761
0.517056110729249 to 0.516725886488523
0.0921308728351912 to 0.0918006485944652
0.0962236853733382 to 0.0958934611326122
Changing layer 0's weights from 
-0.215902787276309 to -0.216233011517035
-0.587829243966144 to -0.58815946820687
-0.822178509660762 to -0.822508733901488
-0.787619259782832 to -0.787949484023558
-0.0397516904554781 to -0.0400819146962041
-0.722393643685382 to -0.722723867926108
-0.568563920088809 to -0.568894144329535
-0.596018326111835 to -0.596348550352561
-0.833510187097591 to -0.833840411338317
-0.877260242231887 to -0.877590466472613
Changing layer 1's weights from 
-0.872163329847854 to -0.87249355408858
-0.270994645186466 to -0.271324869427192
-0.396697383948368 to -0.397027608189094
-0.406602722235721 to -0.406932946476447
-0.591655504532855 to -0.591985728773581
-0.729893338509601 to -0.730223562750327
-0.459174614974063 to -0.459504839214789
-0.844977643915218 to -0.845307868155944
-0.433314663001102 to -0.433644887241828
-0.795961525865596 to -0.796291750106322
Changing layer 2's weights from 
0.0132267059602317 to 0.0128964817195058
-0.540826779433292 to -0.541157003674018
-0.256237488814395 to -0.256567713055121
-0.0850373922071873 to -0.0853676164479133
-0.262290102072757 to -0.262620326313483
-0.468299728461307 to -0.468629952702033
0.0505129875459257 to 0.0501827633051997
-0.0219090400419653 to -0.0222392642826912
-0.753907334633868 to -0.754237558874594
-0.708949816056293 to -0.709280040297019
Changing layer 3's weights from 
-0.0198235927305642 to -0.0201538169712902
-0.283376675673526 to -0.283706899914252
-0.0407667575560032 to -0.0410969817967292
-0.948205989312147 to -0.948536213552873
-0.163442653246921 to -0.163772877487647
-0.293691259451908 to -0.294021483692634
-0.623876106568377 to -0.624206330809103
-0.165363234110874 to -0.1656934583516
-0.497059207984012 to -0.497389432224738
-0.0242293296537822 to -0.0245595538945082
Changing layer 4's weights from 
-0.123503130503696 to -0.123833354744422
-0.332335930892032 to -0.332666155132758
-0.532813530989688 to -0.533143755230414
-0.24587879759888 to -0.246209021839606
-0.485131603308719 to -0.485461827549445
-0.253666382857364 to -0.25399660709809
-0.25802491290192 to -0.258355137142646
-0.719922435112994 to -0.72025265935372
-0.126226347514194 to -0.12655657175492
-0.558528286047977 to -0.558858510288703
Changing layer 5's weights from 
0.0292766870774807 to 0.0289464628367548
-0.41279683692078 to -0.413127061161506
0.0123915256776397 to 0.0120613014369138
-0.923120655693812 to -0.923450879934538
-0.385851841994327 to -0.386182066235053
-0.784319785066646 to -0.784650009307372
-0.484354477950137 to -0.484684702190863
-0.411908250876468 to -0.412238475117194
-0.0223062453947482 to -0.0226364696354742
-0.0288143573484842 to -0.0291445815892102
Trying to learn from memory 57, 0, -0.2
sum 0.0404372549175183 distri -0.0206867427522414
Using diff 0.0510146839403801 and condRate 0.166666666666667
Changed category 0 weights from 
0.131529622392521 to 0.129829132902503
-0.268505341692104 to -0.270205831182122
-0.551026038212909 to -0.552726527702928
-0.399520672245158 to -0.401221161735177
Changing layer 0's weights from 
-0.216233011517035 to -0.217933501007054
-0.58815946820687 to -0.589859957696888
-0.822508733901488 to -0.824209223391506
-0.787949484023558 to -0.789649973513576
-0.0400819146962041 to -0.0417824041862227
-0.722723867926108 to -0.724424357416127
-0.568894144329535 to -0.570594633819553
-0.596348550352561 to -0.598049039842579
-0.833840411338317 to -0.835540900828335
-0.877590466472613 to -0.879290955962631
Changing layer 1's weights from 
-0.87249355408858 to -0.874194043578598
-0.271324869427192 to -0.273025358917211
-0.397027608189094 to -0.398728097679113
-0.406932946476447 to -0.408633435966466
-0.591985728773581 to -0.5936862182636
-0.730223562750327 to -0.731924052240346
-0.459504839214789 to -0.461205328704808
-0.845307868155944 to -0.847008357645962
-0.433644887241828 to -0.435345376731847
-0.796291750106322 to -0.79799223959634
Changing layer 2's weights from 
0.0128964817195058 to 0.0111959922294872
-0.541157003674018 to -0.542857493164036
-0.256567713055121 to -0.25826820254514
-0.0853676164479133 to -0.0870681059379319
-0.262620326313483 to -0.264320815803502
-0.468629952702033 to -0.470330442192052
0.0501827633051997 to 0.0484822738151811
-0.0222392642826912 to -0.0239397537727098
-0.754237558874594 to -0.755938048364613
-0.709280040297019 to -0.710980529787037
Changing layer 3's weights from 
-0.0201538169712902 to -0.0218543064613088
-0.283706899914252 to -0.285407389404271
-0.0410969817967292 to -0.0427974712867478
-0.948536213552873 to -0.950236703042891
-0.163772877487647 to -0.165473366977666
-0.294021483692634 to -0.295721973182653
-0.624206330809103 to -0.625906820299122
-0.1656934583516 to -0.167393947841619
-0.497389432224738 to -0.499089921714757
-0.0245595538945082 to -0.0262600433845268
Changing layer 4's weights from 
-0.123833354744422 to -0.125533844234441
-0.332666155132758 to -0.334366644622777
-0.533143755230414 to -0.534844244720432
-0.246209021839606 to -0.247909511329625
-0.485461827549445 to -0.487162317039464
-0.25399660709809 to -0.255697096588109
-0.258355137142646 to -0.260055626632665
-0.72025265935372 to -0.721953148843739
-0.12655657175492 to -0.128257061244939
-0.558858510288703 to -0.560558999778721
Changing layer 5's weights from 
0.0289464628367548 to 0.0272459733467362
-0.413127061161506 to -0.414827550651525
0.0120613014369138 to 0.0103608119468952
-0.923450879934538 to -0.925151369424556
-0.386182066235053 to -0.387882555725072
-0.784650009307372 to -0.78635049879739
-0.484684702190863 to -0.486385191680882
-0.412238475117194 to -0.413938964607213
-0.0226364696354742 to -0.0243369591254928
-0.0291445815892102 to -0.0308450710792288
Trying to learn from memory 57, 1, -0.2
sum 0.0404372549175183 distri 0.0206365436811324
Using diff 0.00969139750700639 and condRate 0.166666666666667
Changed category 1 weights from 
0.283410299444761 to 0.283087252856381
0.516725886488523 to 0.516402839900142
0.0918006485944652 to 0.0914776020060846
0.0958934611326122 to 0.0955704145442316
Changing layer 0's weights from 
-0.217933501007054 to -0.218256547595434
-0.589859957696888 to -0.590183004285269
-0.824209223391506 to -0.824532269979887
-0.789649973513576 to -0.789973020101957
-0.0417824041862227 to -0.0421054507746033
-0.724424357416127 to -0.724747404004507
-0.570594633819553 to -0.570917680407934
-0.598049039842579 to -0.59837208643096
-0.835540900828335 to -0.835863947416716
-0.879290955962631 to -0.879614002551012
Changing layer 1's weights from 
-0.874194043578598 to -0.874517090166979
-0.273025358917211 to -0.273348405505591
-0.398728097679113 to -0.399051144267493
-0.408633435966466 to -0.408956482554846
-0.5936862182636 to -0.59400926485198
-0.731924052240346 to -0.732247098828726
-0.461205328704808 to -0.461528375293188
-0.847008357645962 to -0.847331404234343
-0.435345376731847 to -0.435668423320227
-0.79799223959634 to -0.798315286184721
Changing layer 2's weights from 
0.0111959922294872 to 0.0108729456411065
-0.542857493164036 to -0.543180539752417
-0.25826820254514 to -0.25859124913352
-0.0870681059379319 to -0.0873911525263125
-0.264320815803502 to -0.264643862391882
-0.470330442192052 to -0.470653488780432
0.0484822738151811 to 0.0481592272268005
-0.0239397537727098 to -0.0242628003610905
-0.755938048364613 to -0.756261094952993
-0.710980529787037 to -0.711303576375418
Changing layer 3's weights from 
-0.0218543064613088 to -0.0221773530496894
-0.285407389404271 to -0.285730435992651
-0.0427974712867478 to -0.0431205178751284
-0.950236703042891 to -0.950559749631272
-0.165473366977666 to -0.165796413566046
-0.295721973182653 to -0.296045019771033
-0.625906820299122 to -0.626229866887502
-0.167393947841619 to -0.167716994429999
-0.499089921714757 to -0.499412968303137
-0.0262600433845268 to -0.0265830899729074
Changing layer 4's weights from 
-0.125533844234441 to -0.125856890822821
-0.334366644622777 to -0.334689691211157
-0.534844244720432 to -0.535167291308813
-0.247909511329625 to -0.248232557918005
-0.487162317039464 to -0.487485363627844
-0.255697096588109 to -0.256020143176489
-0.260055626632665 to -0.260378673221045
-0.721953148843739 to -0.72227619543212
-0.128257061244939 to -0.128580107833319
-0.560558999778721 to -0.560882046367102
Changing layer 5's weights from 
0.0272459733467362 to 0.0269229267583555
-0.414827550651525 to -0.415150597239905
0.0103608119468952 to 0.0100377653585145
-0.925151369424556 to -0.925474416012937
-0.387882555725072 to -0.388205602313452
-0.78635049879739 to -0.786673545385771
-0.486385191680882 to -0.486708238269262
-0.413938964607213 to -0.414262011195593
-0.0243369591254928 to -0.0246600057138735
-0.0308450710792288 to -0.0311681176676094
Trying to learn from memory 58, 0, -0.2
sum 0.0400233670489128 distri -0.0205981108459594
Using diff 0.050615636132644 and condRate 0.166666666666667
Changed category 0 weights from 
0.129829132902503 to 0.128141945006274
-0.270205831182122 to -0.271893019078351
-0.552726527702928 to -0.554413715599157
-0.401221161735177 to -0.402908349631406
Changing layer 0's weights from 
-0.218256547595434 to -0.219943735491664
-0.590183004285269 to -0.591870192181498
-0.824532269979887 to -0.826219457876116
-0.789973020101957 to -0.791660207998186
-0.0421054507746033 to -0.0437926386708325
-0.724747404004507 to -0.726434591900737
-0.570917680407934 to -0.572604868304163
-0.59837208643096 to -0.600059274327189
-0.835863947416716 to -0.837551135312945
-0.879614002551012 to -0.881301190447241
Changing layer 1's weights from 
-0.874517090166979 to -0.876204278063208
-0.273348405505591 to -0.275035593401821
-0.399051144267493 to -0.400738332163723
-0.408956482554846 to -0.410643670451076
-0.59400926485198 to -0.595696452748209
-0.732247098828726 to -0.733934286724955
-0.461528375293188 to -0.463215563189417
-0.847331404234343 to -0.849018592130572
-0.435668423320227 to -0.437355611216456
-0.798315286184721 to -0.80000247408095
Changing layer 2's weights from 
0.0108729456411065 to 0.00918575774487734
-0.543180539752417 to -0.544867727648646
-0.25859124913352 to -0.26027843702975
-0.0873911525263125 to -0.0890783404225417
-0.264643862391882 to -0.266331050288112
-0.470653488780432 to -0.472340676676661
0.0481592272268005 to 0.0464720393305713
-0.0242628003610905 to -0.0259499882573197
-0.756261094952993 to -0.757948282849223
-0.711303576375418 to -0.712990764271647
Changing layer 3's weights from 
-0.0221773530496894 to -0.0238645409459186
-0.285730435992651 to -0.287417623888881
-0.0431205178751284 to -0.0448077057713576
-0.950559749631272 to -0.952246937527501
-0.165796413566046 to -0.167483601462276
-0.296045019771033 to -0.297732207667263
-0.626229866887502 to -0.627917054783732
-0.167716994429999 to -0.169404182326229
-0.499412968303137 to -0.501100156199367
-0.0265830899729074 to -0.0282702778691366
Changing layer 4's weights from 
-0.125856890822821 to -0.12754407871905
-0.334689691211157 to -0.336376879107387
-0.535167291308813 to -0.536854479205042
-0.248232557918005 to -0.249919745814234
-0.487485363627844 to -0.489172551524074
-0.256020143176489 to -0.257707331072718
-0.260378673221045 to -0.262065861117275
-0.72227619543212 to -0.723963383328349
-0.128580107833319 to -0.130267295729549
-0.560882046367102 to -0.562569234263331
Changing layer 5's weights from 
0.0269229267583555 to 0.0252357388621263
-0.415150597239905 to -0.416837785136135
0.0100377653585145 to 0.00835057746228534
-0.925474416012937 to -0.927161603909166
-0.388205602313452 to -0.389892790209682
-0.786673545385771 to -0.788360733282
-0.486708238269262 to -0.488395426165491
-0.414262011195593 to -0.415949199091823
-0.0246600057138735 to -0.0263471936101026
-0.0311681176676094 to -0.0328553055638386
10/5/2016 1:32:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:32:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:12 PMStarting learning phase with deltaScore: 3.2
Modified index 0's learning in memoryPool to 0.64
Modified index 1's learning in memoryPool to 0.64
Modified index 2's learning in memoryPool to 0.64
Modified index 3's learning in memoryPool to 0.64
Modified index 4's learning in memoryPool to 0.64
10/5/2016 1:33:12 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 41, 0, 0.64
sum 0.0381997992319733 distri -0.0222294941933826
Using diff 0.0508793436173625 and condRate 0.166666666666667
Changed category 0 weights from 
0.128141945006274 to 0.13356907487082
-0.271893019078351 to -0.266465889213805
-0.554413715599157 to -0.548986585734611
-0.402908349631406 to -0.39748121976686
Changing layer 0's weights from 
-0.219943735491664 to -0.214516605627117
-0.591870192181498 to -0.586443062316952
-0.826219457876116 to -0.82079232801157
-0.791660207998186 to -0.78623307813364
-0.0437926386708325 to -0.0383655088062863
-0.726434591900737 to -0.72100746203619
-0.572604868304163 to -0.567177738439617
-0.600059274327189 to -0.594632144462643
-0.837551135312945 to -0.832124005448399
-0.881301190447241 to -0.875874060582695
Changing layer 1's weights from 
-0.876204278063208 to -0.870777148198662
-0.275035593401821 to -0.269608463537274
-0.400738332163723 to -0.395311202299176
-0.410643670451076 to -0.405216540586529
-0.595696452748209 to -0.590269322883663
-0.733934286724955 to -0.728507156860409
-0.463215563189417 to -0.457788433324871
-0.849018592130572 to -0.843591462266026
-0.437355611216456 to -0.43192848135191
-0.80000247408095 to -0.794575344216404
Changing layer 2's weights from 
0.00918575774487734 to 0.0146128876094235
-0.544867727648646 to -0.5394405977841
-0.26027843702975 to -0.254851307165203
-0.0890783404225417 to -0.0836512105579955
-0.266331050288112 to -0.260903920423565
-0.472340676676661 to -0.466913546812115
0.0464720393305713 to 0.0518991691951175
-0.0259499882573197 to -0.0205228583927735
-0.757948282849223 to -0.752521152984676
-0.712990764271647 to -0.707563634407101
Changing layer 3's weights from 
-0.0238645409459186 to -0.0184374110813724
-0.287417623888881 to -0.281990494024334
-0.0448077057713576 to -0.0393805759068114
-0.952246937527501 to -0.946819807662955
-0.167483601462276 to -0.162056471597729
-0.297732207667263 to -0.292305077802716
-0.627917054783732 to -0.622489924919186
-0.169404182326229 to -0.163977052461682
-0.501100156199367 to -0.49567302633482
-0.0282702778691366 to -0.0228431480045904
Changing layer 4's weights from 
-0.12754407871905 to -0.122116948854504
-0.336376879107387 to -0.33094974924284
-0.536854479205042 to -0.531427349340496
-0.249919745814234 to -0.244492615949688
-0.489172551524074 to -0.483745421659527
-0.257707331072718 to -0.252280201208172
-0.262065861117275 to -0.256638731252728
-0.723963383328349 to -0.718536253463803
-0.130267295729549 to -0.124840165865002
-0.562569234263331 to -0.557142104398785
Changing layer 5's weights from 
0.0252357388621263 to 0.0306628687266725
-0.416837785136135 to -0.411410655271588
0.00835057746228534 to 0.0137777073268315
-0.927161603909166 to -0.92173447404462
-0.389892790209682 to -0.384465660345135
-0.788360733282 to -0.782933603417454
-0.488395426165491 to -0.482968296300945
-0.415949199091823 to -0.410522069227276
-0.0263471936101026 to -0.0209200637455565
-0.0328553055638386 to -0.0274281756992924
Trying to learn from memory 59, 1, 0.64
sum 0.0380899461365429 distri 0.0203545550096642
Using diff 0.00821290459274301 and condRate 0.166666666666667
Changed category 1 weights from 
0.283087252856381 to 0.283963295993359
0.516402839900142 to 0.517278883037121
0.0914776020060846 to 0.0923536451430628
0.0955704145442316 to 0.0964464576812097
Changing layer 0's weights from 
-0.214516605627117 to -0.213640562490139
-0.586443062316952 to -0.585567019179974
-0.82079232801157 to -0.819916284874592
-0.78623307813364 to -0.785357034996662
-0.0383655088062863 to -0.0374894656693082
-0.72100746203619 to -0.720131418899212
-0.567177738439617 to -0.566301695302639
-0.594632144462643 to -0.593756101325665
-0.832124005448399 to -0.831247962311421
-0.875874060582695 to -0.874998017445717
Changing layer 1's weights from 
-0.870777148198662 to -0.869901105061684
-0.269608463537274 to -0.268732420400296
-0.395311202299176 to -0.394435159162198
-0.405216540586529 to -0.404340497449551
-0.590269322883663 to -0.589393279746685
-0.728507156860409 to -0.727631113723431
-0.457788433324871 to -0.456912390187893
-0.843591462266026 to -0.842715419129048
-0.43192848135191 to -0.431052438214932
-0.794575344216404 to -0.793699301079426
Changing layer 2's weights from 
0.0146128876094235 to 0.0154889307464017
-0.5394405977841 to -0.538564554647122
-0.254851307165203 to -0.253975264028225
-0.0836512105579955 to -0.0827751674210173
-0.260903920423565 to -0.260027877286587
-0.466913546812115 to -0.466037503675137
0.0518991691951175 to 0.0527752123320956
-0.0205228583927735 to -0.0196468152557953
-0.752521152984676 to -0.751645109847698
-0.707563634407101 to -0.706687591270123
Changing layer 3's weights from 
-0.0184374110813724 to -0.0175613679443943
-0.281990494024334 to -0.281114450887356
-0.0393805759068114 to -0.0385045327698332
-0.946819807662955 to -0.945943764525977
-0.162056471597729 to -0.161180428460751
-0.292305077802716 to -0.291429034665738
-0.622489924919186 to -0.621613881782207
-0.163977052461682 to -0.163101009324704
-0.49567302633482 to -0.494796983197842
-0.0228431480045904 to -0.0219671048676123
Changing layer 4's weights from 
-0.122116948854504 to -0.121240905717526
-0.33094974924284 to -0.330073706105862
-0.531427349340496 to -0.530551306203518
-0.244492615949688 to -0.24361657281271
-0.483745421659527 to -0.482869378522549
-0.252280201208172 to -0.251404158071194
-0.256638731252728 to -0.25576268811575
-0.718536253463803 to -0.717660210326824
-0.124840165865002 to -0.123964122728024
-0.557142104398785 to -0.556266061261807
Changing layer 5's weights from 
0.0306628687266725 to 0.0315389118636507
-0.411410655271588 to -0.41053461213461
0.0137777073268315 to 0.0146537504638097
-0.92173447404462 to -0.920858430907642
-0.384465660345135 to -0.383589617208157
-0.782933603417454 to -0.782057560280476
-0.482968296300945 to -0.482092253163967
-0.410522069227276 to -0.409646026090298
-0.0209200637455565 to -0.0200440206085783
-0.0274281756992924 to -0.0265521325623143
Trying to learn from memory 60, 1, 0.64
sum 0.0380899461365429 distri 0.0203545550096642
Using diff 0.00821290459274301 and condRate 0.166666666666667
Changed category 1 weights from 
0.283963295993359 to 0.284839339130337
0.517278883037121 to 0.518154926174099
0.0923536451430628 to 0.0932296882800409
0.0964464576812097 to 0.0973225008181879
Changing layer 0's weights from 
-0.213640562490139 to -0.212764519353161
-0.585567019179974 to -0.584690976042995
-0.819916284874592 to -0.819040241737614
-0.785357034996662 to -0.784480991859684
-0.0374894656693082 to -0.03661342253233
-0.720131418899212 to -0.719255375762234
-0.566301695302639 to -0.565425652165661
-0.593756101325665 to -0.592880058188686
-0.831247962311421 to -0.830371919174443
-0.874998017445717 to -0.874121974308739
Changing layer 1's weights from 
-0.869901105061684 to -0.869025061924706
-0.268732420400296 to -0.267856377263318
-0.394435159162198 to -0.39355911602522
-0.404340497449551 to -0.403464454312573
-0.589393279746685 to -0.588517236609707
-0.727631113723431 to -0.726755070586453
-0.456912390187893 to -0.456036347050915
-0.842715419129048 to -0.84183937599207
-0.431052438214932 to -0.430176395077954
-0.793699301079426 to -0.792823257942448
Changing layer 2's weights from 
0.0154889307464017 to 0.0163649738833799
-0.538564554647122 to -0.537688511510144
-0.253975264028225 to -0.253099220891247
-0.0827751674210173 to -0.0818991242840392
-0.260027877286587 to -0.259151834149609
-0.466037503675137 to -0.465161460538159
0.0527752123320956 to 0.0536512554690738
-0.0196468152557953 to -0.0187707721188171
-0.751645109847698 to -0.75076906671072
-0.706687591270123 to -0.705811548133145
Changing layer 3's weights from 
-0.0175613679443943 to -0.0166853248074161
-0.281114450887356 to -0.280238407750378
-0.0385045327698332 to -0.0376284896328551
-0.945943764525977 to -0.945067721388998
-0.161180428460751 to -0.160304385323773
-0.291429034665738 to -0.29055299152876
-0.621613881782207 to -0.620737838645229
-0.163101009324704 to -0.162224966187726
-0.494796983197842 to -0.493920940060864
-0.0219671048676123 to -0.0210910617306341
Changing layer 4's weights from 
-0.121240905717526 to -0.120364862580548
-0.330073706105862 to -0.329197662968884
-0.530551306203518 to -0.52967526306654
-0.24361657281271 to -0.242740529675732
-0.482869378522549 to -0.481993335385571
-0.251404158071194 to -0.250528114934216
-0.25576268811575 to -0.254886644978772
-0.717660210326824 to -0.716784167189846
-0.123964122728024 to -0.123088079591046
-0.556266061261807 to -0.555390018124829
Changing layer 5's weights from 
0.0315389118636507 to 0.0324149550006289
-0.41053461213461 to -0.409658568997632
0.0146537504638097 to 0.0155297936007879
-0.920858430907642 to -0.919982387770663
-0.383589617208157 to -0.382713574071179
-0.782057560280476 to -0.781181517143498
-0.482092253163967 to -0.481216210026989
-0.409646026090298 to -0.40876998295332
-0.0200440206085783 to -0.0191679774716001
-0.0265521325623143 to -0.0256760894253361
Trying to learn from memory 61, 0, 0.64
sum 0.0380899461365429 distri -0.0229029492064008
Using diff 0.051470408808808 and condRate 0.166666666666667
Changed category 0 weights from 
0.13356907487082 to 0.139059251687711
-0.266465889213805 to -0.260975712396914
-0.548986585734611 to -0.54349640891772
-0.39748121976686 to -0.391991042949969
Changing layer 0's weights from 
-0.212764519353161 to -0.20727434253627
-0.584690976042995 to -0.579200799226104
-0.819040241737614 to -0.813550064920722
-0.784480991859684 to -0.778990815042792
-0.03661342253233 to -0.0311232457154388
-0.719255375762234 to -0.713765198945343
-0.565425652165661 to -0.55993547534877
-0.592880058188686 to -0.587389881371795
-0.830371919174443 to -0.824881742357551
-0.874121974308739 to -0.868631797491847
Changing layer 1's weights from 
-0.869025061924706 to -0.863534885107814
-0.267856377263318 to -0.262366200446427
-0.39355911602522 to -0.388068939208329
-0.403464454312573 to -0.397974277495682
-0.588517236609707 to -0.583027059792816
-0.726755070586453 to -0.721264893769562
-0.456036347050915 to -0.450546170234024
-0.84183937599207 to -0.836349199175178
-0.430176395077954 to -0.424686218261063
-0.792823257942448 to -0.787333081125556
Changing layer 2's weights from 
0.0163649738833799 to 0.021855150700271
-0.537688511510144 to -0.532198334693253
-0.253099220891247 to -0.247609044074356
-0.0818991242840392 to -0.076408947467148
-0.259151834149609 to -0.253661657332718
-0.465161460538159 to -0.459671283721268
0.0536512554690738 to 0.0591414322859649
-0.0187707721188171 to -0.013280595301926
-0.75076906671072 to -0.745278889893829
-0.705811548133145 to -0.700321371316253
Changing layer 3's weights from 
-0.0166853248074161 to -0.0111951479905249
-0.280238407750378 to -0.274748230933487
-0.0376284896328551 to -0.0321383128159639
-0.945067721388998 to -0.939577544572107
-0.160304385323773 to -0.154814208506882
-0.29055299152876 to -0.285062814711869
-0.620737838645229 to -0.615247661828338
-0.162224966187726 to -0.156734789370835
-0.493920940060864 to -0.488430763243973
-0.0210910617306341 to -0.0156008849137429
Changing layer 4's weights from 
-0.120364862580548 to -0.114874685763657
-0.329197662968884 to -0.323707486151993
-0.52967526306654 to -0.524185086249648
-0.242740529675732 to -0.237250352858841
-0.481993335385571 to -0.47650315856868
-0.250528114934216 to -0.245037938117325
-0.254886644978772 to -0.249396468161881
-0.716784167189846 to -0.711293990372955
-0.123088079591046 to -0.117597902774155
-0.555390018124829 to -0.549899841307937
Changing layer 5's weights from 
0.0324149550006289 to 0.03790513181752
-0.409658568997632 to -0.404168392180741
0.0155297936007879 to 0.021019970417679
-0.919982387770663 to -0.914492210953772
-0.382713574071179 to -0.377223397254288
-0.781181517143498 to -0.775691340326606
-0.481216210026989 to -0.475726033210098
-0.40876998295332 to -0.403279806136429
-0.0191679774716001 to -0.013677800654709
-0.0256760894253361 to -0.0201859126084449
Trying to learn from memory 62, 1, 0.64
sum 0.0380899461365429 distri 0.0203545550096642
Using diff 0.00821290459274301 and condRate 0.166666666666667
Changed category 1 weights from 
0.284839339130337 to 0.285715382267315
0.518154926174099 to 0.519030969311077
0.0932296882800409 to 0.0941057314170191
0.0973225008181879 to 0.098198543955166
Changing layer 0's weights from 
-0.20727434253627 to -0.206398299399292
-0.579200799226104 to -0.578324756089126
-0.813550064920722 to -0.812674021783744
-0.778990815042792 to -0.778114771905814
-0.0311232457154388 to -0.0302472025784607
-0.713765198945343 to -0.712889155808365
-0.55993547534877 to -0.559059432211791
-0.587389881371795 to -0.586513838234817
-0.824881742357551 to -0.824005699220573
-0.868631797491847 to -0.867755754354869
Changing layer 1's weights from 
-0.863534885107814 to -0.862658841970836
-0.262366200446427 to -0.261490157309449
-0.388068939208329 to -0.387192896071351
-0.397974277495682 to -0.397098234358704
-0.583027059792816 to -0.582151016655838
-0.721264893769562 to -0.720388850632584
-0.450546170234024 to -0.449670127097046
-0.836349199175178 to -0.8354731560382
-0.424686218261063 to -0.423810175124085
-0.787333081125556 to -0.786457037988578
Changing layer 2's weights from 
0.021855150700271 to 0.0227311938372492
-0.532198334693253 to -0.531322291556274
-0.247609044074356 to -0.246733000937378
-0.076408947467148 to -0.0755329043301699
-0.253661657332718 to -0.25278561419574
-0.459671283721268 to -0.45879524058429
0.0591414322859649 to 0.0600174754229431
-0.013280595301926 to -0.0124045521649478
-0.745278889893829 to -0.744402846756851
-0.700321371316253 to -0.699445328179275
Changing layer 3's weights from 
-0.0111951479905249 to -0.0103191048535468
-0.274748230933487 to -0.273872187796509
-0.0321383128159639 to -0.0312622696789858
-0.939577544572107 to -0.938701501435129
-0.154814208506882 to -0.153938165369904
-0.285062814711869 to -0.284186771574891
-0.615247661828338 to -0.61437161869136
-0.156734789370835 to -0.155858746233857
-0.488430763243973 to -0.487554720106995
-0.0156008849137429 to -0.0147248417767648
Changing layer 4's weights from 
-0.114874685763657 to -0.113998642626679
-0.323707486151993 to -0.322831443015015
-0.524185086249648 to -0.52330904311267
-0.237250352858841 to -0.236374309721863
-0.47650315856868 to -0.475627115431702
-0.245037938117325 to -0.244161894980347
-0.249396468161881 to -0.248520425024903
-0.711293990372955 to -0.710417947235977
-0.117597902774155 to -0.116721859637177
-0.549899841307937 to -0.549023798170959
Changing layer 5's weights from 
0.03790513181752 to 0.0387811749544982
-0.404168392180741 to -0.403292349043763
0.021019970417679 to 0.0218960135546572
-0.914492210953772 to -0.913616167816794
-0.377223397254288 to -0.37634735411731
-0.775691340326606 to -0.774815297189628
-0.475726033210098 to -0.47484999007312
-0.403279806136429 to -0.402403762999451
-0.013677800654709 to -0.0128017575177308
-0.0201859126084449 to -0.0193098694714668
10/5/2016 1:33:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:52 PMStarting learning phase with deltaScore: 3.6
Modified index 0's learning in memoryPool to 0.72
Modified index 1's learning in memoryPool to 0.72
Modified index 2's learning in memoryPool to 0.72
10/5/2016 1:33:52 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 63, 1, 0.72
sum 0.0383862857734183 distri 0.0201461365455766
Using diff 0.00864357778448714 and condRate 0.166666666666667
Changed category 1 weights from 
0.285715382267315 to 0.286752611556803
0.519030969311077 to 0.520068198600565
0.0941057314170191 to 0.0951429607065071
0.098198543955166 to 0.0992357732446541
Changing layer 0's weights from 
-0.206398299399292 to -0.205361070109804
-0.578324756089126 to -0.577287526799638
-0.812674021783744 to -0.811636792494256
-0.778114771905814 to -0.777077542616326
-0.0302472025784607 to -0.0292099732889727
-0.712889155808365 to -0.711851926518877
-0.559059432211791 to -0.558022202922303
-0.586513838234817 to -0.585476608945329
-0.824005699220573 to -0.822968469931085
-0.867755754354869 to -0.866718525065381
Changing layer 1's weights from 
-0.862658841970836 to -0.861621612681348
-0.261490157309449 to -0.260452928019961
-0.387192896071351 to -0.386155666781863
-0.397098234358704 to -0.396061005069216
-0.582151016655838 to -0.58111378736635
-0.720388850632584 to -0.719351621343095
-0.449670127097046 to -0.448632897807558
-0.8354731560382 to -0.834435926748712
-0.423810175124085 to -0.422772945834597
-0.786457037988578 to -0.78541980869909
Changing layer 2's weights from 
0.0227311938372492 to 0.0237684231267372
-0.531322291556274 to -0.530285062266786
-0.246733000937378 to -0.24569577164789
-0.0755329043301699 to -0.0744956750406818
-0.25278561419574 to -0.251748384906252
-0.45879524058429 to -0.457758011294802
0.0600174754229431 to 0.0610547047124311
-0.0124045521649478 to -0.0113673228754598
-0.744402846756851 to -0.743365617467363
-0.699445328179275 to -0.698408098889787
Changing layer 3's weights from 
-0.0103191048535468 to -0.00928187556405876
-0.273872187796509 to -0.272834958507021
-0.0312622696789858 to -0.0302250403894977
-0.938701501435129 to -0.937664272145641
-0.153938165369904 to -0.152900936080416
-0.284186771574891 to -0.283149542285403
-0.61437161869136 to -0.613334389401872
-0.155858746233857 to -0.154821516944369
-0.487554720106995 to -0.486517490817507
-0.0147248417767648 to -0.0136876124872768
Changing layer 4's weights from 
-0.113998642626679 to -0.112961413337191
-0.322831443015015 to -0.321794213725527
-0.52330904311267 to -0.522271813823182
-0.236374309721863 to -0.235337080432375
-0.475627115431702 to -0.474589886142214
-0.244161894980347 to -0.243124665690859
-0.248520425024903 to -0.247483195735415
-0.710417947235977 to -0.709380717946489
-0.116721859637177 to -0.115684630347689
-0.549023798170959 to -0.547986568881471
Changing layer 5's weights from 
0.0387811749544982 to 0.0398184042439862
-0.403292349043763 to -0.402255119754275
0.0218960135546572 to 0.0229332428441452
-0.913616167816794 to -0.912578938527306
-0.37634735411731 to -0.375310124827822
-0.774815297189628 to -0.77377806790014
-0.47484999007312 to -0.473812760783632
-0.402403762999451 to -0.401366533709963
-0.0128017575177308 to -0.0117645282282428
-0.0193098694714668 to -0.0182726401819788
Trying to learn from memory 64, 1, 0.72
sum 0.0383862857734183 distri 0.0201461365455766
Using diff 0.00864357778448714 and condRate 0.166666666666667
Changed category 1 weights from 
0.286752611556803 to 0.287789840846291
0.520068198600565 to 0.521105427890053
0.0951429607065071 to 0.0961801899959951
0.0992357732446541 to 0.100273002534142
Changing layer 0's weights from 
-0.205361070109804 to -0.204323840820316
-0.577287526799638 to -0.57625029751015
-0.811636792494256 to -0.810599563204768
-0.777077542616326 to -0.776040313326838
-0.0292099732889727 to -0.0281727439994846
-0.711851926518877 to -0.710814697229389
-0.558022202922303 to -0.556984973632815
-0.585476608945329 to -0.584439379655841
-0.822968469931085 to -0.821931240641597
-0.866718525065381 to -0.865681295775893
Changing layer 1's weights from 
-0.861621612681348 to -0.86058438339186
-0.260452928019961 to -0.259415698730473
-0.386155666781863 to -0.385118437492375
-0.396061005069216 to -0.395023775779728
-0.58111378736635 to -0.580076558076862
-0.719351621343095 to -0.718314392053607
-0.448632897807558 to -0.44759566851807
-0.834435926748712 to -0.833398697459224
-0.422772945834597 to -0.421735716545109
-0.78541980869909 to -0.784382579409602
Changing layer 2's weights from 
0.0237684231267372 to 0.0248056524162252
-0.530285062266786 to -0.529247832977298
-0.24569577164789 to -0.244658542358402
-0.0744956750406818 to -0.0734584457511938
-0.251748384906252 to -0.250711155616764
-0.457758011294802 to -0.456720782005314
0.0610547047124311 to 0.0620919340019191
-0.0113673228754598 to -0.0103300935859718
-0.743365617467363 to -0.742328388177875
-0.698408098889787 to -0.697370869600299
Changing layer 3's weights from 
-0.00928187556405876 to -0.00824464627457074
-0.272834958507021 to -0.271797729217533
-0.0302250403894977 to -0.0291878111000097
-0.937664272145641 to -0.936627042856153
-0.152900936080416 to -0.151863706790928
-0.283149542285403 to -0.282112312995915
-0.613334389401872 to -0.612297160112384
-0.154821516944369 to -0.153784287654881
-0.486517490817507 to -0.485480261528019
-0.0136876124872768 to -0.0126503831977887
Changing layer 4's weights from 
-0.112961413337191 to -0.111924184047703
-0.321794213725527 to -0.320756984436039
-0.522271813823182 to -0.521234584533694
-0.235337080432375 to -0.234299851142887
-0.474589886142214 to -0.473552656852726
-0.243124665690859 to -0.242087436401371
-0.247483195735415 to -0.246445966445927
-0.709380717946489 to -0.708343488657001
-0.115684630347689 to -0.114647401058201
-0.547986568881471 to -0.546949339591983
Changing layer 5's weights from 
0.0398184042439862 to 0.0408556335334742
-0.402255119754275 to -0.401217890464787
0.0229332428441452 to 0.0239704721336332
-0.912578938527306 to -0.911541709237818
-0.375310124827822 to -0.374272895538334
-0.77377806790014 to -0.772740838610652
-0.473812760783632 to -0.472775531494144
-0.401366533709963 to -0.400329304420475
-0.0117645282282428 to -0.0107272989387548
-0.0182726401819788 to -0.0172354108924907
Trying to learn from memory 65, 1, 0.72
sum 0.0383862857734183 distri 0.0201461365455766
Using diff 0.00864357778448714 and condRate 0.166666666666667
Changed category 1 weights from 
0.287789840846291 to 0.288827070135779
0.521105427890053 to 0.522142657179541
0.0961801899959951 to 0.0972174192854831
0.100273002534142 to 0.10131023182363
Changing layer 0's weights from 
-0.204323840820316 to -0.203286611530828
-0.57625029751015 to -0.575213068220662
-0.810599563204768 to -0.80956233391528
-0.776040313326838 to -0.77500308403735
-0.0281727439994846 to -0.0271355147099966
-0.710814697229389 to -0.709777467939901
-0.556984973632815 to -0.555947744343327
-0.584439379655841 to -0.583402150366353
-0.821931240641597 to -0.820894011352109
-0.865681295775893 to -0.864644066486405
Changing layer 1's weights from 
-0.86058438339186 to -0.859547154102372
-0.259415698730473 to -0.258378469440985
-0.385118437492375 to -0.384081208202887
-0.395023775779728 to -0.39398654649024
-0.580076558076862 to -0.579039328787374
-0.718314392053607 to -0.717277162764119
-0.44759566851807 to -0.446558439228582
-0.833398697459224 to -0.832361468169736
-0.421735716545109 to -0.420698487255621
-0.784382579409602 to -0.783345350120114
Changing layer 2's weights from 
0.0248056524162252 to 0.0258428817057132
-0.529247832977298 to -0.52821060368781
-0.244658542358402 to -0.243621313068914
-0.0734584457511938 to -0.0724212164617058
-0.250711155616764 to -0.249673926327276
-0.456720782005314 to -0.455683552715826
0.0620919340019191 to 0.0631291632914072
-0.0103300935859718 to -0.00929286429648376
-0.742328388177875 to -0.741291158888387
-0.697370869600299 to -0.696333640310811
Changing layer 3's weights from 
-0.00824464627457074 to -0.00720741698508273
-0.271797729217533 to -0.270760499928045
-0.0291878111000097 to -0.0281505818105217
-0.936627042856153 to -0.935589813566665
-0.151863706790928 to -0.15082647750144
-0.282112312995915 to -0.281075083706427
-0.612297160112384 to -0.611259930822896
-0.153784287654881 to -0.152747058365393
-0.485480261528019 to -0.484443032238531
-0.0126503831977887 to -0.0116131539083007
Changing layer 4's weights from 
-0.111924184047703 to -0.110886954758214
-0.320756984436039 to -0.319719755146551
-0.521234584533694 to -0.520197355244206
-0.234299851142887 to -0.233262621853399
-0.473552656852726 to -0.472515427563238
-0.242087436401371 to -0.241050207111883
-0.246445966445927 to -0.245408737156439
-0.708343488657001 to -0.707306259367513
-0.114647401058201 to -0.113610171768713
-0.546949339591983 to -0.545912110302495
Changing layer 5's weights from 
0.0408556335334742 to 0.0418928628229622
-0.401217890464787 to -0.400180661175299
0.0239704721336332 to 0.0250077014231213
-0.911541709237818 to -0.91050447994833
-0.374272895538334 to -0.373235666248846
-0.772740838610652 to -0.771703609321164
-0.472775531494144 to -0.471738302204656
-0.400329304420475 to -0.399292075130987
-0.0107272989387548 to -0.00969006964926674
-0.0172354108924907 to -0.0161981816030027
10/5/2016 1:33:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:33:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:32 PMStarting learning phase with deltaScore: 2.933333
Modified index 0's learning in memoryPool to 0.5866667
Modified index 1's learning in memoryPool to 0.5866667
Modified index 2's learning in memoryPool to 0.5866667
Modified index 3's learning in memoryPool to 0.5866667
Modified index 4's learning in memoryPool to 0.5866667
Modified index 5's learning in memoryPool to 0.5866667
Modified index 6's learning in memoryPool to 0.5866667
Modified index 7's learning in memoryPool to 0.5866667
10/5/2016 1:34:32 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 66, 1, 0.5866667
sum 0.0384830372350319 distri 0.020348099538827
Using diff 0.00851417838744697 and condRate 0.166666666666667
Changed category 1 weights from 
0.288827070135779 to 0.289659567629983
0.522142657179541 to 0.522975154673745
0.0972174192854831 to 0.0980499167796875
0.10131023182363 to 0.102142729317834
Changing layer 0's weights from 
-0.203286611530828 to -0.202454114036623
-0.575213068220662 to -0.574380570726458
-0.80956233391528 to -0.808729836421076
-0.77500308403735 to -0.774170586543146
-0.0271355147099966 to -0.0263030172157923
-0.709777467939901 to -0.708944970445696
-0.555947744343327 to -0.555115246849123
-0.583402150366353 to -0.582569652872149
-0.820894011352109 to -0.820061513857905
-0.864644066486405 to -0.863811568992201
Changing layer 1's weights from 
-0.859547154102372 to -0.858714656608168
-0.258378469440985 to -0.25754597194678
-0.384081208202887 to -0.383248710708682
-0.39398654649024 to -0.393154048996035
-0.579039328787374 to -0.578206831293169
-0.717277162764119 to -0.716444665269915
-0.446558439228582 to -0.445725941734377
-0.832361468169736 to -0.831528970675532
-0.420698487255621 to -0.419865989761416
-0.783345350120114 to -0.78251285262591
Changing layer 2's weights from 
0.0258428817057132 to 0.0266753791999176
-0.52821060368781 to -0.527378106193606
-0.243621313068914 to -0.242788815574709
-0.0724212164617058 to -0.0715887189675015
-0.249673926327276 to -0.248841428833071
-0.455683552715826 to -0.454851055221621
0.0631291632914072 to 0.0639616607856115
-0.00929286429648376 to -0.00846036680227941
-0.741291158888387 to -0.740458661394182
-0.696333640310811 to -0.695501142816607
Changing layer 3's weights from 
-0.00720741698508273 to -0.00637491949087838
-0.270760499928045 to -0.26992800243384
-0.0281505818105217 to -0.0273180843163174
-0.935589813566665 to -0.934757316072461
-0.15082647750144 to -0.149993980007235
-0.281075083706427 to -0.280242586212222
-0.611259930822896 to -0.610427433328691
-0.152747058365393 to -0.151914560871188
-0.484443032238531 to -0.483610534744326
-0.0116131539083007 to -0.0107806564140964
Changing layer 4's weights from 
-0.110886954758214 to -0.11005445726401
-0.319719755146551 to -0.318887257652346
-0.520197355244206 to -0.519364857750002
-0.233262621853399 to -0.232430124359194
-0.472515427563238 to -0.471682930069033
-0.241050207111883 to -0.240217709617678
-0.245408737156439 to -0.244576239662234
-0.707306259367513 to -0.706473761873309
-0.113610171768713 to -0.112777674274508
-0.545912110302495 to -0.545079612808291
Changing layer 5's weights from 
0.0418928628229622 to 0.0427253603171666
-0.400180661175299 to -0.399348163681094
0.0250077014231213 to 0.0258401989173256
-0.91050447994833 to -0.909671982454126
-0.373235666248846 to -0.372403168754641
-0.771703609321164 to -0.77087111182696
-0.471738302204656 to -0.470905804710451
-0.399292075130987 to -0.398459577636782
-0.00969006964926674 to -0.00885757215506239
-0.0161981816030027 to -0.0153656841087984
Trying to learn from memory 67, 0, 0.5866667
sum 0.0384830372350319 distri -0.0215764926963346
Using diff 0.0504387706226085 and condRate 0.166666666666667
Changed category 0 weights from 
0.139059251687711 to 0.143991042900352
-0.260975712396914 to -0.256043921184273
-0.54349640891772 to -0.538564617705079
-0.391991042949969 to -0.387059251737328
Changing layer 0's weights from 
-0.202454114036623 to -0.197522322823982
-0.574380570726458 to -0.569448779513817
-0.808729836421076 to -0.803798045208435
-0.774170586543146 to -0.769238795330505
-0.0263030172157923 to -0.0213712260031512
-0.708944970445696 to -0.704013179233055
-0.555115246849123 to -0.550183455636482
-0.582569652872149 to -0.577637861659508
-0.820061513857905 to -0.815129722645264
-0.863811568992201 to -0.85887977777956
Changing layer 1's weights from 
-0.858714656608168 to -0.853782865395527
-0.25754597194678 to -0.252614180734139
-0.383248710708682 to -0.378316919496041
-0.393154048996035 to -0.388222257783394
-0.578206831293169 to -0.573275040080528
-0.716444665269915 to -0.711512874057274
-0.445725941734377 to -0.440794150521736
-0.831528970675532 to -0.826597179462891
-0.419865989761416 to -0.414934198548775
-0.78251285262591 to -0.777581061413269
Changing layer 2's weights from 
0.0266753791999176 to 0.0316071704125587
-0.527378106193606 to -0.522446314980965
-0.242788815574709 to -0.237857024362068
-0.0715887189675015 to -0.0666569277548604
-0.248841428833071 to -0.24390963762043
-0.454851055221621 to -0.44991926400898
0.0639616607856115 to 0.0688934519982526
-0.00846036680227941 to -0.00352857558963833
-0.740458661394182 to -0.735526870181541
-0.695501142816607 to -0.690569351603966
Changing layer 3's weights from 
-0.00637491949087838 to -0.0014431282782373
-0.26992800243384 to -0.264996211221199
-0.0273180843163174 to -0.0223862931036763
-0.934757316072461 to -0.92982552485982
-0.149993980007235 to -0.145062188794594
-0.280242586212222 to -0.275310794999581
-0.610427433328691 to -0.60549564211605
-0.151914560871188 to -0.146982769658547
-0.483610534744326 to -0.478678743531685
-0.0107806564140964 to -0.00584886520145528
Changing layer 4's weights from 
-0.11005445726401 to -0.105122666051369
-0.318887257652346 to -0.313955466439705
-0.519364857750002 to -0.514433066537361
-0.232430124359194 to -0.227498333146553
-0.471682930069033 to -0.466751138856392
-0.240217709617678 to -0.235285918405037
-0.244576239662234 to -0.239644448449593
-0.706473761873309 to -0.701541970660667
-0.112777674274508 to -0.107845883061867
-0.545079612808291 to -0.54014782159565
Changing layer 5's weights from 
0.0427253603171666 to 0.0476571515298077
-0.399348163681094 to -0.394416372468453
0.0258401989173256 to 0.0307719901299667
-0.909671982454126 to -0.904740191241485
-0.372403168754641 to -0.367471377542
-0.77087111182696 to -0.765939320614319
-0.470905804710451 to -0.46597401349781
-0.398459577636782 to -0.393527786424141
-0.00885757215506239 to -0.00392578094242131
-0.0153656841087984 to -0.0104338928961573
Trying to learn from memory 68, 0, 0.5866667
sum 0.0384830372350319 distri -0.0215764926963346
Using diff 0.0504387706226085 and condRate 0.166666666666667
Changed category 0 weights from 
0.143991042900352 to 0.148922834112993
-0.256043921184273 to -0.251112129971632
-0.538564617705079 to -0.533632826492438
-0.387059251737328 to -0.382127460524687
Changing layer 0's weights from 
-0.197522322823982 to -0.192590531611341
-0.569448779513817 to -0.564516988301176
-0.803798045208435 to -0.798866253995794
-0.769238795330505 to -0.764307004117864
-0.0213712260031512 to -0.0164394347905101
-0.704013179233055 to -0.699081388020414
-0.550183455636482 to -0.545251664423841
-0.577637861659508 to -0.572706070446867
-0.815129722645264 to -0.810197931432623
-0.85887977777956 to -0.853947986566919
Changing layer 1's weights from 
-0.853782865395527 to -0.848851074182886
-0.252614180734139 to -0.247682389521498
-0.378316919496041 to -0.3733851282834
-0.388222257783394 to -0.383290466570753
-0.573275040080528 to -0.568343248867887
-0.711512874057274 to -0.706581082844633
-0.440794150521736 to -0.435862359309095
-0.826597179462891 to -0.82166538825025
-0.414934198548775 to -0.410002407336134
-0.777581061413269 to -0.772649270200628
Changing layer 2's weights from 
0.0316071704125587 to 0.0365389616251997
-0.522446314980965 to -0.517514523768324
-0.237857024362068 to -0.232925233149427
-0.0666569277548604 to -0.0617251365422193
-0.24390963762043 to -0.238977846407789
-0.44991926400898 to -0.444987472796339
0.0688934519982526 to 0.0738252432108937
-0.00352857558963833 to 0.00140321562300275
-0.735526870181541 to -0.7305950789689
-0.690569351603966 to -0.685637560391325
Changing layer 3's weights from 
-0.0014431282782373 to 0.00348866293440378
-0.264996211221199 to -0.260064420008558
-0.0223862931036763 to -0.0174545018910352
-0.92982552485982 to -0.924893733647179
-0.145062188794594 to -0.140130397581953
-0.275310794999581 to -0.27037900378694
-0.60549564211605 to -0.600563850903409
-0.146982769658547 to -0.142050978445906
-0.478678743531685 to -0.473746952319044
-0.00584886520145528 to -0.000917073988814205
Changing layer 4's weights from 
-0.105122666051369 to -0.100190874838728
-0.313955466439705 to -0.309023675227064
-0.514433066537361 to -0.50950127532472
-0.227498333146553 to -0.222566541933912
-0.466751138856392 to -0.461819347643751
-0.235285918405037 to -0.230354127192396
-0.239644448449593 to -0.234712657236952
-0.701541970660667 to -0.696610179448026
-0.107845883061867 to -0.102914091849226
-0.54014782159565 to -0.535216030383009
Changing layer 5's weights from 
0.0476571515298077 to 0.0525889427424487
-0.394416372468453 to -0.389484581255812
0.0307719901299667 to 0.0357037813426078
-0.904740191241485 to -0.899808400028844
-0.367471377542 to -0.362539586329359
-0.765939320614319 to -0.761007529401678
-0.46597401349781 to -0.461042222285169
-0.393527786424141 to -0.3885959952115
-0.00392578094242131 to 0.00100601027021977
-0.0104338928961573 to -0.00550210168351621
Trying to learn from memory 69, 0, 0.5866667
sum 0.0384830372350319 distri -0.0215764926963346
Using diff 0.0504387706226085 and condRate 0.166666666666667
Changed category 0 weights from 
0.148922834112993 to 0.153854625325634
-0.251112129971632 to -0.246180338758991
-0.533632826492438 to -0.528701035279797
-0.382127460524687 to -0.377195669312046
Changing layer 0's weights from 
-0.192590531611341 to -0.1876587403987
-0.564516988301176 to -0.559585197088534
-0.798866253995794 to -0.793934462783153
-0.764307004117864 to -0.759375212905223
-0.0164394347905101 to -0.011507643577869
-0.699081388020414 to -0.694149596807773
-0.545251664423841 to -0.5403198732112
-0.572706070446867 to -0.567774279234225
-0.810197931432623 to -0.805266140219982
-0.853947986566919 to -0.849016195354278
Changing layer 1's weights from 
-0.848851074182886 to -0.843919282970245
-0.247682389521498 to -0.242750598308857
-0.3733851282834 to -0.368453337070759
-0.383290466570753 to -0.378358675358112
-0.568343248867887 to -0.563411457655246
-0.706581082844633 to -0.701649291631992
-0.435862359309095 to -0.430930568096454
-0.82166538825025 to -0.816733597037609
-0.410002407336134 to -0.405070616123493
-0.772649270200628 to -0.767717478987987
Changing layer 2's weights from 
0.0365389616251997 to 0.0414707528378408
-0.517514523768324 to -0.512582732555683
-0.232925233149427 to -0.227993441936786
-0.0617251365422193 to -0.0567933453295782
-0.238977846407789 to -0.234046055195148
-0.444987472796339 to -0.440055681583698
0.0738252432108937 to 0.0787570344235347
0.00140321562300275 to 0.00633500683564383
-0.7305950789689 to -0.725663287756259
-0.685637560391325 to -0.680705769178684
Changing layer 3's weights from 
0.00348866293440378 to 0.00842045414704486
-0.260064420008558 to -0.255132628795917
-0.0174545018910352 to -0.0125227106783941
-0.924893733647179 to -0.919961942434537
-0.140130397581953 to -0.135198606369312
-0.27037900378694 to -0.265447212574299
-0.600563850903409 to -0.595632059690768
-0.142050978445906 to -0.137119187233265
-0.473746952319044 to -0.468815161106403
-0.000917073988814205 to 0.00401471722382687
Changing layer 4's weights from 
-0.100190874838728 to -0.0952590836260869
-0.309023675227064 to -0.304091884014423
-0.50950127532472 to -0.504569484112079
-0.222566541933912 to -0.217634750721271
-0.461819347643751 to -0.45688755643111
-0.230354127192396 to -0.225422335979755
-0.234712657236952 to -0.229780866024311
-0.696610179448026 to -0.691678388235385
-0.102914091849226 to -0.097982300636585
-0.535216030383009 to -0.530284239170368
Changing layer 5's weights from 
0.0525889427424487 to 0.0575207339550898
-0.389484581255812 to -0.384552790043171
0.0357037813426078 to 0.0406355725552488
-0.899808400028844 to -0.894876608816202
-0.362539586329359 to -0.357607795116718
-0.761007529401678 to -0.756075738189037
-0.461042222285169 to -0.456110431072528
-0.3885959952115 to -0.383664203998859
0.00100601027021977 to 0.00593780148286085
-0.00550210168351621 to -0.000570310470875135
Trying to learn from memory 70, 0, 0.5866667
sum 0.0384830372350319 distri -0.0215764926963346
Using diff 0.0504387706226085 and condRate 0.166666666666667
Changed category 0 weights from 
0.153854625325634 to 0.158786416538275
-0.246180338758991 to -0.24124854754635
-0.528701035279797 to -0.523769244067156
-0.377195669312046 to -0.372263878099405
Changing layer 0's weights from 
-0.1876587403987 to -0.182726949186059
-0.559585197088534 to -0.554653405875893
-0.793934462783153 to -0.789002671570512
-0.759375212905223 to -0.754443421692582
-0.011507643577869 to -0.00657585236522796
-0.694149596807773 to -0.689217805595132
-0.5403198732112 to -0.535388081998559
-0.567774279234225 to -0.562842488021584
-0.805266140219982 to -0.80033434900734
-0.849016195354278 to -0.844084404141636
Changing layer 1's weights from 
-0.843919282970245 to -0.838987491757603
-0.242750598308857 to -0.237818807096216
-0.368453337070759 to -0.363521545858118
-0.378358675358112 to -0.373426884145471
-0.563411457655246 to -0.558479666442605
-0.701649291631992 to -0.696717500419351
-0.430930568096454 to -0.425998776883813
-0.816733597037609 to -0.811801805824968
-0.405070616123493 to -0.400138824910852
-0.767717478987987 to -0.762785687775346
Changing layer 2's weights from 
0.0414707528378408 to 0.0464025440504819
-0.512582732555683 to -0.507650941343042
-0.227993441936786 to -0.223061650724145
-0.0567933453295782 to -0.0518615541169372
-0.234046055195148 to -0.229114263982507
-0.440055681583698 to -0.435123890371057
0.0787570344235347 to 0.0836888256361758
0.00633500683564383 to 0.0112667980482849
-0.725663287756259 to -0.720731496543618
-0.680705769178684 to -0.675773977966043
Changing layer 3's weights from 
0.00842045414704486 to 0.0133522453596859
-0.255132628795917 to -0.250200837583276
-0.0125227106783941 to -0.00759091946575303
-0.919961942434537 to -0.915030151221896
-0.135198606369312 to -0.130266815156671
-0.265447212574299 to -0.260515421361658
-0.595632059690768 to -0.590700268478127
-0.137119187233265 to -0.132187396020624
-0.468815161106403 to -0.463883369893762
0.00401471722382687 to 0.00894650843646795
Changing layer 4's weights from 
-0.0952590836260869 to -0.0903272924134458
-0.304091884014423 to -0.299160092801782
-0.504569484112079 to -0.499637692899437
-0.217634750721271 to -0.21270295950863
-0.45688755643111 to -0.451955765218469
-0.225422335979755 to -0.220490544767114
-0.229780866024311 to -0.22484907481167
-0.691678388235385 to -0.686746597022744
-0.097982300636585 to -0.093050509423944
-0.530284239170368 to -0.525352447957727
Changing layer 5's weights from 
0.0575207339550898 to 0.0624525251677309
-0.384552790043171 to -0.37962099883053
0.0406355725552488 to 0.0455673637678899
-0.894876608816202 to -0.889944817603561
-0.357607795116718 to -0.352676003904077
-0.756075738189037 to -0.751143946976396
-0.456110431072528 to -0.451178639859887
-0.383664203998859 to -0.378732412786218
0.00593780148286085 to 0.0108695926955019
-0.000570310470875135 to 0.00436148074176594
Trying to learn from memory 71, 1, 0.5866667
sum 0.0384830372350319 distri 0.020348099538827
Using diff 0.00851417838744697 and condRate 0.166666666666667
Changed category 1 weights from 
0.289659567629983 to 0.290492065124188
0.522975154673745 to 0.52380765216795
0.0980499167796875 to 0.0988824142738918
0.102142729317834 to 0.102975226812039
Changing layer 0's weights from 
-0.182726949186059 to -0.181894451691855
-0.554653405875893 to -0.553820908381689
-0.789002671570512 to -0.788170174076307
-0.754443421692582 to -0.753610924198377
-0.00657585236522796 to -0.00574335487102361
-0.689217805595132 to -0.688385308100928
-0.535388081998559 to -0.534555584504354
-0.562842488021584 to -0.56200999052738
-0.80033434900734 to -0.799501851513136
-0.844084404141636 to -0.843251906647432
Changing layer 1's weights from 
-0.838987491757603 to -0.838154994263399
-0.237818807096216 to -0.236986309602012
-0.363521545858118 to -0.362689048363914
-0.373426884145471 to -0.372594386651267
-0.558479666442605 to -0.557647168948401
-0.696717500419351 to -0.695885002925146
-0.425998776883813 to -0.425166279389609
-0.811801805824968 to -0.810969308330763
-0.400138824910852 to -0.399306327416648
-0.762785687775346 to -0.761953190281141
Changing layer 2's weights from 
0.0464025440504819 to 0.0472350415446863
-0.507650941343042 to -0.506818443848837
-0.223061650724145 to -0.222229153229941
-0.0518615541169372 to -0.0510290566227328
-0.229114263982507 to -0.228281766488303
-0.435123890371057 to -0.434291392876853
0.0836888256361758 to 0.0845213231303802
0.0112667980482849 to 0.0120992955424893
-0.720731496543618 to -0.719898999049414
-0.675773977966043 to -0.674941480471838
Changing layer 3's weights from 
0.0133522453596859 to 0.0141847428538903
-0.250200837583276 to -0.249368340089072
-0.00759091946575303 to -0.00675842197154868
-0.915030151221896 to -0.914197653727692
-0.130266815156671 to -0.129434317662467
-0.260515421361658 to -0.259682923867454
-0.590700268478127 to -0.589867770983923
-0.132187396020624 to -0.13135489852642
-0.463883369893762 to -0.463050872399558
0.00894650843646795 to 0.0097790059306723
Changing layer 4's weights from 
-0.0903272924134458 to -0.0894947949192415
-0.299160092801782 to -0.298327595307578
-0.499637692899437 to -0.498805195405233
-0.21270295950863 to -0.211870462014425
-0.451955765218469 to -0.451123267724265
-0.220490544767114 to -0.21965804727291
-0.22484907481167 to -0.224016577317466
-0.686746597022744 to -0.68591409952854
-0.093050509423944 to -0.0922180119297396
-0.525352447957727 to -0.524519950463522
Changing layer 5's weights from 
0.0624525251677309 to 0.0632850226619352
-0.37962099883053 to -0.378788501336326
0.0455673637678899 to 0.0463998612620943
-0.889944817603561 to -0.889112320109357
-0.352676003904077 to -0.351843506409873
-0.751143946976396 to -0.750311449482191
-0.451178639859887 to -0.450346142365683
-0.378732412786218 to -0.377899915292014
0.0108695926955019 to 0.0117020901897063
0.00436148074176594 to 0.00519397823597029
Trying to learn from memory 71, 0, 0.5866667
sum 0.0384830372350319 distri -0.0215764926963346
Using diff 0.0504387706226085 and condRate 0.166666666666667
Changed category 0 weights from 
0.158786416538275 to 0.163718207750916
-0.24124854754635 to -0.236316756333709
-0.523769244067156 to -0.518837452854514
-0.372263878099405 to -0.367332086886763
Changing layer 0's weights from 
-0.181894451691855 to -0.176962660479213
-0.553820908381689 to -0.548889117169048
-0.788170174076307 to -0.783238382863666
-0.753610924198377 to -0.748679132985736
-0.00574335487102361 to -0.000811563658382533
-0.688385308100928 to -0.683453516888286
-0.534555584504354 to -0.529623793291713
-0.56200999052738 to -0.557078199314739
-0.799501851513136 to -0.794570060300495
-0.843251906647432 to -0.838320115434791
Changing layer 1's weights from 
-0.838154994263399 to -0.833223203050758
-0.236986309602012 to -0.232054518389371
-0.362689048363914 to -0.357757257151273
-0.372594386651267 to -0.367662595438626
-0.557647168948401 to -0.552715377735759
-0.695885002925146 to -0.690953211712505
-0.425166279389609 to -0.420234488176968
-0.810969308330763 to -0.806037517118122
-0.399306327416648 to -0.394374536204006
-0.761953190281141 to -0.7570213990685
Changing layer 2's weights from 
0.0472350415446863 to 0.0521668327573273
-0.506818443848837 to -0.501886652636196
-0.222229153229941 to -0.2172973620173
-0.0510290566227328 to -0.0460972654100917
-0.228281766488303 to -0.223349975275662
-0.434291392876853 to -0.429359601664212
0.0845213231303802 to 0.0894531143430212
0.0120992955424893 to 0.0170310867551303
-0.719898999049414 to -0.714967207836773
-0.674941480471838 to -0.670009689259197
Changing layer 3's weights from 
0.0141847428538903 to 0.0191165340665314
-0.249368340089072 to -0.244436548876431
-0.00675842197154868 to -0.0018266307589076
-0.914197653727692 to -0.909265862515051
-0.129434317662467 to -0.124502526449826
-0.259682923867454 to -0.254751132654813
-0.589867770983923 to -0.584935979771282
-0.13135489852642 to -0.126423107313778
-0.463050872399558 to -0.458119081186917
0.0097790059306723 to 0.0147107971433134
Changing layer 4's weights from 
-0.0894947949192415 to -0.0845630037066004
-0.298327595307578 to -0.293395804094937
-0.498805195405233 to -0.493873404192592
-0.211870462014425 to -0.206938670801784
-0.451123267724265 to -0.446191476511624
-0.21965804727291 to -0.214726256060268
-0.224016577317466 to -0.219084786104825
-0.68591409952854 to -0.680982308315899
-0.0922180119297396 to -0.0872862207170985
-0.524519950463522 to -0.519588159250881
Changing layer 5's weights from 
0.0632850226619352 to 0.0682168138745763
-0.378788501336326 to -0.373856710123685
0.0463998612620943 to 0.0513316524747354
-0.889112320109357 to -0.884180528896716
-0.351843506409873 to -0.346911715197232
-0.750311449482191 to -0.74537965826955
-0.450346142365683 to -0.445414351153042
-0.377899915292014 to -0.372968124079373
0.0117020901897063 to 0.0166338814023474
0.00519397823597029 to 0.0101257694486114
Trying to learn from memory 71, 1, 0.5866667
sum 0.0384830372350319 distri 0.020348099538827
Using diff 0.00851417838744697 and condRate 0.166666666666667
Changed category 1 weights from 
0.290492065124188 to 0.291324562618392
0.52380765216795 to 0.524640149662154
0.0988824142738918 to 0.0997149117680962
0.102975226812039 to 0.103807724306243
Changing layer 0's weights from 
-0.176962660479213 to -0.176130162985009
-0.548889117169048 to -0.548056619674844
-0.783238382863666 to -0.782405885369462
-0.748679132985736 to -0.747846635491532
-0.000811563658382533 to 2.0933835821817E-05
-0.683453516888286 to -0.682621019394082
-0.529623793291713 to -0.528791295797509
-0.557078199314739 to -0.556245701820535
-0.794570060300495 to -0.793737562806291
-0.838320115434791 to -0.837487617940587
Changing layer 1's weights from 
-0.833223203050758 to -0.832390705556554
-0.232054518389371 to -0.231222020895166
-0.357757257151273 to -0.356924759657068
-0.367662595438626 to -0.366830097944421
-0.552715377735759 to -0.551882880241555
-0.690953211712505 to -0.690120714218301
-0.420234488176968 to -0.419401990682763
-0.806037517118122 to -0.805205019623918
-0.394374536204006 to -0.393542038709802
-0.7570213990685 to -0.756188901574296
Changing layer 2's weights from 
0.0521668327573273 to 0.0529993302515317
-0.501886652636196 to -0.501054155141992
-0.2172973620173 to -0.216464864523095
-0.0460972654100917 to -0.0452647679158874
-0.223349975275662 to -0.222517477781457
-0.429359601664212 to -0.428527104170007
0.0894531143430212 to 0.0902856118372256
0.0170310867551303 to 0.0178635842493347
-0.714967207836773 to -0.714134710342568
-0.670009689259197 to -0.669177191764993
Changing layer 3's weights from 
0.0191165340665314 to 0.0199490315607357
-0.244436548876431 to -0.243604051382226
-0.0018266307589076 to -0.000994133264703252
-0.909265862515051 to -0.908433365020847
-0.124502526449826 to -0.123670028955621
-0.254751132654813 to -0.253918635160608
-0.584935979771282 to -0.584103482277077
-0.126423107313778 to -0.125590609819574
-0.458119081186917 to -0.457286583692712
0.0147107971433134 to 0.0155432946375177
Changing layer 4's weights from 
-0.0845630037066004 to -0.0837305062123961
-0.293395804094937 to -0.292563306600732
-0.493873404192592 to -0.493040906698388
-0.206938670801784 to -0.20610617330758
-0.446191476511624 to -0.445358979017419
-0.214726256060268 to -0.213893758566064
-0.219084786104825 to -0.21825228861062
-0.680982308315899 to -0.680149810821694
-0.0872862207170985 to -0.0864537232228942
-0.519588159250881 to -0.518755661756677
Changing layer 5's weights from 
0.0682168138745763 to 0.0690493113687807
-0.373856710123685 to -0.37302421262948
0.0513316524747354 to 0.0521641499689397
-0.884180528896716 to -0.883348031402512
-0.346911715197232 to -0.346079217703027
-0.74537965826955 to -0.744547160775346
-0.445414351153042 to -0.444581853658837
-0.372968124079373 to -0.372135626585168
0.0166338814023474 to 0.0174663788965517
0.0101257694486114 to 0.0109582669428157
10/5/2016 1:34:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:34:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:12 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
Modified index 9's learning in memoryPool to -0.2
Modified index 10's learning in memoryPool to -0.2
Modified index 11's learning in memoryPool to -0.2
Modified index 12's learning in memoryPool to -0.2
Modified index 13's learning in memoryPool to -0.2
Modified index 14's learning in memoryPool to -0.2
Modified index 15's learning in memoryPool to -0.2
Modified index 16's learning in memoryPool to -0.2
Modified index 17's learning in memoryPool to -0.2
Modified index 18's learning in memoryPool to -0.2
Modified index 19's learning in memoryPool to -0.2
Modified index 20's learning in memoryPool to -0.2
Modified index 21's learning in memoryPool to -0.2
Modified index 22's learning in memoryPool to -0.2
Modified index 23's learning in memoryPool to -0.2
Modified index 24's learning in memoryPool to -0.2
Modified index 25's learning in memoryPool to -0.2
10/5/2016 1:35:13 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 72, 1, -0.2
sum 0.0397773104331494 distri 0.0200506094370613
Using diff 0.00978237338780073 and condRate 0.166666666666667
Changed category 1 weights from 
0.291324562618392 to 0.290998483500606
0.524640149662154 to 0.524314070544368
0.0997149117680962 to 0.0993888326503105
0.103807724306243 to 0.103481645188457
Changing layer 0's weights from 
-0.176130162985009 to -0.176456242102795
-0.548056619674844 to -0.548382698792629
-0.782405885369462 to -0.782731964487247
-0.747846635491532 to -0.748172714609317
2.0933835821817E-05 to -0.000305145281963831
-0.682621019394082 to -0.682947098511868
-0.528791295797509 to -0.529117374915294
-0.556245701820535 to -0.55657178093832
-0.793737562806291 to -0.794063641924076
-0.837487617940587 to -0.837813697058372
Changing layer 1's weights from 
-0.832390705556554 to -0.832716784674339
-0.231222020895166 to -0.231548100012952
-0.356924759657068 to -0.357250838774854
-0.366830097944421 to -0.367156177062207
-0.551882880241555 to -0.552208959359341
-0.690120714218301 to -0.690446793336087
-0.419401990682763 to -0.419728069800549
-0.805205019623918 to -0.805531098741703
-0.393542038709802 to -0.393868117827588
-0.756188901574296 to -0.756514980692081
Changing layer 2's weights from 
0.0529993302515317 to 0.052673251133746
-0.501054155141992 to -0.501380234259777
-0.216464864523095 to -0.216790943640881
-0.0452647679158874 to -0.045590847033673
-0.222517477781457 to -0.222843556899243
-0.428527104170007 to -0.428853183287793
0.0902856118372256 to 0.0899595327194399
0.0178635842493347 to 0.017537505131549
-0.714134710342568 to -0.714460789460354
-0.669177191764993 to -0.669503270882778
Changing layer 3's weights from 
0.0199490315607357 to 0.0196229524429501
-0.243604051382226 to -0.243930130500012
-0.000994133264703252 to -0.0013202123824889
-0.908433365020847 to -0.908759444138632
-0.123670028955621 to -0.123996108073407
-0.253918635160608 to -0.254244714278394
-0.584103482277077 to -0.584429561394863
-0.125590609819574 to -0.12591668893736
-0.457286583692712 to -0.457612662810498
0.0155432946375177 to 0.0152172155197321
Changing layer 4's weights from 
-0.0837305062123961 to -0.0840565853301817
-0.292563306600732 to -0.292889385718518
-0.493040906698388 to -0.493366985816173
-0.20610617330758 to -0.206432252425366
-0.445358979017419 to -0.445685058135205
-0.213893758566064 to -0.21421983768385
-0.21825228861062 to -0.218578367728406
-0.680149810821694 to -0.68047588993948
-0.0864537232228942 to -0.0867798023406798
-0.518755661756677 to -0.519081740874462
Changing layer 5's weights from 
0.0690493113687807 to 0.068723232250995
-0.37302421262948 to -0.373350291747266
0.0521641499689397 to 0.0518380708511541
-0.883348031402512 to -0.883674110520297
-0.346079217703027 to -0.346405296820813
-0.744547160775346 to -0.744873239893131
-0.444581853658837 to -0.444907932776623
-0.372135626585168 to -0.372461705702954
0.0174663788965517 to 0.0171402997787661
0.0109582669428157 to 0.0106321878250301
Trying to learn from memory 73, 1, -0.2
sum 0.0397773104331494 distri 0.0200506094370613
Using diff 0.00978237338780073 and condRate 0.166666666666667
Changed category 1 weights from 
0.290998483500606 to 0.290672404382821
0.524314070544368 to 0.523987991426583
0.0993888326503105 to 0.0990627535325249
0.103481645188457 to 0.103155566070672
Changing layer 0's weights from 
-0.176456242102795 to -0.17678232122058
-0.548382698792629 to -0.548708777910415
-0.782731964487247 to -0.783058043605033
-0.748172714609317 to -0.748498793727103
-0.000305145281963831 to -0.00063122439974948
-0.682947098511868 to -0.683273177629653
-0.529117374915294 to -0.52944345403308
-0.55657178093832 to -0.556897860056106
-0.794063641924076 to -0.794389721041862
-0.837813697058372 to -0.838139776176158
Changing layer 1's weights from 
-0.832716784674339 to -0.833042863792125
-0.231548100012952 to -0.231874179130738
-0.357250838774854 to -0.35757691789264
-0.367156177062207 to -0.367482256179993
-0.552208959359341 to -0.552535038477126
-0.690446793336087 to -0.690772872453872
-0.419728069800549 to -0.420054148918334
-0.805531098741703 to -0.805857177859489
-0.393868117827588 to -0.394194196945373
-0.756514980692081 to -0.756841059809867
Changing layer 2's weights from 
0.052673251133746 to 0.0523471720159604
-0.501380234259777 to -0.501706313377563
-0.216790943640881 to -0.217117022758666
-0.045590847033673 to -0.0459169261514587
-0.222843556899243 to -0.223169636017029
-0.428853183287793 to -0.429179262405578
0.0899595327194399 to 0.0896334536016543
0.017537505131549 to 0.0172114260137634
-0.714460789460354 to -0.714786868578139
-0.669503270882778 to -0.669829350000564
Changing layer 3's weights from 
0.0196229524429501 to 0.0192968733251644
-0.243930130500012 to -0.244256209617798
-0.0013202123824889 to -0.00164629150027455
-0.908759444138632 to -0.909085523256418
-0.123996108073407 to -0.124322187191193
-0.254244714278394 to -0.25457079339618
-0.584429561394863 to -0.584755640512649
-0.12591668893736 to -0.126242768055145
-0.457612662810498 to -0.457938741928284
0.0152172155197321 to 0.0148911364019464
Changing layer 4's weights from 
-0.0840565853301817 to -0.0843826644479674
-0.292889385718518 to -0.293215464836304
-0.493366985816173 to -0.493693064933959
-0.206432252425366 to -0.206758331543151
-0.445685058135205 to -0.446011137252991
-0.21421983768385 to -0.214545916801635
-0.218578367728406 to -0.218904446846191
-0.68047588993948 to -0.680801969057266
-0.0867798023406798 to -0.0871058814584655
-0.519081740874462 to -0.519407819992248
Changing layer 5's weights from 
0.068723232250995 to 0.0683971531332094
-0.373350291747266 to -0.373676370865052
0.0518380708511541 to 0.0515119917333684
-0.883674110520297 to -0.884000189638083
-0.346405296820813 to -0.346731375938599
-0.744873239893131 to -0.745199319010917
-0.444907932776623 to -0.445234011894408
-0.372461705702954 to -0.37278778482074
0.0171402997787661 to 0.0168142206609804
0.0106321878250301 to 0.0103061087072444
Trying to learn from memory 74, 0, -0.2
sum 0.0397773104331494 distri -0.019031287861157
Using diff 0.0488642706860191 and condRate 0.166666666666667
Changed category 0 weights from 
0.163718207750916 to 0.162089398703778
-0.236316756333709 to -0.237945565380847
-0.518837452854514 to -0.520466261901653
-0.367332086886763 to -0.368960895933902
Changing layer 0's weights from 
-0.17678232122058 to -0.178411130267719
-0.548708777910415 to -0.550337586957553
-0.783058043605033 to -0.784686852652171
-0.748498793727103 to -0.750127602774241
-0.00063122439974948 to -0.00226003344688793
-0.683273177629653 to -0.684901986676792
-0.52944345403308 to -0.531072263080218
-0.556897860056106 to -0.558526669103244
-0.794389721041862 to -0.796018530089
-0.838139776176158 to -0.839768585223296
Changing layer 1's weights from 
-0.833042863792125 to -0.834671672839263
-0.231874179130738 to -0.233502988177876
-0.35757691789264 to -0.359205726939778
-0.367482256179993 to -0.369111065227131
-0.552535038477126 to -0.554163847524265
-0.690772872453872 to -0.692401681501011
-0.420054148918334 to -0.421682957965473
-0.805857177859489 to -0.807485986906627
-0.394194196945373 to -0.395823005992512
-0.756841059809867 to -0.758469868857005
Changing layer 2's weights from 
0.0523471720159604 to 0.0507183629688219
-0.501706313377563 to -0.503335122424701
-0.217117022758666 to -0.218745831805805
-0.0459169261514587 to -0.0475457351985971
-0.223169636017029 to -0.224798445064167
-0.429179262405578 to -0.430808071452717
0.0896334536016543 to 0.0880046445545158
0.0172114260137634 to 0.0155826169666249
-0.714786868578139 to -0.716415677625278
-0.669829350000564 to -0.671458159047702
Changing layer 3's weights from 
0.0192968733251644 to 0.017668064278026
-0.244256209617798 to -0.245885018664936
-0.00164629150027455 to -0.003275100547413
-0.909085523256418 to -0.910714332303556
-0.124322187191193 to -0.125950996238331
-0.25457079339618 to -0.256199602443318
-0.584755640512649 to -0.586384449559787
-0.126242768055145 to -0.127871577102284
-0.457938741928284 to -0.459567550975422
0.0148911364019464 to 0.013262327354808
Changing layer 4's weights from 
-0.0843826644479674 to -0.0860114734951058
-0.293215464836304 to -0.294844273883442
-0.493693064933959 to -0.495321873981098
-0.206758331543151 to -0.20838714059029
-0.446011137252991 to -0.447639946300129
-0.214545916801635 to -0.216174725848774
-0.218904446846191 to -0.22053325589333
-0.680801969057266 to -0.682430778104404
-0.0871058814584655 to -0.088734690505604
-0.519407819992248 to -0.521036629039386
Changing layer 5's weights from 
0.0683971531332094 to 0.0667683440860709
-0.373676370865052 to -0.37530517991219
0.0515119917333684 to 0.04988318268623
-0.884000189638083 to -0.885628998685221
-0.346731375938599 to -0.348360184985737
-0.745199319010917 to -0.746828128058055
-0.445234011894408 to -0.446862820941547
-0.37278778482074 to -0.374416593867878
0.0168142206609804 to 0.015185411613842
0.0103061087072444 to 0.00867729966010598
Trying to learn from memory 75, 0, -0.2
sum 0.0397773104331494 distri -0.019031287861157
Using diff 0.0488642706860191 and condRate 0.166666666666667
Changed category 0 weights from 
0.162089398703778 to 0.16046058965664
-0.237945565380847 to -0.239574374427985
-0.520466261901653 to -0.522095070948791
-0.368960895933902 to -0.37058970498104
Changing layer 0's weights from 
-0.178411130267719 to -0.180039939314857
-0.550337586957553 to -0.551966396004692
-0.784686852652171 to -0.78631566169931
-0.750127602774241 to -0.75175641182138
-0.00226003344688793 to -0.00388884249402638
-0.684901986676792 to -0.68653079572393
-0.531072263080218 to -0.532701072127357
-0.558526669103244 to -0.560155478150383
-0.796018530089 to -0.797647339136139
-0.839768585223296 to -0.841397394270435
Changing layer 1's weights from 
-0.834671672839263 to -0.836300481886402
-0.233502988177876 to -0.235131797225014
-0.359205726939778 to -0.360834535986916
-0.369111065227131 to -0.370739874274269
-0.554163847524265 to -0.555792656571403
-0.692401681501011 to -0.694030490548149
-0.421682957965473 to -0.423311767012611
-0.807485986906627 to -0.809114795953766
-0.395823005992512 to -0.39745181503965
-0.758469868857005 to -0.760098677904144
Changing layer 2's weights from 
0.0507183629688219 to 0.0490895539216835
-0.503335122424701 to -0.50496393147184
-0.218745831805805 to -0.220374640852943
-0.0475457351985971 to -0.0491745442457356
-0.224798445064167 to -0.226427254111305
-0.430808071452717 to -0.432436880499855
0.0880046445545158 to 0.0863758355073774
0.0155826169666249 to 0.0139538079194865
-0.716415677625278 to -0.718044486672416
-0.671458159047702 to -0.673086968094841
Changing layer 3's weights from 
0.017668064278026 to 0.0160392552308875
-0.245885018664936 to -0.247513827712074
-0.003275100547413 to -0.00490390959455145
-0.910714332303556 to -0.912343141350695
-0.125950996238331 to -0.127579805285469
-0.256199602443318 to -0.257828411490456
-0.586384449559787 to -0.588013258606925
-0.127871577102284 to -0.129500386149422
-0.459567550975422 to -0.46119636002256
0.013262327354808 to 0.0116335183076695
Changing layer 4's weights from 
-0.0860114734951058 to -0.0876402825422443
-0.294844273883442 to -0.296473082930581
-0.495321873981098 to -0.496950683028236
-0.20838714059029 to -0.210015949637428
-0.447639946300129 to -0.449268755347267
-0.216174725848774 to -0.217803534895912
-0.22053325589333 to -0.222162064940468
-0.682430778104404 to -0.684059587151543
-0.088734690505604 to -0.0903634995527424
-0.521036629039386 to -0.522665438086525
Changing layer 5's weights from 
0.0667683440860709 to 0.0651395350389325
-0.37530517991219 to -0.376933988959329
0.04988318268623 to 0.0482543736390915
-0.885628998685221 to -0.88725780773236
-0.348360184985737 to -0.349988994032876
-0.746828128058055 to -0.748456937105194
-0.446862820941547 to -0.448491629988685
-0.374416593867878 to -0.376045402915016
0.015185411613842 to 0.0135566025667035
0.00867729966010598 to 0.00704849061296753
Trying to learn from memory 76, 0, -0.2
sum 0.0397773104331494 distri -0.019031287861157
Using diff 0.0488642706860191 and condRate 0.166666666666667
Changed category 0 weights from 
0.16046058965664 to 0.158831780609501
-0.239574374427985 to -0.241203183475124
-0.522095070948791 to -0.52372387999593
-0.37058970498104 to -0.372218514028179
Changing layer 0's weights from 
-0.180039939314857 to -0.181668748361996
-0.551966396004692 to -0.55359520505183
-0.78631566169931 to -0.787944470746448
-0.75175641182138 to -0.753385220868518
-0.00388884249402638 to -0.00551765154116483
-0.68653079572393 to -0.688159604771069
-0.532701072127357 to -0.534329881174495
-0.560155478150383 to -0.561784287197521
-0.797647339136139 to -0.799276148183277
-0.841397394270435 to -0.843026203317573
Changing layer 1's weights from 
-0.836300481886402 to -0.83792929093354
-0.235131797225014 to -0.236760606272153
-0.360834535986916 to -0.362463345034055
-0.370739874274269 to -0.372368683321408
-0.555792656571403 to -0.557421465618542
-0.694030490548149 to -0.695659299595288
-0.423311767012611 to -0.42494057605975
-0.809114795953766 to -0.810743605000904
-0.39745181503965 to -0.399080624086789
-0.760098677904144 to -0.761727486951282
Changing layer 2's weights from 
0.0490895539216835 to 0.047460744874545
-0.50496393147184 to -0.506592740518978
-0.220374640852943 to -0.222003449900082
-0.0491745442457356 to -0.050803353292874
-0.226427254111305 to -0.228056063158444
-0.432436880499855 to -0.434065689546994
0.0863758355073774 to 0.0847470264602389
0.0139538079194865 to 0.012324998872348
-0.718044486672416 to -0.719673295719555
-0.673086968094841 to -0.674715777141979
Changing layer 3's weights from 
0.0160392552308875 to 0.0144104461837491
-0.247513827712074 to -0.249142636759213
-0.00490390959455145 to -0.0065327186416899
-0.912343141350695 to -0.913971950397833
-0.127579805285469 to -0.129208614332608
-0.257828411490456 to -0.259457220537595
-0.588013258606925 to -0.589642067654064
-0.129500386149422 to -0.131129195196561
-0.46119636002256 to -0.462825169069699
0.0116335183076695 to 0.0100047092605311
Changing layer 4's weights from 
-0.0876402825422443 to -0.0892690915893827
-0.296473082930581 to -0.298101891977719
-0.496950683028236 to -0.498579492075374
-0.210015949637428 to -0.211644758684567
-0.449268755347267 to -0.450897564394406
-0.217803534895912 to -0.219432343943051
-0.222162064940468 to -0.223790873987607
-0.684059587151543 to -0.685688396198681
-0.0903634995527424 to -0.0919923085998809
-0.522665438086525 to -0.524294247133663
Changing layer 5's weights from 
0.0651395350389325 to 0.063510725991794
-0.376933988959329 to -0.378562798006467
0.0482543736390915 to 0.0466255645919531
-0.88725780773236 to -0.888886616779498
-0.349988994032876 to -0.351617803080014
-0.748456937105194 to -0.750085746152332
-0.448491629988685 to -0.450120439035824
-0.376045402915016 to -0.377674211962155
0.0135566025667035 to 0.0119277935195651
0.00704849061296753 to 0.00541968156582908
Trying to learn from memory 77, 0, -0.2
sum 0.0397773104331494 distri -0.019031287861157
Using diff 0.0488642706860191 and condRate 0.166666666666667
Changed category 0 weights from 
0.158831780609501 to 0.157202971562363
-0.241203183475124 to -0.242831992522262
-0.52372387999593 to -0.525352689043068
-0.372218514028179 to -0.373847323075317
Changing layer 0's weights from 
-0.181668748361996 to -0.183297557409134
-0.55359520505183 to -0.555224014098969
-0.787944470746448 to -0.789573279793587
-0.753385220868518 to -0.755014029915657
-0.00551765154116483 to -0.00714646058830328
-0.688159604771069 to -0.689788413818207
-0.534329881174495 to -0.535958690221634
-0.561784287197521 to -0.56341309624466
-0.799276148183277 to -0.800904957230416
-0.843026203317573 to -0.844655012364712
Changing layer 1's weights from 
-0.83792929093354 to -0.839558099980679
-0.236760606272153 to -0.238389415319291
-0.362463345034055 to -0.364092154081193
-0.372368683321408 to -0.373997492368546
-0.557421465618542 to -0.55905027466568
-0.695659299595288 to -0.697288108642426
-0.42494057605975 to -0.426569385106888
-0.810743605000904 to -0.812372414048043
-0.399080624086789 to -0.400709433133927
-0.761727486951282 to -0.763356295998421
Changing layer 2's weights from 
0.047460744874545 to 0.0458319358274066
-0.506592740518978 to -0.508221549566117
-0.222003449900082 to -0.22363225894722
-0.050803353292874 to -0.0524321623400125
-0.228056063158444 to -0.229684872205582
-0.434065689546994 to -0.435694498594132
0.0847470264602389 to 0.0831182174131005
0.012324998872348 to 0.0106961898252096
-0.719673295719555 to -0.721302104766693
-0.674715777141979 to -0.676344586189118
Changing layer 3's weights from 
0.0144104461837491 to 0.0127816371366106
-0.249142636759213 to -0.250771445806351
-0.0065327186416899 to -0.00816152768882835
-0.913971950397833 to -0.915600759444972
-0.129208614332608 to -0.130837423379746
-0.259457220537595 to -0.261086029584733
-0.589642067654064 to -0.591270876701202
-0.131129195196561 to -0.132758004243699
-0.462825169069699 to -0.464453978116837
0.0100047092605311 to 0.00837590021339264
Changing layer 4's weights from 
-0.0892690915893827 to -0.0908979006365212
-0.298101891977719 to -0.299730701024857
-0.498579492075374 to -0.500208301122513
-0.211644758684567 to -0.213273567731705
-0.450897564394406 to -0.452526373441544
-0.219432343943051 to -0.221061152990189
-0.223790873987607 to -0.225419683034745
-0.685688396198681 to -0.68731720524582
-0.0919923085998809 to -0.0936211176470193
-0.524294247133663 to -0.525923056180802
Changing layer 5's weights from 
0.063510725991794 to 0.0618819169446556
-0.378562798006467 to -0.380191607053605
0.0466255645919531 to 0.0449967555448146
-0.888886616779498 to -0.890515425826637
-0.351617803080014 to -0.353246612127152
-0.750085746152332 to -0.751714555199471
-0.450120439035824 to -0.451749248082962
-0.377674211962155 to -0.379303021009293
0.0119277935195651 to 0.0102989844724266
0.00541968156582908 to 0.00379087251869063
Trying to learn from memory 77, 1, -0.2
sum 0.0397773104331494 distri 0.0200506094370613
Using diff 0.00978237338780073 and condRate 0.166666666666667
Changed category 1 weights from 
0.290672404382821 to 0.290346325265035
0.523987991426583 to 0.523661912308797
0.0990627535325249 to 0.0987366744147392
0.103155566070672 to 0.102829486952886
Changing layer 0's weights from 
-0.183297557409134 to -0.18362363652692
-0.555224014098969 to -0.555550093216754
-0.789573279793587 to -0.789899358911372
-0.755014029915657 to -0.755340109033442
-0.00714646058830328 to -0.00747253970608892
-0.689788413818207 to -0.690114492935993
-0.535958690221634 to -0.536284769339419
-0.56341309624466 to -0.563739175362445
-0.800904957230416 to -0.801231036348201
-0.844655012364712 to -0.844981091482497
Changing layer 1's weights from 
-0.839558099980679 to -0.839884179098464
-0.238389415319291 to -0.238715494437077
-0.364092154081193 to -0.364418233198979
-0.373997492368546 to -0.374323571486332
-0.55905027466568 to -0.559376353783466
-0.697288108642426 to -0.697614187760212
-0.426569385106888 to -0.426895464224674
-0.812372414048043 to -0.812698493165828
-0.400709433133927 to -0.401035512251713
-0.763356295998421 to -0.763682375116206
Changing layer 2's weights from 
0.0458319358274066 to 0.045505856709621
-0.508221549566117 to -0.508547628683902
-0.22363225894722 to -0.223958338065006
-0.0524321623400125 to -0.0527582414577981
-0.229684872205582 to -0.230010951323368
-0.435694498594132 to -0.436020577711918
0.0831182174131005 to 0.0827921382953148
0.0106961898252096 to 0.0103701107074239
-0.721302104766693 to -0.721628183884479
-0.676344586189118 to -0.676670665306903
Changing layer 3's weights from 
0.0127816371366106 to 0.012455558018825
-0.250771445806351 to -0.251097524924137
-0.00816152768882835 to -0.00848760680661399
-0.915600759444972 to -0.915926838562757
-0.130837423379746 to -0.131163502497532
-0.261086029584733 to -0.261412108702519
-0.591270876701202 to -0.591596955818988
-0.132758004243699 to -0.133084083361485
-0.464453978116837 to -0.464780057234623
0.00837590021339264 to 0.00804982109560699
Changing layer 4's weights from 
-0.0908979006365212 to -0.0912239797543068
-0.299730701024857 to -0.300056780142643
-0.500208301122513 to -0.500534380240299
-0.213273567731705 to -0.213599646849491
-0.452526373441544 to -0.45285245255933
-0.221061152990189 to -0.221387232107975
-0.225419683034745 to -0.225745762152531
-0.68731720524582 to -0.687643284363605
-0.0936211176470193 to -0.093947196764805
-0.525923056180802 to -0.526249135298587
Changing layer 5's weights from 
0.0618819169446556 to 0.0615558378268699
-0.380191607053605 to -0.380517686171391
0.0449967555448146 to 0.044670676427029
-0.890515425826637 to -0.890841504944422
-0.353246612127152 to -0.353572691244938
-0.751714555199471 to -0.752040634317256
-0.451749248082962 to -0.452075327200748
-0.379303021009293 to -0.379629100127079
0.0102989844724266 to 0.00997290535464096
0.00379087251869063 to 0.00346479340090498
Trying to learn from memory 78, 0, -0.2
sum 0.0397773104331494 distri -0.019031287861157
Using diff 0.0488642706860191 and condRate 0.166666666666667
Changed category 0 weights from 
0.157202971562363 to 0.155574162515224
-0.242831992522262 to -0.244460801569401
-0.525352689043068 to -0.526981498090207
-0.373847323075317 to -0.375476132122456
Changing layer 0's weights from 
-0.18362363652692 to -0.185252445574058
-0.555550093216754 to -0.557178902263893
-0.789899358911372 to -0.791528167958511
-0.755340109033442 to -0.756968918080581
-0.00747253970608892 to -0.00910134875322737
-0.690114492935993 to -0.691743301983131
-0.536284769339419 to -0.537913578386558
-0.563739175362445 to -0.565367984409584
-0.801231036348201 to -0.80285984539534
-0.844981091482497 to -0.846609900529636
Changing layer 1's weights from 
-0.839884179098464 to -0.841512988145603
-0.238715494437077 to -0.240344303484215
-0.364418233198979 to -0.366047042246118
-0.374323571486332 to -0.375952380533471
-0.559376353783466 to -0.561005162830604
-0.697614187760212 to -0.69924299680735
-0.426895464224674 to -0.428524273271812
-0.812698493165828 to -0.814327302212967
-0.401035512251713 to -0.402664321298851
-0.763682375116206 to -0.765311184163345
Changing layer 2's weights from 
0.045505856709621 to 0.0438770476624825
-0.508547628683902 to -0.510176437731041
-0.223958338065006 to -0.225587147112144
-0.0527582414577981 to -0.0543870505049366
-0.230010951323368 to -0.231639760370506
-0.436020577711918 to -0.437649386759056
0.0827921382953148 to 0.0811633292481764
0.0103701107074239 to 0.00874130166028549
-0.721628183884479 to -0.723256992931617
-0.676670665306903 to -0.678299474354042
Changing layer 3's weights from 
0.012455558018825 to 0.0108267489716865
-0.251097524924137 to -0.252726333971275
-0.00848760680661399 to -0.0101164158537524
-0.915926838562757 to -0.917555647609896
-0.131163502497532 to -0.13279231154467
-0.261412108702519 to -0.263040917749658
-0.591596955818988 to -0.593225764866126
-0.133084083361485 to -0.134712892408623
-0.464780057234623 to -0.466408866281762
0.00804982109560699 to 0.00642101204846854
Changing layer 4's weights from 
-0.0912239797543068 to -0.0928527888014453
-0.300056780142643 to -0.301685589189782
-0.500534380240299 to -0.502163189287437
-0.213599646849491 to -0.215228455896629
-0.45285245255933 to -0.454481261606469
-0.221387232107975 to -0.223016041155113
-0.225745762152531 to -0.227374571199669
-0.687643284363605 to -0.689272093410744
-0.093947196764805 to -0.0955760058119434
-0.526249135298587 to -0.527877944345726
Changing layer 5's weights from 
0.0615558378268699 to 0.0599270287797315
-0.380517686171391 to -0.38214649521853
0.044670676427029 to 0.0430418673798905
-0.890841504944422 to -0.892470313991561
-0.353572691244938 to -0.355201500292077
-0.752040634317256 to -0.753669443364395
-0.452075327200748 to -0.453704136247886
-0.379629100127079 to -0.381257909174218
0.00997290535464096 to 0.00834409630750251
0.00346479340090498 to 0.00183598435376653
Trying to learn from memory 78, 1, -0.2
sum 0.0397773104331494 distri 0.0200506094370613
Using diff 0.00978237338780073 and condRate 0.166666666666667
Changed category 1 weights from 
0.290346325265035 to 0.290020246147249
0.523661912308797 to 0.523335833191012
0.0987366744147392 to 0.0984105952969536
0.102829486952886 to 0.102503407835101
Changing layer 0's weights from 
-0.185252445574058 to -0.185578524691844
-0.557178902263893 to -0.557504981381678
-0.791528167958511 to -0.791854247076296
-0.756968918080581 to -0.757294997198366
-0.00910134875322737 to -0.00942742787101302
-0.691743301983131 to -0.692069381100917
-0.537913578386558 to -0.538239657504344
-0.565367984409584 to -0.565694063527369
-0.80285984539534 to -0.803185924513125
-0.846609900529636 to -0.846935979647421
Changing layer 1's weights from 
-0.841512988145603 to -0.841839067263388
-0.240344303484215 to -0.240670382602001
-0.366047042246118 to -0.366373121363903
-0.375952380533471 to -0.376278459651256
-0.561005162830604 to -0.56133124194839
-0.69924299680735 to -0.699569075925136
-0.428524273271812 to -0.428850352389598
-0.814327302212967 to -0.814653381330752
-0.402664321298851 to -0.402990400416637
-0.765311184163345 to -0.76563726328113
Changing layer 2's weights from 
0.0438770476624825 to 0.0435509685446969
-0.510176437731041 to -0.510502516848827
-0.225587147112144 to -0.22591322622993
-0.0543870505049366 to -0.0547131296227222
-0.231639760370506 to -0.231965839488292
-0.437649386759056 to -0.437975465876842
0.0811633292481764 to 0.0808372501303907
0.00874130166028549 to 0.00841522254249985
-0.723256992931617 to -0.723583072049403
-0.678299474354042 to -0.678625553471828
Changing layer 3's weights from 
0.0108267489716865 to 0.0105006698539009
-0.252726333971275 to -0.253052413089061
-0.0101164158537524 to -0.0104424949715381
-0.917555647609896 to -0.917881726727681
-0.13279231154467 to -0.133118390662456
-0.263040917749658 to -0.263366996867443
-0.593225764866126 to -0.593551843983912
-0.134712892408623 to -0.135038971526409
-0.466408866281762 to -0.466734945399547
0.00642101204846854 to 0.00609493293068289
Changing layer 4's weights from 
-0.0928527888014453 to -0.0931788679192309
-0.301685589189782 to -0.302011668307567
-0.502163189287437 to -0.502489268405223
-0.215228455896629 to -0.215554535014415
-0.454481261606469 to -0.454807340724254
-0.223016041155113 to -0.223342120272899
-0.227374571199669 to -0.227700650317455
-0.689272093410744 to -0.689598172528529
-0.0955760058119434 to -0.0959020849297291
-0.527877944345726 to -0.528204023463511
Changing layer 5's weights from 
0.0599270287797315 to 0.0596009496619458
-0.38214649521853 to -0.382472574336315
0.0430418673798905 to 0.0427157882621049
-0.892470313991561 to -0.892796393109346
-0.355201500292077 to -0.355527579409862
-0.753669443364395 to -0.75399552248218
-0.453704136247886 to -0.454030215365672
-0.381257909174218 to -0.381583988292003
0.00834409630750251 to 0.00801801718971686
0.00183598435376653 to 0.00150990523598088
Trying to learn from memory 79, 0, -0.2
sum 0.0397772500311353 distri -0.0190314063896876
Using diff 0.0488643439130391 and condRate 0.166666666666667
Changed category 0 weights from 
0.155574162515224 to 0.153945351027185
-0.244460801569401 to -0.24608961305744
-0.526981498090207 to -0.528610309578246
-0.375476132122456 to -0.377104943610495
Changing layer 0's weights from 
-0.185578524691844 to -0.187207336179883
-0.557504981381678 to -0.559133792869718
-0.791854247076296 to -0.793483058564336
-0.757294997198366 to -0.758923808686406
-0.00942742787101302 to -0.0110562393590522
-0.692069381100917 to -0.693698192588956
-0.538239657504344 to -0.539868468992383
-0.565694063527369 to -0.567322875015408
-0.803185924513125 to -0.804814736001165
-0.846935979647421 to -0.848564791135461
Changing layer 1's weights from 
-0.841839067263388 to -0.843467878751428
-0.240670382602001 to -0.24229919409004
-0.366373121363903 to -0.368001932851942
-0.376278459651256 to -0.377907271139295
-0.56133124194839 to -0.562960053436429
-0.699569075925136 to -0.701197887413175
-0.428850352389598 to -0.430479163877637
-0.814653381330752 to -0.816282192818792
-0.402990400416637 to -0.404619211904676
-0.76563726328113 to -0.76726607476917
Changing layer 2's weights from 
0.0435509685446969 to 0.0419221570566577
-0.510502516848827 to -0.512131328336866
-0.22591322622993 to -0.227542037717969
-0.0547131296227222 to -0.0563419411107614
-0.231965839488292 to -0.233594650976331
-0.437975465876842 to -0.439604277364881
0.0808372501303907 to 0.0792084386423516
0.00841522254249985 to 0.00678641105446069
-0.723583072049403 to -0.725211883537442
-0.678625553471828 to -0.680254364959867
Changing layer 3's weights from 
0.0105006698539009 to 0.00887185836586173
-0.253052413089061 to -0.2546812245771
-0.0104424949715381 to -0.0120713064595772
-0.917881726727681 to -0.91951053821572
-0.133118390662456 to -0.134747202150495
-0.263366996867443 to -0.264995808355482
-0.593551843983912 to -0.595180655471951
-0.135038971526409 to -0.136667783014448
-0.466734945399547 to -0.468363756887586
0.00609493293068289 to 0.00446612144264374
Changing layer 4's weights from 
-0.0931788679192309 to -0.0948076794072701
-0.302011668307567 to -0.303640479795606
-0.502489268405223 to -0.504118079893262
-0.215554535014415 to -0.217183346502454
-0.454807340724254 to -0.456436152212293
-0.223342120272899 to -0.224970931760938
-0.227700650317455 to -0.229329461805494
-0.689598172528529 to -0.691226984016568
-0.0959020849297291 to -0.0975308964177682
-0.528204023463511 to -0.529832834951551
Changing layer 5's weights from 
0.0596009496619458 to 0.0579721381739067
-0.382472574336315 to -0.384101385824354
0.0427157882621049 to 0.0410869767740657
-0.892796393109346 to -0.894425204597386
-0.355527579409862 to -0.357156390897901
-0.75399552248218 to -0.75562433397022
-0.454030215365672 to -0.455659026853711
-0.381583988292003 to -0.383212799780042
0.00801801718971686 to 0.00638920570167771
0.00150990523598088 to -0.000118906252058269
10/5/2016 1:35:13 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 80, 1, -0.2
sum 0.0397784020903686 distri 0.0200515202246657
Using diff 0.00978228134311081 and condRate 0.166666666666667
Changed category 1 weights from 
0.290020246147249 to 0.28969417009762
0.523335833191012 to 0.523009757141382
0.0984105952969536 to 0.0980845192473243
0.102503407835101 to 0.102177331785471
Changing layer 0's weights from 
-0.187207336179883 to -0.187533412229512
-0.559133792869718 to -0.559459868919347
-0.793483058564336 to -0.793809134613965
-0.758923808686406 to -0.759249884736035
-0.0110562393590522 to -0.0113823154086814
-0.693698192588956 to -0.694024268638585
-0.539868468992383 to -0.540194545042012
-0.567322875015408 to -0.567648951065038
-0.804814736001165 to -0.805140812050794
-0.848564791135461 to -0.84889086718509
Changing layer 1's weights from 
-0.843467878751428 to -0.843793954801057
-0.24229919409004 to -0.24262527013967
-0.368001932851942 to -0.368328008901572
-0.377907271139295 to -0.378233347188925
-0.562960053436429 to -0.563286129486058
-0.701197887413175 to -0.701523963462804
-0.430479163877637 to -0.430805239927267
-0.816282192818792 to -0.816608268868421
-0.404619211904676 to -0.404945287954306
-0.76726607476917 to -0.767592150818799
Changing layer 2's weights from 
0.0419221570566577 to 0.0415960810070284
-0.512131328336866 to -0.512457404386495
-0.227542037717969 to -0.227868113767598
-0.0563419411107614 to -0.0566680171603906
-0.233594650976331 to -0.23392072702596
-0.439604277364881 to -0.439930353414511
0.0792084386423516 to 0.0788823625927223
0.00678641105446069 to 0.00646033500483142
-0.725211883537442 to -0.725537959587071
-0.680254364959867 to -0.680580441009496
Changing layer 3's weights from 
0.00887185836586173 to 0.00854578231623246
-0.2546812245771 to -0.255007300626729
-0.0120713064595772 to -0.0123973825092065
-0.91951053821572 to -0.91983661426535
-0.134747202150495 to -0.135073278200124
-0.264995808355482 to -0.265321884405112
-0.595180655471951 to -0.595506731521581
-0.136667783014448 to -0.136993859064077
-0.468363756887586 to -0.468689832937216
0.00446612144264374 to 0.00414004539301447
Changing layer 4's weights from 
-0.0948076794072701 to -0.0951337554568994
-0.303640479795606 to -0.303966555845236
-0.504118079893262 to -0.504444155942891
-0.217183346502454 to -0.217509422552083
-0.456436152212293 to -0.456762228261923
-0.224970931760938 to -0.225297007810567
-0.229329461805494 to -0.229655537855123
-0.691226984016568 to -0.691553060066198
-0.0975308964177682 to -0.0978569724673975
-0.529832834951551 to -0.53015891100118
Changing layer 5's weights from 
0.0579721381739067 to 0.0576460621242774
-0.384101385824354 to -0.384427461873984
0.0410869767740657 to 0.0407609007244364
-0.894425204597386 to -0.894751280647015
-0.357156390897901 to -0.357482466947531
-0.75562433397022 to -0.755950410019849
-0.455659026853711 to -0.455985102903341
-0.383212799780042 to -0.383538875829672
0.00638920570167771 to 0.00606312965204844
-0.000118906252058269 to -0.000444982301687541
Trying to learn from memory 81, 0, -0.2
sum 0.0397774910776678 distri -0.019031436778076
Using diff 0.0488645550863268 and condRate 0.166666666666667
Changed category 0 weights from 
0.153945351027185 to 0.152316532500036
-0.24608961305744 to -0.247718431584589
-0.528610309578246 to -0.530239128105395
-0.377104943610495 to -0.378733762137644
Changing layer 0's weights from 
-0.187533412229512 to -0.189162230756661
-0.559459868919347 to -0.561088687446496
-0.793809134613965 to -0.795437953141114
-0.759249884736035 to -0.760878703263184
-0.0113823154086814 to -0.0130111339358303
-0.694024268638585 to -0.695653087165734
-0.540194545042012 to -0.541823363569161
-0.567648951065038 to -0.569277769592187
-0.805140812050794 to -0.806769630577943
-0.84889086718509 to -0.850519685712239
Changing layer 1's weights from 
-0.843793954801057 to -0.845422773328206
-0.24262527013967 to -0.244254088666818
-0.368328008901572 to -0.36995682742872
-0.378233347188925 to -0.379862165716073
-0.563286129486058 to -0.564914948013207
-0.701523963462804 to -0.703152781989953
-0.430805239927267 to -0.432434058454415
-0.816608268868421 to -0.81823708739557
-0.404945287954306 to -0.406574106481454
-0.767592150818799 to -0.769220969345948
Changing layer 2's weights from 
0.0415960810070284 to 0.0399672624798796
-0.512457404386495 to -0.514086222913644
-0.227868113767598 to -0.229496932294747
-0.0566680171603906 to -0.0582968356875395
-0.23392072702596 to -0.235549545553109
-0.439930353414511 to -0.441559171941659
0.0788823625927223 to 0.0772535440655734
0.00646033500483142 to 0.00483151647768257
-0.725537959587071 to -0.72716677811422
-0.680580441009496 to -0.682209259536645
Changing layer 3's weights from 
0.00854578231623246 to 0.00691696378908361
-0.255007300626729 to -0.256636119153878
-0.0123973825092065 to -0.0140262010363554
-0.91983661426535 to -0.921465432792499
-0.135073278200124 to -0.136702096727273
-0.265321884405112 to -0.26695070293226
-0.595506731521581 to -0.597135550048729
-0.136993859064077 to -0.138622677591226
-0.468689832937216 to -0.470318651464364
0.00414004539301447 to 0.00251122686586562
Changing layer 4's weights from 
-0.0951337554568994 to -0.0967625739840482
-0.303966555845236 to -0.305595374372385
-0.504444155942891 to -0.50607297447004
-0.217509422552083 to -0.219138241079232
-0.456762228261923 to -0.458391046789071
-0.225297007810567 to -0.226925826337716
-0.229655537855123 to -0.231284356382272
-0.691553060066198 to -0.693181878593346
-0.0978569724673975 to -0.0994857909945463
-0.53015891100118 to -0.531787729528329
Changing layer 5's weights from 
0.0576460621242774 to 0.0560172435971285
-0.384427461873984 to -0.386056280401132
0.0407609007244364 to 0.0391320821972876
-0.894751280647015 to -0.896380099174164
-0.357482466947531 to -0.35911128547468
-0.755950410019849 to -0.757579228546998
-0.455985102903341 to -0.457613921430489
-0.383538875829672 to -0.38516769435682
0.00606312965204844 to 0.00443431112489959
-0.000444982301687541 to -0.00207380082883639
Trying to learn from memory 81, 1, -0.2
sum 0.0397774910776678 distri 0.0200507096299985
Using diff 0.00978240867825233 and condRate 0.166666666666667
Changed category 1 weights from 
0.28969417009762 to 0.289368089803486
0.523009757141382 to 0.522683676847248
0.0980845192473243 to 0.0977584389531902
0.102177331785471 to 0.101851251491337
Changing layer 0's weights from 
-0.189162230756661 to -0.189488311050795
-0.561088687446496 to -0.56141476774063
-0.795437953141114 to -0.795764033435248
-0.760878703263184 to -0.761204783557318
-0.0130111339358303 to -0.0133372142299643
-0.695653087165734 to -0.695979167459868
-0.541823363569161 to -0.542149443863295
-0.569277769592187 to -0.569603849886321
-0.806769630577943 to -0.807095710872077
-0.850519685712239 to -0.850845766006373
Changing layer 1's weights from 
-0.845422773328206 to -0.84574885362234
-0.244254088666818 to -0.244580168960952
-0.36995682742872 to -0.370282907722854
-0.379862165716073 to -0.380188246010208
-0.564914948013207 to -0.565241028307341
-0.703152781989953 to -0.703478862284087
-0.432434058454415 to -0.432760138748549
-0.81823708739557 to -0.818563167689704
-0.406574106481454 to -0.406900186775588
-0.769220969345948 to -0.769547049640082
Changing layer 2's weights from 
0.0399672624798796 to 0.0396411821857455
-0.514086222913644 to -0.514412303207778
-0.229496932294747 to -0.229823012588881
-0.0582968356875395 to -0.0586229159816735
-0.235549545553109 to -0.235875625847243
-0.441559171941659 to -0.441885252235793
0.0772535440655734 to 0.0769274637714394
0.00483151647768257 to 0.00450543618354852
-0.72716677811422 to -0.727492858408354
-0.682209259536645 to -0.682535339830779
Changing layer 3's weights from 
0.00691696378908361 to 0.00659088349494956
-0.256636119153878 to -0.256962199448012
-0.0140262010363554 to -0.0143522813304894
-0.921465432792499 to -0.921791513086633
-0.136702096727273 to -0.137028177021407
-0.26695070293226 to -0.267276783226395
-0.597135550048729 to -0.597461630342863
-0.138622677591226 to -0.13894875788536
-0.470318651464364 to -0.470644731758499
0.00251122686586562 to 0.00218514657173157
Changing layer 4's weights from 
-0.0967625739840482 to -0.0970886542781823
-0.305595374372385 to -0.305921454666519
-0.50607297447004 to -0.506399054764174
-0.219138241079232 to -0.219464321373366
-0.458391046789071 to -0.458717127083206
-0.226925826337716 to -0.22725190663185
-0.231284356382272 to -0.231610436676406
-0.693181878593346 to -0.693507958887481
-0.0994857909945463 to -0.0998118712886804
-0.531787729528329 to -0.532113809822463
Changing layer 5's weights from 
0.0560172435971285 to 0.0556911633029945
-0.386056280401132 to -0.386382360695267
0.0391320821972876 to 0.0388060019031535
-0.896380099174164 to -0.896706179468298
-0.35911128547468 to -0.359437365768814
-0.757579228546998 to -0.757905308841132
-0.457613921430489 to -0.457940001724623
-0.38516769435682 to -0.385493774650955
0.00443431112489959 to 0.00410823083076554
-0.00207380082883639 to -0.00239988112297044
Trying to learn from memory 82, 0, -0.2
sum 0.0397772967928684 distri -0.0190313117231645
Using diff 0.0488642843178158 and condRate 0.166666666666667
Changed category 0 weights from 
0.152316532500036 to 0.150687722998505
-0.247718431584589 to -0.24934724108612
-0.530239128105395 to -0.531867937606926
-0.378733762137644 to -0.380362571639175
Changing layer 0's weights from 
-0.189488311050795 to -0.191117120552327
-0.56141476774063 to -0.563043577242161
-0.795764033435248 to -0.797392842936779
-0.761204783557318 to -0.762833593058849
-0.0133372142299643 to -0.014966023731496
-0.695979167459868 to -0.6976079769614
-0.542149443863295 to -0.543778253364827
-0.569603849886321 to -0.571232659387852
-0.807095710872077 to -0.808724520373608
-0.850845766006373 to -0.852474575507904
Changing layer 1's weights from 
-0.84574885362234 to -0.847377663123871
-0.244580168960952 to -0.246208978462484
-0.370282907722854 to -0.371911717224386
-0.380188246010208 to -0.381817055511739
-0.565241028307341 to -0.566869837808873
-0.703478862284087 to -0.705107671785619
-0.432760138748549 to -0.434388948250081
-0.818563167689704 to -0.820191977191235
-0.406900186775588 to -0.40852899627712
-0.769547049640082 to -0.771175859141613
Changing layer 2's weights from 
0.0396411821857455 to 0.0380123726842138
-0.514412303207778 to -0.51604111270931
-0.229823012588881 to -0.231451822090413
-0.0586229159816735 to -0.0602517254832052
-0.235875625847243 to -0.237504435348775
-0.441885252235793 to -0.443514061737325
0.0769274637714394 to 0.0752986542699077
0.00450543618354852 to 0.00287662668201684
-0.727492858408354 to -0.729121667909886
-0.682535339830779 to -0.684164149332311
Changing layer 3's weights from 
0.00659088349494956 to 0.00496207399341788
-0.256962199448012 to -0.258591008949544
-0.0143522813304894 to -0.0159810908320211
-0.921791513086633 to -0.923420322588164
-0.137028177021407 to -0.138656986522939
-0.267276783226395 to -0.268905592727926
-0.597461630342863 to -0.599090439844395
-0.13894875788536 to -0.140577567386892
-0.470644731758499 to -0.47227354126003
0.00218514657173157 to 0.00055633707019989
Changing layer 4's weights from 
-0.0970886542781823 to -0.0987174637797139
-0.305921454666519 to -0.30755026416805
-0.506399054764174 to -0.508027864265706
-0.219464321373366 to -0.221093130874898
-0.458717127083206 to -0.460345936584737
-0.22725190663185 to -0.228880716133382
-0.231610436676406 to -0.233239246177938
-0.693507958887481 to -0.695136768389012
-0.0998118712886804 to -0.101440680790212
-0.532113809822463 to -0.533742619323994
Changing layer 5's weights from 
0.0556911633029945 to 0.0540623538014628
-0.386382360695267 to -0.388011170196798
0.0388060019031535 to 0.0371771924016219
-0.896706179468298 to -0.898334988969829
-0.359437365768814 to -0.361066175270345
-0.757905308841132 to -0.759534118342663
-0.457940001724623 to -0.459568811226155
-0.385493774650955 to -0.387122584152486
0.00410823083076554 to 0.00247942132923386
-0.00239988112297044 to -0.00402869062450212
Trying to learn from memory 77, 1, -0.2
sum 0.0397773104331494 distri 0.0200506094370613
Using diff 0.00978237338780073 and condRate 0.166666666666667
Changed category 1 weights from 
0.289368089803486 to 0.289042010685701
0.522683676847248 to 0.522357597729463
0.0977584389531902 to 0.0974323598354046
0.101851251491337 to 0.101525172373552
Changing layer 0's weights from 
-0.191117120552327 to -0.191443199670113
-0.563043577242161 to -0.563369656359947
-0.797392842936779 to -0.797718922054565
-0.762833593058849 to -0.763159672176635
-0.014966023731496 to -0.0152921028492817
-0.6976079769614 to -0.697934056079185
-0.543778253364827 to -0.544104332482612
-0.571232659387852 to -0.571558738505638
-0.808724520373608 to -0.809050599491394
-0.852474575507904 to -0.85280065462569
Changing layer 1's weights from 
-0.847377663123871 to -0.847703742241657
-0.246208978462484 to -0.24653505758027
-0.371911717224386 to -0.372237796342172
-0.381817055511739 to -0.382143134629525
-0.566869837808873 to -0.567195916926658
-0.705107671785619 to -0.705433750903404
-0.434388948250081 to -0.434715027367867
-0.820191977191235 to -0.820518056309021
-0.40852899627712 to -0.408855075394906
-0.771175859141613 to -0.771501938259399
Changing layer 2's weights from 
0.0380123726842138 to 0.0376862935664282
-0.51604111270931 to -0.516367191827095
-0.231451822090413 to -0.231777901208199
-0.0602517254832052 to -0.0605778046009909
-0.237504435348775 to -0.237830514466561
-0.443514061737325 to -0.443840140855111
0.0752986542699077 to 0.0749725751521221
0.00287662668201684 to 0.00255054756423119
-0.729121667909886 to -0.729447747027672
-0.684164149332311 to -0.684490228450096
Changing layer 3's weights from 
0.00496207399341788 to 0.00463599487563223
-0.258591008949544 to -0.25891708806733
-0.0159810908320211 to -0.0163071699498067
-0.923420322588164 to -0.92374640170595
-0.138656986522939 to -0.138983065640725
-0.268905592727926 to -0.269231671845712
-0.599090439844395 to -0.599416518962181
-0.140577567386892 to -0.140903646504678
-0.47227354126003 to -0.472599620377816
0.00055633707019989 to 0.000230257952414242
Changing layer 4's weights from 
-0.0987174637797139 to -0.0990435428974996
-0.30755026416805 to -0.307876343285836
-0.508027864265706 to -0.508353943383491
-0.221093130874898 to -0.221419209992684
-0.460345936584737 to -0.460672015702523
-0.228880716133382 to -0.229206795251168
-0.233239246177938 to -0.233565325295724
-0.695136768389012 to -0.695462847506798
-0.101440680790212 to -0.101766759907998
-0.533742619323994 to -0.53406869844178
Changing layer 5's weights from 
0.0540623538014628 to 0.0537362746836772
-0.388011170196798 to -0.388337249314584
0.0371771924016219 to 0.0368511132838362
-0.898334988969829 to -0.898661068087615
-0.361066175270345 to -0.361392254388131
-0.759534118342663 to -0.759860197460449
-0.459568811226155 to -0.459894890343941
-0.387122584152486 to -0.387448663270272
0.00247942132923386 to 0.00215334221144821
-0.00402869062450212 to -0.00435476974228777
Trying to learn from memory 78, 0, -0.2
sum 0.0397773104331494 distri -0.019031287861157
Using diff 0.0488642706860191 and condRate 0.166666666666667
Changed category 0 weights from 
0.150687722998505 to 0.149058913951366
-0.24934724108612 to -0.250976050133259
-0.531867937606926 to -0.533496746654065
-0.380362571639175 to -0.381991380686314
Changing layer 0's weights from 
-0.191443199670113 to -0.193072008717251
-0.563369656359947 to -0.564998465407085
-0.797718922054565 to -0.799347731101704
-0.763159672176635 to -0.764788481223774
-0.0152921028492817 to -0.0169209118964201
-0.697934056079185 to -0.699562865126324
-0.544104332482612 to -0.545733141529751
-0.571558738505638 to -0.573187547552776
-0.809050599491394 to -0.810679408538532
-0.85280065462569 to -0.854429463672828
Changing layer 1's weights from 
-0.847703742241657 to -0.849332551288795
-0.24653505758027 to -0.248163866627408
-0.372237796342172 to -0.37386660538931
-0.382143134629525 to -0.383771943676663
-0.567195916926658 to -0.568824725973797
-0.705433750903404 to -0.707062559950543
-0.434715027367867 to -0.436343836415005
-0.820518056309021 to -0.82214686535616
-0.408855075394906 to -0.410483884442044
-0.771501938259399 to -0.773130747306538
Changing layer 2's weights from 
0.0376862935664282 to 0.0360574845192898
-0.516367191827095 to -0.517996000874234
-0.231777901208199 to -0.233406710255337
-0.0605778046009909 to -0.0622066136481293
-0.237830514466561 to -0.239459323513699
-0.443840140855111 to -0.445468949902249
0.0749725751521221 to 0.0733437661049836
0.00255054756423119 to 0.000921738517092745
-0.729447747027672 to -0.73107655607481
-0.684490228450096 to -0.686119037497235
Changing layer 3's weights from 
0.00463599487563223 to 0.00300718582849378
-0.25891708806733 to -0.260545897114468
-0.0163071699498067 to -0.0179359789969452
-0.92374640170595 to -0.925375210753088
-0.138983065640725 to -0.140611874687863
-0.269231671845712 to -0.27086048089285
-0.599416518962181 to -0.601045328009319
-0.140903646504678 to -0.142532455551816
-0.472599620377816 to -0.474228429424954
0.000230257952414242 to -0.00139855109472421
Changing layer 4's weights from 
-0.0990435428974996 to -0.100672351944638
-0.307876343285836 to -0.309505152332974
-0.508353943383491 to -0.50998275243063
-0.221419209992684 to -0.223048019039822
-0.460672015702523 to -0.462300824749661
-0.229206795251168 to -0.230835604298306
-0.233565325295724 to -0.235194134342862
-0.695462847506798 to -0.697091656553936
-0.101766759907998 to -0.103395568955136
-0.53406869844178 to -0.535697507488919
Changing layer 5's weights from 
0.0537362746836772 to 0.0521074656365387
-0.388337249314584 to -0.389966058361722
0.0368511132838362 to 0.0352223042366978
-0.898661068087615 to -0.900289877134753
-0.361392254388131 to -0.363021063435269
-0.759860197460449 to -0.761489006507588
-0.459894890343941 to -0.461523699391079
-0.387448663270272 to -0.38907747231741
0.00215334221144821 to 0.000524533164309763
-0.00435476974228777 to -0.00598357878942622
Trying to learn from memory 78, 0, -0.2
sum 0.0397773104331494 distri -0.019031287861157
Using diff 0.0488642706860191 and condRate 0.166666666666667
Changed category 0 weights from 
0.149058913951366 to 0.147430104904228
-0.250976050133259 to -0.252604859180397
-0.533496746654065 to -0.535125555701203
-0.381991380686314 to -0.383620189733452
Changing layer 0's weights from 
-0.193072008717251 to -0.194700817764389
-0.564998465407085 to -0.566627274454224
-0.799347731101704 to -0.800976540148842
-0.764788481223774 to -0.766417290270912
-0.0169209118964201 to -0.0185497209435586
-0.699562865126324 to -0.701191674173462
-0.545733141529751 to -0.547361950576889
-0.573187547552776 to -0.574816356599915
-0.810679408538532 to -0.812308217585671
-0.854429463672828 to -0.856058272719967
Changing layer 1's weights from 
-0.849332551288795 to -0.850961360335934
-0.248163866627408 to -0.249792675674547
-0.37386660538931 to -0.375495414436449
-0.383771943676663 to -0.385400752723802
-0.568824725973797 to -0.570453535020935
-0.707062559950543 to -0.708691368997681
-0.436343836415005 to -0.437972645462144
-0.82214686535616 to -0.823775674403298
-0.410483884442044 to -0.412112693489183
-0.773130747306538 to -0.774759556353676
Changing layer 2's weights from 
0.0360574845192898 to 0.0344286754721513
-0.517996000874234 to -0.519624809921372
-0.233406710255337 to -0.235035519302476
-0.0622066136481293 to -0.0638354226952678
-0.239459323513699 to -0.241088132560838
-0.445468949902249 to -0.447097758949388
0.0733437661049836 to 0.0717149570578452
0.000921738517092745 to -0.000707070530045705
-0.73107655607481 to -0.732705365121949
-0.686119037497235 to -0.687747846544373
Changing layer 3's weights from 
0.00300718582849378 to 0.00137837678135533
-0.260545897114468 to -0.262174706161607
-0.0179359789969452 to -0.0195647880440836
-0.925375210753088 to -0.927004019800227
-0.140611874687863 to -0.142240683735002
-0.27086048089285 to -0.272489289939989
-0.601045328009319 to -0.602674137056458
-0.142532455551816 to -0.144161264598954
-0.474228429424954 to -0.475857238472093
-0.00139855109472421 to -0.00302736014186266
Changing layer 4's weights from 
-0.100672351944638 to -0.102301160991777
-0.309505152332974 to -0.311133961380113
-0.50998275243063 to -0.511611561477768
-0.223048019039822 to -0.22467682808696
-0.462300824749661 to -0.4639296337968
-0.230835604298306 to -0.232464413345444
-0.235194134342862 to -0.236822943390001
-0.697091656553936 to -0.698720465601075
-0.103395568955136 to -0.105024378002275
-0.535697507488919 to -0.537326316536057
Changing layer 5's weights from 
0.0521074656365387 to 0.0504786565894003
-0.389966058361722 to -0.391594867408861
0.0352223042366978 to 0.0335934951895593
-0.900289877134753 to -0.901918686181892
-0.363021063435269 to -0.364649872482408
-0.761489006507588 to -0.763117815554726
-0.461523699391079 to -0.463152508438218
-0.38907747231741 to -0.390706281364549
0.000524533164309763 to -0.00110427588282869
-0.00598357878942622 to -0.00761238783656467
Trying to learn from memory 78, 1, -0.2
sum 0.0397773104331494 distri 0.0200506094370613
Using diff 0.00978237338780073 and condRate 0.166666666666667
Changed category 1 weights from 
0.289042010685701 to 0.288715931567915
0.522357597729463 to 0.522031518611677
0.0974323598354046 to 0.0971062807176189
0.101525172373552 to 0.101199093255766
Changing layer 0's weights from 
-0.194700817764389 to -0.195026896882175
-0.566627274454224 to -0.566953353572009
-0.800976540148842 to -0.801302619266628
-0.766417290270912 to -0.766743369388698
-0.0185497209435586 to -0.0188758000613442
-0.701191674173462 to -0.701517753291248
-0.547361950576889 to -0.547688029694675
-0.574816356599915 to -0.5751424357177
-0.812308217585671 to -0.812634296703457
-0.856058272719967 to -0.856384351837753
Changing layer 1's weights from 
-0.850961360335934 to -0.85128743945372
-0.249792675674547 to -0.250118754792332
-0.375495414436449 to -0.375821493554234
-0.385400752723802 to -0.385726831841587
-0.570453535020935 to -0.570779614138721
-0.708691368997681 to -0.709017448115467
-0.437972645462144 to -0.438298724579929
-0.823775674403298 to -0.824101753521084
-0.412112693489183 to -0.412438772606968
-0.774759556353676 to -0.775085635471462
Changing layer 2's weights from 
0.0344286754721513 to 0.0341025963543657
-0.519624809921372 to -0.519950889039158
-0.235035519302476 to -0.235361598420261
-0.0638354226952678 to -0.0641615018130534
-0.241088132560838 to -0.241414211678623
-0.447097758949388 to -0.447423838067173
0.0717149570578452 to 0.0713888779400595
-0.000707070530045705 to -0.00103314964783135
-0.732705365121949 to -0.733031444239734
-0.687747846544373 to -0.688073925662159
Changing layer 3's weights from 
0.00137837678135533 to 0.00105229766356968
-0.262174706161607 to -0.262500785279392
-0.0195647880440836 to -0.0198908671618693
-0.927004019800227 to -0.927330098918012
-0.142240683735002 to -0.142566762852787
-0.272489289939989 to -0.272815369057774
-0.602674137056458 to -0.603000216174243
-0.144161264598954 to -0.14448734371674
-0.475857238472093 to -0.476183317589878
-0.00302736014186266 to -0.0033534392596483
Changing layer 4's weights from 
-0.102301160991777 to -0.102627240109562
-0.311133961380113 to -0.311460040497898
-0.511611561477768 to -0.511937640595554
-0.22467682808696 to -0.225002907204746
-0.4639296337968 to -0.464255712914585
-0.232464413345444 to -0.23279049246323
-0.236822943390001 to -0.237149022507786
-0.698720465601075 to -0.69904654471886
-0.105024378002275 to -0.10535045712006
-0.537326316536057 to -0.537652395653843
Changing layer 5's weights from 
0.0504786565894003 to 0.0501525774716146
-0.391594867408861 to -0.391920946526646
0.0335934951895593 to 0.0332674160717737
-0.901918686181892 to -0.902244765299677
-0.364649872482408 to -0.364975951600193
-0.763117815554726 to -0.763443894672512
-0.463152508438218 to -0.463478587556003
-0.390706281364549 to -0.391032360482334
-0.00110427588282869 to -0.00143035500061433
-0.00761238783656467 to -0.00793846695435031
Trying to learn from memory 79, 0, -0.2
sum 0.0397772500311353 distri -0.0190314063896876
Using diff 0.0488643439130391 and condRate 0.166666666666667
Changed category 0 weights from 
0.147430104904228 to 0.145801293416189
-0.252604859180397 to -0.254233670668437
-0.535125555701203 to -0.536754367189243
-0.383620189733452 to -0.385249001221492
Changing layer 0's weights from 
-0.195026896882175 to -0.196655708370214
-0.566953353572009 to -0.568582165060049
-0.801302619266628 to -0.802931430754667
-0.766743369388698 to -0.768372180876737
-0.0188758000613442 to -0.0205046115493834
-0.701517753291248 to -0.703146564779287
-0.547688029694675 to -0.549316841182714
-0.5751424357177 to -0.57677124720574
-0.812634296703457 to -0.814263108191496
-0.856384351837753 to -0.858013163325792
Changing layer 1's weights from 
-0.85128743945372 to -0.852916250941759
-0.250118754792332 to -0.251747566280371
-0.375821493554234 to -0.377450305042274
-0.385726831841587 to -0.387355643329627
-0.570779614138721 to -0.57240842562676
-0.709017448115467 to -0.710646259603506
-0.438298724579929 to -0.439927536067969
-0.824101753521084 to -0.825730565009123
-0.412438772606968 to -0.414067584095007
-0.775085635471462 to -0.776714446959501
Changing layer 2's weights from 
0.0341025963543657 to 0.0324737848663265
-0.519950889039158 to -0.521579700527197
-0.235361598420261 to -0.2369904099083
-0.0641615018130534 to -0.0657903133010926
-0.241414211678623 to -0.243043023166662
-0.447423838067173 to -0.449052649555213
0.0713888779400595 to 0.0697600664520203
-0.00103314964783135 to -0.00266196113587051
-0.733031444239734 to -0.734660255727773
-0.688073925662159 to -0.689702737150198
Changing layer 3's weights from 
0.00105229766356968 to -0.000576513824469469
-0.262500785279392 to -0.264129596767431
-0.0198908671618693 to -0.0215196786499084
-0.927330098918012 to -0.928958910406052
-0.142566762852787 to -0.144195574340826
-0.272815369057774 to -0.274444180545814
-0.603000216174243 to -0.604629027662282
-0.14448734371674 to -0.146116155204779
-0.476183317589878 to -0.477812129077918
-0.0033534392596483 to -0.00498225074768746
Changing layer 4's weights from 
-0.102627240109562 to -0.104256051597601
-0.311460040497898 to -0.313088851985938
-0.511937640595554 to -0.513566452083593
-0.225002907204746 to -0.226631718692785
-0.464255712914585 to -0.465884524402625
-0.23279049246323 to -0.234419303951269
-0.237149022507786 to -0.238777833995825
-0.69904654471886 to -0.700675356206899
-0.10535045712006 to -0.106979268608099
-0.537652395653843 to -0.539281207141882
Changing layer 5's weights from 
0.0501525774716146 to 0.0485237659835755
-0.391920946526646 to -0.393549758014686
0.0332674160717737 to 0.0316386045837345
-0.902244765299677 to -0.903873576787717
-0.364975951600193 to -0.366604763088233
-0.763443894672512 to -0.765072706160551
-0.463478587556003 to -0.465107399044043
-0.391032360482334 to -0.392661171970374
-0.00143035500061433 to -0.00305916648865349
-0.00793846695435031 to -0.00956727844238947
Trying to learn from memory 80, 2, -0.2
sum 0.0397784020903686 distri 0.0387599889923221
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.491144798026144 to 0.457811464196105
0.769586763606133 to 0.736253429776094
0.682546398863853 to 0.649213065033814
0.0279746942973731 to -0.00535863953266557
Changing layer 0's weights from 
-0.196655708370214 to -0.229989042200253
-0.568582165060049 to -0.601915498890087
-0.802931430754667 to -0.836264764584705
-0.768372180876737 to -0.801705514706775
-0.0205046115493834 to -0.0538379453794221
-0.703146564779287 to -0.736479898609326
-0.549316841182714 to -0.582650175012753
-0.57677124720574 to -0.610104581035778
-0.814263108191496 to -0.847596442021534
-0.858013163325792 to -0.89134649715583
Changing layer 1's weights from 
-0.852916250941759 to -0.886249584771797
-0.251747566280371 to -0.28508090011041
-0.377450305042274 to -0.410783638872312
-0.387355643329627 to -0.420688977159665
-0.57240842562676 to -0.605741759456799
-0.710646259603506 to -0.743979593433545
-0.439927536067969 to -0.473260869898007
-0.825730565009123 to -0.859063898839161
-0.414067584095007 to -0.447400917925046
-0.776714446959501 to -0.810047780789539
Changing layer 2's weights from 
0.0324737848663265 to -0.000859548963712202
-0.521579700527197 to -0.554913034357235
-0.2369904099083 to -0.270323743738339
-0.0657903133010926 to -0.0991236471311313
-0.243043023166662 to -0.276376356996701
-0.449052649555213 to -0.482385983385251
0.0697600664520203 to 0.0364267326219816
-0.00266196113587051 to -0.0359952949659092
-0.734660255727773 to -0.767993589557812
-0.689702737150198 to -0.723036070980236
Changing layer 3's weights from 
-0.000576513824469469 to -0.0339098476545082
-0.264129596767431 to -0.29746293059747
-0.0215196786499084 to -0.0548530124799471
-0.928958910406052 to -0.96229224423609
-0.144195574340826 to -0.177528908170865
-0.274444180545814 to -0.307777514375852
-0.604629027662282 to -0.637962361492321
-0.146116155204779 to -0.179449489034818
-0.477812129077918 to -0.511145462907956
-0.00498225074768746 to -0.0383155845777262
Changing layer 4's weights from 
-0.104256051597601 to -0.13758938542764
-0.313088851985938 to -0.346422185815976
-0.513566452083593 to -0.546899785913632
-0.226631718692785 to -0.259965052522824
-0.465884524402625 to -0.499217858232663
-0.234419303951269 to -0.267752637781308
-0.238777833995825 to -0.272111167825864
-0.700675356206899 to -0.734008690036938
-0.106979268608099 to -0.140312602438138
-0.539281207141882 to -0.57261454097192
Changing layer 5's weights from 
0.0485237659835755 to 0.0151904321535368
-0.393549758014686 to -0.426883091844724
0.0316386045837345 to -0.00169472924630418
-0.903873576787717 to -0.937206910617755
-0.366604763088233 to -0.399938096918271
-0.765072706160551 to -0.798406039990589
-0.465107399044043 to -0.498440732874081
-0.392661171970374 to -0.425994505800412
-0.00305916648865349 to -0.0363925003186922
-0.00956727844238947 to -0.0429006122724282
10/5/2016 1:35:14 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 83, 0, -0.2
sum 0.0399142220175729 distri -0.0191413046758735
Using diff 0.0490769711890532 and condRate 0.166666666666667
Changed category 0 weights from 
0.145801293416189 to 0.144165394352177
-0.254233670668437 to -0.255869569732448
-0.536754367189243 to -0.538390266253254
-0.385249001221492 to -0.386884900285503
Changing layer 0's weights from 
-0.229989042200253 to -0.231624941264265
-0.601915498890087 to -0.603551397954099
-0.836264764584705 to -0.837900663648717
-0.801705514706775 to -0.803341413770787
-0.0538379453794221 to -0.055473844443434
-0.736479898609326 to -0.738115797673338
-0.582650175012753 to -0.584286074076764
-0.610104581035778 to -0.61174048009979
-0.847596442021534 to -0.849232341085546
-0.89134649715583 to -0.892982396219842
Changing layer 1's weights from 
-0.886249584771797 to -0.887885483835809
-0.28508090011041 to -0.286716799174422
-0.410783638872312 to -0.412419537936324
-0.420688977159665 to -0.422324876223677
-0.605741759456799 to -0.607377658520811
-0.743979593433545 to -0.745615492497557
-0.473260869898007 to -0.474896768962019
-0.859063898839161 to -0.860699797903173
-0.447400917925046 to -0.449036816989058
-0.810047780789539 to -0.811683679853551
Changing layer 2's weights from 
-0.000859548963712202 to -0.0024954480277241
-0.554913034357235 to -0.556548933421247
-0.270323743738339 to -0.271959642802351
-0.0991236471311313 to -0.100759546195143
-0.276376356996701 to -0.278012256060713
-0.482385983385251 to -0.484021882449263
0.0364267326219816 to 0.0347908335579697
-0.0359952949659092 to -0.0376311940299211
-0.767993589557812 to -0.769629488621824
-0.723036070980236 to -0.724671970044248
Changing layer 3's weights from 
-0.0339098476545082 to -0.0355457467185201
-0.29746293059747 to -0.299098829661482
-0.0548530124799471 to -0.056488911543959
-0.96229224423609 to -0.963928143300102
-0.177528908170865 to -0.179164807234877
-0.307777514375852 to -0.309413413439864
-0.637962361492321 to -0.639598260556333
-0.179449489034818 to -0.18108538809883
-0.511145462907956 to -0.512781361971968
-0.0383155845777262 to -0.0399514836417381
Changing layer 4's weights from 
-0.13758938542764 to -0.139225284491652
-0.346422185815976 to -0.348058084879988
-0.546899785913632 to -0.548535684977643
-0.259965052522824 to -0.261600951586836
-0.499217858232663 to -0.500853757296675
-0.267752637781308 to -0.26938853684532
-0.272111167825864 to -0.273747066889876
-0.734008690036938 to -0.73564458910095
-0.140312602438138 to -0.14194850150215
-0.57261454097192 to -0.574250440035932
Changing layer 5's weights from 
0.0151904321535368 to 0.0135545330895249
-0.426883091844724 to -0.428518990908736
-0.00169472924630418 to -0.00333062831031608
-0.937206910617755 to -0.938842809681767
-0.399938096918271 to -0.401573995982283
-0.798406039990589 to -0.800041939054601
-0.498440732874081 to -0.500076631938093
-0.425994505800412 to -0.427630404864424
-0.0363925003186922 to -0.0380283993827041
-0.0429006122724282 to -0.0445365113364401
Trying to learn from memory 84, 0, -0.2
sum 0.0401082841210203 distri -0.0192983412125967
Using diff 0.0493795543033619 and condRate 0.166666666666667
Changed category 0 weights from 
0.144165394352177 to 0.142519409184204
-0.255869569732448 to -0.257515554900421
-0.538390266253254 to -0.540036251421227
-0.386884900285503 to -0.388530885453476
Changing layer 0's weights from 
-0.231624941264265 to -0.233270926432237
-0.603551397954099 to -0.605197383122072
-0.837900663648717 to -0.83954664881669
-0.803341413770787 to -0.80498739893876
-0.055473844443434 to -0.0571198296114065
-0.738115797673338 to -0.73976178284131
-0.584286074076764 to -0.585932059244737
-0.61174048009979 to -0.613386465267763
-0.849232341085546 to -0.850878326253519
-0.892982396219842 to -0.894628381387815
Changing layer 1's weights from 
-0.887885483835809 to -0.889531469003782
-0.286716799174422 to -0.288362784342395
-0.412419537936324 to -0.414065523104297
-0.422324876223677 to -0.42397086139165
-0.607377658520811 to -0.609023643688783
-0.745615492497557 to -0.747261477665529
-0.474896768962019 to -0.476542754129992
-0.860699797903173 to -0.862345783071146
-0.449036816989058 to -0.450682802157031
-0.811683679853551 to -0.813329665021524
Changing layer 2's weights from 
-0.0024954480277241 to -0.00414143319569659
-0.556548933421247 to -0.55819491858922
-0.271959642802351 to -0.273605627970323
-0.100759546195143 to -0.102405531363116
-0.278012256060713 to -0.279658241228685
-0.484021882449263 to -0.485667867617236
0.0347908335579697 to 0.0331448483899973
-0.0376311940299211 to -0.0392771791978936
-0.769629488621824 to -0.771275473789796
-0.724671970044248 to -0.726317955212221
Changing layer 3's weights from 
-0.0355457467185201 to -0.0371917318864926
-0.299098829661482 to -0.300744814829455
-0.056488911543959 to -0.0581348967119315
-0.963928143300102 to -0.965574128468075
-0.179164807234877 to -0.180810792402849
-0.309413413439864 to -0.311059398607837
-0.639598260556333 to -0.641244245724305
-0.18108538809883 to -0.182731373266802
-0.512781361971968 to -0.514427347139941
-0.0399514836417381 to -0.0415974688097105
Changing layer 4's weights from 
-0.139225284491652 to -0.140871269659624
-0.348058084879988 to -0.349704070047961
-0.548535684977643 to -0.550181670145616
-0.261600951586836 to -0.263246936754808
-0.500853757296675 to -0.502499742464648
-0.26938853684532 to -0.271034522013292
-0.273747066889876 to -0.275393052057848
-0.73564458910095 to -0.737290574268923
-0.14194850150215 to -0.143594486670123
-0.574250440035932 to -0.575896425203905
Changing layer 5's weights from 
0.0135545330895249 to 0.0119085479215524
-0.428518990908736 to -0.430164976076709
-0.00333062831031608 to -0.00497661347828857
-0.938842809681767 to -0.94048879484974
-0.401573995982283 to -0.403219981150256
-0.800041939054601 to -0.801687924222574
-0.500076631938093 to -0.501722617106066
-0.427630404864424 to -0.429276390032397
-0.0380283993827041 to -0.0396743845506766
-0.0445365113364401 to -0.0461824965044126
Trying to learn from memory 84, 0, -0.2
sum 0.0401082841210203 distri -0.0192983412125967
Using diff 0.0493795543033619 and condRate 0.166666666666667
Changed category 0 weights from 
0.142519409184204 to 0.140873424016232
-0.257515554900421 to -0.259161540068393
-0.540036251421227 to -0.541682236589199
-0.388530885453476 to -0.390176870621448
Changing layer 0's weights from 
-0.233270926432237 to -0.23491691160021
-0.605197383122072 to -0.606843368290044
-0.83954664881669 to -0.841192633984662
-0.80498739893876 to -0.806633384106732
-0.0571198296114065 to -0.058765814779379
-0.73976178284131 to -0.741407768009283
-0.585932059244737 to -0.587578044412709
-0.613386465267763 to -0.615032450435735
-0.850878326253519 to -0.852524311421491
-0.894628381387815 to -0.896274366555787
Changing layer 1's weights from 
-0.889531469003782 to -0.891177454171754
-0.288362784342395 to -0.290008769510367
-0.414065523104297 to -0.415711508272269
-0.42397086139165 to -0.425616846559622
-0.609023643688783 to -0.610669628856756
-0.747261477665529 to -0.748907462833502
-0.476542754129992 to -0.478188739297964
-0.862345783071146 to -0.863991768239118
-0.450682802157031 to -0.452328787325003
-0.813329665021524 to -0.814975650189496
Changing layer 2's weights from 
-0.00414143319569659 to -0.00578741836366908
-0.55819491858922 to -0.559840903757192
-0.273605627970323 to -0.275251613138296
-0.102405531363116 to -0.104051516531088
-0.279658241228685 to -0.281304226396658
-0.485667867617236 to -0.487313852785208
0.0331448483899973 to 0.0314988632220248
-0.0392771791978936 to -0.0409231643658661
-0.771275473789796 to -0.772921458957769
-0.726317955212221 to -0.727963940380193
Changing layer 3's weights from 
-0.0371917318864926 to -0.0388377170544651
-0.300744814829455 to -0.302390799997427
-0.0581348967119315 to -0.059780881879904
-0.965574128468075 to -0.967220113636047
-0.180810792402849 to -0.182456777570822
-0.311059398607837 to -0.312705383775809
-0.641244245724305 to -0.642890230892278
-0.182731373266802 to -0.184377358434775
-0.514427347139941 to -0.516073332307913
-0.0415974688097105 to -0.043243453977683
Changing layer 4's weights from 
-0.140871269659624 to -0.142517254827597
-0.349704070047961 to -0.351350055215933
-0.550181670145616 to -0.551827655313588
-0.263246936754808 to -0.264892921922781
-0.502499742464648 to -0.50414572763262
-0.271034522013292 to -0.272680507181265
-0.275393052057848 to -0.277039037225821
-0.737290574268923 to -0.738936559436895
-0.143594486670123 to -0.145240471838095
-0.575896425203905 to -0.577542410371877
Changing layer 5's weights from 
0.0119085479215524 to 0.0102625627535799
-0.430164976076709 to -0.431810961244681
-0.00497661347828857 to -0.00662259864626106
-0.94048879484974 to -0.942134780017712
-0.403219981150256 to -0.404865966318228
-0.801687924222574 to -0.803333909390546
-0.501722617106066 to -0.503368602274038
-0.429276390032397 to -0.430922375200369
-0.0396743845506766 to -0.0413203697186491
-0.0461824965044126 to -0.047828481672385
Trying to learn from memory 84, 1, -0.2
sum 0.0401082841210203 distri 0.0202460623515483
Using diff 0.00983515073921691 and condRate 0.166666666666667
Changed category 1 weights from 
0.288715931567915 to 0.288388093205056
0.522031518611677 to 0.521703680248818
0.0971062807176189 to 0.0967784423547599
0.101199093255766 to 0.100871254892907
Changing layer 0's weights from 
-0.23491691160021 to -0.235244749963069
-0.606843368290044 to -0.607171206652903
-0.841192633984662 to -0.841520472347521
-0.806633384106732 to -0.806961222469591
-0.058765814779379 to -0.059093653142238
-0.741407768009283 to -0.741735606372142
-0.587578044412709 to -0.587905882775569
-0.615032450435735 to -0.615360288798594
-0.852524311421491 to -0.85285214978435
-0.896274366555787 to -0.896602204918646
Changing layer 1's weights from 
-0.891177454171754 to -0.891505292534613
-0.290008769510367 to -0.290336607873226
-0.415711508272269 to -0.416039346635128
-0.425616846559622 to -0.425944684922481
-0.610669628856756 to -0.610997467219615
-0.748907462833502 to -0.749235301196361
-0.478188739297964 to -0.478516577660823
-0.863991768239118 to -0.864319606601977
-0.452328787325003 to -0.452656625687862
-0.814975650189496 to -0.815303488552355
Changing layer 2's weights from 
-0.00578741836366908 to -0.00611525672652815
-0.559840903757192 to -0.560168742120052
-0.275251613138296 to -0.275579451501155
-0.104051516531088 to -0.104379354893947
-0.281304226396658 to -0.281632064759517
-0.487313852785208 to -0.487641691148067
0.0314988632220248 to 0.0311710248591657
-0.0409231643658661 to -0.0412510027287252
-0.772921458957769 to -0.773249297320628
-0.727963940380193 to -0.728291778743053
Changing layer 3's weights from 
-0.0388377170544651 to -0.0391655554173241
-0.302390799997427 to -0.302718638360286
-0.059780881879904 to -0.0601087202427631
-0.967220113636047 to -0.967547951998906
-0.182456777570822 to -0.182784615933681
-0.312705383775809 to -0.313033222138668
-0.642890230892278 to -0.643218069255137
-0.184377358434775 to -0.184705196797634
-0.516073332307913 to -0.516401170670772
-0.043243453977683 to -0.0435712923405421
Changing layer 4's weights from 
-0.142517254827597 to -0.142845093190456
-0.351350055215933 to -0.351677893578792
-0.551827655313588 to -0.552155493676448
-0.264892921922781 to -0.26522076028564
-0.50414572763262 to -0.504473565995479
-0.272680507181265 to -0.273008345544124
-0.277039037225821 to -0.27736687558868
-0.738936559436895 to -0.739264397799754
-0.145240471838095 to -0.145568310200954
-0.577542410371877 to -0.577870248734736
Changing layer 5's weights from 
0.0102625627535799 to 0.00993472439072081
-0.431810961244681 to -0.43213879960754
-0.00662259864626106 to -0.00695043700912013
-0.942134780017712 to -0.942462618380571
-0.404865966318228 to -0.405193804681087
-0.803333909390546 to -0.803661747753405
-0.503368602274038 to -0.503696440636897
-0.430922375200369 to -0.431250213563228
-0.0413203697186491 to -0.0416482080815081
-0.047828481672385 to -0.0481563200352441
Trying to learn from memory 85, 1, -0.2
sum 0.0402817810186142 distri 0.0202600110934909
Using diff 0.00995132467046968 and condRate 0.166666666666667
Changed category 1 weights from 
0.288388093205056 to 0.288056382377764
0.521703680248818 to 0.521371969421526
0.0967784423547599 to 0.096446731527468
0.100871254892907 to 0.100539544065615
Changing layer 0's weights from 
-0.235244749963069 to -0.235576460790361
-0.607171206652903 to -0.607502917480195
-0.841520472347521 to -0.841852183174813
-0.806961222469591 to -0.807292933296883
-0.059093653142238 to -0.0594253639695299
-0.741735606372142 to -0.742067317199434
-0.587905882775569 to -0.58823759360286
-0.615360288798594 to -0.615691999625886
-0.85285214978435 to -0.853183860611642
-0.896602204918646 to -0.896933915745938
Changing layer 1's weights from 
-0.891505292534613 to -0.891837003361905
-0.290336607873226 to -0.290668318700518
-0.416039346635128 to -0.41637105746242
-0.425944684922481 to -0.426276395749773
-0.610997467219615 to -0.611329178046907
-0.749235301196361 to -0.749567012023653
-0.478516577660823 to -0.478848288488115
-0.864319606601977 to -0.864651317429269
-0.452656625687862 to -0.452988336515154
-0.815303488552355 to -0.815635199379647
Changing layer 2's weights from 
-0.00611525672652815 to -0.00644696755382001
-0.560168742120052 to -0.560500452947343
-0.275579451501155 to -0.275911162328447
-0.104379354893947 to -0.104711065721239
-0.281632064759517 to -0.281963775586809
-0.487641691148067 to -0.487973401975359
0.0311710248591657 to 0.0308393140318738
-0.0412510027287252 to -0.041582713556017
-0.773249297320628 to -0.77358100814792
-0.728291778743053 to -0.728623489570344
Changing layer 3's weights from 
-0.0391655554173241 to -0.039497266244616
-0.302718638360286 to -0.303050349187578
-0.0601087202427631 to -0.060440431070055
-0.967547951998906 to -0.967879662826198
-0.182784615933681 to -0.183116326760973
-0.313033222138668 to -0.31336493296596
-0.643218069255137 to -0.643549780082429
-0.184705196797634 to -0.185036907624926
-0.516401170670772 to -0.516732881498064
-0.0435712923405421 to -0.043903003167834
Changing layer 4's weights from 
-0.142845093190456 to -0.143176804017748
-0.351677893578792 to -0.352009604406084
-0.552155493676448 to -0.552487204503739
-0.26522076028564 to -0.265552471112932
-0.504473565995479 to -0.504805276822771
-0.273008345544124 to -0.273340056371416
-0.27736687558868 to -0.277698586415972
-0.739264397799754 to -0.739596108627046
-0.145568310200954 to -0.145900021028246
-0.577870248734736 to -0.578201959562028
Changing layer 5's weights from 
0.00993472439072081 to 0.00960301356342894
-0.43213879960754 to -0.432470510434832
-0.00695043700912013 to -0.00728214783641199
-0.942462618380571 to -0.942794329207863
-0.405193804681087 to -0.405525515508379
-0.803661747753405 to -0.803993458580697
-0.503696440636897 to -0.504028151464189
-0.431250213563228 to -0.43158192439052
-0.0416482080815081 to -0.0419799189088
-0.0481563200352441 to -0.048488030862536
Trying to learn from memory 86, 0, -0.2
sum 0.0398597951871382 distri -0.0190440742487575
Using diff 0.0489389206391112 and condRate 0.166666666666667
Changed category 0 weights from 
0.140873424016232 to 0.139242126637286
-0.259161540068393 to -0.260792837447339
-0.541682236589199 to -0.543313533968145
-0.390176870621448 to -0.391808168000394
Changing layer 0's weights from 
-0.235576460790361 to -0.237207758169306
-0.607502917480195 to -0.609134214859141
-0.841852183174813 to -0.843483480553759
-0.807292933296883 to -0.808924230675829
-0.0594253639695299 to -0.0610566613484752
-0.742067317199434 to -0.743698614578379
-0.58823759360286 to -0.589868890981806
-0.615691999625886 to -0.617323297004831
-0.853183860611642 to -0.854815157990588
-0.896933915745938 to -0.898565213124884
Changing layer 1's weights from 
-0.891837003361905 to -0.893468300740851
-0.290668318700518 to -0.292299616079463
-0.41637105746242 to -0.418002354841365
-0.426276395749773 to -0.427907693128718
-0.611329178046907 to -0.612960475425852
-0.749567012023653 to -0.751198309402598
-0.478848288488115 to -0.48047958586706
-0.864651317429269 to -0.866282614808215
-0.452988336515154 to -0.454619633894099
-0.815635199379647 to -0.817266496758593
Changing layer 2's weights from 
-0.00644696755382001 to -0.00807826493276528
-0.560500452947343 to -0.562131750326289
-0.275911162328447 to -0.277542459707392
-0.104711065721239 to -0.106342363100184
-0.281963775586809 to -0.283595072965754
-0.487973401975359 to -0.489604699354304
0.0308393140318738 to 0.0292080166529286
-0.041582713556017 to -0.0432140109349623
-0.77358100814792 to -0.775212305526865
-0.728623489570344 to -0.73025478694929
Changing layer 3's weights from 
-0.039497266244616 to -0.0411285636235613
-0.303050349187578 to -0.304681646566523
-0.060440431070055 to -0.0620717284490002
-0.967879662826198 to -0.969510960205143
-0.183116326760973 to -0.184747624139918
-0.31336493296596 to -0.314996230344905
-0.643549780082429 to -0.645181077461374
-0.185036907624926 to -0.186668205003871
-0.516732881498064 to -0.518364178877009
-0.043903003167834 to -0.0455343005467792
Changing layer 4's weights from 
-0.143176804017748 to -0.144808101396693
-0.352009604406084 to -0.353640901785029
-0.552487204503739 to -0.554118501882685
-0.265552471112932 to -0.267183768491877
-0.504805276822771 to -0.506436574201717
-0.273340056371416 to -0.274971353750361
-0.277698586415972 to -0.279329883794917
-0.739596108627046 to -0.741227406005991
-0.145900021028246 to -0.147531318407191
-0.578201959562028 to -0.579833256940974
Changing layer 5's weights from 
0.00960301356342894 to 0.00797171618448368
-0.432470510434832 to -0.434101807813777
-0.00728214783641199 to -0.00891344521535726
-0.942794329207863 to -0.944425626586809
-0.405525515508379 to -0.407156812887324
-0.803993458580697 to -0.805624755959643
-0.504028151464189 to -0.505659448843135
-0.43158192439052 to -0.433213221769465
-0.0419799189088 to -0.0436112162877453
-0.048488030862536 to -0.0501193282414812
10/5/2016 1:35:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:53 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
Modified index 9's learning in memoryPool to -0.2
Modified index 10's learning in memoryPool to -0.2
Modified index 11's learning in memoryPool to -0.2
Modified index 12's learning in memoryPool to -0.2
Modified index 13's learning in memoryPool to -0.2
Modified index 14's learning in memoryPool to -0.2
Modified index 15's learning in memoryPool to -0.2
Modified index 16's learning in memoryPool to -0.2
Modified index 17's learning in memoryPool to -0.2
Modified index 18's learning in memoryPool to -0.2
Modified index 19's learning in memoryPool to -0.2
Modified index 20's learning in memoryPool to -0.2
Modified index 21's learning in memoryPool to -0.2
Modified index 22's learning in memoryPool to -0.2
Modified index 23's learning in memoryPool to -0.2
Modified index 24's learning in memoryPool to -0.2
10/5/2016 1:35:53 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 87, 1, -0.2
sum 0.0397773104331494 distri 0.0200506094370613
Using diff 0.00978237338780073 and condRate 0.166666666666667
Changed category 1 weights from 
0.288056382377764 to 0.287730303259978
0.521371969421526 to 0.52104589030374
0.096446731527468 to 0.0961206524096823
0.100539544065615 to 0.100213464947829
Changing layer 0's weights from 
-0.237207758169306 to -0.237533837287092
-0.609134214859141 to -0.609460293976926
-0.843483480553759 to -0.843809559671544
-0.808924230675829 to -0.809250309793614
-0.0610566613484752 to -0.0613827404662608
-0.743698614578379 to -0.744024693696165
-0.589868890981806 to -0.590194970099591
-0.617323297004831 to -0.617649376122617
-0.854815157990588 to -0.855141237108373
-0.898565213124884 to -0.898891292242669
Changing layer 1's weights from 
-0.893468300740851 to -0.893794379858636
-0.292299616079463 to -0.292625695197249
-0.418002354841365 to -0.418328433959151
-0.427907693128718 to -0.428233772246504
-0.612960475425852 to -0.613286554543638
-0.751198309402598 to -0.751524388520384
-0.48047958586706 to -0.480805664984846
-0.866282614808215 to -0.866608693926
-0.454619633894099 to -0.454945713011885
-0.817266496758593 to -0.817592575876378
Changing layer 2's weights from 
-0.00807826493276528 to -0.00840434405055092
-0.562131750326289 to -0.562457829444074
-0.277542459707392 to -0.277868538825178
-0.106342363100184 to -0.10666844221797
-0.283595072965754 to -0.28392115208354
-0.489604699354304 to -0.48993077847209
0.0292080166529286 to 0.0288819375351429
-0.0432140109349623 to -0.0435400900527479
-0.775212305526865 to -0.775538384644651
-0.73025478694929 to -0.730580866067075
Changing layer 3's weights from 
-0.0411285636235613 to -0.0414546427413469
-0.304681646566523 to -0.305007725684309
-0.0620717284490002 to -0.0623978075667859
-0.969510960205143 to -0.969837039322929
-0.184747624139918 to -0.185073703257704
-0.314996230344905 to -0.315322309462691
-0.645181077461374 to -0.64550715657916
-0.186668205003871 to -0.186994284121657
-0.518364178877009 to -0.518690257994795
-0.0455343005467792 to -0.0458603796645649
Changing layer 4's weights from 
-0.144808101396693 to -0.145134180514479
-0.353640901785029 to -0.353966980902815
-0.554118501882685 to -0.55444458100047
-0.267183768491877 to -0.267509847609663
-0.506436574201717 to -0.506762653319502
-0.274971353750361 to -0.275297432868147
-0.279329883794917 to -0.279655962912703
-0.741227406005991 to -0.741553485123777
-0.147531318407191 to -0.147857397524977
-0.579833256940974 to -0.580159336058759
Changing layer 5's weights from 
0.00797171618448368 to 0.00764563706669803
-0.434101807813777 to -0.434427886931563
-0.00891344521535726 to -0.00923952433314291
-0.944425626586809 to -0.944751705704594
-0.407156812887324 to -0.40748289200511
-0.805624755959643 to -0.805950835077428
-0.505659448843135 to -0.50598552796092
-0.433213221769465 to -0.433539300887251
-0.0436112162877453 to -0.0439372954055309
-0.0501193282414812 to -0.0504454073592669
Trying to learn from memory 72, 0, -0.2
sum 0.0379741785216576 distri -0.0230963272711135
Using diff 0.0515769611623567 and condRate 0.166666666666667
Changed category 0 weights from 
0.139242126637286 to 0.137522894572923
-0.260792837447339 to -0.262512069511702
-0.543313533968145 to -0.545032766032509
-0.391808168000394 to -0.393527400064757
Changing layer 0's weights from 
-0.237533837287092 to -0.239253069351455
-0.609460293976926 to -0.61117952604129
-0.843809559671544 to -0.845528791735908
-0.809250309793614 to -0.810969541857978
-0.0613827404662608 to -0.0631019725306246
-0.744024693696165 to -0.745743925760528
-0.590194970099591 to -0.591914202163955
-0.617649376122617 to -0.619368608186981
-0.855141237108373 to -0.856860469172737
-0.898891292242669 to -0.900610524307033
Changing layer 1's weights from 
-0.893794379858636 to -0.895513611923
-0.292625695197249 to -0.294344927261613
-0.418328433959151 to -0.420047666023515
-0.428233772246504 to -0.429953004310868
-0.613286554543638 to -0.615005786608001
-0.751524388520384 to -0.753243620584747
-0.480805664984846 to -0.48252489704921
-0.866608693926 to -0.868327925990364
-0.454945713011885 to -0.456664945076249
-0.817592575876378 to -0.819311807940742
Changing layer 2's weights from 
-0.00840434405055092 to -0.0101235761149147
-0.562457829444074 to -0.564177061508438
-0.277868538825178 to -0.279587770889541
-0.10666844221797 to -0.108387674282334
-0.28392115208354 to -0.285640384147904
-0.48993077847209 to -0.491650010536454
0.0288819375351429 to 0.0271627054707791
-0.0435400900527479 to -0.0452593221171117
-0.775538384644651 to -0.777257616709015
-0.730580866067075 to -0.732300098131439
Changing layer 3's weights from 
-0.0414546427413469 to -0.0431738748057107
-0.305007725684309 to -0.306726957748673
-0.0623978075667859 to -0.0641170396311496
-0.969837039322929 to -0.971556271387293
-0.185073703257704 to -0.186792935322068
-0.315322309462691 to -0.317041541527055
-0.64550715657916 to -0.647226388643524
-0.186994284121657 to -0.18871351618602
-0.518690257994795 to -0.520409490059159
-0.0458603796645649 to -0.0475796117289287
Changing layer 4's weights from 
-0.145134180514479 to -0.146853412578843
-0.353966980902815 to -0.355686212967179
-0.55444458100047 to -0.556163813064834
-0.267509847609663 to -0.269229079674026
-0.506762653319502 to -0.508481885383866
-0.275297432868147 to -0.27701666493251
-0.279655962912703 to -0.281375194977066
-0.741553485123777 to -0.743272717188141
-0.147857397524977 to -0.149576629589341
-0.580159336058759 to -0.581878568123123
Changing layer 5's weights from 
0.00764563706669803 to 0.00592640500233425
-0.434427886931563 to -0.436147118995927
-0.00923952433314291 to -0.0109587563975067
-0.944751705704594 to -0.946470937768958
-0.40748289200511 to -0.409202124069474
-0.805950835077428 to -0.807670067141792
-0.50598552796092 to -0.507704760025284
-0.433539300887251 to -0.435258532951615
-0.0439372954055309 to -0.0456565274698947
-0.0504454073592669 to -0.0521646394236307
Trying to learn from memory 73, 0, -0.2
sum 0.0379741785216576 distri -0.0230963272711135
Using diff 0.0515769611623567 and condRate 0.166666666666667
Changed category 0 weights from 
0.137522894572923 to 0.135803662508559
-0.262512069511702 to -0.264231301576066
-0.545032766032509 to -0.546751998096872
-0.393527400064757 to -0.395246632129121
Changing layer 0's weights from 
-0.239253069351455 to -0.240972301415819
-0.61117952604129 to -0.612898758105654
-0.845528791735908 to -0.847248023800272
-0.810969541857978 to -0.812688773922342
-0.0631019725306246 to -0.0648212045949884
-0.745743925760528 to -0.747463157824892
-0.591914202163955 to -0.593633434228319
-0.619368608186981 to -0.621087840251345
-0.856860469172737 to -0.858579701237101
-0.900610524307033 to -0.902329756371397
Changing layer 1's weights from 
-0.895513611923 to -0.897232843987364
-0.294344927261613 to -0.296064159325976
-0.420047666023515 to -0.421766898087878
-0.429953004310868 to -0.431672236375232
-0.615005786608001 to -0.616725018672365
-0.753243620584747 to -0.754962852649111
-0.48252489704921 to -0.484244129113573
-0.868327925990364 to -0.870047158054728
-0.456664945076249 to -0.458384177140612
-0.819311807940742 to -0.821031040005106
Changing layer 2's weights from 
-0.0101235761149147 to -0.0118428081792785
-0.564177061508438 to -0.565896293572802
-0.279587770889541 to -0.281307002953905
-0.108387674282334 to -0.110106906346698
-0.285640384147904 to -0.287359616212267
-0.491650010536454 to -0.493369242600817
0.0271627054707791 to 0.0254434734064154
-0.0452593221171117 to -0.0469785541814755
-0.777257616709015 to -0.778976848773378
-0.732300098131439 to -0.734019330195803
Changing layer 3's weights from 
-0.0431738748057107 to -0.0448931068700745
-0.306726957748673 to -0.308446189813036
-0.0641170396311496 to -0.0658362716955134
-0.971556271387293 to -0.973275503451657
-0.186792935322068 to -0.188512167386431
-0.317041541527055 to -0.318760773591419
-0.647226388643524 to -0.648945620707887
-0.18871351618602 to -0.190432748250384
-0.520409490059159 to -0.522128722123523
-0.0475796117289287 to -0.0492988437932924
Changing layer 4's weights from 
-0.146853412578843 to -0.148572644643206
-0.355686212967179 to -0.357405445031543
-0.556163813064834 to -0.557883045129198
-0.269229079674026 to -0.27094831173839
-0.508481885383866 to -0.51020111744823
-0.27701666493251 to -0.278735896996874
-0.281375194977066 to -0.28309442704143
-0.743272717188141 to -0.744991949252504
-0.149576629589341 to -0.151295861653704
-0.581878568123123 to -0.583597800187487
Changing layer 5's weights from 
0.00592640500233425 to 0.00420717293797048
-0.436147118995927 to -0.437866351060291
-0.0109587563975067 to -0.0126779884618705
-0.946470937768958 to -0.948190169833322
-0.409202124069474 to -0.410921356133838
-0.807670067141792 to -0.809389299206156
-0.507704760025284 to -0.509423992089648
-0.435258532951615 to -0.436977765015979
-0.0456565274698947 to -0.0473757595342585
-0.0521646394236307 to -0.0538838714879945
Trying to learn from memory 88, 0, -0.2
sum 0.0379741785216576 distri -0.0230963272711135
Using diff 0.0515769611623567 and condRate 0.166666666666667
Changed category 0 weights from 
0.135803662508559 to 0.134084430444195
-0.264231301576066 to -0.26595053364043
-0.546751998096872 to -0.548471230161236
-0.395246632129121 to -0.396965864193485
Changing layer 0's weights from 
-0.240972301415819 to -0.242691533480183
-0.612898758105654 to -0.614617990170017
-0.847248023800272 to -0.848967255864636
-0.812688773922342 to -0.814408005986705
-0.0648212045949884 to -0.0665404366593521
-0.747463157824892 to -0.749182389889256
-0.593633434228319 to -0.595352666292683
-0.621087840251345 to -0.622807072315708
-0.858579701237101 to -0.860298933301464
-0.902329756371397 to -0.90404898843576
Changing layer 1's weights from 
-0.897232843987364 to -0.898952076051727
-0.296064159325976 to -0.29778339139034
-0.421766898087878 to -0.423486130152242
-0.431672236375232 to -0.433391468439595
-0.616725018672365 to -0.618444250736729
-0.754962852649111 to -0.756682084713475
-0.484244129113573 to -0.485963361177937
-0.870047158054728 to -0.871766390119092
-0.458384177140612 to -0.460103409204976
-0.821031040005106 to -0.822750272069469
Changing layer 2's weights from 
-0.0118428081792785 to -0.0135620402436423
-0.565896293572802 to -0.567615525637166
-0.281307002953905 to -0.283026235018269
-0.110106906346698 to -0.111826138411061
-0.287359616212267 to -0.289078848276631
-0.493369242600817 to -0.495088474665181
0.0254434734064154 to 0.0237242413420516
-0.0469785541814755 to -0.0486977862458393
-0.778976848773378 to -0.780696080837742
-0.734019330195803 to -0.735738562260167
Changing layer 3's weights from 
-0.0448931068700745 to -0.0466123389344382
-0.308446189813036 to -0.3101654218774
-0.0658362716955134 to -0.0675555037598772
-0.973275503451657 to -0.97499473551602
-0.188512167386431 to -0.190231399450795
-0.318760773591419 to -0.320480005655782
-0.648945620707887 to -0.650664852772251
-0.190432748250384 to -0.192151980314748
-0.522128722123523 to -0.523847954187886
-0.0492988437932924 to -0.0510180758576562
Changing layer 4's weights from 
-0.148572644643206 to -0.15029187670757
-0.357405445031543 to -0.359124677095906
-0.557883045129198 to -0.559602277193562
-0.27094831173839 to -0.272667543802754
-0.51020111744823 to -0.511920349512593
-0.278735896996874 to -0.280455129061238
-0.28309442704143 to -0.284813659105794
-0.744991949252504 to -0.746711181316868
-0.151295861653704 to -0.153015093718068
-0.583597800187487 to -0.585317032251851
Changing layer 5's weights from 
0.00420717293797048 to 0.0024879408736067
-0.437866351060291 to -0.439585583124654
-0.0126779884618705 to -0.0143972205262342
-0.948190169833322 to -0.949909401897685
-0.410921356133838 to -0.412640588198201
-0.809389299206156 to -0.81110853127052
-0.509423992089648 to -0.511143224154011
-0.436977765015979 to -0.438696997080342
-0.0473757595342585 to -0.0490949915986223
-0.0538838714879945 to -0.0556031035523582
Trying to learn from memory 89, 0, -0.2
sum 0.0379741785216576 distri -0.0230963272711135
Using diff 0.0515769611623567 and condRate 0.166666666666667
Changed category 0 weights from 
0.134084430444195 to 0.132365198379831
-0.26595053364043 to -0.267669765704794
-0.548471230161236 to -0.5501904622256
-0.396965864193485 to -0.398685096257849
Changing layer 0's weights from 
-0.242691533480183 to -0.244410765544547
-0.614617990170017 to -0.616337222234381
-0.848967255864636 to -0.850686487928999
-0.814408005986705 to -0.816127238051069
-0.0665404366593521 to -0.0682596687237159
-0.749182389889256 to -0.75090162195362
-0.595352666292683 to -0.597071898357046
-0.622807072315708 to -0.624526304380072
-0.860298933301464 to -0.862018165365828
-0.90404898843576 to -0.905768220500124
Changing layer 1's weights from 
-0.898952076051727 to -0.900671308116091
-0.29778339139034 to -0.299502623454704
-0.423486130152242 to -0.425205362216606
-0.433391468439595 to -0.435110700503959
-0.618444250736729 to -0.620163482801093
-0.756682084713475 to -0.758401316777839
-0.485963361177937 to -0.487682593242301
-0.871766390119092 to -0.873485622183455
-0.460103409204976 to -0.46182264126934
-0.822750272069469 to -0.824469504133833
Changing layer 2's weights from 
-0.0135620402436423 to -0.015281272308006
-0.567615525637166 to -0.569334757701529
-0.283026235018269 to -0.284745467082633
-0.111826138411061 to -0.113545370475425
-0.289078848276631 to -0.290798080340995
-0.495088474665181 to -0.496807706729545
0.0237242413420516 to 0.0220050092776878
-0.0486977862458393 to -0.0504170183102031
-0.780696080837742 to -0.782415312902106
-0.735738562260167 to -0.73745779432453
Changing layer 3's weights from 
-0.0466123389344382 to -0.048331570998802
-0.3101654218774 to -0.311884653941764
-0.0675555037598772 to -0.069274735824241
-0.97499473551602 to -0.976713967580384
-0.190231399450795 to -0.191950631515159
-0.320480005655782 to -0.322199237720146
-0.650664852772251 to -0.652384084836615
-0.192151980314748 to -0.193871212379112
-0.523847954187886 to -0.52556718625225
-0.0510180758576562 to -0.05273730792202
Changing layer 4's weights from 
-0.15029187670757 to -0.152011108771934
-0.359124677095906 to -0.36084390916027
-0.559602277193562 to -0.561321509257925
-0.272667543802754 to -0.274386775867118
-0.511920349512593 to -0.513639581576957
-0.280455129061238 to -0.282174361125602
-0.284813659105794 to -0.286532891170158
-0.746711181316868 to -0.748430413381232
-0.153015093718068 to -0.154734325782432
-0.585317032251851 to -0.587036264316214
Changing layer 5's weights from 
0.0024879408736067 to 0.000768708809242923
-0.439585583124654 to -0.441304815189018
-0.0143972205262342 to -0.016116452590598
-0.949909401897685 to -0.951628633962049
-0.412640588198201 to -0.414359820262565
-0.81110853127052 to -0.812827763334883
-0.511143224154011 to -0.512862456218375
-0.438696997080342 to -0.440416229144706
-0.0490949915986223 to -0.050814223662986
-0.0556031035523582 to -0.057322335616722
Trying to learn from memory 90, 0, -0.2
sum 0.0379741785216576 distri -0.0230963272711135
Using diff 0.0515769611623567 and condRate 0.166666666666667
Changed category 0 weights from 
0.132365198379831 to 0.130645966315468
-0.267669765704794 to -0.269388997769157
-0.5501904622256 to -0.551909694289964
-0.398685096257849 to -0.400404328322212
Changing layer 0's weights from 
-0.244410765544547 to -0.246129997608911
-0.616337222234381 to -0.618056454298745
-0.850686487928999 to -0.852405719993363
-0.816127238051069 to -0.817846470115433
-0.0682596687237159 to -0.0699789007880797
-0.75090162195362 to -0.752620854017983
-0.597071898357046 to -0.59879113042141
-0.624526304380072 to -0.626245536444436
-0.862018165365828 to -0.863737397430192
-0.905768220500124 to -0.907487452564488
Changing layer 1's weights from 
-0.900671308116091 to -0.902390540180455
-0.299502623454704 to -0.301221855519068
-0.425205362216606 to -0.42692459428097
-0.435110700503959 to -0.436829932568323
-0.620163482801093 to -0.621882714865456
-0.758401316777839 to -0.760120548842202
-0.487682593242301 to -0.489401825306665
-0.873485622183455 to -0.875204854247819
-0.46182264126934 to -0.463541873333704
-0.824469504133833 to -0.826188736198197
Changing layer 2's weights from 
-0.015281272308006 to -0.0170005043723698
-0.569334757701529 to -0.571053989765893
-0.284745467082633 to -0.286464699146996
-0.113545370475425 to -0.115264602539789
-0.290798080340995 to -0.292517312405359
-0.496807706729545 to -0.498526938793909
0.0220050092776878 to 0.020285777213324
-0.0504170183102031 to -0.0521362503745668
-0.782415312902106 to -0.78413454496647
-0.73745779432453 to -0.739177026388894
Changing layer 3's weights from 
-0.048331570998802 to -0.0500508030631658
-0.311884653941764 to -0.313603886006128
-0.069274735824241 to -0.0709939678886048
-0.976713967580384 to -0.978433199644748
-0.191950631515159 to -0.193669863579523
-0.322199237720146 to -0.32391846978451
-0.652384084836615 to -0.654103316900979
-0.193871212379112 to -0.195590444443476
-0.52556718625225 to -0.527286418316614
-0.05273730792202 to -0.0544565399863838
Changing layer 4's weights from 
-0.152011108771934 to -0.153730340836298
-0.36084390916027 to -0.362563141224634
-0.561321509257925 to -0.563040741322289
-0.274386775867118 to -0.276106007931481
-0.513639581576957 to -0.515358813641321
-0.282174361125602 to -0.283893593189965
-0.286532891170158 to -0.288252123234521
-0.748430413381232 to -0.750149645445596
-0.154734325782432 to -0.156453557846796
-0.587036264316214 to -0.588755496380578
Changing layer 5's weights from 
0.000768708809242923 to -0.000950523255120853
-0.441304815189018 to -0.443024047253382
-0.016116452590598 to -0.0178356846549618
-0.951628633962049 to -0.953347866026413
-0.414359820262565 to -0.416079052326929
-0.812827763334883 to -0.814546995399247
-0.512862456218375 to -0.514581688282739
-0.440416229144706 to -0.44213546120907
-0.050814223662986 to -0.0525334557273498
-0.057322335616722 to -0.0590415676810858
Trying to learn from memory 91, 1, -0.2
sum 0.0379741785216576 distri 0.0216399287059932
Using diff 0.00684070518525006 and condRate 0.166666666666667
Changed category 1 weights from 
0.287730303259978 to 0.287502279750405
0.52104589030374 to 0.520817866794168
0.0961206524096823 to 0.0958926289001095
0.100213464947829 to 0.0999854414382565
Changing layer 0's weights from 
-0.246129997608911 to -0.246358021118483
-0.618056454298745 to -0.618284477808318
-0.852405719993363 to -0.852633743502936
-0.817846470115433 to -0.818074493625006
-0.0699789007880797 to -0.0702069242976525
-0.752620854017983 to -0.752848877527556
-0.59879113042141 to -0.599019153930983
-0.626245536444436 to -0.626473559954009
-0.863737397430192 to -0.863965420939765
-0.907487452564488 to -0.907715476074061
Changing layer 1's weights from 
-0.902390540180455 to -0.902618563690028
-0.301221855519068 to -0.30144987902864
-0.42692459428097 to -0.427152617790543
-0.436829932568323 to -0.437057956077896
-0.621882714865456 to -0.622110738375029
-0.760120548842202 to -0.760348572351775
-0.489401825306665 to -0.489629848816238
-0.875204854247819 to -0.875432877757392
-0.463541873333704 to -0.463769896843276
-0.826188736198197 to -0.82641675970777
Changing layer 2's weights from 
-0.0170005043723698 to -0.0172285278819426
-0.571053989765893 to -0.571282013275466
-0.286464699146996 to -0.286692722656569
-0.115264602539789 to -0.115492626049362
-0.292517312405359 to -0.292745335914931
-0.498526938793909 to -0.498754962303481
0.020285777213324 to 0.0200577537037512
-0.0521362503745668 to -0.0523642738841396
-0.78413454496647 to -0.784362568476042
-0.739177026388894 to -0.739405049898467
Changing layer 3's weights from 
-0.0500508030631658 to -0.0502788265727386
-0.313603886006128 to -0.3138319095157
-0.0709939678886048 to -0.0712219913981776
-0.978433199644748 to -0.978661223154321
-0.193669863579523 to -0.193897887089096
-0.32391846978451 to -0.324146493294083
-0.654103316900979 to -0.654331340410551
-0.195590444443476 to -0.195818467953048
-0.527286418316614 to -0.527514441826187
-0.0544565399863838 to -0.0546845634959566
Changing layer 4's weights from 
-0.153730340836298 to -0.15395836434587
-0.362563141224634 to -0.362791164734207
-0.563040741322289 to -0.563268764831862
-0.276106007931481 to -0.276334031441054
-0.515358813641321 to -0.515586837150894
-0.283893593189965 to -0.284121616699538
-0.288252123234521 to -0.288480146744094
-0.750149645445596 to -0.750377668955169
-0.156453557846796 to -0.156681581356369
-0.588755496380578 to -0.588983519890151
Changing layer 5's weights from 
-0.000950523255120853 to -0.00117854676469367
-0.443024047253382 to -0.443252070762955
-0.0178356846549618 to -0.0180637081645346
-0.953347866026413 to -0.953575889535986
-0.416079052326929 to -0.416307075836502
-0.814546995399247 to -0.81477501890882
-0.514581688282739 to -0.514809711792312
-0.44213546120907 to -0.442363484718643
-0.0525334557273498 to -0.0527614792369226
-0.0590415676810858 to -0.0592695911906586
Trying to learn from memory 91, 0, -0.2
sum 0.0379741785216576 distri -0.0230963272711135
Using diff 0.0515769611623567 and condRate 0.166666666666667
Changed category 0 weights from 
0.130645966315468 to 0.128926734251104
-0.269388997769157 to -0.271108229833521
-0.551909694289964 to -0.553628926354327
-0.400404328322212 to -0.402123560386576
Changing layer 0's weights from 
-0.246358021118483 to -0.248077253182847
-0.618284477808318 to -0.620003709872682
-0.852633743502936 to -0.8543529755673
-0.818074493625006 to -0.81979372568937
-0.0702069242976525 to -0.0719261563620163
-0.752848877527556 to -0.75456810959192
-0.599019153930983 to -0.600738385995347
-0.626473559954009 to -0.628192792018372
-0.863965420939765 to -0.865684653004129
-0.907715476074061 to -0.909434708138425
Changing layer 1's weights from 
-0.902618563690028 to -0.904337795754392
-0.30144987902864 to -0.303169111093004
-0.427152617790543 to -0.428871849854906
-0.437057956077896 to -0.438777188142259
-0.622110738375029 to -0.623829970439393
-0.760348572351775 to -0.762067804416139
-0.489629848816238 to -0.491349080880601
-0.875432877757392 to -0.877152109821756
-0.463769896843276 to -0.46548912890764
-0.82641675970777 to -0.828135991772134
Changing layer 2's weights from 
-0.0172285278819426 to -0.0189477599463064
-0.571282013275466 to -0.57300124533983
-0.286692722656569 to -0.288411954720933
-0.115492626049362 to -0.117211858113726
-0.292745335914931 to -0.294464567979295
-0.498754962303481 to -0.500474194367845
0.0200577537037512 to 0.0183385216393874
-0.0523642738841396 to -0.0540835059485034
-0.784362568476042 to -0.786081800540406
-0.739405049898467 to -0.741124281962831
Changing layer 3's weights from 
-0.0502788265727386 to -0.0519980586371024
-0.3138319095157 to -0.315551141580064
-0.0712219913981776 to -0.0729412234625414
-0.978661223154321 to -0.980380455218684
-0.193897887089096 to -0.195617119153459
-0.324146493294083 to -0.325865725358446
-0.654331340410551 to -0.656050572474915
-0.195818467953048 to -0.197537700017412
-0.527514441826187 to -0.52923367389055
-0.0546845634959566 to -0.0564037955603204
Changing layer 4's weights from 
-0.15395836434587 to -0.155677596410234
-0.362791164734207 to -0.36451039679857
-0.563268764831862 to -0.564987996896226
-0.276334031441054 to -0.278053263505418
-0.515586837150894 to -0.517306069215257
-0.284121616699538 to -0.285840848763902
-0.288480146744094 to -0.290199378808458
-0.750377668955169 to -0.752096901019532
-0.156681581356369 to -0.158400813420732
-0.588983519890151 to -0.590702751954515
Changing layer 5's weights from 
-0.00117854676469367 to -0.00289777882905745
-0.443252070762955 to -0.444971302827318
-0.0180637081645346 to -0.0197829402288984
-0.953575889535986 to -0.955295121600349
-0.416307075836502 to -0.418026307900865
-0.81477501890882 to -0.816494250973184
-0.514809711792312 to -0.516528943856675
-0.442363484718643 to -0.444082716783006
-0.0527614792369226 to -0.0544807113012864
-0.0592695911906586 to -0.0609888232550224
Trying to learn from memory 91, 1, -0.2
sum 0.0379741785216576 distri 0.0216399287059932
Using diff 0.00684070518525006 and condRate 0.166666666666667
Changed category 1 weights from 
0.287502279750405 to 0.287274256240833
0.520817866794168 to 0.520589843284595
0.0958926289001095 to 0.0956646053905367
0.0999854414382565 to 0.0997574179286837
Changing layer 0's weights from 
-0.248077253182847 to -0.24830527669242
-0.620003709872682 to -0.620231733382254
-0.8543529755673 to -0.854580999076872
-0.81979372568937 to -0.820021749198942
-0.0719261563620163 to -0.0721541798715891
-0.75456810959192 to -0.754796133101493
-0.600738385995347 to -0.60096640950492
-0.628192792018372 to -0.628420815527945
-0.865684653004129 to -0.865912676513701
-0.909434708138425 to -0.909662731647997
Changing layer 1's weights from 
-0.904337795754392 to -0.904565819263964
-0.303169111093004 to -0.303397134602577
-0.428871849854906 to -0.429099873364479
-0.438777188142259 to -0.439005211651832
-0.623829970439393 to -0.624057993948966
-0.762067804416139 to -0.762295827925712
-0.491349080880601 to -0.491577104390174
-0.877152109821756 to -0.877380133331328
-0.46548912890764 to -0.465717152417213
-0.828135991772134 to -0.828364015281706
Changing layer 2's weights from 
-0.0189477599463064 to -0.0191757834558792
-0.57300124533983 to -0.573229268849403
-0.288411954720933 to -0.288639978230506
-0.117211858113726 to -0.117439881623298
-0.294464567979295 to -0.294692591488868
-0.500474194367845 to -0.500702217877418
0.0183385216393874 to 0.0181104981298146
-0.0540835059485034 to -0.0543115294580762
-0.786081800540406 to -0.786309824049979
-0.741124281962831 to -0.741352305472403
Changing layer 3's weights from 
-0.0519980586371024 to -0.0522260821466752
-0.315551141580064 to -0.315779165089637
-0.0729412234625414 to -0.0731692469721142
-0.980380455218684 to -0.980608478728257
-0.195617119153459 to -0.195845142663032
-0.325865725358446 to -0.326093748868019
-0.656050572474915 to -0.656278595984488
-0.197537700017412 to -0.197765723526985
-0.52923367389055 to -0.529461697400123
-0.0564037955603204 to -0.0566318190698932
Changing layer 4's weights from 
-0.155677596410234 to -0.155905619919807
-0.36451039679857 to -0.364738420308143
-0.564987996896226 to -0.565216020405799
-0.278053263505418 to -0.278281287014991
-0.517306069215257 to -0.51753409272483
-0.285840848763902 to -0.286068872273475
-0.290199378808458 to -0.290427402318031
-0.752096901019532 to -0.752324924529105
-0.158400813420732 to -0.158628836930305
-0.590702751954515 to -0.590930775464087
Changing layer 5's weights from 
-0.00289777882905745 to -0.00312580233863026
-0.444971302827318 to -0.445199326336891
-0.0197829402288984 to -0.0200109637384712
-0.955295121600349 to -0.955523145109922
-0.418026307900865 to -0.418254331410438
-0.816494250973184 to -0.816722274482756
-0.516528943856675 to -0.516756967366248
-0.444082716783006 to -0.444310740292579
-0.0544807113012864 to -0.0547087348108592
-0.0609888232550224 to -0.0612168467645952
Trying to learn from memory 92, 0, -0.2
sum 0.0379741785216576 distri -0.0230963272711135
Using diff 0.0515769611623567 and condRate 0.166666666666667
Changed category 0 weights from 
0.128926734251104 to 0.12720750218674
-0.271108229833521 to -0.272827461897885
-0.553628926354327 to -0.555348158418691
-0.402123560386576 to -0.40384279245094
Changing layer 0's weights from 
-0.24830527669242 to -0.250024508756784
-0.620231733382254 to -0.621950965446618
-0.854580999076872 to -0.856300231141236
-0.820021749198942 to -0.821740981263306
-0.0721541798715891 to -0.0738734119359529
-0.754796133101493 to -0.756515365165857
-0.60096640950492 to -0.602685641569283
-0.628420815527945 to -0.630140047592309
-0.865912676513701 to -0.867631908578065
-0.909662731647997 to -0.911381963712361
Changing layer 1's weights from 
-0.904565819263964 to -0.906285051328328
-0.303397134602577 to -0.305116366666941
-0.429099873364479 to -0.430819105428843
-0.439005211651832 to -0.440724443716196
-0.624057993948966 to -0.62577722601333
-0.762295827925712 to -0.764015059990075
-0.491577104390174 to -0.493296336454538
-0.877380133331328 to -0.879099365395692
-0.465717152417213 to -0.467436384481577
-0.828364015281706 to -0.83008324734607
Changing layer 2's weights from 
-0.0191757834558792 to -0.020895015520243
-0.573229268849403 to -0.574948500913766
-0.288639978230506 to -0.29035921029487
-0.117439881623298 to -0.119159113687662
-0.294692591488868 to -0.296411823553232
-0.500702217877418 to -0.502421449941782
0.0181104981298146 to 0.0163912660654509
-0.0543115294580762 to -0.05603076152244
-0.786309824049979 to -0.788029056114343
-0.741352305472403 to -0.743071537536767
Changing layer 3's weights from 
-0.0522260821466752 to -0.053945314211039
-0.315779165089637 to -0.317498397154001
-0.0731692469721142 to -0.074888479036478
-0.980608478728257 to -0.982327710792621
-0.195845142663032 to -0.197564374727396
-0.326093748868019 to -0.327812980932383
-0.656278595984488 to -0.657997828048852
-0.197765723526985 to -0.199484955591349
-0.529461697400123 to -0.531180929464487
-0.0566318190698932 to -0.058351051134257
Changing layer 4's weights from 
-0.155905619919807 to -0.157624851984171
-0.364738420308143 to -0.366457652372507
-0.565216020405799 to -0.566935252470162
-0.278281287014991 to -0.280000519079354
-0.51753409272483 to -0.519253324789194
-0.286068872273475 to -0.287788104337838
-0.290427402318031 to -0.292146634382395
-0.752324924529105 to -0.754044156593469
-0.158628836930305 to -0.160348068994669
-0.590930775464087 to -0.592650007528451
Changing layer 5's weights from 
-0.00312580233863026 to -0.00484503440299404
-0.445199326336891 to -0.446918558401255
-0.0200109637384712 to -0.021730195802835
-0.955523145109922 to -0.957242377174286
-0.418254331410438 to -0.419973563474802
-0.816722274482756 to -0.81844150654712
-0.516756967366248 to -0.518476199430612
-0.444310740292579 to -0.446029972356943
-0.0547087348108592 to -0.056427966875223
-0.0612168467645952 to -0.062936078828959
10/5/2016 1:35:53 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 92, 1, -0.2
sum 0.0379741785216576 distri 0.0216399287059932
Using diff 0.00684070518525006 and condRate 0.166666666666667
Changed category 1 weights from 
0.287274256240833 to 0.28704623273126
0.520589843284595 to 0.520361819775022
0.0956646053905367 to 0.0954365818809639
0.0997574179286837 to 0.0995293944191109
Changing layer 0's weights from 
-0.250024508756784 to -0.250252532266357
-0.621950965446618 to -0.622178988956191
-0.856300231141236 to -0.856528254650809
-0.821740981263306 to -0.821969004772879
-0.0738734119359529 to -0.0741014354455257
-0.756515365165857 to -0.756743388675429
-0.602685641569283 to -0.602913665078856
-0.630140047592309 to -0.630368071101882
-0.867631908578065 to -0.867859932087638
-0.911381963712361 to -0.911609987221934
Changing layer 1's weights from 
-0.906285051328328 to -0.906513074837901
-0.305116366666941 to -0.305344390176514
-0.430819105428843 to -0.431047128938416
-0.440724443716196 to -0.440952467225769
-0.62577722601333 to -0.626005249522902
-0.764015059990075 to -0.764243083499648
-0.493296336454538 to -0.493524359964111
-0.879099365395692 to -0.879327388905265
-0.467436384481577 to -0.46766440799115
-0.83008324734607 to -0.830311270855643
Changing layer 2's weights from 
-0.020895015520243 to -0.0211230390298158
-0.574948500913766 to -0.575176524423339
-0.29035921029487 to -0.290587233804442
-0.119159113687662 to -0.119387137197235
-0.296411823553232 to -0.296639847062804
-0.502421449941782 to -0.502649473451355
0.0163912660654509 to 0.016163242555878
-0.05603076152244 to -0.0562587850320128
-0.788029056114343 to -0.788257079623916
-0.743071537536767 to -0.74329956104634
Changing layer 3's weights from 
-0.053945314211039 to -0.0541733377206118
-0.317498397154001 to -0.317726420663574
-0.074888479036478 to -0.0751165025460508
-0.982327710792621 to -0.982555734302194
-0.197564374727396 to -0.197792398236969
-0.327812980932383 to -0.328041004441956
-0.657997828048852 to -0.658225851558425
-0.199484955591349 to -0.199712979100922
-0.531180929464487 to -0.53140895297406
-0.058351051134257 to -0.0585790746438298
Changing layer 4's weights from 
-0.157624851984171 to -0.157852875493744
-0.366457652372507 to -0.36668567588208
-0.566935252470162 to -0.567163275979735
-0.280000519079354 to -0.280228542588927
-0.519253324789194 to -0.519481348298767
-0.287788104337838 to -0.288016127847411
-0.292146634382395 to -0.292374657891967
-0.754044156593469 to -0.754272180103042
-0.160348068994669 to -0.160576092504242
-0.592650007528451 to -0.592878031038024
Changing layer 5's weights from 
-0.00484503440299404 to -0.00507305791256686
-0.446918558401255 to -0.447146581910828
-0.021730195802835 to -0.0219582193124078
-0.957242377174286 to -0.957470400683859
-0.419973563474802 to -0.420201586984375
-0.81844150654712 to -0.818669530056693
-0.518476199430612 to -0.518704222940185
-0.446029972356943 to -0.446257995866516
-0.056427966875223 to -0.0566559903847958
-0.062936078828959 to -0.0631641023385318
Trying to learn from memory 93, 1, -0.2
sum 0.0379736998758543 distri 0.0216405844789848
Using diff 0.00683969042790595 and condRate 0.166666666666667
Changed category 1 weights from 
0.28704623273126 to 0.286818243046932
0.520361819775022 to 0.520133830090694
0.0954365818809639 to 0.0952085921966364
0.0995293944191109 to 0.0993014047347833
Changing layer 0's weights from 
-0.250252532266357 to -0.250480521950684
-0.622178988956191 to -0.622406978640518
-0.856528254650809 to -0.856756244335137
-0.821969004772879 to -0.822196994457207
-0.0741014354455257 to -0.0743294251298532
-0.756743388675429 to -0.756971378359757
-0.602913665078856 to -0.603141654763184
-0.630368071101882 to -0.630596060786209
-0.867859932087638 to -0.868087921771965
-0.911609987221934 to -0.911837976906261
Changing layer 1's weights from 
-0.906513074837901 to -0.906741064522228
-0.305344390176514 to -0.305572379860841
-0.431047128938416 to -0.431275118622743
-0.440952467225769 to -0.441180456910096
-0.626005249522902 to -0.62623323920723
-0.764243083499648 to -0.764471073183976
-0.493524359964111 to -0.493752349648438
-0.879327388905265 to -0.879555378589593
-0.46766440799115 to -0.467892397675477
-0.830311270855643 to -0.830539260539971
Changing layer 2's weights from 
-0.0211230390298158 to -0.0213510287141433
-0.575176524423339 to -0.575404514107667
-0.290587233804442 to -0.29081522348877
-0.119387137197235 to -0.119615126881562
-0.296639847062804 to -0.296867836747132
-0.502649473451355 to -0.502877463135682
0.016163242555878 to 0.0159352528715505
-0.0562587850320128 to -0.0564867747163403
-0.788257079623916 to -0.788485069308243
-0.74329956104634 to -0.743527550730668
Changing layer 3's weights from 
-0.0541733377206118 to -0.0544013274049393
-0.317726420663574 to -0.317954410347901
-0.0751165025460508 to -0.0753444922303783
-0.982555734302194 to -0.982783723986521
-0.197792398236969 to -0.198020387921296
-0.328041004441956 to -0.328268994126283
-0.658225851558425 to -0.658453841242752
-0.199712979100922 to -0.199940968785249
-0.53140895297406 to -0.531636942658387
-0.0585790746438298 to -0.0588070643281573
Changing layer 4's weights from 
-0.157852875493744 to -0.158080865178071
-0.36668567588208 to -0.366913665566407
-0.567163275979735 to -0.567391265664063
-0.280228542588927 to -0.280456532273255
-0.519481348298767 to -0.519709337983094
-0.288016127847411 to -0.288244117531739
-0.292374657891967 to -0.292602647576295
-0.754272180103042 to -0.754500169787369
-0.160576092504242 to -0.160804082188569
-0.592878031038024 to -0.593106020722352
Changing layer 5's weights from 
-0.00507305791256686 to -0.00530104759689437
-0.447146581910828 to -0.447374571595155
-0.0219582193124078 to -0.0221862089967353
-0.957470400683859 to -0.957698390368186
-0.420201586984375 to -0.420429576668702
-0.818669530056693 to -0.818897519741021
-0.518704222940185 to -0.518932212624512
-0.446257995866516 to -0.446485985550843
-0.0566559903847958 to -0.0568839800691233
-0.0631641023385318 to -0.0633920920228593
Trying to learn from memory 94, 0, -0.2
sum 0.0379741196205569 distri -0.0230965037467695
Using diff 0.0515770934621872 and condRate 0.166666666666667
Changed category 0 weights from 
0.12720750218674 to 0.125488265712382
-0.272827461897885 to -0.274546698372243
-0.555348158418691 to -0.557067394893049
-0.40384279245094 to -0.405562028925298
Changing layer 0's weights from 
-0.250480521950684 to -0.252199758425042
-0.622406978640518 to -0.624126215114877
-0.856756244335137 to -0.858475480809495
-0.822196994457207 to -0.823916230931565
-0.0743294251298532 to -0.0760486616042114
-0.756971378359757 to -0.758690614834115
-0.603141654763184 to -0.604860891237542
-0.630596060786209 to -0.632315297260568
-0.868087921771965 to -0.869807158246324
-0.911837976906261 to -0.91355721338062
Changing layer 1's weights from 
-0.906741064522228 to -0.908460300996587
-0.305572379860841 to -0.307291616335199
-0.431275118622743 to -0.432994355097101
-0.441180456910096 to -0.442899693384454
-0.62623323920723 to -0.627952475681588
-0.764471073183976 to -0.766190309658334
-0.493752349648438 to -0.495471586122796
-0.879555378589593 to -0.881274615063951
-0.467892397675477 to -0.469611634149835
-0.830539260539971 to -0.832258497014329
Changing layer 2's weights from 
-0.0213510287141433 to -0.0230702651885015
-0.575404514107667 to -0.577123750582025
-0.29081522348877 to -0.292534459963128
-0.119615126881562 to -0.121334363355921
-0.296867836747132 to -0.29858707322149
-0.502877463135682 to -0.50459669961004
0.0159352528715505 to 0.0142160163971923
-0.0564867747163403 to -0.0582060111906985
-0.788485069308243 to -0.790204305782601
-0.743527550730668 to -0.745246787205026
Changing layer 3's weights from 
-0.0544013274049393 to -0.0561205638792975
-0.317954410347901 to -0.319673646822259
-0.0753444922303783 to -0.0770637287047365
-0.982783723986521 to -0.98450296046088
-0.198020387921296 to -0.199739624395654
-0.328268994126283 to -0.329988230600641
-0.658453841242752 to -0.66017307771711
-0.199940968785249 to -0.201660205259607
-0.531636942658387 to -0.533356179132746
-0.0588070643281573 to -0.0605263008025155
Changing layer 4's weights from 
-0.158080865178071 to -0.159800101652429
-0.366913665566407 to -0.368632902040765
-0.567391265664063 to -0.569110502138421
-0.280456532273255 to -0.282175768747613
-0.519709337983094 to -0.521428574457453
-0.288244117531739 to -0.289963354006097
-0.292602647576295 to -0.294321884050653
-0.754500169787369 to -0.756219406261727
-0.160804082188569 to -0.162523318662928
-0.593106020722352 to -0.59482525719671
Changing layer 5's weights from 
-0.00530104759689437 to -0.00702028407125256
-0.447374571595155 to -0.449093808069513
-0.0221862089967353 to -0.0239054454710935
-0.957698390368186 to -0.959417626842545
-0.420429576668702 to -0.42214881314306
-0.818897519741021 to -0.820616756215379
-0.518932212624512 to -0.520651449098871
-0.446485985550843 to -0.448205222025201
-0.0568839800691233 to -0.0586032165434815
-0.0633920920228593 to -0.0651113284972175
Trying to learn from memory 94, 1, -0.2
sum 0.0379741196205569 distri 0.0216399753584521
Using diff 0.00684061435696564 and condRate 0.166666666666667
Changed category 1 weights from 
0.286818243046932 to 0.286590222564969
0.520133830090694 to 0.519905809608731
0.0952085921966364 to 0.0949805717146731
0.0993014047347833 to 0.09907338425282
Changing layer 0's weights from 
-0.252199758425042 to -0.252427778907006
-0.624126215114877 to -0.62435423559684
-0.858475480809495 to -0.858703501291458
-0.823916230931565 to -0.824144251413528
-0.0760486616042114 to -0.0762766820861747
-0.758690614834115 to -0.758918635316078
-0.604860891237542 to -0.605088911719505
-0.632315297260568 to -0.632543317742531
-0.869807158246324 to -0.870035178728287
-0.91355721338062 to -0.913785233862583
Changing layer 1's weights from 
-0.908460300996587 to -0.90868832147855
-0.307291616335199 to -0.307519636817163
-0.432994355097101 to -0.433222375579065
-0.442899693384454 to -0.443127713866418
-0.627952475681588 to -0.628180496163551
-0.766190309658334 to -0.766418330140297
-0.495471586122796 to -0.49569960660476
-0.881274615063951 to -0.881502635545914
-0.469611634149835 to -0.469839654631799
-0.832258497014329 to -0.832486517496292
Changing layer 2's weights from 
-0.0230702651885015 to -0.0232982856704648
-0.577123750582025 to -0.577351771063988
-0.292534459963128 to -0.292762480445091
-0.121334363355921 to -0.121562383837884
-0.29858707322149 to -0.298815093703453
-0.50459669961004 to -0.504824720092004
0.0142160163971923 to 0.013987995915229
-0.0582060111906985 to -0.0584340316726618
-0.790204305782601 to -0.790432326264565
-0.745246787205026 to -0.745474807686989
Changing layer 3's weights from 
-0.0561205638792975 to -0.0563485843612608
-0.319673646822259 to -0.319901667304223
-0.0770637287047365 to -0.0772917491866998
-0.98450296046088 to -0.984730980942843
-0.199739624395654 to -0.199967644877618
-0.329988230600641 to -0.330216251082605
-0.66017307771711 to -0.660401098199074
-0.201660205259607 to -0.201888225741571
-0.533356179132746 to -0.533584199614709
-0.0605263008025155 to -0.0607543212844788
Changing layer 4's weights from 
-0.159800101652429 to -0.160028122134393
-0.368632902040765 to -0.368860922522729
-0.569110502138421 to -0.569338522620384
-0.282175768747613 to -0.282403789229576
-0.521428574457453 to -0.521656594939416
-0.289963354006097 to -0.29019137448806
-0.294321884050653 to -0.294549904532616
-0.756219406261727 to -0.756447426743691
-0.162523318662928 to -0.162751339144891
-0.59482525719671 to -0.595053277678673
Changing layer 5's weights from 
-0.00702028407125256 to -0.00724830455321585
-0.449093808069513 to -0.449321828551477
-0.0239054454710935 to -0.0241334659530568
-0.959417626842545 to -0.959645647324508
-0.42214881314306 to -0.422376833625024
-0.820616756215379 to -0.820844776697342
-0.520651449098871 to -0.520879469580834
-0.448205222025201 to -0.448433242507165
-0.0586032165434815 to -0.0588312370254448
-0.0651113284972175 to -0.0653393489791808
Trying to learn from memory 91, 0, -0.2
sum 0.0379741785216576 distri -0.0230963272711135
Using diff 0.0515769611623567 and condRate 0.166666666666667
Changed category 0 weights from 
0.125488265712382 to 0.123769033648018
-0.274546698372243 to -0.276265930436607
-0.557067394893049 to -0.558786626957413
-0.405562028925298 to -0.407281260989662
Changing layer 0's weights from 
-0.252427778907006 to -0.254147010971369
-0.62435423559684 to -0.626073467661204
-0.858703501291458 to -0.860422733355822
-0.824144251413528 to -0.825863483477892
-0.0762766820861747 to -0.0779959141505385
-0.758918635316078 to -0.760637867380442
-0.605088911719505 to -0.606808143783869
-0.632543317742531 to -0.634262549806895
-0.870035178728287 to -0.871754410792651
-0.913785233862583 to -0.915504465926947
Changing layer 1's weights from 
-0.90868832147855 to -0.910407553542914
-0.307519636817163 to -0.309238868881526
-0.433222375579065 to -0.434941607643428
-0.443127713866418 to -0.444846945930781
-0.628180496163551 to -0.629899728227915
-0.766418330140297 to -0.768137562204661
-0.49569960660476 to -0.497418838669123
-0.881502635545914 to -0.883221867610278
-0.469839654631799 to -0.471558886696162
-0.832486517496292 to -0.834205749560656
Changing layer 2's weights from 
-0.0232982856704648 to -0.0250175177348286
-0.577351771063988 to -0.579071003128352
-0.292762480445091 to -0.294481712509455
-0.121562383837884 to -0.123281615902248
-0.298815093703453 to -0.300534325767817
-0.504824720092004 to -0.506543952156367
0.013987995915229 to 0.0122687638508653
-0.0584340316726618 to -0.0601532637370256
-0.790432326264565 to -0.792151558328928
-0.745474807686989 to -0.747194039751353
Changing layer 3's weights from 
-0.0563485843612608 to -0.0580678164256246
-0.319901667304223 to -0.321620899368586
-0.0772917491866998 to -0.0790109812510636
-0.984730980942843 to -0.986450213007207
-0.199967644877618 to -0.201686876941982
-0.330216251082605 to -0.331935483146968
-0.660401098199074 to -0.662120330263437
-0.201888225741571 to -0.203607457805934
-0.533584199614709 to -0.535303431679073
-0.0607543212844788 to -0.0624735533488426
Changing layer 4's weights from 
-0.160028122134393 to -0.161747354198756
-0.368860922522729 to -0.370580154587092
-0.569338522620384 to -0.571057754684748
-0.282403789229576 to -0.28412302129394
-0.521656594939416 to -0.52337582700378
-0.29019137448806 to -0.291910606552424
-0.294549904532616 to -0.29626913659698
-0.756447426743691 to -0.758166658808054
-0.162751339144891 to -0.164470571209255
-0.595053277678673 to -0.596772509743037
Changing layer 5's weights from 
-0.00724830455321585 to -0.00896753661757963
-0.449321828551477 to -0.45104106061584
-0.0241334659530568 to -0.0258526980174206
-0.959645647324508 to -0.961364879388872
-0.422376833625024 to -0.424096065689387
-0.820844776697342 to -0.822564008761706
-0.520879469580834 to -0.522598701645198
-0.448433242507165 to -0.450152474571528
-0.0588312370254448 to -0.0605504690898086
-0.0653393489791808 to -0.0670585810435446
Trying to learn from memory 91, 0, -0.2
sum 0.0379741785216576 distri -0.0230963272711135
Using diff 0.0515769611623567 and condRate 0.166666666666667
Changed category 0 weights from 
0.123769033648018 to 0.122049801583654
-0.276265930436607 to -0.277985162500971
-0.558786626957413 to -0.560505859021777
-0.407281260989662 to -0.409000493054026
Changing layer 0's weights from 
-0.254147010971369 to -0.255866243035733
-0.626073467661204 to -0.627792699725567
-0.860422733355822 to -0.862141965420186
-0.825863483477892 to -0.827582715542255
-0.0779959141505385 to -0.0797151462149023
-0.760637867380442 to -0.762357099444806
-0.606808143783869 to -0.608527375848233
-0.634262549806895 to -0.635981781871258
-0.871754410792651 to -0.873473642857014
-0.915504465926947 to -0.91722369799131
Changing layer 1's weights from 
-0.910407553542914 to -0.912126785607277
-0.309238868881526 to -0.31095810094589
-0.434941607643428 to -0.436660839707792
-0.444846945930781 to -0.446566177995145
-0.629899728227915 to -0.631618960292279
-0.768137562204661 to -0.769856794269025
-0.497418838669123 to -0.499138070733487
-0.883221867610278 to -0.884941099674642
-0.471558886696162 to -0.473278118760526
-0.834205749560656 to -0.835924981625019
Changing layer 2's weights from 
-0.0250175177348286 to -0.0267367497991924
-0.579071003128352 to -0.580790235192716
-0.294481712509455 to -0.296200944573819
-0.123281615902248 to -0.125000847966611
-0.300534325767817 to -0.302253557832181
-0.506543952156367 to -0.508263184220731
0.0122687638508653 to 0.0105495317865015
-0.0601532637370256 to -0.0618724958013894
-0.792151558328928 to -0.793870790393292
-0.747194039751353 to -0.748913271815717
Changing layer 3's weights from 
-0.0580678164256246 to -0.0597870484899884
-0.321620899368586 to -0.32334013143295
-0.0790109812510636 to -0.0807302133154273
-0.986450213007207 to -0.98816944507157
-0.201686876941982 to -0.203406109006345
-0.331935483146968 to -0.333654715211332
-0.662120330263437 to -0.663839562327801
-0.203607457805934 to -0.205326689870298
-0.535303431679073 to -0.537022663743436
-0.0624735533488426 to -0.0641927854132063
Changing layer 4's weights from 
-0.161747354198756 to -0.16346658626312
-0.370580154587092 to -0.372299386651456
-0.571057754684748 to -0.572776986749112
-0.28412302129394 to -0.285842253358304
-0.52337582700378 to -0.525095059068143
-0.291910606552424 to -0.293629838616788
-0.29626913659698 to -0.297988368661344
-0.758166658808054 to -0.759885890872418
-0.164470571209255 to -0.166189803273618
-0.596772509743037 to -0.598491741807401
Changing layer 5's weights from 
-0.00896753661757963 to -0.0106867686819434
-0.45104106061584 to -0.452760292680204
-0.0258526980174206 to -0.0275719300817843
-0.961364879388872 to -0.963084111453235
-0.424096065689387 to -0.425815297753751
-0.822564008761706 to -0.82428324082607
-0.522598701645198 to -0.524317933709561
-0.450152474571528 to -0.451871706635892
-0.0605504690898086 to -0.0622697011541724
-0.0670585810435446 to -0.0687778131079084
Trying to learn from memory 91, 1, -0.2
sum 0.0379741785216576 distri 0.0216399287059932
Using diff 0.00684070518525006 and condRate 0.166666666666667
Changed category 1 weights from 
0.286590222564969 to 0.286362199055396
0.519905809608731 to 0.519677786099158
0.0949805717146731 to 0.0947525482051002
0.09907338425282 to 0.0988453607432472
Changing layer 0's weights from 
-0.255866243035733 to -0.256094266545306
-0.627792699725567 to -0.62802072323514
-0.862141965420186 to -0.862369988929758
-0.827582715542255 to -0.827810739051828
-0.0797151462149023 to -0.0799431697244751
-0.762357099444806 to -0.762585122954379
-0.608527375848233 to -0.608755399357805
-0.635981781871258 to -0.636209805380831
-0.873473642857014 to -0.873701666366587
-0.91722369799131 to -0.917451721500883
Changing layer 1's weights from 
-0.912126785607277 to -0.91235480911685
-0.31095810094589 to -0.311186124455463
-0.436660839707792 to -0.436888863217365
-0.446566177995145 to -0.446794201504718
-0.631618960292279 to -0.631846983801852
-0.769856794269025 to -0.770084817778598
-0.499138070733487 to -0.49936609424306
-0.884941099674642 to -0.885169123184214
-0.473278118760526 to -0.473506142270099
-0.835924981625019 to -0.836153005134592
Changing layer 2's weights from 
-0.0267367497991924 to -0.0269647733087652
-0.580790235192716 to -0.581018258702288
-0.296200944573819 to -0.296428968083392
-0.125000847966611 to -0.125228871476184
-0.302253557832181 to -0.302481581341754
-0.508263184220731 to -0.508491207730304
0.0105495317865015 to 0.0103215082769287
-0.0618724958013894 to -0.0621005193109622
-0.793870790393292 to -0.794098813902865
-0.748913271815717 to -0.749141295325289
Changing layer 3's weights from 
-0.0597870484899884 to -0.0600150719995612
-0.32334013143295 to -0.323568154942523
-0.0807302133154273 to -0.0809582368250002
-0.98816944507157 to -0.988397468581143
-0.203406109006345 to -0.203634132515918
-0.333654715211332 to -0.333882738720905
-0.663839562327801 to -0.664067585837374
-0.205326689870298 to -0.205554713379871
-0.537022663743436 to -0.537250687253009
-0.0641927854132063 to -0.0644208089227792
Changing layer 4's weights from 
-0.16346658626312 to -0.163694609772693
-0.372299386651456 to -0.372527410161029
-0.572776986749112 to -0.573005010258684
-0.285842253358304 to -0.286070276867877
-0.525095059068143 to -0.525323082577716
-0.293629838616788 to -0.293857862126361
-0.297988368661344 to -0.298216392170917
-0.759885890872418 to -0.760113914381991
-0.166189803273618 to -0.166417826783191
-0.598491741807401 to -0.598719765316973
Changing layer 5's weights from 
-0.0106867686819434 to -0.0109147921915162
-0.452760292680204 to -0.452988316189777
-0.0275719300817843 to -0.0277999535913572
-0.963084111453235 to -0.963312134962808
-0.425815297753751 to -0.426043321263324
-0.82428324082607 to -0.824511264335642
-0.524317933709561 to -0.524545957219134
-0.451871706635892 to -0.452099730145465
-0.0622697011541724 to -0.0624977246637452
-0.0687778131079084 to -0.0690058366174812
Trying to learn from memory 92, 1, -0.2
sum 0.0379741785216576 distri 0.0216399287059932
Using diff 0.00684070518525006 and condRate 0.166666666666667
Changed category 1 weights from 
0.286362199055396 to 0.286134175545823
0.519677786099158 to 0.519449762589586
0.0947525482051002 to 0.0945245246955274
0.0988453607432472 to 0.0986173372336744
Changing layer 0's weights from 
-0.256094266545306 to -0.256322290054879
-0.62802072323514 to -0.628248746744713
-0.862369988929758 to -0.862598012439331
-0.827810739051828 to -0.828038762561401
-0.0799431697244751 to -0.0801711932340479
-0.762585122954379 to -0.762813146463952
-0.608755399357805 to -0.608983422867378
-0.636209805380831 to -0.636437828890404
-0.873701666366587 to -0.87392968987616
-0.917451721500883 to -0.917679745010456
Changing layer 1's weights from 
-0.91235480911685 to -0.912582832626423
-0.311186124455463 to -0.311414147965036
-0.436888863217365 to -0.437116886726938
-0.446794201504718 to -0.447022225014291
-0.631846983801852 to -0.632075007311425
-0.770084817778598 to -0.77031284128817
-0.49936609424306 to -0.499594117752633
-0.885169123184214 to -0.885397146693787
-0.473506142270099 to -0.473734165779672
-0.836153005134592 to -0.836381028644165
Changing layer 2's weights from 
-0.0269647733087652 to -0.027192796818338
-0.581018258702288 to -0.581246282211861
-0.296428968083392 to -0.296656991592964
-0.125228871476184 to -0.125456894985757
-0.302481581341754 to -0.302709604851327
-0.508491207730304 to -0.508719231239877
0.0103215082769287 to 0.0100934847673559
-0.0621005193109622 to -0.062328542820535
-0.794098813902865 to -0.794326837412438
-0.749141295325289 to -0.749369318834862
Changing layer 3's weights from 
-0.0600150719995612 to -0.060243095509134
-0.323568154942523 to -0.323796178452096
-0.0809582368250002 to -0.081186260334573
-0.988397468581143 to -0.988625492090716
-0.203634132515918 to -0.203862156025491
-0.333882738720905 to -0.334110762230478
-0.664067585837374 to -0.664295609346947
-0.205554713379871 to -0.205782736889444
-0.537250687253009 to -0.537478710762582
-0.0644208089227792 to -0.064648832432352
Changing layer 4's weights from 
-0.163694609772693 to -0.163922633282266
-0.372527410161029 to -0.372755433670602
-0.573005010258684 to -0.573233033768257
-0.286070276867877 to -0.286298300377449
-0.525323082577716 to -0.525551106087289
-0.293857862126361 to -0.294085885635933
-0.298216392170917 to -0.298444415680489
-0.760113914381991 to -0.760341937891564
-0.166417826783191 to -0.166645850292764
-0.598719765316973 to -0.598947788826546
Changing layer 5's weights from 
-0.0109147921915162 to -0.011142815701089
-0.452988316189777 to -0.45321633969935
-0.0277999535913572 to -0.02802797710093
-0.963312134962808 to -0.963540158472381
-0.426043321263324 to -0.426271344772897
-0.824511264335642 to -0.824739287845215
-0.524545957219134 to -0.524773980728707
-0.452099730145465 to -0.452327753655038
-0.0624977246637452 to -0.062725748173318
-0.0690058366174812 to -0.069233860127054
Trying to learn from memory 95, 0, -0.2
sum 0.0379738527058933 distri -0.0230970312613137
Using diff 0.0515774207907337 and condRate 0.166666666666667
Changed category 0 weights from 
0.122049801583654 to 0.120330554198344
-0.277985162500971 to -0.27970440988628
-0.560505859021777 to -0.562225106407087
-0.409000493054026 to -0.410719740439336
Changing layer 0's weights from 
-0.256322290054879 to -0.258041537440189
-0.628248746744713 to -0.629967994130023
-0.862598012439331 to -0.864317259824641
-0.828038762561401 to -0.829758009946711
-0.0801711932340479 to -0.0818904406193578
-0.762813146463952 to -0.764532393849262
-0.608983422867378 to -0.610702670252688
-0.636437828890404 to -0.638157076275714
-0.87392968987616 to -0.87564893726147
-0.917679745010456 to -0.919398992395766
Changing layer 1's weights from 
-0.912582832626423 to -0.914302080011733
-0.311414147965036 to -0.313133395350346
-0.437116886726938 to -0.438836134112248
-0.447022225014291 to -0.448741472399601
-0.632075007311425 to -0.633794254696734
-0.77031284128817 to -0.77203208867348
-0.499594117752633 to -0.501313365137943
-0.885397146693787 to -0.887116394079097
-0.473734165779672 to -0.475453413164982
-0.836381028644165 to -0.838100276029475
Changing layer 2's weights from 
-0.027192796818338 to -0.0289120442036479
-0.581246282211861 to -0.582965529597171
-0.296656991592964 to -0.298376238978274
-0.125456894985757 to -0.127176142371067
-0.302709604851327 to -0.304428852236636
-0.508719231239877 to -0.510438478625187
0.0100934847673559 to 0.00837423738204595
-0.062328542820535 to -0.0640477902058449
-0.794326837412438 to -0.796046084797748
-0.749369318834862 to -0.751088566220172
Changing layer 3's weights from 
-0.060243095509134 to -0.0619623428944439
-0.323796178452096 to -0.325515425837406
-0.081186260334573 to -0.0829055077198829
-0.988625492090716 to -0.990344739476026
-0.203862156025491 to -0.205581403410801
-0.334110762230478 to -0.335830009615788
-0.664295609346947 to -0.666014856732257
-0.205782736889444 to -0.207501984274754
-0.537478710762582 to -0.539197958147892
-0.064648832432352 to -0.0663680798176619
Changing layer 4's weights from 
-0.163922633282266 to -0.165641880667576
-0.372755433670602 to -0.374474681055912
-0.573233033768257 to -0.574952281153567
-0.286298300377449 to -0.288017547762759
-0.525551106087289 to -0.527270353472599
-0.294085885635933 to -0.295805133021243
-0.298444415680489 to -0.300163663065799
-0.760341937891564 to -0.762061185276874
-0.166645850292764 to -0.168365097678074
-0.598947788826546 to -0.600667036211856
Changing layer 5's weights from 
-0.011142815701089 to -0.0128620630863989
-0.45321633969935 to -0.45493558708466
-0.02802797710093 to -0.0297472244862399
-0.963540158472381 to -0.965259405857691
-0.426271344772897 to -0.427990592158207
-0.824739287845215 to -0.826458535230525
-0.524773980728707 to -0.526493228114017
-0.452327753655038 to -0.454047001040348
-0.062725748173318 to -0.0644449955586279
-0.069233860127054 to -0.0709531075123639
Trying to learn from memory 96, 1, -0.2
sum 0.0379832345209898 distri 0.021646063153054
Using diff 0.0068413627376883 and condRate 0.166666666666667
Changed category 1 weights from 
0.286134175545823 to 0.285906130117836
0.519449762589586 to 0.519221717161598
0.0945245246955274 to 0.0942964792675397
0.0986173372336744 to 0.0983892918056866
Changing layer 0's weights from 
-0.258041537440189 to -0.258269582868176
-0.629967994130023 to -0.630196039558011
-0.864317259824641 to -0.864545305252629
-0.829758009946711 to -0.829986055374699
-0.0818904406193578 to -0.0821184860473456
-0.764532393849262 to -0.764760439277249
-0.610702670252688 to -0.610930715680676
-0.638157076275714 to -0.638385121703702
-0.87564893726147 to -0.875876982689458
-0.919398992395766 to -0.919627037823754
Changing layer 1's weights from 
-0.914302080011733 to -0.914530125439721
-0.313133395350346 to -0.313361440778333
-0.438836134112248 to -0.439064179540235
-0.448741472399601 to -0.448969517827588
-0.633794254696734 to -0.634022300124722
-0.77203208867348 to -0.772260134101468
-0.501313365137943 to -0.50154141056593
-0.887116394079097 to -0.887344439507085
-0.475453413164982 to -0.475681458592969
-0.838100276029475 to -0.838328321457463
Changing layer 2's weights from 
-0.0289120442036479 to -0.0291400896316357
-0.582965529597171 to -0.583193575025159
-0.298376238978274 to -0.298604284406262
-0.127176142371067 to -0.127404187799055
-0.304428852236636 to -0.304656897664624
-0.510438478625187 to -0.510666524053174
0.00837423738204595 to 0.00814619195405819
-0.0640477902058449 to -0.0642758356338327
-0.796046084797748 to -0.796274130225735
-0.751088566220172 to -0.75131661164816
Changing layer 3's weights from 
-0.0619623428944439 to -0.0621903883224316
-0.325515425837406 to -0.325743471265393
-0.0829055077198829 to -0.0831335531478706
-0.990344739476026 to -0.990572784904014
-0.205581403410801 to -0.205809448838789
-0.335830009615788 to -0.336058055043775
-0.666014856732257 to -0.666242902160244
-0.207501984274754 to -0.207730029702741
-0.539197958147892 to -0.53942600357588
-0.0663680798176619 to -0.0665961252456496
Changing layer 4's weights from 
-0.165641880667576 to -0.165869926095564
-0.374474681055912 to -0.3747027264839
-0.574952281153567 to -0.575180326581555
-0.288017547762759 to -0.288245593190747
-0.527270353472599 to -0.527498398900587
-0.295805133021243 to -0.296033178449231
-0.300163663065799 to -0.300391708493787
-0.762061185276874 to -0.762289230704862
-0.168365097678074 to -0.168593143106062
-0.600667036211856 to -0.600895081639844
Changing layer 5's weights from 
-0.0128620630863989 to -0.0130901085143867
-0.45493558708466 to -0.455163632512647
-0.0297472244862399 to -0.0299752699142276
-0.965259405857691 to -0.965487451285679
-0.427990592158207 to -0.428218637586194
-0.826458535230525 to -0.826686580658513
-0.526493228114017 to -0.526721273542005
-0.454047001040348 to -0.454275046468335
-0.0644449955586279 to -0.0646730409866157
-0.0709531075123639 to -0.0711811529403517
10/5/2016 1:35:54 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 97, 1, -0.2
sum 0.0379749885680422 distri 0.0216404634166433
Using diff 0.00684077800938834 and condRate 0.166666666666667
Changed category 1 weights from 
0.285906130117836 to 0.285678104180792
0.519221717161598 to 0.518993691224554
0.0942964792675397 to 0.0940684533304955
0.0983892918056866 to 0.0981612658686425
Changing layer 0's weights from 
-0.258269582868176 to -0.25849760880522
-0.630196039558011 to -0.630424065495055
-0.864545305252629 to -0.864773331189673
-0.829986055374699 to -0.830214081311743
-0.0821184860473456 to -0.0823465119843897
-0.764760439277249 to -0.764988465214293
-0.610930715680676 to -0.61115874161772
-0.638385121703702 to -0.638613147640746
-0.875876982689458 to -0.876105008626502
-0.919627037823754 to -0.919855063760798
Changing layer 1's weights from 
-0.914530125439721 to -0.914758151376765
-0.313361440778333 to -0.313589466715377
-0.439064179540235 to -0.43929220547728
-0.448969517827588 to -0.449197543764633
-0.634022300124722 to -0.634250326061766
-0.772260134101468 to -0.772488160038512
-0.50154141056593 to -0.501769436502974
-0.887344439507085 to -0.887572465444129
-0.475681458592969 to -0.475909484530013
-0.838328321457463 to -0.838556347394507
Changing layer 2's weights from 
-0.0291400896316357 to -0.0293681155686798
-0.583193575025159 to -0.583421600962203
-0.298604284406262 to -0.298832310343306
-0.127404187799055 to -0.127632213736099
-0.304656897664624 to -0.304884923601668
-0.510666524053174 to -0.510894549990219
0.00814619195405819 to 0.00791816601701406
-0.0642758356338327 to -0.0645038615708768
-0.796274130225735 to -0.796502156162779
-0.75131661164816 to -0.751544637585204
Changing layer 3's weights from 
-0.0621903883224316 to -0.0624184142594758
-0.325743471265393 to -0.325971497202437
-0.0831335531478706 to -0.0833615790849148
-0.990572784904014 to -0.990800810841058
-0.205809448838789 to -0.206037474775833
-0.336058055043775 to -0.33628608098082
-0.666242902160244 to -0.666470928097288
-0.207730029702741 to -0.207958055639786
-0.53942600357588 to -0.539654029512924
-0.0665961252456496 to -0.0668241511826938
Changing layer 4's weights from 
-0.165869926095564 to -0.166097952032608
-0.3747027264839 to -0.374930752420944
-0.575180326581555 to -0.575408352518599
-0.288245593190747 to -0.288473619127791
-0.527498398900587 to -0.527726424837631
-0.296033178449231 to -0.296261204386275
-0.300391708493787 to -0.300619734430831
-0.762289230704862 to -0.762517256641906
-0.168593143106062 to -0.168821169043106
-0.600895081639844 to -0.601123107576888
Changing layer 5's weights from 
-0.0130901085143867 to -0.0133181344514308
-0.455163632512647 to -0.455391658449692
-0.0299752699142276 to -0.0302032958512718
-0.965487451285679 to -0.965715477222723
-0.428218637586194 to -0.428446663523239
-0.826686580658513 to -0.826914606595557
-0.526721273542005 to -0.526949299479049
-0.454275046468335 to -0.45450307240538
-0.0646730409866157 to -0.0649010669236598
-0.0711811529403517 to -0.0714091788773958
Trying to learn from memory 94, 0, -0.2
sum 0.0379741196205569 distri -0.0230965037467695
Using diff 0.0515770934621872 and condRate 0.166666666666667
Changed category 0 weights from 
0.120330554198344 to 0.118611317723986
-0.27970440988628 to -0.281423646360639
-0.562225106407087 to -0.563944342881445
-0.410719740439336 to -0.412438976913694
Changing layer 0's weights from 
-0.25849760880522 to -0.260216845279579
-0.630424065495055 to -0.632143301969413
-0.864773331189673 to -0.866492567664031
-0.830214081311743 to -0.831933317786101
-0.0823465119843897 to -0.0840657484587479
-0.764988465214293 to -0.766707701688652
-0.61115874161772 to -0.612877978092078
-0.638613147640746 to -0.640332384115104
-0.876105008626502 to -0.87782424510086
-0.919855063760798 to -0.921574300235156
Changing layer 1's weights from 
-0.914758151376765 to -0.916477387851123
-0.313589466715377 to -0.315308703189736
-0.43929220547728 to -0.441011441951638
-0.449197543764633 to -0.450916780238991
-0.634250326061766 to -0.635969562536124
-0.772488160038512 to -0.77420739651287
-0.501769436502974 to -0.503488672977333
-0.887572465444129 to -0.889291701918487
-0.475909484530013 to -0.477628721004372
-0.838556347394507 to -0.840275583868865
Changing layer 2's weights from 
-0.0293681155686798 to -0.031087352043038
-0.583421600962203 to -0.585140837436561
-0.298832310343306 to -0.300551546817664
-0.127632213736099 to -0.129351450210457
-0.304884923601668 to -0.306604160076027
-0.510894549990219 to -0.512613786464577
0.00791816601701406 to 0.00619892954265587
-0.0645038615708768 to -0.066223098045235
-0.796502156162779 to -0.798221392637138
-0.751544637585204 to -0.753263874059562
Changing layer 3's weights from 
-0.0624184142594758 to -0.064137650733834
-0.325971497202437 to -0.327690733676796
-0.0833615790849148 to -0.085080815559273
-0.990800810841058 to -0.992520047315416
-0.206037474775833 to -0.207756711250191
-0.33628608098082 to -0.338005317455178
-0.666470928097288 to -0.668190164571647
-0.207958055639786 to -0.209677292114144
-0.539654029512924 to -0.541373265987282
-0.0668241511826938 to -0.068543387657052
Changing layer 4's weights from 
-0.166097952032608 to -0.167817188506966
-0.374930752420944 to -0.376649988895302
-0.575408352518599 to -0.577127588992957
-0.288473619127791 to -0.290192855602149
-0.527726424837631 to -0.529445661311989
-0.296261204386275 to -0.297980440860633
-0.300619734430831 to -0.302338970905189
-0.762517256641906 to -0.764236493116264
-0.168821169043106 to -0.170540405517464
-0.601123107576888 to -0.602842344051246
Changing layer 5's weights from 
-0.0133181344514308 to -0.015037370925789
-0.455391658449692 to -0.45711089492405
-0.0302032958512718 to -0.03192253232563
-0.965715477222723 to -0.967434713697081
-0.428446663523239 to -0.430165899997597
-0.826914606595557 to -0.828633843069915
-0.526949299479049 to -0.528668535953407
-0.45450307240538 to -0.456222308879738
-0.0649010669236598 to -0.066620303398018
-0.0714091788773958 to -0.073128415351754
Trying to learn from memory 91, 1, -0.2
sum 0.0379741785216576 distri 0.0216399287059932
Using diff 0.00684070518525006 and condRate 0.166666666666667
Changed category 1 weights from 
0.285678104180792 to 0.285450080671219
0.518993691224554 to 0.518765667714981
0.0940684533304955 to 0.0938404298209227
0.0981612658686425 to 0.0979332423590697
Changing layer 0's weights from 
-0.260216845279579 to -0.260444868789151
-0.632143301969413 to -0.632371325478986
-0.866492567664031 to -0.866720591173604
-0.831933317786101 to -0.832161341295674
-0.0840657484587479 to -0.0842937719683207
-0.766707701688652 to -0.766935725198224
-0.612877978092078 to -0.613106001601651
-0.640332384115104 to -0.640560407624677
-0.87782424510086 to -0.878052268610433
-0.921574300235156 to -0.921802323744729
Changing layer 1's weights from 
-0.916477387851123 to -0.916705411360696
-0.315308703189736 to -0.315536726699308
-0.441011441951638 to -0.441239465461211
-0.450916780238991 to -0.451144803748564
-0.635969562536124 to -0.636197586045697
-0.77420739651287 to -0.774435420022443
-0.503488672977333 to -0.503716696486905
-0.889291701918487 to -0.88951972542806
-0.477628721004372 to -0.477856744513944
-0.840275583868865 to -0.840503607378438
Changing layer 2's weights from 
-0.031087352043038 to -0.0313153755526108
-0.585140837436561 to -0.585368860946134
-0.300551546817664 to -0.300779570327237
-0.129351450210457 to -0.12957947372003
-0.306604160076027 to -0.306832183585599
-0.512613786464577 to -0.51284180997415
0.00619892954265587 to 0.00597090603308305
-0.066223098045235 to -0.0664511215548078
-0.798221392637138 to -0.798449416146711
-0.753263874059562 to -0.753491897569135
Changing layer 3's weights from 
-0.064137650733834 to -0.0643656742434068
-0.327690733676796 to -0.327918757186368
-0.085080815559273 to -0.0853088390688458
-0.992520047315416 to -0.992748070824989
-0.207756711250191 to -0.207984734759764
-0.338005317455178 to -0.338233340964751
-0.668190164571647 to -0.66841818808122
-0.209677292114144 to -0.209905315623717
-0.541373265987282 to -0.541601289496855
-0.068543387657052 to -0.0687714111666248
Changing layer 4's weights from 
-0.167817188506966 to -0.168045212016539
-0.376649988895302 to -0.376878012404875
-0.577127588992957 to -0.57735561250253
-0.290192855602149 to -0.290420879111722
-0.529445661311989 to -0.529673684821562
-0.297980440860633 to -0.298208464370206
-0.302338970905189 to -0.302566994414762
-0.764236493116264 to -0.764464516625837
-0.170540405517464 to -0.170768429027037
-0.602842344051246 to -0.603070367560819
Changing layer 5's weights from 
-0.015037370925789 to -0.0152653944353618
-0.45711089492405 to -0.457338918433623
-0.03192253232563 to -0.0321505558352028
-0.967434713697081 to -0.967662737206654
-0.430165899997597 to -0.43039392350717
-0.828633843069915 to -0.828861866579488
-0.528668535953407 to -0.52889655946298
-0.456222308879738 to -0.456450332389311
-0.066620303398018 to -0.0668483269075908
-0.073128415351754 to -0.0733564388613268
Trying to learn from memory 92, 0, -0.2
sum 0.0379741785216576 distri -0.0230963272711135
Using diff 0.0515769611623567 and condRate 0.166666666666667
Changed category 0 weights from 
0.118611317723986 to 0.116892085659622
-0.281423646360639 to -0.283142878425002
-0.563944342881445 to -0.565663574945809
-0.412438976913694 to -0.414158208978058
Changing layer 0's weights from 
-0.260444868789151 to -0.262164100853515
-0.632371325478986 to -0.63409055754335
-0.866720591173604 to -0.868439823237968
-0.832161341295674 to -0.833880573360038
-0.0842937719683207 to -0.0860130040326845
-0.766935725198224 to -0.768654957262588
-0.613106001601651 to -0.614825233666015
-0.640560407624677 to -0.642279639689041
-0.878052268610433 to -0.879771500674797
-0.921802323744729 to -0.923521555809093
Changing layer 1's weights from 
-0.916705411360696 to -0.91842464342506
-0.315536726699308 to -0.317255958763672
-0.441239465461211 to -0.442958697525574
-0.451144803748564 to -0.452864035812927
-0.636197586045697 to -0.637916818110061
-0.774435420022443 to -0.776154652086807
-0.503716696486905 to -0.505435928551269
-0.88951972542806 to -0.891238957492424
-0.477856744513944 to -0.479575976578308
-0.840503607378438 to -0.842222839442802
Changing layer 2's weights from 
-0.0313153755526108 to -0.0330346076169746
-0.585368860946134 to -0.587088093010498
-0.300779570327237 to -0.302498802391601
-0.12957947372003 to -0.131298705784394
-0.306832183585599 to -0.308551415649963
-0.51284180997415 to -0.514561042038513
0.00597090603308305 to 0.00425167396871928
-0.0664511215548078 to -0.0681703536191716
-0.798449416146711 to -0.800168648211074
-0.753491897569135 to -0.755211129633499
Changing layer 3's weights from 
-0.0643656742434068 to -0.0660849063077706
-0.327918757186368 to -0.329637989250732
-0.0853088390688458 to -0.0870280711332096
-0.992748070824989 to -0.994467302889353
-0.207984734759764 to -0.209703966824128
-0.338233340964751 to -0.339952573029114
-0.66841818808122 to -0.670137420145583
-0.209905315623717 to -0.21162454768808
-0.541601289496855 to -0.543320521561219
-0.0687714111666248 to -0.0704906432309886
Changing layer 4's weights from 
-0.168045212016539 to -0.169764444080902
-0.376878012404875 to -0.378597244469238
-0.57735561250253 to -0.579074844566894
-0.290420879111722 to -0.292140111176086
-0.529673684821562 to -0.531392916885926
-0.298208464370206 to -0.29992769643457
-0.302566994414762 to -0.304286226479126
-0.764464516625837 to -0.7661837486902
-0.170768429027037 to -0.172487661091401
-0.603070367560819 to -0.604789599625183
Changing layer 5's weights from 
-0.0152653944353618 to -0.0169846264997256
-0.457338918433623 to -0.459058150497986
-0.0321505558352028 to -0.0338697878995666
-0.967662737206654 to -0.969381969271018
-0.43039392350717 to -0.432113155571533
-0.828861866579488 to -0.830581098643852
-0.52889655946298 to -0.530615791527344
-0.456450332389311 to -0.458169564453674
-0.0668483269075908 to -0.0685675589719546
-0.0733564388613268 to -0.0750756709256906
Trying to learn from memory 92, 0, -0.2
sum 0.0379741785216576 distri -0.0230963272711135
Using diff 0.0515769611623567 and condRate 0.166666666666667
Changed category 0 weights from 
0.116892085659622 to 0.115172853595259
-0.283142878425002 to -0.284862110489366
-0.565663574945809 to -0.567382807010172
-0.414158208978058 to -0.415877441042421
Changing layer 0's weights from 
-0.262164100853515 to -0.263883332917879
-0.63409055754335 to -0.635809789607713
-0.868439823237968 to -0.870159055302332
-0.833880573360038 to -0.835599805424401
-0.0860130040326845 to -0.0877322360970483
-0.768654957262588 to -0.770374189326952
-0.614825233666015 to -0.616544465730379
-0.642279639689041 to -0.643998871753404
-0.879771500674797 to -0.88149073273916
-0.923521555809093 to -0.925240787873456
Changing layer 1's weights from 
-0.91842464342506 to -0.920143875489423
-0.317255958763672 to -0.318975190828036
-0.442958697525574 to -0.444677929589938
-0.452864035812927 to -0.454583267877291
-0.637916818110061 to -0.639636050174425
-0.776154652086807 to -0.777873884151171
-0.505435928551269 to -0.507155160615633
-0.891238957492424 to -0.892958189556788
-0.479575976578308 to -0.481295208642672
-0.842222839442802 to -0.843942071507165
Changing layer 2's weights from 
-0.0330346076169746 to -0.0347538396813384
-0.587088093010498 to -0.588807325074862
-0.302498802391601 to -0.304218034455965
-0.131298705784394 to -0.133017937848757
-0.308551415649963 to -0.310270647714327
-0.514561042038513 to -0.516280274102877
0.00425167396871928 to 0.0025324419043555
-0.0681703536191716 to -0.0698895856835354
-0.800168648211074 to -0.801887880275438
-0.755211129633499 to -0.756930361697863
Changing layer 3's weights from 
-0.0660849063077706 to -0.0678041383721344
-0.329637989250732 to -0.331357221315096
-0.0870280711332096 to -0.0887473031975733
-0.994467302889353 to -0.996186534953716
-0.209703966824128 to -0.211423198888491
-0.339952573029114 to -0.341671805093478
-0.670137420145583 to -0.671856652209947
-0.21162454768808 to -0.213343779752444
-0.543320521561219 to -0.545039753625582
-0.0704906432309886 to -0.0722098752953523
Changing layer 4's weights from 
-0.169764444080902 to -0.171483676145266
-0.378597244469238 to -0.380316476533602
-0.579074844566894 to -0.580794076631258
-0.292140111176086 to -0.29385934324045
-0.531392916885926 to -0.533112148950289
-0.29992769643457 to -0.301646928498934
-0.304286226479126 to -0.30600545854349
-0.7661837486902 to -0.767902980754564
-0.172487661091401 to -0.174206893155764
-0.604789599625183 to -0.606508831689547
Changing layer 5's weights from 
-0.0169846264997256 to -0.0187038585640894
-0.459058150497986 to -0.46077738256235
-0.0338697878995666 to -0.0355890199639303
-0.969381969271018 to -0.971101201335381
-0.432113155571533 to -0.433832387635897
-0.830581098643852 to -0.832300330708216
-0.530615791527344 to -0.532335023591707
-0.458169564453674 to -0.459888796518038
-0.0685675589719546 to -0.0702867910363184
-0.0750756709256906 to -0.0767949029900544
10/5/2016 1:35:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:35:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:44 PMStarting AI
Reading weights from 10/5/2016 1:36:44 PMStarting AI
Weights.txt
Layer 0's weights: -0.263883332917879 -0.635809789607713 -0.870159055302332 -0.835599805424401 -0.0877322360970483 -0.770374189326952 -0.616544465730379 -0.643998871753404 -0.88149073273916 -0.925240787873456 14 11 
Layer 1's weights: -0.920143875489423 -0.318975190828036 -0.444677929589938 -0.454583267877291 -0.639636050174425 -0.777873884151171 -0.507155160615633 -0.892958189556788 -0.481295208642672 -0.843942071507165 12 9 
Layer 2's weights: -0.0347538396813384 -0.588807325074862 -0.304218034455965 -0.133017937848757 -0.310270647714327 -0.516280274102877 0.0025324419043555 -0.0698895856835354 -0.801887880275438 -0.756930361697863 10 7 
Layer 3's weights: -0.0678041383721344 -0.331357221315096 -0.0887473031975733 -0.996186534953716 -0.211423198888491 -0.341671805093478 -0.671856652209947 -0.213343779752444 -0.545039753625582 -0.0722098752953523 8 5 
Layer 4's weights: -0.171483676145266 -0.380316476533602 -0.580794076631258 -0.29385934324045 -0.533112148950289 -0.301646928498934 -0.30600545854349 -0.767902980754564 -0.174206893155764 -0.606508831689547 6 3 
Layer 5's weights: -0.0187038585640894 -0.46077738256235 -0.0355890199639303 -0.971101201335381 -0.433832387635897 -0.832300330708216 -0.532335023591707 -0.459888796518038 -0.0702867910363184 -0.0767949029900544 4 1 
Layer 6's weights: 0.115172853595259 -0.284862110489366 -0.567382807010172 -0.415877441042421 
Layer 7's weights: 0.285450080671219 0.518765667714981 0.0938404298209227 0.0979332423590697 
Layer 8's weights: 0.457811464196105 0.736253429776094 0.649213065033814 -0.00535863953266557 
10/5/2016 1:36:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:36:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:25 PMStarting learning phase with deltaScore: 2.266667
Modified index 0's learning in memoryPool to 0.4533333
Modified index 1's learning in memoryPool to 0.4533333
Modified index 2's learning in memoryPool to 0.4533333
Modified index 3's learning in memoryPool to 0.4533333
Modified index 4's learning in memoryPool to 0.4533333
Modified index 5's learning in memoryPool to 0.4533333
Modified index 6's learning in memoryPool to 0.4533333
Modified index 7's learning in memoryPool to 0.4533333
Modified index 8's learning in memoryPool to 0.4533333
Modified index 9's learning in memoryPool to 0.4533333
Modified index 10's learning in memoryPool to 0.4533333
Modified index 11's learning in memoryPool to 0.4533333
10/5/2016 1:37:25 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 0, 1, 0.4533333
sum 0.0394100926299693 distri 0.0236870777282942
Using diff 0.00587049174418285 and condRate 0.166666666666667
Changed category 1 weights from 
0.285450080671219 to 0.285893628921561
0.518765667714981 to 0.519209215965323
0.0938404298209227 to 0.0942839780712648
0.0979332423590697 to 0.0983767906094118
Changing layer 0's weights from 
-0.263883332917879 to -0.263439784667537
-0.635809789607713 to -0.635366241357371
-0.870159055302332 to -0.86971550705199
-0.835599805424401 to -0.835156257174059
-0.0877322360970483 to -0.0872886878467062
-0.770374189326952 to -0.76993064107661
-0.616544465730379 to -0.616100917480037
-0.643998871753404 to -0.643555323503062
-0.88149073273916 to -0.881047184488818
-0.925240787873456 to -0.924797239623114
Changing layer 1's weights from 
-0.920143875489423 to -0.919700327239081
-0.318975190828036 to -0.318531642577694
-0.444677929589938 to -0.444234381339596
-0.454583267877291 to -0.454139719626949
-0.639636050174425 to -0.639192501924083
-0.777873884151171 to -0.777430335900829
-0.507155160615633 to -0.506711612365291
-0.892958189556788 to -0.892514641306446
-0.481295208642672 to -0.48085166039233
-0.843942071507165 to -0.843498523256823
Changing layer 2's weights from 
-0.0347538396813384 to -0.0343102914309963
-0.588807325074862 to -0.58836377682452
-0.304218034455965 to -0.303774486205623
-0.133017937848757 to -0.132574389598415
-0.310270647714327 to -0.309827099463985
-0.516280274102877 to -0.515836725852535
0.0025324419043555 to 0.00297599015469762
-0.0698895856835354 to -0.0694460374331933
-0.801887880275438 to -0.801444332025096
-0.756930361697863 to -0.756486813447521
Changing layer 3's weights from 
-0.0678041383721344 to -0.0673605901217923
-0.331357221315096 to -0.330913673064754
-0.0887473031975733 to -0.0883037549472312
-0.996186534953716 to -0.995742986703374
-0.211423198888491 to -0.210979650638149
-0.341671805093478 to -0.341228256843136
-0.671856652209947 to -0.671413103959605
-0.213343779752444 to -0.212900231502102
-0.545039753625582 to -0.54459620537524
-0.0722098752953523 to -0.0717663270450102
Changing layer 4's weights from 
-0.171483676145266 to -0.171040127894924
-0.380316476533602 to -0.37987292828326
-0.580794076631258 to -0.580350528380916
-0.29385934324045 to -0.293415794990108
-0.533112148950289 to -0.532668600699947
-0.301646928498934 to -0.301203380248592
-0.30600545854349 to -0.305561910293148
-0.767902980754564 to -0.767459432504222
-0.174206893155764 to -0.173763344905422
-0.606508831689547 to -0.606065283439205
Changing layer 5's weights from 
-0.0187038585640894 to -0.0182603103137473
-0.46077738256235 to -0.460333834312008
-0.0355890199639303 to -0.0351454717135882
-0.971101201335381 to -0.970657653085039
-0.433832387635897 to -0.433388839385555
-0.832300330708216 to -0.831856782457874
-0.532335023591707 to -0.531891475341365
-0.459888796518038 to -0.459445248267696
-0.0702867910363184 to -0.0698432427859763
-0.0767949029900544 to -0.0763513547397123
Trying to learn from memory 1, 0, 0.4533333
sum 0.0394103028758372 distri -0.0278207623090746
Using diff 0.0573784894659525 and condRate 0.166666666666667
Changed category 0 weights from 
0.115172853595259 to 0.119508117099397
-0.284862110489366 to -0.280526846985228
-0.567382807010172 to -0.563047543506034
-0.415877441042421 to -0.411542177538283
Changing layer 0's weights from 
-0.263439784667537 to -0.259104521163399
-0.635366241357371 to -0.631030977853233
-0.86971550705199 to -0.865380243547852
-0.835156257174059 to -0.830820993669921
-0.0872886878467062 to -0.0829534243425686
-0.76993064107661 to -0.765595377572472
-0.616100917480037 to -0.611765653975899
-0.643555323503062 to -0.639220059998924
-0.881047184488818 to -0.87671192098468
-0.924797239623114 to -0.920461976118976
Changing layer 1's weights from 
-0.919700327239081 to -0.915365063734943
-0.318531642577694 to -0.314196379073556
-0.444234381339596 to -0.439899117835458
-0.454139719626949 to -0.449804456122811
-0.639192501924083 to -0.634857238419945
-0.777430335900829 to -0.773095072396691
-0.506711612365291 to -0.502376348861153
-0.892514641306446 to -0.888179377802308
-0.48085166039233 to -0.476516396888192
-0.843498523256823 to -0.839163259752685
Changing layer 2's weights from 
-0.0343102914309963 to -0.0299750279268587
-0.58836377682452 to -0.584028513320382
-0.303774486205623 to -0.299439222701485
-0.132574389598415 to -0.128239126094277
-0.309827099463985 to -0.305491835959847
-0.515836725852535 to -0.511501462348397
0.00297599015469762 to 0.00731125365883522
-0.0694460374331933 to -0.0651107739290557
-0.801444332025096 to -0.797109068520958
-0.756486813447521 to -0.752151549943383
Changing layer 3's weights from 
-0.0673605901217923 to -0.0630253266176547
-0.330913673064754 to -0.326578409560616
-0.0883037549472312 to -0.0839684914430936
-0.995742986703374 to -0.991407723199236
-0.210979650638149 to -0.206644387134011
-0.341228256843136 to -0.336892993338998
-0.671413103959605 to -0.667077840455467
-0.212900231502102 to -0.208564967997964
-0.54459620537524 to -0.540260941871102
-0.0717663270450102 to -0.0674310635408726
Changing layer 4's weights from 
-0.171040127894924 to -0.166704864390786
-0.37987292828326 to -0.375537664779122
-0.580350528380916 to -0.576015264876778
-0.293415794990108 to -0.28908053148597
-0.532668600699947 to -0.528333337195809
-0.301203380248592 to -0.296868116744454
-0.305561910293148 to -0.30122664678901
-0.767459432504222 to -0.763124169000084
-0.173763344905422 to -0.169428081401284
-0.606065283439205 to -0.601730019935067
Changing layer 5's weights from 
-0.0182603103137473 to -0.0139250468096097
-0.460333834312008 to -0.45599857080787
-0.0351454717135882 to -0.0308102082094506
-0.970657653085039 to -0.966322389580901
-0.433388839385555 to -0.429053575881417
-0.831856782457874 to -0.827521518953736
-0.531891475341365 to -0.527556211837227
-0.459445248267696 to -0.455109984763558
-0.0698432427859763 to -0.0655079792818387
-0.0763513547397123 to -0.0720160912355747
Trying to learn from memory 2, 0, 0.4533333
sum 0.039415476811152 distri -0.0278233793273486
Using diff 0.0573849869357125 and condRate 0.166666666666667
Changed category 0 weights from 
0.119508117099397 to 0.123843871523455
-0.280526846985228 to -0.27619109256117
-0.563047543506034 to -0.558711789081976
-0.411542177538283 to -0.407206423114225
Changing layer 0's weights from 
-0.259104521163399 to -0.254768766739341
-0.631030977853233 to -0.626695223429175
-0.865380243547852 to -0.861044489123794
-0.830820993669921 to -0.826485239245863
-0.0829534243425686 to -0.0786176699185099
-0.765595377572472 to -0.761259623148414
-0.611765653975899 to -0.607429899551841
-0.639220059998924 to -0.634884305574866
-0.87671192098468 to -0.872376166560622
-0.920461976118976 to -0.916126221694918
Changing layer 1's weights from 
-0.915365063734943 to -0.911029309310885
-0.314196379073556 to -0.309860624649498
-0.439899117835458 to -0.4355633634114
-0.449804456122811 to -0.445468701698753
-0.634857238419945 to -0.630521483995887
-0.773095072396691 to -0.768759317972633
-0.502376348861153 to -0.498040594437095
-0.888179377802308 to -0.88384362337825
-0.476516396888192 to -0.472180642464134
-0.839163259752685 to -0.834827505328627
Changing layer 2's weights from 
-0.0299750279268587 to -0.0256392735028
-0.584028513320382 to -0.579692758896324
-0.299439222701485 to -0.295103468277427
-0.128239126094277 to -0.123903371670219
-0.305491835959847 to -0.301156081535789
-0.511501462348397 to -0.507165707924339
0.00731125365883522 to 0.0116470080828939
-0.0651107739290557 to -0.060775019504997
-0.797109068520958 to -0.7927733140969
-0.752151549943383 to -0.747815795519325
Changing layer 3's weights from 
-0.0630253266176547 to -0.058689572193596
-0.326578409560616 to -0.322242655136558
-0.0839684914430936 to -0.0796327370190349
-0.991407723199236 to -0.987071968775178
-0.206644387134011 to -0.202308632709953
-0.336892993338998 to -0.33255723891494
-0.667077840455467 to -0.662742086031409
-0.208564967997964 to -0.204229213573906
-0.540260941871102 to -0.535925187447044
-0.0674310635408726 to -0.0630953091168139
Changing layer 4's weights from 
-0.166704864390786 to -0.162369109966728
-0.375537664779122 to -0.371201910355064
-0.576015264876778 to -0.57167951045272
-0.28908053148597 to -0.284744777061912
-0.528333337195809 to -0.523997582771751
-0.296868116744454 to -0.292532362320396
-0.30122664678901 to -0.296890892364952
-0.763124169000084 to -0.758788414576026
-0.169428081401284 to -0.165092326977226
-0.601730019935067 to -0.597394265511009
Changing layer 5's weights from 
-0.0139250468096097 to -0.00958929238555101
-0.45599857080787 to -0.451662816383812
-0.0308102082094506 to -0.0264744537853919
-0.966322389580901 to -0.961986635156843
-0.429053575881417 to -0.424717821457359
-0.827521518953736 to -0.823185764529678
-0.527556211837227 to -0.523220457413169
-0.455109984763558 to -0.4507742303395
-0.0655079792818387 to -0.06117222485778
-0.0720160912355747 to -0.067680336811516
Trying to learn from memory 3, 0, 0.4533333
sum 0.0394148423617248 distri -0.0278195026559643
Using diff 0.0573806344272579 and condRate 0.166666666666667
Changed category 0 weights from 
0.123843871523455 to 0.128179297091331
-0.27619109256117 to -0.271855666993295
-0.558711789081976 to -0.5543763635141
-0.407206423114225 to -0.40287099754635
Changing layer 0's weights from 
-0.254768766739341 to -0.250433341171465
-0.626695223429175 to -0.622359797861299
-0.861044489123794 to -0.856709063555918
-0.826485239245863 to -0.822149813677987
-0.0786176699185099 to -0.0742822443506346
-0.761259623148414 to -0.756924197580538
-0.607429899551841 to -0.603094473983965
-0.634884305574866 to -0.63054888000699
-0.872376166560622 to -0.868040740992746
-0.916126221694918 to -0.911790796127042
Changing layer 1's weights from 
-0.911029309310885 to -0.906693883743009
-0.309860624649498 to -0.305525199081622
-0.4355633634114 to -0.431227937843524
-0.445468701698753 to -0.441133276130877
-0.630521483995887 to -0.626186058428011
-0.768759317972633 to -0.764423892404757
-0.498040594437095 to -0.493705168869219
-0.88384362337825 to -0.879508197810374
-0.472180642464134 to -0.467845216896258
-0.834827505328627 to -0.830492079760751
Changing layer 2's weights from 
-0.0256392735028 to -0.0213038479349247
-0.579692758896324 to -0.575357333328448
-0.295103468277427 to -0.290768042709551
-0.123903371670219 to -0.119567946102343
-0.301156081535789 to -0.296820655967913
-0.507165707924339 to -0.502830282356463
0.0116470080828939 to 0.0159824336507692
-0.060775019504997 to -0.0564395939371217
-0.7927733140969 to -0.788437888529024
-0.747815795519325 to -0.743480369951449
Changing layer 3's weights from 
-0.058689572193596 to -0.0543541466257207
-0.322242655136558 to -0.317907229568682
-0.0796327370190349 to -0.0752973114511596
-0.987071968775178 to -0.982736543207302
-0.202308632709953 to -0.197973207142077
-0.33255723891494 to -0.328221813347064
-0.662742086031409 to -0.658406660463533
-0.204229213573906 to -0.19989378800603
-0.535925187447044 to -0.531589761879168
-0.0630953091168139 to -0.0587598835489386
Changing layer 4's weights from 
-0.162369109966728 to -0.158033684398852
-0.371201910355064 to -0.366866484787188
-0.57167951045272 to -0.567344084884844
-0.284744777061912 to -0.280409351494036
-0.523997582771751 to -0.519662157203875
-0.292532362320396 to -0.28819693675252
-0.296890892364952 to -0.292555466797076
-0.758788414576026 to -0.75445298900815
-0.165092326977226 to -0.16075690140935
-0.597394265511009 to -0.593058839943133
Changing layer 5's weights from 
-0.00958929238555101 to -0.00525386681767573
-0.451662816383812 to -0.447327390815936
-0.0264744537853919 to -0.0221390282175166
-0.961986635156843 to -0.957651209588967
-0.424717821457359 to -0.420382395889483
-0.823185764529678 to -0.818850338961802
-0.523220457413169 to -0.518885031845293
-0.4507742303395 to -0.446438804771624
-0.06117222485778 to -0.0568367992899047
-0.067680336811516 to -0.0633449112436407
Trying to learn from memory 4, 0, 0.4533333
sum 0.0394112804778756 distri -0.0278186308781267
Using diff 0.0573770912365334 and condRate 0.166666666666667
Changed category 0 weights from 
0.128179297091331 to 0.132514454951471
-0.271855666993295 to -0.267520509133154
-0.5543763635141 to -0.55004120565396
-0.40287099754635 to -0.398535839686209
Changing layer 0's weights from 
-0.250433341171465 to -0.246098183311325
-0.622359797861299 to -0.618024640001159
-0.856709063555918 to -0.852373905695778
-0.822149813677987 to -0.817814655817847
-0.0742822443506346 to -0.0699470864904941
-0.756924197580538 to -0.752589039720398
-0.603094473983965 to -0.598759316123825
-0.63054888000699 to -0.62621372214685
-0.868040740992746 to -0.863705583132606
-0.911790796127042 to -0.907455638266902
Changing layer 1's weights from 
-0.906693883743009 to -0.902358725882869
-0.305525199081622 to -0.301190041221482
-0.431227937843524 to -0.426892779983384
-0.441133276130877 to -0.436798118270737
-0.626186058428011 to -0.621850900567871
-0.764423892404757 to -0.760088734544617
-0.493705168869219 to -0.489370011009079
-0.879508197810374 to -0.875173039950234
-0.467845216896258 to -0.463510059036118
-0.830492079760751 to -0.826156921900611
Changing layer 2's weights from 
-0.0213038479349247 to -0.0169686900747842
-0.575357333328448 to -0.571022175468308
-0.290768042709551 to -0.286432884849411
-0.119567946102343 to -0.115232788242203
-0.296820655967913 to -0.292485498107773
-0.502830282356463 to -0.498495124496323
0.0159824336507692 to 0.0203175915109097
-0.0564395939371217 to -0.0521044360769812
-0.788437888529024 to -0.784102730668884
-0.743480369951449 to -0.739145212091309
Changing layer 3's weights from 
-0.0543541466257207 to -0.0500189887655802
-0.317907229568682 to -0.313572071708542
-0.0752973114511596 to -0.0709621535910191
-0.982736543207302 to -0.978401385347162
-0.197973207142077 to -0.193638049281937
-0.328221813347064 to -0.323886655486924
-0.658406660463533 to -0.654071502603393
-0.19989378800603 to -0.19555863014589
-0.531589761879168 to -0.527254604019028
-0.0587598835489386 to -0.0544247256887981
Changing layer 4's weights from 
-0.158033684398852 to -0.153698526538712
-0.366866484787188 to -0.362531326927048
-0.567344084884844 to -0.563008927024704
-0.280409351494036 to -0.276074193633896
-0.519662157203875 to -0.515326999343735
-0.28819693675252 to -0.28386177889238
-0.292555466797076 to -0.288220308936936
-0.75445298900815 to -0.75011783114801
-0.16075690140935 to -0.15642174354921
-0.593058839943133 to -0.588723682082993
Changing layer 5's weights from 
-0.00525386681767573 to -0.000918708957535166
-0.447327390815936 to -0.442992232955796
-0.0221390282175166 to -0.0178038703573761
-0.957651209588967 to -0.953316051728827
-0.420382395889483 to -0.416047238029343
-0.818850338961802 to -0.814515181101662
-0.518885031845293 to -0.514549873985153
-0.446438804771624 to -0.442103646911484
-0.0568367992899047 to -0.0525016414297642
-0.0633449112436407 to -0.0590097533835002
Trying to learn from memory 4, 0, 0.4533333
sum 0.0394112804778756 distri -0.0278186308781267
Using diff 0.0573770912365334 and condRate 0.166666666666667
Changed category 0 weights from 
0.132514454951471 to 0.136849612811612
-0.267520509133154 to -0.263185351273013
-0.55004120565396 to -0.545706047793819
-0.398535839686209 to -0.394200681826068
Changing layer 0's weights from 
-0.246098183311325 to -0.241763025451184
-0.618024640001159 to -0.613689482141018
-0.852373905695778 to -0.848038747835637
-0.817814655817847 to -0.813479497957706
-0.0699470864904941 to -0.0656119286303535
-0.752589039720398 to -0.748253881860257
-0.598759316123825 to -0.594424158263684
-0.62621372214685 to -0.621878564286709
-0.863705583132606 to -0.859370425272465
-0.907455638266902 to -0.903120480406761
Changing layer 1's weights from 
-0.902358725882869 to -0.898023568022728
-0.301190041221482 to -0.296854883361341
-0.426892779983384 to -0.422557622123243
-0.436798118270737 to -0.432462960410596
-0.621850900567871 to -0.61751574270773
-0.760088734544617 to -0.755753576684476
-0.489370011009079 to -0.485034853148938
-0.875173039950234 to -0.870837882090093
-0.463510059036118 to -0.459174901175977
-0.826156921900611 to -0.82182176404047
Changing layer 2's weights from 
-0.0169686900747842 to -0.0126335322146436
-0.571022175468308 to -0.566687017608167
-0.286432884849411 to -0.28209772698927
-0.115232788242203 to -0.110897630382062
-0.292485498107773 to -0.288150340247632
-0.498495124496323 to -0.494159966636182
0.0203175915109097 to 0.0246527493710503
-0.0521044360769812 to -0.0477692782168406
-0.784102730668884 to -0.779767572808743
-0.739145212091309 to -0.734810054231168
Changing layer 3's weights from 
-0.0500189887655802 to -0.0456838309054396
-0.313572071708542 to -0.309236913848401
-0.0709621535910191 to -0.0666269957308785
-0.978401385347162 to -0.974066227487021
-0.193638049281937 to -0.189302891421796
-0.323886655486924 to -0.319551497626783
-0.654071502603393 to -0.649736344743252
-0.19555863014589 to -0.191223472285749
-0.527254604019028 to -0.522919446158887
-0.0544247256887981 to -0.0500895678286575
Changing layer 4's weights from 
-0.153698526538712 to -0.149363368678571
-0.362531326927048 to -0.358196169066907
-0.563008927024704 to -0.558673769164563
-0.276074193633896 to -0.271739035773755
-0.515326999343735 to -0.510991841483594
-0.28386177889238 to -0.279526621032239
-0.288220308936936 to -0.283885151076795
-0.75011783114801 to -0.745782673287869
-0.15642174354921 to -0.152086585689069
-0.588723682082993 to -0.584388524222852
Changing layer 5's weights from 
-0.000918708957535166 to 0.0034164489026054
-0.442992232955796 to -0.438657075095655
-0.0178038703573761 to -0.0134687124972355
-0.953316051728827 to -0.948980893868686
-0.416047238029343 to -0.411712080169202
-0.814515181101662 to -0.810180023241521
-0.514549873985153 to -0.510214716125012
-0.442103646911484 to -0.437768489051343
-0.0525016414297642 to -0.0481664835696236
-0.0590097533835002 to -0.0546745955233596
Trying to learn from memory 4, 0, 0.4533333
sum 0.0394112804778756 distri -0.0278186308781267
Using diff 0.0573770912365334 and condRate 0.166666666666667
Changed category 0 weights from 
0.136849612811612 to 0.141184770671752
-0.263185351273013 to -0.258850193412873
-0.545706047793819 to -0.541370889933679
-0.394200681826068 to -0.389865523965928
Changing layer 0's weights from 
-0.241763025451184 to -0.237427867591044
-0.613689482141018 to -0.609354324280878
-0.848038747835637 to -0.843703589975497
-0.813479497957706 to -0.809144340097566
-0.0656119286303535 to -0.0612767707702129
-0.748253881860257 to -0.743918724000117
-0.594424158263684 to -0.590089000403544
-0.621878564286709 to -0.617543406426569
-0.859370425272465 to -0.855035267412325
-0.903120480406761 to -0.898785322546621
Changing layer 1's weights from 
-0.898023568022728 to -0.893688410162588
-0.296854883361341 to -0.292519725501201
-0.422557622123243 to -0.418222464263103
-0.432462960410596 to -0.428127802550456
-0.61751574270773 to -0.61318058484759
-0.755753576684476 to -0.751418418824336
-0.485034853148938 to -0.480699695288798
-0.870837882090093 to -0.866502724229953
-0.459174901175977 to -0.454839743315837
-0.82182176404047 to -0.81748660618033
Changing layer 2's weights from 
-0.0126335322146436 to -0.00829837435450303
-0.566687017608167 to -0.562351859748027
-0.28209772698927 to -0.27776256912913
-0.110897630382062 to -0.106562472521922
-0.288150340247632 to -0.283815182387492
-0.494159966636182 to -0.489824808776042
0.0246527493710503 to 0.0289879072311909
-0.0477692782168406 to -0.0434341203567
-0.779767572808743 to -0.775432414948603
-0.734810054231168 to -0.730474896371028
Changing layer 3's weights from 
-0.0456838309054396 to -0.041348673045299
-0.309236913848401 to -0.304901755988261
-0.0666269957308785 to -0.0622918378707379
-0.974066227487021 to -0.969731069626881
-0.189302891421796 to -0.184967733561656
-0.319551497626783 to -0.315216339766643
-0.649736344743252 to -0.645401186883112
-0.191223472285749 to -0.186888314425609
-0.522919446158887 to -0.518584288298747
-0.0500895678286575 to -0.0457544099685169
Changing layer 4's weights from 
-0.149363368678571 to -0.145028210818431
-0.358196169066907 to -0.353861011206767
-0.558673769164563 to -0.554338611304423
-0.271739035773755 to -0.267403877913615
-0.510991841483594 to -0.506656683623454
-0.279526621032239 to -0.275191463172099
-0.283885151076795 to -0.279549993216655
-0.745782673287869 to -0.741447515427729
-0.152086585689069 to -0.147751427828929
-0.584388524222852 to -0.580053366362712
Changing layer 5's weights from 
0.0034164489026054 to 0.00775160676274597
-0.438657075095655 to -0.434321917235515
-0.0134687124972355 to -0.00913355463709494
-0.948980893868686 to -0.944645736008546
-0.411712080169202 to -0.407376922309062
-0.810180023241521 to -0.805844865381381
-0.510214716125012 to -0.505879558264872
-0.437768489051343 to -0.433433331191203
-0.0481664835696236 to -0.043831325709483
-0.0546745955233596 to -0.050339437663219
Trying to learn from memory 4, 1, 0.4533333
sum 0.0394112804778756 distri 0.0236875383356886
Using diff 0.00587092202271815 and condRate 0.166666666666667
Changed category 1 weights from 
0.285893628921561 to 0.286337209681836
0.519209215965323 to 0.519652796725598
0.0942839780712648 to 0.0947275588315396
0.0983767906094118 to 0.0988203713696866
Changing layer 0's weights from 
-0.237427867591044 to -0.236984286830769
-0.609354324280878 to -0.608910743520603
-0.843703589975497 to -0.843260009215222
-0.809144340097566 to -0.808700759337291
-0.0612767707702129 to -0.0608331900099381
-0.743918724000117 to -0.743475143239842
-0.590089000403544 to -0.589645419643269
-0.617543406426569 to -0.617099825666294
-0.855035267412325 to -0.85459168665205
-0.898785322546621 to -0.898341741786346
Changing layer 1's weights from 
-0.893688410162588 to -0.893244829402313
-0.292519725501201 to -0.292076144740926
-0.418222464263103 to -0.417778883502828
-0.428127802550456 to -0.427684221790181
-0.61318058484759 to -0.612737004087315
-0.751418418824336 to -0.750974838064061
-0.480699695288798 to -0.480256114528523
-0.866502724229953 to -0.866059143469678
-0.454839743315837 to -0.454396162555562
-0.81748660618033 to -0.817043025420055
Changing layer 2's weights from 
-0.00829837435450303 to -0.00785479359422822
-0.562351859748027 to -0.561908278987752
-0.27776256912913 to -0.277318988368855
-0.106562472521922 to -0.106118891761647
-0.283815182387492 to -0.283371601627217
-0.489824808776042 to -0.489381228015767
0.0289879072311909 to 0.0294314879914657
-0.0434341203567 to -0.0429905395964252
-0.775432414948603 to -0.774988834188328
-0.730474896371028 to -0.730031315610753
Changing layer 3's weights from 
-0.041348673045299 to -0.0409050922850242
-0.304901755988261 to -0.304458175227986
-0.0622918378707379 to -0.0618482571104631
-0.969731069626881 to -0.969287488866606
-0.184967733561656 to -0.184524152801381
-0.315216339766643 to -0.314772759006368
-0.645401186883112 to -0.644957606122837
-0.186888314425609 to -0.186444733665334
-0.518584288298747 to -0.518140707538472
-0.0457544099685169 to -0.0453108292082421
Changing layer 4's weights from 
-0.145028210818431 to -0.144584630058156
-0.353861011206767 to -0.353417430446492
-0.554338611304423 to -0.553895030544148
-0.267403877913615 to -0.26696029715334
-0.506656683623454 to -0.506213102863179
-0.275191463172099 to -0.274747882411824
-0.279549993216655 to -0.27910641245638
-0.741447515427729 to -0.741003934667454
-0.147751427828929 to -0.147307847068654
-0.580053366362712 to -0.579609785602437
Changing layer 5's weights from 
0.00775160676274597 to 0.00819518752302078
-0.434321917235515 to -0.43387833647524
-0.00913355463709494 to -0.00868997387682012
-0.944645736008546 to -0.944202155248271
-0.407376922309062 to -0.406933341548787
-0.805844865381381 to -0.805401284621106
-0.505879558264872 to -0.505435977504597
-0.433433331191203 to -0.432989750430928
-0.043831325709483 to -0.0433877449492082
-0.050339437663219 to -0.0498958569029442
Trying to learn from memory 4, 0, 0.4533333
sum 0.0394112804778756 distri -0.0278186308781267
Using diff 0.0573770912365334 and condRate 0.166666666666667
Changed category 0 weights from 
0.141184770671752 to 0.145519928531893
-0.258850193412873 to -0.254515035552732
-0.541370889933679 to -0.537035732073538
-0.389865523965928 to -0.385530366105787
Changing layer 0's weights from 
-0.236984286830769 to -0.232649128970628
-0.608910743520603 to -0.604575585660462
-0.843260009215222 to -0.838924851355081
-0.808700759337291 to -0.80436560147715
-0.0608331900099381 to -0.0564980321497976
-0.743475143239842 to -0.739139985379701
-0.589645419643269 to -0.585310261783128
-0.617099825666294 to -0.612764667806153
-0.85459168665205 to -0.850256528791909
-0.898341741786346 to -0.894006583926205
Changing layer 1's weights from 
-0.893244829402313 to -0.888909671542172
-0.292076144740926 to -0.287740986880785
-0.417778883502828 to -0.413443725642687
-0.427684221790181 to -0.42334906393004
-0.612737004087315 to -0.608401846227174
-0.750974838064061 to -0.74663968020392
-0.480256114528523 to -0.475920956668382
-0.866059143469678 to -0.861723985609537
-0.454396162555562 to -0.450061004695421
-0.817043025420055 to -0.812707867559914
Changing layer 2's weights from 
-0.00785479359422822 to -0.00351963573408765
-0.561908278987752 to -0.557573121127611
-0.277318988368855 to -0.272983830508714
-0.106118891761647 to -0.101783733901506
-0.283371601627217 to -0.279036443767076
-0.489381228015767 to -0.485046070155626
0.0294314879914657 to 0.0337666458516063
-0.0429905395964252 to -0.0386553817362846
-0.774988834188328 to -0.770653676328187
-0.730031315610753 to -0.725696157750612
Changing layer 3's weights from 
-0.0409050922850242 to -0.0365699344248836
-0.304458175227986 to -0.300123017367845
-0.0618482571104631 to -0.0575130992503226
-0.969287488866606 to -0.964952331006465
-0.184524152801381 to -0.18018899494124
-0.314772759006368 to -0.310437601146227
-0.644957606122837 to -0.640622448262696
-0.186444733665334 to -0.182109575805193
-0.518140707538472 to -0.513805549678331
-0.0453108292082421 to -0.0409756713481015
Changing layer 4's weights from 
-0.144584630058156 to -0.140249472198015
-0.353417430446492 to -0.349082272586351
-0.553895030544148 to -0.549559872684007
-0.26696029715334 to -0.262625139293199
-0.506213102863179 to -0.501877945003038
-0.274747882411824 to -0.270412724551683
-0.27910641245638 to -0.274771254596239
-0.741003934667454 to -0.736668776807313
-0.147307847068654 to -0.142972689208513
-0.579609785602437 to -0.575274627742296
Changing layer 5's weights from 
0.00819518752302078 to 0.0125303453831614
-0.43387833647524 to -0.429543178615099
-0.00868997387682012 to -0.00435481601667956
-0.944202155248271 to -0.93986699738813
-0.406933341548787 to -0.402598183688646
-0.805401284621106 to -0.801066126760965
-0.505435977504597 to -0.501100819644456
-0.432989750430928 to -0.428654592570787
-0.0433877449492082 to -0.0390525870890676
-0.0498958569029442 to -0.0455606990428036
Trying to learn from memory 4, 0, 0.4533333
sum 0.0394112804778756 distri -0.0278186308781267
Using diff 0.0573770912365334 and condRate 0.166666666666667
Changed category 0 weights from 
0.145519928531893 to 0.149855086392033
-0.254515035552732 to -0.250179877692592
-0.537035732073538 to -0.532700574213397
-0.385530366105787 to -0.381195208245647
Changing layer 0's weights from 
-0.232649128970628 to -0.228313971110488
-0.604575585660462 to -0.600240427800322
-0.838924851355081 to -0.834589693494941
-0.80436560147715 to -0.80003044361701
-0.0564980321497976 to -0.052162874289657
-0.739139985379701 to -0.734804827519561
-0.585310261783128 to -0.580975103922988
-0.612764667806153 to -0.608429509946013
-0.850256528791909 to -0.845921370931769
-0.894006583926205 to -0.889671426066065
Changing layer 1's weights from 
-0.888909671542172 to -0.884574513682032
-0.287740986880785 to -0.283405829020645
-0.413443725642687 to -0.409108567782547
-0.42334906393004 to -0.4190139060699
-0.608401846227174 to -0.604066688367034
-0.74663968020392 to -0.74230452234378
-0.475920956668382 to -0.471585798808242
-0.861723985609537 to -0.857388827749397
-0.450061004695421 to -0.445725846835281
-0.812707867559914 to -0.808372709699774
Changing layer 2's weights from 
-0.00351963573408765 to 0.000815522126052917
-0.557573121127611 to -0.553237963267471
-0.272983830508714 to -0.268648672648574
-0.101783733901506 to -0.0974485760413657
-0.279036443767076 to -0.274701285906936
-0.485046070155626 to -0.480710912295486
0.0337666458516063 to 0.0381018037117468
-0.0386553817362846 to -0.0343202238761441
-0.770653676328187 to -0.766318518468047
-0.725696157750612 to -0.721360999890472
Changing layer 3's weights from 
-0.0365699344248836 to -0.0322347765647431
-0.300123017367845 to -0.295787859507705
-0.0575130992503226 to -0.053177941390182
-0.964952331006465 to -0.960617173146325
-0.18018899494124 to -0.1758538370811
-0.310437601146227 to -0.306102443286087
-0.640622448262696 to -0.636287290402555
-0.182109575805193 to -0.177774417945053
-0.513805549678331 to -0.509470391818191
-0.0409756713481015 to -0.036640513487961
Changing layer 4's weights from 
-0.140249472198015 to -0.135914314337875
-0.349082272586351 to -0.344747114726211
-0.549559872684007 to -0.545224714823867
-0.262625139293199 to -0.258289981433059
-0.501877945003038 to -0.497542787142898
-0.270412724551683 to -0.266077566691543
-0.274771254596239 to -0.270436096736099
-0.736668776807313 to -0.732333618947173
-0.142972689208513 to -0.138637531348373
-0.575274627742296 to -0.570939469882155
Changing layer 5's weights from 
0.0125303453831614 to 0.0168655032433019
-0.429543178615099 to -0.425208020754959
-0.00435481601667956 to -1.96581565389889E-05
-0.93986699738813 to -0.93553183952799
-0.402598183688646 to -0.398263025828506
-0.801066126760965 to -0.796730968900825
-0.501100819644456 to -0.496765661784316
-0.428654592570787 to -0.424319434710647
-0.0390525870890676 to -0.0347174292289271
-0.0455606990428036 to -0.0412255411826631
10/5/2016 1:37:25 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 4, 0, 0.4533333
sum 0.0394112804778756 distri -0.0278186308781267
Using diff 0.0573770912365334 and condRate 0.166666666666667
Changed category 0 weights from 
0.149855086392033 to 0.154190244252174
-0.250179877692592 to -0.245844719832451
-0.532700574213397 to -0.528365416353257
-0.381195208245647 to -0.376860050385506
Changing layer 0's weights from 
-0.228313971110488 to -0.223978813250347
-0.600240427800322 to -0.595905269940181
-0.834589693494941 to -0.8302545356348
-0.80003044361701 to -0.795695285756869
-0.052162874289657 to -0.0478277164295164
-0.734804827519561 to -0.73046966965942
-0.580975103922988 to -0.576639946062847
-0.608429509946013 to -0.604094352085872
-0.845921370931769 to -0.841586213071628
-0.889671426066065 to -0.885336268205924
Changing layer 1's weights from 
-0.884574513682032 to -0.880239355821891
-0.283405829020645 to -0.279070671160504
-0.409108567782547 to -0.404773409922406
-0.4190139060699 to -0.414678748209759
-0.604066688367034 to -0.599731530506893
-0.74230452234378 to -0.737969364483639
-0.471585798808242 to -0.467250640948101
-0.857388827749397 to -0.853053669889256
-0.445725846835281 to -0.44139068897514
-0.808372709699774 to -0.804037551839633
Changing layer 2's weights from 
0.000815522126052917 to 0.00515067998619348
-0.553237963267471 to -0.54890280540733
-0.268648672648574 to -0.264313514788433
-0.0974485760413657 to -0.0931134181812251
-0.274701285906936 to -0.270366128046795
-0.480710912295486 to -0.476375754435345
0.0381018037117468 to 0.0424369615718874
-0.0343202238761441 to -0.0299850660160035
-0.766318518468047 to -0.761983360607906
-0.721360999890472 to -0.717025842030331
Changing layer 3's weights from 
-0.0322347765647431 to -0.0278996187046025
-0.295787859507705 to -0.291452701647564
-0.053177941390182 to -0.0488427835300414
-0.960617173146325 to -0.956282015286184
-0.1758538370811 to -0.171518679220959
-0.306102443286087 to -0.301767285425946
-0.636287290402555 to -0.631952132542415
-0.177774417945053 to -0.173439260084912
-0.509470391818191 to -0.50513523395805
-0.036640513487961 to -0.0323053556278204
Changing layer 4's weights from 
-0.135914314337875 to -0.131579156477734
-0.344747114726211 to -0.34041195686607
-0.545224714823867 to -0.540889556963726
-0.258289981433059 to -0.253954823572918
-0.497542787142898 to -0.493207629282757
-0.266077566691543 to -0.261742408831402
-0.270436096736099 to -0.266100938875958
-0.732333618947173 to -0.727998461087032
-0.138637531348373 to -0.134302373488232
-0.570939469882155 to -0.566604312022015
Changing layer 5's weights from 
0.0168655032433019 to 0.0212006611034425
-0.425208020754959 to -0.420872862894818
-1.96581565389889E-05 to 0.00431549970360158
-0.93553183952799 to -0.931196681667849
-0.398263025828506 to -0.393927867968365
-0.796730968900825 to -0.792395811040684
-0.496765661784316 to -0.492430503924175
-0.424319434710647 to -0.419984276850506
-0.0347174292289271 to -0.0303822713687865
-0.0412255411826631 to -0.0368903833225225
Trying to learn from memory 4, 1, 0.4533333
sum 0.0394112804778756 distri 0.0236875383356886
Using diff 0.00587092202271815 and condRate 0.166666666666667
Changed category 1 weights from 
0.286337209681836 to 0.286780790442111
0.519652796725598 to 0.520096377485873
0.0947275588315396 to 0.0951711395918144
0.0988203713696866 to 0.0992639521299614
Changing layer 0's weights from 
-0.223978813250347 to -0.223535232490072
-0.595905269940181 to -0.595461689179906
-0.8302545356348 to -0.829810954874525
-0.795695285756869 to -0.795251704996594
-0.0478277164295164 to -0.0473841356692416
-0.73046966965942 to -0.730026088899145
-0.576639946062847 to -0.576196365302572
-0.604094352085872 to -0.603650771325597
-0.841586213071628 to -0.841142632311353
-0.885336268205924 to -0.884892687445649
Changing layer 1's weights from 
-0.880239355821891 to -0.879795775061616
-0.279070671160504 to -0.278627090400229
-0.404773409922406 to -0.404329829162131
-0.414678748209759 to -0.414235167449484
-0.599731530506893 to -0.599287949746618
-0.737969364483639 to -0.737525783723364
-0.467250640948101 to -0.466807060187826
-0.853053669889256 to -0.852610089128981
-0.44139068897514 to -0.440947108214865
-0.804037551839633 to -0.803593971079358
Changing layer 2's weights from 
0.00515067998619348 to 0.0055942607464683
-0.54890280540733 to -0.548459224647055
-0.264313514788433 to -0.263869934028158
-0.0931134181812251 to -0.0926698374209503
-0.270366128046795 to -0.26992254728652
-0.476375754435345 to -0.47593217367507
0.0424369615718874 to 0.0428805423321622
-0.0299850660160035 to -0.0295414852557287
-0.761983360607906 to -0.761539779847631
-0.717025842030331 to -0.716582261270056
Changing layer 3's weights from 
-0.0278996187046025 to -0.0274560379443277
-0.291452701647564 to -0.291009120887289
-0.0488427835300414 to -0.0483992027697666
-0.956282015286184 to -0.955838434525909
-0.171518679220959 to -0.171075098460684
-0.301767285425946 to -0.301323704665671
-0.631952132542415 to -0.63150855178214
-0.173439260084912 to -0.172995679324637
-0.50513523395805 to -0.504691653197775
-0.0323053556278204 to -0.0318617748675456
Changing layer 4's weights from 
-0.131579156477734 to -0.131135575717459
-0.34041195686607 to -0.339968376105795
-0.540889556963726 to -0.540445976203451
-0.253954823572918 to -0.253511242812643
-0.493207629282757 to -0.492764048522482
-0.261742408831402 to -0.261298828071127
-0.266100938875958 to -0.265657358115683
-0.727998461087032 to -0.727554880326757
-0.134302373488232 to -0.133858792727957
-0.566604312022015 to -0.56616073126174
Changing layer 5's weights from 
0.0212006611034425 to 0.0216442418637173
-0.420872862894818 to -0.420429282134543
0.00431549970360158 to 0.00475908046387639
-0.931196681667849 to -0.930753100907574
-0.393927867968365 to -0.39348428720809
-0.792395811040684 to -0.791952230280409
-0.492430503924175 to -0.4919869231639
-0.419984276850506 to -0.419540696090231
-0.0303822713687865 to -0.0299386906085117
-0.0368903833225225 to -0.0364468025622477
10/5/2016 1:37:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:28 PMStarting learning phase with deltaScore: 2.6
Modified index 0's learning in memoryPool to 0.52
Modified index 1's learning in memoryPool to 0.52
Modified index 2's learning in memoryPool to 0.52
10/5/2016 1:37:28 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 5, 1, 0.52
sum 0.0385625614037886 distri 0.0211422781364124
Using diff 0.00777964291642909 and condRate 0.166666666666667
Changed category 1 weights from 
0.286780790442111 to 0.287455026136804
0.520096377485873 to 0.520770613180566
0.0951711395918144 to 0.0958453752865075
0.0992639521299614 to 0.0999381878246545
Changing layer 0's weights from 
-0.223535232490072 to -0.222860996795379
-0.595461689179906 to -0.594787453485213
-0.829810954874525 to -0.829136719179832
-0.795251704996594 to -0.794577469301901
-0.0473841356692416 to -0.0467098999745486
-0.730026088899145 to -0.729351853204452
-0.576196365302572 to -0.575522129607879
-0.603650771325597 to -0.602976535630904
-0.841142632311353 to -0.84046839661666
-0.884892687445649 to -0.884218451750956
Changing layer 1's weights from 
-0.879795775061616 to -0.879121539366923
-0.278627090400229 to -0.277952854705536
-0.404329829162131 to -0.403655593467438
-0.414235167449484 to -0.413560931754791
-0.599287949746618 to -0.598613714051925
-0.737525783723364 to -0.736851548028671
-0.466807060187826 to -0.466132824493133
-0.852610089128981 to -0.851935853434288
-0.440947108214865 to -0.440272872520172
-0.803593971079358 to -0.802919735384665
Changing layer 2's weights from 
0.0055942607464683 to 0.00626849644116134
-0.548459224647055 to -0.547784988952362
-0.263869934028158 to -0.263195698333465
-0.0926698374209503 to -0.0919956017262573
-0.26992254728652 to -0.269248311591827
-0.47593217367507 to -0.475257937980377
0.0428805423321622 to 0.0435547780268552
-0.0295414852557287 to -0.0288672495610356
-0.761539779847631 to -0.760865544152938
-0.716582261270056 to -0.715908025575363
Changing layer 3's weights from 
-0.0274560379443277 to -0.0267818022496347
-0.291009120887289 to -0.290334885192596
-0.0483992027697666 to -0.0477249670750736
-0.955838434525909 to -0.955164198831216
-0.171075098460684 to -0.170400862765991
-0.301323704665671 to -0.300649468970978
-0.63150855178214 to -0.630834316087447
-0.172995679324637 to -0.172321443629944
-0.504691653197775 to -0.504017417503082
-0.0318617748675456 to -0.0311875391728526
Changing layer 4's weights from 
-0.131135575717459 to -0.130461340022766
-0.339968376105795 to -0.339294140411102
-0.540445976203451 to -0.539771740508758
-0.253511242812643 to -0.25283700711795
-0.492764048522482 to -0.492089812827789
-0.261298828071127 to -0.260624592376434
-0.265657358115683 to -0.26498312242099
-0.727554880326757 to -0.726880644632064
-0.133858792727957 to -0.133184557033264
-0.56616073126174 to -0.565486495567047
Changing layer 5's weights from 
0.0216442418637173 to 0.0223184775584103
-0.420429282134543 to -0.41975504643985
0.00475908046387639 to 0.00543331615856943
-0.930753100907574 to -0.930078865212881
-0.39348428720809 to -0.392810051513397
-0.791952230280409 to -0.791277994585716
-0.4919869231639 to -0.491312687469207
-0.419540696090231 to -0.418866460395538
-0.0299386906085117 to -0.0292644549138186
-0.0364468025622477 to -0.0357725668675546
Trying to learn from memory 6, 1, 0.52
sum 0.0385625089101845 distri 0.021142311532796
Using diff 0.00777957014984238 and condRate 0.166666666666667
Changed category 1 weights from 
0.287455026136804 to 0.28812925552506
0.520770613180566 to 0.521444842568822
0.0958453752865075 to 0.0965196046747632
0.0999381878246545 to 0.10061241721291
Changing layer 0's weights from 
-0.222860996795379 to -0.222186767407124
-0.594787453485213 to -0.594113224096957
-0.829136719179832 to -0.828462489791576
-0.794577469301901 to -0.793903239913645
-0.0467098999745486 to -0.0460356705862928
-0.729351853204452 to -0.728677623816196
-0.575522129607879 to -0.574847900219623
-0.602976535630904 to -0.602302306242648
-0.84046839661666 to -0.839794167228404
-0.884218451750956 to -0.8835442223627
Changing layer 1's weights from 
-0.879121539366923 to -0.878447309978667
-0.277952854705536 to -0.277278625317281
-0.403655593467438 to -0.402981364079183
-0.413560931754791 to -0.412886702366536
-0.598613714051925 to -0.597939484663669
-0.736851548028671 to -0.736177318640415
-0.466132824493133 to -0.465458595104878
-0.851935853434288 to -0.851261624046032
-0.440272872520172 to -0.439598643131917
-0.802919735384665 to -0.802245505996409
Changing layer 2's weights from 
0.00626849644116134 to 0.00694272582941709
-0.547784988952362 to -0.547110759564106
-0.263195698333465 to -0.26252146894521
-0.0919956017262573 to -0.0913213723380016
-0.269248311591827 to -0.268574082203572
-0.475257937980377 to -0.474583708592122
0.0435547780268552 to 0.044229007415111
-0.0288672495610356 to -0.0281930201727799
-0.760865544152938 to -0.760191314764682
-0.715908025575363 to -0.715233796187107
Changing layer 3's weights from 
-0.0267818022496347 to -0.0261075728613789
-0.290334885192596 to -0.289660655804341
-0.0477249670750736 to -0.0470507376868178
-0.955164198831216 to -0.95448996944296
-0.170400862765991 to -0.169726633377735
-0.300649468970978 to -0.299975239582723
-0.630834316087447 to -0.630160086699191
-0.172321443629944 to -0.171647214241688
-0.504017417503082 to -0.503343188114826
-0.0311875391728526 to -0.0305133097845968
Changing layer 4's weights from 
-0.130461340022766 to -0.12978711063451
-0.339294140411102 to -0.338619911022847
-0.539771740508758 to -0.539097511120502
-0.25283700711795 to -0.252162777729695
-0.492089812827789 to -0.491415583439533
-0.260624592376434 to -0.259950362988179
-0.26498312242099 to -0.264308893032735
-0.726880644632064 to -0.726206415243808
-0.133184557033264 to -0.132510327645008
-0.565486495567047 to -0.564812266178791
Changing layer 5's weights from 
0.0223184775584103 to 0.0229927069466661
-0.41975504643985 to -0.419080817051595
0.00543331615856943 to 0.00610754554682518
-0.930078865212881 to -0.929404635824625
-0.392810051513397 to -0.392135822125142
-0.791277994585716 to -0.79060376519746
-0.491312687469207 to -0.490638458080951
-0.418866460395538 to -0.418192231007283
-0.0292644549138186 to -0.0285902255255629
-0.0357725668675546 to -0.0350983374792989
Trying to learn from memory 7, 1, 0.52
sum 0.0385632294360253 distri 0.021142917956177
Using diff 0.00777950412084204 and condRate 0.166666666666667
Changed category 1 weights from 
0.28812925552506 to 0.288803479190802
0.521444842568822 to 0.522119066234564
0.0965196046747632 to 0.0971938283405058
0.10061241721291 to 0.101286640878653
Changing layer 0's weights from 
-0.222186767407124 to -0.221512543741381
-0.594113224096957 to -0.593439000431215
-0.828462489791576 to -0.827788266125834
-0.793903239913645 to -0.793229016247903
-0.0460356705862928 to -0.0453614469205502
-0.728677623816196 to -0.728003400150454
-0.574847900219623 to -0.574173676553881
-0.602302306242648 to -0.601628082576906
-0.839794167228404 to -0.839119943562662
-0.8835442223627 to -0.882869998696958
Changing layer 1's weights from 
-0.878447309978667 to -0.877773086312925
-0.277278625317281 to -0.276604401651538
-0.402981364079183 to -0.40230714041344
-0.412886702366536 to -0.412212478700793
-0.597939484663669 to -0.597265260997927
-0.736177318640415 to -0.735503094974673
-0.465458595104878 to -0.464784371439135
-0.851261624046032 to -0.85058740038029
-0.439598643131917 to -0.438924419466174
-0.802245505996409 to -0.801571282330667
Changing layer 2's weights from 
0.00694272582941709 to 0.00761694949515969
-0.547110759564106 to -0.546436535898364
-0.26252146894521 to -0.261847245279467
-0.0913213723380016 to -0.090647148672259
-0.268574082203572 to -0.267899858537829
-0.474583708592122 to -0.473909484926379
0.044229007415111 to 0.0449032310808536
-0.0281930201727799 to -0.0275187965070373
-0.760191314764682 to -0.75951709109894
-0.715233796187107 to -0.714559572521365
Changing layer 3's weights from 
-0.0261075728613789 to -0.0254333491956363
-0.289660655804341 to -0.288986432138598
-0.0470507376868178 to -0.0463765140210752
-0.95448996944296 to -0.953815745777218
-0.169726633377735 to -0.169052409711993
-0.299975239582723 to -0.29930101591698
-0.630160086699191 to -0.629485863033449
-0.171647214241688 to -0.170972990575946
-0.503343188114826 to -0.502668964449084
-0.0305133097845968 to -0.0298390861188542
Changing layer 4's weights from 
-0.12978711063451 to -0.129112886968768
-0.338619911022847 to -0.337945687357104
-0.539097511120502 to -0.53842328745476
-0.252162777729695 to -0.251488554063952
-0.491415583439533 to -0.490741359773791
-0.259950362988179 to -0.259276139322436
-0.264308893032735 to -0.263634669366992
-0.726206415243808 to -0.725532191578066
-0.132510327645008 to -0.131836103979266
-0.564812266178791 to -0.564138042513049
Changing layer 5's weights from 
0.0229927069466661 to 0.0236669306124087
-0.419080817051595 to -0.418406593385852
0.00610754554682518 to 0.00678176921256778
-0.929404635824625 to -0.928730412158883
-0.392135822125142 to -0.391461598459399
-0.79060376519746 to -0.789929541531718
-0.490638458080951 to -0.489964234415209
-0.418192231007283 to -0.41751800734154
-0.0285902255255629 to -0.0279160018598203
-0.0350983374792989 to -0.0344241138135563
10/5/2016 1:37:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:37:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:05 PMStarting learning phase with deltaScore: 1.2
Modified index 0's learning in memoryPool to 0.24
Modified index 1's learning in memoryPool to 0.24
Modified index 2's learning in memoryPool to 0.24
Modified index 3's learning in memoryPool to 0.24
Modified index 4's learning in memoryPool to 0.24
Modified index 5's learning in memoryPool to 0.24
Modified index 6's learning in memoryPool to 0.24
Modified index 7's learning in memoryPool to 0.24
10/5/2016 1:38:05 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 8, 1, 0.24
sum 0.0385756701015214 distri 0.0212196025252722
Using diff 0.00771215005086885 and condRate 0.166666666666667
Changed category 1 weights from 
0.288803479190802 to 0.289111965205095
0.522119066234564 to 0.522427552248857
0.0971938283405058 to 0.0975023143547987
0.101286640878653 to 0.101595126892946
Changing layer 0's weights from 
-0.221512543741381 to -0.221204057727088
-0.593439000431215 to -0.593130514416922
-0.827788266125834 to -0.827479780111541
-0.793229016247903 to -0.79292053023361
-0.0453614469205502 to -0.0450529609062573
-0.728003400150454 to -0.727694914136161
-0.574173676553881 to -0.573865190539588
-0.601628082576906 to -0.601319596562613
-0.839119943562662 to -0.838811457548369
-0.882869998696958 to -0.882561512682665
Changing layer 1's weights from 
-0.877773086312925 to -0.877464600298632
-0.276604401651538 to -0.276295915637245
-0.40230714041344 to -0.401998654399147
-0.412212478700793 to -0.4119039926865
-0.597265260997927 to -0.596956774983634
-0.735503094974673 to -0.73519460896038
-0.464784371439135 to -0.464475885424842
-0.85058740038029 to -0.850278914365997
-0.438924419466174 to -0.438615933451881
-0.801571282330667 to -0.801262796316374
Changing layer 2's weights from 
0.00761694949515969 to 0.00792543550945257
-0.546436535898364 to -0.546128049884071
-0.261847245279467 to -0.261538759265174
-0.090647148672259 to -0.0903386626579661
-0.267899858537829 to -0.267591372523536
-0.473909484926379 to -0.473600998912086
0.0449032310808536 to 0.0452117170951465
-0.0275187965070373 to -0.0272103104927444
-0.75951709109894 to -0.759208605084647
-0.714559572521365 to -0.714251086507072
Changing layer 3's weights from 
-0.0254333491956363 to -0.0251248631813434
-0.288986432138598 to -0.288677946124305
-0.0463765140210752 to -0.0460680280067823
-0.953815745777218 to -0.953507259762925
-0.169052409711993 to -0.1687439236977
-0.29930101591698 to -0.298992529902687
-0.629485863033449 to -0.629177377019156
-0.170972990575946 to -0.170664504561653
-0.502668964449084 to -0.502360478434791
-0.0298390861188542 to -0.0295306001045613
Changing layer 4's weights from 
-0.129112886968768 to -0.128804400954475
-0.337945687357104 to -0.337637201342811
-0.53842328745476 to -0.538114801440467
-0.251488554063952 to -0.251180068049659
-0.490741359773791 to -0.490432873759498
-0.259276139322436 to -0.258967653308143
-0.263634669366992 to -0.263326183352699
-0.725532191578066 to -0.725223705563773
-0.131836103979266 to -0.131527617964973
-0.564138042513049 to -0.563829556498756
Changing layer 5's weights from 
0.0236669306124087 to 0.0239754166267016
-0.418406593385852 to -0.418098107371559
0.00678176921256778 to 0.00709025522686067
-0.928730412158883 to -0.92842192614459
-0.391461598459399 to -0.391153112445106
-0.789929541531718 to -0.789621055517425
-0.489964234415209 to -0.489655748400916
-0.41751800734154 to -0.417209521327247
-0.0279160018598203 to -0.0276075158455274
-0.0344241138135563 to -0.0341156277992634
Trying to learn from memory 9, 0, 0.24
sum 0.0385732461904812 distri -0.0211948337290851
Using diff 0.0501247683719459 and condRate 0.166666666666667
Changed category 0 weights from 
0.154190244252174 to 0.156195235066723
-0.245844719832451 to -0.243839729017902
-0.528365416353257 to -0.526360425538708
-0.376860050385506 to -0.374855059570957
Changing layer 0's weights from 
-0.221204057727088 to -0.219199066912539
-0.593130514416922 to -0.591125523602373
-0.827479780111541 to -0.825474789296992
-0.79292053023361 to -0.790915539419061
-0.0450529609062573 to -0.0430479700917083
-0.727694914136161 to -0.725689923321612
-0.573865190539588 to -0.571860199725039
-0.601319596562613 to -0.599314605748064
-0.838811457548369 to -0.83680646673382
-0.882561512682665 to -0.880556521868116
Changing layer 1's weights from 
-0.877464600298632 to -0.875459609484083
-0.276295915637245 to -0.274290924822696
-0.401998654399147 to -0.399993663584598
-0.4119039926865 to -0.409899001871951
-0.596956774983634 to -0.594951784169085
-0.73519460896038 to -0.733189618145831
-0.464475885424842 to -0.462470894610293
-0.850278914365997 to -0.848273923551448
-0.438615933451881 to -0.436610942637332
-0.801262796316374 to -0.799257805501825
Changing layer 2's weights from 
0.00792543550945257 to 0.00993042632400159
-0.546128049884071 to -0.544123059069522
-0.261538759265174 to -0.259533768450625
-0.0903386626579661 to -0.0883336718434171
-0.267591372523536 to -0.265586381708987
-0.473600998912086 to -0.471596008097537
0.0452117170951465 to 0.0472167079096955
-0.0272103104927444 to -0.0252053196781954
-0.759208605084647 to -0.757203614270098
-0.714251086507072 to -0.712246095692523
Changing layer 3's weights from 
-0.0251248631813434 to -0.0231198723667944
-0.288677946124305 to -0.286672955309756
-0.0460680280067823 to -0.0440630371922333
-0.953507259762925 to -0.951502268948376
-0.1687439236977 to -0.166738932883151
-0.298992529902687 to -0.296987539088138
-0.629177377019156 to -0.627172386204607
-0.170664504561653 to -0.168659513747104
-0.502360478434791 to -0.500355487620242
-0.0295306001045613 to -0.0275256092900123
Changing layer 4's weights from 
-0.128804400954475 to -0.126799410139926
-0.337637201342811 to -0.335632210528262
-0.538114801440467 to -0.536109810625918
-0.251180068049659 to -0.24917507723511
-0.490432873759498 to -0.488427882944949
-0.258967653308143 to -0.256962662493594
-0.263326183352699 to -0.26132119253815
-0.725223705563773 to -0.723218714749224
-0.131527617964973 to -0.129522627150424
-0.563829556498756 to -0.561824565684207
Changing layer 5's weights from 
0.0239754166267016 to 0.0259804074412506
-0.418098107371559 to -0.41609311655701
0.00709025522686067 to 0.00909524604140968
-0.92842192614459 to -0.926416935330041
-0.391153112445106 to -0.389148121630557
-0.789621055517425 to -0.787616064702876
-0.489655748400916 to -0.487650757586367
-0.417209521327247 to -0.415204530512698
-0.0276075158455274 to -0.0256025250309784
-0.0341156277992634 to -0.0321106369847144
Trying to learn from memory 9, 0, 0.24
sum 0.0385732461904812 distri -0.0211948337290851
Using diff 0.0501247683719459 and condRate 0.166666666666667
Changed category 0 weights from 
0.156195235066723 to 0.158200225881272
-0.243839729017902 to -0.241834738203353
-0.526360425538708 to -0.524355434724159
-0.374855059570957 to -0.372850068756408
Changing layer 0's weights from 
-0.219199066912539 to -0.21719407609799
-0.591125523602373 to -0.589120532787824
-0.825474789296992 to -0.823469798482443
-0.790915539419061 to -0.788910548604512
-0.0430479700917083 to -0.0410429792771593
-0.725689923321612 to -0.723684932507063
-0.571860199725039 to -0.56985520891049
-0.599314605748064 to -0.597309614933515
-0.83680646673382 to -0.834801475919271
-0.880556521868116 to -0.878551531053567
Changing layer 1's weights from 
-0.875459609484083 to -0.873454618669534
-0.274290924822696 to -0.272285934008147
-0.399993663584598 to -0.397988672770049
-0.409899001871951 to -0.407894011057402
-0.594951784169085 to -0.592946793354536
-0.733189618145831 to -0.731184627331282
-0.462470894610293 to -0.460465903795744
-0.848273923551448 to -0.846268932736899
-0.436610942637332 to -0.434605951822783
-0.799257805501825 to -0.797252814687276
Changing layer 2's weights from 
0.00993042632400159 to 0.0119354171385506
-0.544123059069522 to -0.542118068254973
-0.259533768450625 to -0.257528777636076
-0.0883336718434171 to -0.0863286810288681
-0.265586381708987 to -0.263581390894438
-0.471596008097537 to -0.469591017282988
0.0472167079096955 to 0.0492216987242445
-0.0252053196781954 to -0.0232003288636464
-0.757203614270098 to -0.755198623455549
-0.712246095692523 to -0.710241104877974
Changing layer 3's weights from 
-0.0231198723667944 to -0.0211148815522454
-0.286672955309756 to -0.284667964495207
-0.0440630371922333 to -0.0420580463776843
-0.951502268948376 to -0.949497278133827
-0.166738932883151 to -0.164733942068602
-0.296987539088138 to -0.294982548273589
-0.627172386204607 to -0.625167395390058
-0.168659513747104 to -0.166654522932555
-0.500355487620242 to -0.498350496805693
-0.0275256092900123 to -0.0255206184754633
Changing layer 4's weights from 
-0.126799410139926 to -0.124794419325377
-0.335632210528262 to -0.333627219713713
-0.536109810625918 to -0.534104819811369
-0.24917507723511 to -0.247170086420561
-0.488427882944949 to -0.4864228921304
-0.256962662493594 to -0.254957671679045
-0.26132119253815 to -0.259316201723601
-0.723218714749224 to -0.721213723934675
-0.129522627150424 to -0.127517636335875
-0.561824565684207 to -0.559819574869658
Changing layer 5's weights from 
0.0259804074412506 to 0.0279853982557996
-0.41609311655701 to -0.414088125742461
0.00909524604140968 to 0.0111002368559587
-0.926416935330041 to -0.924411944515492
-0.389148121630557 to -0.387143130816008
-0.787616064702876 to -0.785611073888327
-0.487650757586367 to -0.485645766771818
-0.415204530512698 to -0.413199539698149
-0.0256025250309784 to -0.0235975342164294
-0.0321106369847144 to -0.0301056461701654
Trying to learn from memory 9, 0, 0.24
sum 0.0385732461904812 distri -0.0211948337290851
Using diff 0.0501247683719459 and condRate 0.166666666666667
Changed category 0 weights from 
0.158200225881272 to 0.160205216695821
-0.241834738203353 to -0.239829747388804
-0.524355434724159 to -0.52235044390961
-0.372850068756408 to -0.370845077941859
Changing layer 0's weights from 
-0.21719407609799 to -0.215189085283441
-0.589120532787824 to -0.587115541973275
-0.823469798482443 to -0.821464807667894
-0.788910548604512 to -0.786905557789963
-0.0410429792771593 to -0.0390379884626103
-0.723684932507063 to -0.721679941692514
-0.56985520891049 to -0.567850218095941
-0.597309614933515 to -0.595304624118966
-0.834801475919271 to -0.832796485104722
-0.878551531053567 to -0.876546540239018
Changing layer 1's weights from 
-0.873454618669534 to -0.871449627854985
-0.272285934008147 to -0.270280943193598
-0.397988672770049 to -0.3959836819555
-0.407894011057402 to -0.405889020242853
-0.592946793354536 to -0.590941802539987
-0.731184627331282 to -0.729179636516733
-0.460465903795744 to -0.458460912981195
-0.846268932736899 to -0.84426394192235
-0.434605951822783 to -0.432600961008234
-0.797252814687276 to -0.795247823872727
Changing layer 2's weights from 
0.0119354171385506 to 0.0139404079530996
-0.542118068254973 to -0.540113077440424
-0.257528777636076 to -0.255523786821527
-0.0863286810288681 to -0.084323690214319
-0.263581390894438 to -0.261576400079889
-0.469591017282988 to -0.467586026468439
0.0492216987242445 to 0.0512266895387935
-0.0232003288636464 to -0.0211953380490974
-0.755198623455549 to -0.753193632641
-0.710241104877974 to -0.708236114063425
Changing layer 3's weights from 
-0.0211148815522454 to -0.0191098907376964
-0.284667964495207 to -0.282662973680658
-0.0420580463776843 to -0.0400530555631353
-0.949497278133827 to -0.947492287319278
-0.164733942068602 to -0.162728951254053
-0.294982548273589 to -0.29297755745904
-0.625167395390058 to -0.623162404575509
-0.166654522932555 to -0.164649532118006
-0.498350496805693 to -0.496345505991144
-0.0255206184754633 to -0.0235156276609143
Changing layer 4's weights from 
-0.124794419325377 to -0.122789428510828
-0.333627219713713 to -0.331622228899164
-0.534104819811369 to -0.53209982899682
-0.247170086420561 to -0.245165095606012
-0.4864228921304 to -0.484417901315851
-0.254957671679045 to -0.252952680864496
-0.259316201723601 to -0.257311210909052
-0.721213723934675 to -0.719208733120126
-0.127517636335875 to -0.125512645521326
-0.559819574869658 to -0.557814584055109
Changing layer 5's weights from 
0.0279853982557996 to 0.0299903890703486
-0.414088125742461 to -0.412083134927912
0.0111002368559587 to 0.0131052276705077
-0.924411944515492 to -0.922406953700943
-0.387143130816008 to -0.385138140001459
-0.785611073888327 to -0.783606083073778
-0.485645766771818 to -0.483640775957269
-0.413199539698149 to -0.4111945488836
-0.0235975342164294 to -0.0215925434018804
-0.0301056461701654 to -0.0281006553556164
Trying to learn from memory 9, 1, 0.24
sum 0.0385732461904812 distri 0.0212185301728318
Using diff 0.00771140447002905 and condRate 0.166666666666667
Changed category 1 weights from 
0.289111965205095 to 0.289420421396153
0.522427552248857 to 0.522736008439915
0.0975023143547987 to 0.0978107705458568
0.101595126892946 to 0.101903583084004
Changing layer 0's weights from 
-0.215189085283441 to -0.214880629092383
-0.587115541973275 to -0.586807085782217
-0.821464807667894 to -0.821156351476836
-0.786905557789963 to -0.786597101598905
-0.0390379884626103 to -0.0387295322715522
-0.721679941692514 to -0.721371485501456
-0.567850218095941 to -0.567541761904883
-0.595304624118966 to -0.594996167927908
-0.832796485104722 to -0.832488028913664
-0.876546540239018 to -0.87623808404796
Changing layer 1's weights from 
-0.871449627854985 to -0.871141171663927
-0.270280943193598 to -0.26997248700254
-0.3959836819555 to -0.395675225764442
-0.405889020242853 to -0.405580564051795
-0.590941802539987 to -0.590633346348929
-0.729179636516733 to -0.728871180325675
-0.458460912981195 to -0.458152456790137
-0.84426394192235 to -0.843955485731292
-0.432600961008234 to -0.432292504817176
-0.795247823872727 to -0.794939367681669
Changing layer 2's weights from 
0.0139404079530996 to 0.0142488641441577
-0.540113077440424 to -0.539804621249366
-0.255523786821527 to -0.255215330630469
-0.084323690214319 to -0.0840152340232609
-0.261576400079889 to -0.261267943888831
-0.467586026468439 to -0.467277570277381
0.0512266895387935 to 0.0515351457298516
-0.0211953380490974 to -0.0208868818580393
-0.753193632641 to -0.752885176449942
-0.708236114063425 to -0.707927657872367
Changing layer 3's weights from 
-0.0191098907376964 to -0.0188014345466383
-0.282662973680658 to -0.2823545174896
-0.0400530555631353 to -0.0397445993720772
-0.947492287319278 to -0.94718383112822
-0.162728951254053 to -0.162420495062995
-0.29297755745904 to -0.292669101267982
-0.623162404575509 to -0.622853948384451
-0.164649532118006 to -0.164341075926948
-0.496345505991144 to -0.496037049800086
-0.0235156276609143 to -0.0232071714698562
Changing layer 4's weights from 
-0.122789428510828 to -0.12248097231977
-0.331622228899164 to -0.331313772708106
-0.53209982899682 to -0.531791372805762
-0.245165095606012 to -0.244856639414954
-0.484417901315851 to -0.484109445124793
-0.252952680864496 to -0.252644224673438
-0.257311210909052 to -0.257002754717994
-0.719208733120126 to -0.718900276929068
-0.125512645521326 to -0.125204189330268
-0.557814584055109 to -0.557506127864051
Changing layer 5's weights from 
0.0299903890703486 to 0.0302988452614067
-0.412083134927912 to -0.411774678736854
0.0131052276705077 to 0.0134136838615658
-0.922406953700943 to -0.922098497509885
-0.385138140001459 to -0.384829683810401
-0.783606083073778 to -0.78329762688272
-0.483640775957269 to -0.483332319766211
-0.4111945488836 to -0.410886092692542
-0.0215925434018804 to -0.0212840872108223
-0.0281006553556164 to -0.0277921991645583
Trying to learn from memory 9, 0, 0.24
sum 0.0385732461904812 distri -0.0211948337290851
Using diff 0.0501247683719459 and condRate 0.166666666666667
Changed category 0 weights from 
0.160205216695821 to 0.16221020751037
-0.239829747388804 to -0.237824756574255
-0.52235044390961 to -0.520345453095061
-0.370845077941859 to -0.36884008712731
Changing layer 0's weights from 
-0.214880629092383 to -0.212875638277834
-0.586807085782217 to -0.584802094967668
-0.821156351476836 to -0.819151360662287
-0.786597101598905 to -0.784592110784356
-0.0387295322715522 to -0.0367245414570032
-0.721371485501456 to -0.719366494686907
-0.567541761904883 to -0.565536771090334
-0.594996167927908 to -0.592991177113359
-0.832488028913664 to -0.830483038099115
-0.87623808404796 to -0.874233093233411
Changing layer 1's weights from 
-0.871141171663927 to -0.869136180849378
-0.26997248700254 to -0.267967496187991
-0.395675225764442 to -0.393670234949893
-0.405580564051795 to -0.403575573237246
-0.590633346348929 to -0.58862835553438
-0.728871180325675 to -0.726866189511126
-0.458152456790137 to -0.456147465975588
-0.843955485731292 to -0.841950494916743
-0.432292504817176 to -0.430287514002627
-0.794939367681669 to -0.79293437686712
Changing layer 2's weights from 
0.0142488641441577 to 0.0162538549587067
-0.539804621249366 to -0.537799630434817
-0.255215330630469 to -0.25321033981592
-0.0840152340232609 to -0.0820102432087119
-0.261267943888831 to -0.259262953074282
-0.467277570277381 to -0.465272579462832
0.0515351457298516 to 0.0535401365444006
-0.0208868818580393 to -0.0188818910434903
-0.752885176449942 to -0.750880185635393
-0.707927657872367 to -0.705922667057818
Changing layer 3's weights from 
-0.0188014345466383 to -0.0167964437320893
-0.2823545174896 to -0.280349526675051
-0.0397445993720772 to -0.0377396085575282
-0.94718383112822 to -0.945178840313671
-0.162420495062995 to -0.160415504248446
-0.292669101267982 to -0.290664110453433
-0.622853948384451 to -0.620848957569902
-0.164341075926948 to -0.162336085112399
-0.496037049800086 to -0.494032058985537
-0.0232071714698562 to -0.0212021806553072
Changing layer 4's weights from 
-0.12248097231977 to -0.120475981505221
-0.331313772708106 to -0.329308781893557
-0.531791372805762 to -0.529786381991213
-0.244856639414954 to -0.242851648600405
-0.484109445124793 to -0.482104454310244
-0.252644224673438 to -0.250639233858889
-0.257002754717994 to -0.254997763903445
-0.718900276929068 to -0.716895286114519
-0.125204189330268 to -0.123199198515719
-0.557506127864051 to -0.555501137049502
Changing layer 5's weights from 
0.0302988452614067 to 0.0323038360759557
-0.411774678736854 to -0.409769687922305
0.0134136838615658 to 0.0154186746761148
-0.922098497509885 to -0.920093506695336
-0.384829683810401 to -0.382824692995852
-0.78329762688272 to -0.781292636068171
-0.483332319766211 to -0.481327328951662
-0.410886092692542 to -0.408881101877993
-0.0212840872108223 to -0.0192790963962733
-0.0277921991645583 to -0.0257872083500093
Trying to learn from memory 9, 1, 0.24
sum 0.0385732461904812 distri 0.0212185301728318
Using diff 0.00771140447002905 and condRate 0.166666666666667
Changed category 1 weights from 
0.289420421396153 to 0.289728877587211
0.522736008439915 to 0.523044464630973
0.0978107705458568 to 0.0981192267369149
0.101903583084004 to 0.102212039275062
Changing layer 0's weights from 
-0.212875638277834 to -0.212567182086776
-0.584802094967668 to -0.58449363877661
-0.819151360662287 to -0.818842904471229
-0.784592110784356 to -0.784283654593298
-0.0367245414570032 to -0.0364160852659451
-0.719366494686907 to -0.719058038495849
-0.565536771090334 to -0.565228314899276
-0.592991177113359 to -0.592682720922301
-0.830483038099115 to -0.830174581908057
-0.874233093233411 to -0.873924637042353
Changing layer 1's weights from 
-0.869136180849378 to -0.86882772465832
-0.267967496187991 to -0.267659039996933
-0.393670234949893 to -0.393361778758835
-0.403575573237246 to -0.403267117046188
-0.58862835553438 to -0.588319899343322
-0.726866189511126 to -0.726557733320068
-0.456147465975588 to -0.45583900978453
-0.841950494916743 to -0.841642038725685
-0.430287514002627 to -0.429979057811569
-0.79293437686712 to -0.792625920676062
Changing layer 2's weights from 
0.0162538549587067 to 0.0165623111497648
-0.537799630434817 to -0.537491174243759
-0.25321033981592 to -0.252901883624862
-0.0820102432087119 to -0.0817017870176538
-0.259262953074282 to -0.258954496883224
-0.465272579462832 to -0.464964123271774
0.0535401365444006 to 0.0538485927354587
-0.0188818910434903 to -0.0185734348524322
-0.750880185635393 to -0.750571729444335
-0.705922667057818 to -0.70561421086676
Changing layer 3's weights from 
-0.0167964437320893 to -0.0164879875410312
-0.280349526675051 to -0.280041070483993
-0.0377396085575282 to -0.0374311523664701
-0.945178840313671 to -0.944870384122613
-0.160415504248446 to -0.160107048057388
-0.290664110453433 to -0.290355654262375
-0.620848957569902 to -0.620540501378844
-0.162336085112399 to -0.162027628921341
-0.494032058985537 to -0.493723602794479
-0.0212021806553072 to -0.0208937244642491
Changing layer 4's weights from 
-0.120475981505221 to -0.120167525314163
-0.329308781893557 to -0.329000325702499
-0.529786381991213 to -0.529477925800155
-0.242851648600405 to -0.242543192409347
-0.482104454310244 to -0.481795998119186
-0.250639233858889 to -0.250330777667831
-0.254997763903445 to -0.254689307712387
-0.716895286114519 to -0.716586829923461
-0.123199198515719 to -0.122890742324661
-0.555501137049502 to -0.555192680858444
Changing layer 5's weights from 
0.0323038360759557 to 0.0326122922670138
-0.409769687922305 to -0.409461231731247
0.0154186746761148 to 0.0157271308671729
-0.920093506695336 to -0.919785050504278
-0.382824692995852 to -0.382516236804794
-0.781292636068171 to -0.780984179877113
-0.481327328951662 to -0.481018872760604
-0.408881101877993 to -0.408572645686935
-0.0192790963962733 to -0.0189706402052151
-0.0257872083500093 to -0.0254787521589512
Trying to learn from memory 10, 1, 0.24
sum 0.0385732461904812 distri 0.0212185301728318
Using diff 0.00771140447002905 and condRate 0.166666666666667
Changed category 1 weights from 
0.289728877587211 to 0.290037333778269
0.523044464630973 to 0.523352920822031
0.0981192267369149 to 0.098427682927973
0.102212039275062 to 0.10252049546612
Changing layer 0's weights from 
-0.212567182086776 to -0.212258725895718
-0.58449363877661 to -0.584185182585552
-0.818842904471229 to -0.818534448280171
-0.784283654593298 to -0.78397519840224
-0.0364160852659451 to -0.036107629074887
-0.719058038495849 to -0.718749582304791
-0.565228314899276 to -0.564919858708218
-0.592682720922301 to -0.592374264731243
-0.830174581908057 to -0.829866125716999
-0.873924637042353 to -0.873616180851295
Changing layer 1's weights from 
-0.86882772465832 to -0.868519268467262
-0.267659039996933 to -0.267350583805875
-0.393361778758835 to -0.393053322567777
-0.403267117046188 to -0.40295866085513
-0.588319899343322 to -0.588011443152264
-0.726557733320068 to -0.72624927712901
-0.45583900978453 to -0.455530553593472
-0.841642038725685 to -0.841333582534627
-0.429979057811569 to -0.429670601620511
-0.792625920676062 to -0.792317464485004
Changing layer 2's weights from 
0.0165623111497648 to 0.0168707673408229
-0.537491174243759 to -0.537182718052701
-0.252901883624862 to -0.252593427433804
-0.0817017870176538 to -0.0813933308265957
-0.258954496883224 to -0.258646040692166
-0.464964123271774 to -0.464655667080716
0.0538485927354587 to 0.0541570489265169
-0.0185734348524322 to -0.018264978661374
-0.750571729444335 to -0.750263273253277
-0.70561421086676 to -0.705305754675702
Changing layer 3's weights from 
-0.0164879875410312 to -0.0161795313499731
-0.280041070483993 to -0.279732614292935
-0.0374311523664701 to -0.037122696175412
-0.944870384122613 to -0.944561927931555
-0.160107048057388 to -0.15979859186633
-0.290355654262375 to -0.290047198071317
-0.620540501378844 to -0.620232045187786
-0.162027628921341 to -0.161719172730283
-0.493723602794479 to -0.49341514660342
-0.0208937244642491 to -0.0205852682731909
Changing layer 4's weights from 
-0.120167525314163 to -0.119859069123105
-0.329000325702499 to -0.328691869511441
-0.529477925800155 to -0.529169469609097
-0.242543192409347 to -0.242234736218289
-0.481795998119186 to -0.481487541928127
-0.250330777667831 to -0.250022321476773
-0.254689307712387 to -0.254380851521329
-0.716586829923461 to -0.716278373732403
-0.122890742324661 to -0.122582286133603
-0.555192680858444 to -0.554884224667386
Changing layer 5's weights from 
0.0326122922670138 to 0.0329207484580719
-0.409461231731247 to -0.409152775540189
0.0157271308671729 to 0.016035587058231
-0.919785050504278 to -0.91947659431322
-0.382516236804794 to -0.382207780613736
-0.780984179877113 to -0.780675723686055
-0.481018872760604 to -0.480710416569546
-0.408572645686935 to -0.408264189495877
-0.0189706402052151 to -0.018662184014157
-0.0254787521589512 to -0.025170295967893
10/5/2016 1:38:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:45 PMStarting learning phase with deltaScore: 1.866667
Modified index 0's learning in memoryPool to 0.3733333
Modified index 1's learning in memoryPool to 0.3733333
Modified index 2's learning in memoryPool to 0.3733333
Modified index 3's learning in memoryPool to 0.3733333
Modified index 4's learning in memoryPool to 0.3733333
Modified index 5's learning in memoryPool to 0.3733333
Modified index 6's learning in memoryPool to 0.3733333
Modified index 7's learning in memoryPool to 0.3733333
Modified index 8's learning in memoryPool to 0.3733333
Modified index 9's learning in memoryPool to 0.3733333
Modified index 10's learning in memoryPool to 0.3733333
Modified index 11's learning in memoryPool to 0.3733333
Modified index 12's learning in memoryPool to 0.3733333
Modified index 13's learning in memoryPool to 0.3733333
Modified index 14's learning in memoryPool to 0.3733333
10/5/2016 1:38:45 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 11, 1, 0.3733333
sum 0.0386781845564306 distri 0.0209401658764524
Using diff 0.00806847254087053 and condRate 0.166666666666667
Changed category 1 weights from 
0.290037333778269 to 0.290539372071839
0.523352920822031 to 0.523854959115601
0.098427682927973 to 0.0989297212215424
0.10252049546612 to 0.103022533759689
Changing layer 0's weights from 
-0.212258725895718 to -0.211756687602148
-0.584185182585552 to -0.583683144291982
-0.818534448280171 to -0.818032409986601
-0.78397519840224 to -0.78347316010867
-0.036107629074887 to -0.0356055907813176
-0.718749582304791 to -0.718247544011221
-0.564919858708218 to -0.564417820414648
-0.592374264731243 to -0.591872226437673
-0.829866125716999 to -0.829364087423429
-0.873616180851295 to -0.873114142557725
Changing layer 1's weights from 
-0.868519268467262 to -0.868017230173692
-0.267350583805875 to -0.266848545512305
-0.393053322567777 to -0.392551284274207
-0.40295866085513 to -0.40245662256156
-0.588011443152264 to -0.587509404858694
-0.72624927712901 to -0.72574723883544
-0.455530553593472 to -0.455028515299902
-0.841333582534627 to -0.840831544241057
-0.429670601620511 to -0.429168563326941
-0.792317464485004 to -0.791815426191434
Changing layer 2's weights from 
0.0168707673408229 to 0.0173728056343923
-0.537182718052701 to -0.536680679759131
-0.252593427433804 to -0.252091389140234
-0.0813933308265957 to -0.0808912925330263
-0.258646040692166 to -0.258144002398596
-0.464655667080716 to -0.464153628787146
0.0541570489265169 to 0.0546590872200862
-0.018264978661374 to -0.0177629403678047
-0.750263273253277 to -0.749761234959707
-0.705305754675702 to -0.704803716382132
Changing layer 3's weights from 
-0.0161795313499731 to -0.0156774930564037
-0.279732614292935 to -0.279230575999365
-0.037122696175412 to -0.0366206578818426
-0.944561927931555 to -0.944059889637985
-0.15979859186633 to -0.15929655357276
-0.290047198071317 to -0.289545159777747
-0.620232045187786 to -0.619730006894216
-0.161719172730283 to -0.161217134436713
-0.49341514660342 to -0.492913108309851
-0.0205852682731909 to -0.0200832299796216
Changing layer 4's weights from 
-0.119859069123105 to -0.119357030829535
-0.328691869511441 to -0.328189831217871
-0.529169469609097 to -0.528667431315527
-0.242234736218289 to -0.241732697924719
-0.481487541928127 to -0.480985503634558
-0.250022321476773 to -0.249520283183203
-0.254380851521329 to -0.253878813227759
-0.716278373732403 to -0.715776335438833
-0.122582286133603 to -0.122080247840033
-0.554884224667386 to -0.554382186373816
Changing layer 5's weights from 
0.0329207484580719 to 0.0334227867516413
-0.409152775540189 to -0.408650737246619
0.016035587058231 to 0.0165376253518004
-0.91947659431322 to -0.91897455601965
-0.382207780613736 to -0.381705742320166
-0.780675723686055 to -0.780173685392485
-0.480710416569546 to -0.480208378275976
-0.408264189495877 to -0.407762151202307
-0.018662184014157 to -0.0181601457205877
-0.025170295967893 to -0.0246682576743237
Trying to learn from memory 12, 1, 0.3733333
sum 0.0386781839857946 distri 0.0209401660634952
Using diff 0.00806847192585076 and condRate 0.166666666666667
Changed category 1 weights from 
0.290539372071839 to 0.29104141032714
0.523854959115601 to 0.524356997370902
0.0989297212215424 to 0.0994317594768439
0.103022533759689 to 0.103524572014991
Changing layer 0's weights from 
-0.211756687602148 to -0.211254649346847
-0.583683144291982 to -0.583181106036681
-0.818032409986601 to -0.8175303717313
-0.78347316010867 to -0.782971121853369
-0.0356055907813176 to -0.0351035525260161
-0.718247544011221 to -0.71774550575592
-0.564417820414648 to -0.563915782159347
-0.591872226437673 to -0.591370188182372
-0.829364087423429 to -0.828862049168128
-0.873114142557725 to -0.872612104302424
Changing layer 1's weights from 
-0.868017230173692 to -0.867515191918391
-0.266848545512305 to -0.266346507257004
-0.392551284274207 to -0.392049246018906
-0.40245662256156 to -0.401954584306259
-0.587509404858694 to -0.587007366603393
-0.72574723883544 to -0.725245200580139
-0.455028515299902 to -0.454526477044601
-0.840831544241057 to -0.840329505985756
-0.429168563326941 to -0.42866652507164
-0.791815426191434 to -0.791313387936133
Changing layer 2's weights from 
0.0173728056343923 to 0.0178748438896938
-0.536680679759131 to -0.53617864150383
-0.252091389140234 to -0.251589350884933
-0.0808912925330263 to -0.0803892542777249
-0.258144002398596 to -0.257641964143295
-0.464153628787146 to -0.463651590531845
0.0546590872200862 to 0.0551611254753877
-0.0177629403678047 to -0.0172609021125032
-0.749761234959707 to -0.749259196704406
-0.704803716382132 to -0.704301678126831
Changing layer 3's weights from 
-0.0156774930564037 to -0.0151754548011022
-0.279230575999365 to -0.278728537744064
-0.0366206578818426 to -0.0361186196265411
-0.944059889637985 to -0.943557851382684
-0.15929655357276 to -0.158794515317459
-0.289545159777747 to -0.289043121522446
-0.619730006894216 to -0.619227968638915
-0.161217134436713 to -0.160715096181412
-0.492913108309851 to -0.49241107005455
-0.0200832299796216 to -0.0195811917243201
Changing layer 4's weights from 
-0.119357030829535 to -0.118854992574234
-0.328189831217871 to -0.32768779296257
-0.528667431315527 to -0.528165393060226
-0.241732697924719 to -0.241230659669418
-0.480985503634558 to -0.480483465379257
-0.249520283183203 to -0.249018244927902
-0.253878813227759 to -0.253376774972458
-0.715776335438833 to -0.715274297183532
-0.122080247840033 to -0.121578209584732
-0.554382186373816 to -0.553880148118515
Changing layer 5's weights from 
0.0334227867516413 to 0.0339248250069428
-0.408650737246619 to -0.408148698991318
0.0165376253518004 to 0.0170396636071019
-0.91897455601965 to -0.918472517764349
-0.381705742320166 to -0.381203704064865
-0.780173685392485 to -0.779671647137184
-0.480208378275976 to -0.479706340020675
-0.407762151202307 to -0.407260112947006
-0.0181601457205877 to -0.0176581074652862
-0.0246682576743237 to -0.0241662194190222
Trying to learn from memory 13, 0, 0.3733333
sum 0.0386782843758121 distri -0.0201320252620538
Using diff 0.0491407385439128 and condRate 0.166666666666667
Changed category 0 weights from 
0.16221020751037 to 0.165267853477231
-0.237824756574255 to -0.234767110607394
-0.520345453095061 to -0.5172878071282
-0.36884008712731 to -0.365782441160449
Changing layer 0's weights from 
-0.211254649346847 to -0.208197003379986
-0.583181106036681 to -0.580123460069819
-0.8175303717313 to -0.814472725764438
-0.782971121853369 to -0.779913475886508
-0.0351035525260161 to -0.0320459065591548
-0.71774550575592 to -0.714687859789058
-0.563915782159347 to -0.560858136192485
-0.591370188182372 to -0.58831254221551
-0.828862049168128 to -0.825804403201266
-0.872612104302424 to -0.869554458335562
Changing layer 1's weights from 
-0.867515191918391 to -0.864457545951529
-0.266346507257004 to -0.263288861290143
-0.392049246018906 to -0.388991600052045
-0.401954584306259 to -0.398896938339397
-0.587007366603393 to -0.583949720636532
-0.725245200580139 to -0.722187554613277
-0.454526477044601 to -0.45146883107774
-0.840329505985756 to -0.837271860018894
-0.42866652507164 to -0.425608879104779
-0.791313387936133 to -0.788255741969272
Changing layer 2's weights from 
0.0178748438896938 to 0.0209324898565551
-0.53617864150383 to -0.533120995536968
-0.251589350884933 to -0.248531704918072
-0.0803892542777249 to -0.0773316083108636
-0.257641964143295 to -0.254584318176434
-0.463651590531845 to -0.460593944564984
0.0551611254753877 to 0.058218771442249
-0.0172609021125032 to -0.0142032561456419
-0.749259196704406 to -0.746201550737544
-0.704301678126831 to -0.701244032159969
Changing layer 3's weights from 
-0.0151754548011022 to -0.0121178088342409
-0.278728537744064 to -0.275670891777203
-0.0361186196265411 to -0.0330609736596798
-0.943557851382684 to -0.940500205415822
-0.158794515317459 to -0.155736869350597
-0.289043121522446 to -0.285985475555585
-0.619227968638915 to -0.616170322672053
-0.160715096181412 to -0.15765745021455
-0.49241107005455 to -0.489353424087688
-0.0195811917243201 to -0.0165235457574588
Changing layer 4's weights from 
-0.118854992574234 to -0.115797346607372
-0.32768779296257 to -0.324630146995709
-0.528165393060226 to -0.525107747093365
-0.241230659669418 to -0.238173013702557
-0.480483465379257 to -0.477425819412395
-0.249018244927902 to -0.245960598961041
-0.253376774972458 to -0.250319129005597
-0.715274297183532 to -0.71221665121667
-0.121578209584732 to -0.11852056361787
-0.553880148118515 to -0.550822502151653
Changing layer 5's weights from 
0.0339248250069428 to 0.0369824709738041
-0.408148698991318 to -0.405091053024457
0.0170396636071019 to 0.0200973095739632
-0.918472517764349 to -0.915414871797487
-0.381203704064865 to -0.378146058098004
-0.779671647137184 to -0.776614001170322
-0.479706340020675 to -0.476648694053813
-0.407260112947006 to -0.404202466980145
-0.0176581074652862 to -0.0146004614984249
-0.0241662194190222 to -0.0211085734521609
Trying to learn from memory 14, 1, 0.3733333
sum 0.0386800545449334 distri 0.0209412668711054
Using diff 0.00806877403759458 and condRate 0.166666666666667
Changed category 1 weights from 
0.29104141032714 to 0.291543467380506
0.524356997370902 to 0.524859054424268
0.0994317594768439 to 0.0999338165302095
0.103524572014991 to 0.104026629068356
Changing layer 0's weights from 
-0.208197003379986 to -0.20769494632662
-0.580123460069819 to -0.579621403016454
-0.814472725764438 to -0.813970668711073
-0.779913475886508 to -0.779411418833142
-0.0320459065591548 to -0.0315438495057892
-0.714687859789058 to -0.714185802735693
-0.560858136192485 to -0.56035607913912
-0.58831254221551 to -0.587810485162145
-0.825804403201266 to -0.825302346147901
-0.869554458335562 to -0.869052401282197
Changing layer 1's weights from 
-0.864457545951529 to -0.863955488898164
-0.263288861290143 to -0.262786804236777
-0.388991600052045 to -0.388489542998679
-0.398896938339397 to -0.398394881286032
-0.583949720636532 to -0.583447663583166
-0.722187554613277 to -0.721685497559912
-0.45146883107774 to -0.450966774024374
-0.837271860018894 to -0.836769802965529
-0.425608879104779 to -0.425106822051413
-0.788255741969272 to -0.787753684915906
Changing layer 2's weights from 
0.0209324898565551 to 0.0214345469099207
-0.533120995536968 to -0.532618938483603
-0.248531704918072 to -0.248029647864706
-0.0773316083108636 to -0.076829551257498
-0.254584318176434 to -0.254082261123068
-0.460593944564984 to -0.460091887511618
0.058218771442249 to 0.0587208284956146
-0.0142032561456419 to -0.0137011990922763
-0.746201550737544 to -0.745699493684179
-0.701244032159969 to -0.700741975106604
Changing layer 3's weights from 
-0.0121178088342409 to -0.0116157517808753
-0.275670891777203 to -0.275168834723837
-0.0330609736596798 to -0.0325589166063142
-0.940500205415822 to -0.939998148362457
-0.155736869350597 to -0.155234812297232
-0.285985475555585 to -0.285483418502219
-0.616170322672053 to -0.615668265618688
-0.15765745021455 to -0.157155393161185
-0.489353424087688 to -0.488851367034323
-0.0165235457574588 to -0.0160214887040932
Changing layer 4's weights from 
-0.115797346607372 to -0.115295289554007
-0.324630146995709 to -0.324128089942343
-0.525107747093365 to -0.524605690039999
-0.238173013702557 to -0.237670956649191
-0.477425819412395 to -0.47692376235903
-0.245960598961041 to -0.245458541907675
-0.250319129005597 to -0.249817071952231
-0.71221665121667 to -0.711714594163305
-0.11852056361787 to -0.118018506564505
-0.550822502151653 to -0.550320445098288
Changing layer 5's weights from 
0.0369824709738041 to 0.0374845280271697
-0.405091053024457 to -0.404588995971091
0.0200973095739632 to 0.0205993666273288
-0.915414871797487 to -0.914912814744122
-0.378146058098004 to -0.377644001044638
-0.776614001170322 to -0.776111944116957
-0.476648694053813 to -0.476146637000448
-0.404202466980145 to -0.403700409926779
-0.0146004614984249 to -0.0140984044450593
-0.0211085734521609 to -0.0206065163987953
Trying to learn from memory 15, 1, 0.3733333
sum 0.038680616502965 distri 0.0209412896906555
Using diff 0.0080691726865683 and condRate 0.166666666666667
Changed category 1 weights from 
0.291543467380506 to 0.292045549238697
0.524859054424268 to 0.525361136282459
0.0999338165302095 to 0.1004358983884
0.104026629068356 to 0.104528710926547
Changing layer 0's weights from 
-0.20769494632662 to -0.207192864468429
-0.579621403016454 to -0.579119321158263
-0.813970668711073 to -0.813468586852882
-0.779411418833142 to -0.778909336974951
-0.0315438495057892 to -0.0310417676475985
-0.714185802735693 to -0.713683720877502
-0.56035607913912 to -0.559853997280929
-0.587810485162145 to -0.587308403303954
-0.825302346147901 to -0.82480026428971
-0.869052401282197 to -0.868550319424006
Changing layer 1's weights from 
-0.863955488898164 to -0.863453407039973
-0.262786804236777 to -0.262284722378586
-0.388489542998679 to -0.387987461140488
-0.398394881286032 to -0.397892799427841
-0.583447663583166 to -0.582945581724975
-0.721685497559912 to -0.721183415701721
-0.450966774024374 to -0.450464692166183
-0.836769802965529 to -0.836267721107338
-0.425106822051413 to -0.424604740193222
-0.787753684915906 to -0.787251603057715
Changing layer 2's weights from 
0.0214345469099207 to 0.0219366287681114
-0.532618938483603 to -0.532116856625412
-0.248029647864706 to -0.247527566006515
-0.076829551257498 to -0.0763274693993072
-0.254082261123068 to -0.253580179264877
-0.460091887511618 to -0.459589805653427
0.0587208284956146 to 0.0592229103538053
-0.0137011990922763 to -0.0131991172340856
-0.745699493684179 to -0.745197411825988
-0.700741975106604 to -0.700239893248413
Changing layer 3's weights from 
-0.0116157517808753 to -0.0111136699226846
-0.275168834723837 to -0.274666752865646
-0.0325589166063142 to -0.0320568347481235
-0.939998148362457 to -0.939496066504266
-0.155234812297232 to -0.154732730439041
-0.285483418502219 to -0.284981336644028
-0.615668265618688 to -0.615166183760497
-0.157155393161185 to -0.156653311302994
-0.488851367034323 to -0.488349285176132
-0.0160214887040932 to -0.0155194068459025
Changing layer 4's weights from 
-0.115295289554007 to -0.114793207695816
-0.324128089942343 to -0.323626008084152
-0.524605690039999 to -0.524103608181808
-0.237670956649191 to -0.237168874791
-0.47692376235903 to -0.476421680500839
-0.245458541907675 to -0.244956460049484
-0.249817071952231 to -0.24931499009404
-0.711714594163305 to -0.711212512305114
-0.118018506564505 to -0.117516424706314
-0.550320445098288 to -0.549818363240097
Changing layer 5's weights from 
0.0374845280271697 to 0.0379866098853604
-0.404588995971091 to -0.4040869141129
0.0205993666273288 to 0.0211014484855195
-0.914912814744122 to -0.914410732885931
-0.377644001044638 to -0.377141919186447
-0.776111944116957 to -0.775609862258766
-0.476146637000448 to -0.475644555142257
-0.403700409926779 to -0.403198328068588
-0.0140984044450593 to -0.0135963225868686
-0.0206065163987953 to -0.0201044345406046
Trying to learn from memory 16, 1, 0.3733333
sum 0.0386788603352959 distri 0.0209404227840417
Using diff 0.00806872246743025 and condRate 0.166666666666667
Changed category 1 weights from 
0.292045549238697 to 0.292547603083252
0.525361136282459 to 0.525863190127014
0.1004358983884 to 0.100937952232956
0.104528710926547 to 0.105030764771103
Changing layer 0's weights from 
-0.207192864468429 to -0.206690810623874
-0.579119321158263 to -0.578617267313708
-0.813468586852882 to -0.812966533008327
-0.778909336974951 to -0.778407283130396
-0.0310417676475985 to -0.0305397138030431
-0.713683720877502 to -0.713181667032947
-0.559853997280929 to -0.559351943436374
-0.587308403303954 to -0.586806349459399
-0.82480026428971 to -0.824298210445155
-0.868550319424006 to -0.868048265579451
Changing layer 1's weights from 
-0.863453407039973 to -0.862951353195418
-0.262284722378586 to -0.261782668534031
-0.387987461140488 to -0.387485407295933
-0.397892799427841 to -0.397390745583286
-0.582945581724975 to -0.58244352788042
-0.721183415701721 to -0.720681361857166
-0.450464692166183 to -0.449962638321628
-0.836267721107338 to -0.835765667262783
-0.424604740193222 to -0.424102686348667
-0.787251603057715 to -0.78674954921316
Changing layer 2's weights from 
0.0219366287681114 to 0.0224386826126668
-0.532116856625412 to -0.531614802780857
-0.247527566006515 to -0.24702551216196
-0.0763274693993072 to -0.0758254155547519
-0.253580179264877 to -0.253078125420322
-0.459589805653427 to -0.459087751808872
0.0592229103538053 to 0.0597249641983607
-0.0131991172340856 to -0.0126970633895302
-0.745197411825988 to -0.744695357981433
-0.700239893248413 to -0.699737839403858
Changing layer 3's weights from 
-0.0111136699226846 to -0.0106116160781292
-0.274666752865646 to -0.274164699021091
-0.0320568347481235 to -0.0315547809035681
-0.939496066504266 to -0.938994012659711
-0.154732730439041 to -0.154230676594486
-0.284981336644028 to -0.284479282799473
-0.615166183760497 to -0.614664129915942
-0.156653311302994 to -0.156151257458439
-0.488349285176132 to -0.487847231331577
-0.0155194068459025 to -0.0150173530013471
Changing layer 4's weights from 
-0.114793207695816 to -0.114291153851261
-0.323626008084152 to -0.323123954239597
-0.524103608181808 to -0.523601554337253
-0.237168874791 to -0.236666820946445
-0.476421680500839 to -0.475919626656284
-0.244956460049484 to -0.244454406204929
-0.24931499009404 to -0.248812936249485
-0.711212512305114 to -0.710710458460559
-0.117516424706314 to -0.117014370861759
-0.549818363240097 to -0.549316309395542
Changing layer 5's weights from 
0.0379866098853604 to 0.0384886637299158
-0.4040869141129 to -0.403584860268345
0.0211014484855195 to 0.0216035023300749
-0.914410732885931 to -0.913908679041376
-0.377141919186447 to -0.376639865341892
-0.775609862258766 to -0.775107808414211
-0.475644555142257 to -0.475142501297702
-0.403198328068588 to -0.402696274224033
-0.0135963225868686 to -0.0130942687423132
-0.0201044345406046 to -0.0196023806960492
Trying to learn from memory 16, 0, 0.3733333
sum 0.0386788603352959 distri -0.0201314353520458
Using diff 0.0491405806035177 and condRate 0.166666666666667
Changed category 0 weights from 
0.165267853477231 to 0.16832548961669
-0.234767110607394 to -0.231709474467935
-0.5172878071282 to -0.514230170988741
-0.365782441160449 to -0.36272480502099
Changing layer 0's weights from 
-0.206690810623874 to -0.203633174484415
-0.578617267313708 to -0.575559631174249
-0.812966533008327 to -0.809908896868868
-0.778407283130396 to -0.775349646990937
-0.0305397138030431 to -0.0274820776635842
-0.713181667032947 to -0.710124030893488
-0.559351943436374 to -0.556294307296915
-0.586806349459399 to -0.58374871331994
-0.824298210445155 to -0.821240574305696
-0.868048265579451 to -0.864990629439992
Changing layer 1's weights from 
-0.862951353195418 to -0.859893717055959
-0.261782668534031 to -0.258725032394572
-0.387485407295933 to -0.384427771156474
-0.397390745583286 to -0.394333109443827
-0.58244352788042 to -0.579385891740961
-0.720681361857166 to -0.717623725717707
-0.449962638321628 to -0.446905002182169
-0.835765667262783 to -0.832708031123324
-0.424102686348667 to -0.421045050209208
-0.78674954921316 to -0.783691913073701
Changing layer 2's weights from 
0.0224386826126668 to 0.0254963187521257
-0.531614802780857 to -0.528557166641398
-0.24702551216196 to -0.243967876022501
-0.0758254155547519 to -0.0727677794152929
-0.253078125420322 to -0.250020489280863
-0.459087751808872 to -0.456030115669413
0.0597249641983607 to 0.0627826003378196
-0.0126970633895302 to -0.00963942725007129
-0.744695357981433 to -0.741637721841974
-0.699737839403858 to -0.696680203264399
Changing layer 3's weights from 
-0.0106116160781292 to -0.0075539799386703
-0.274164699021091 to -0.271107062881632
-0.0315547809035681 to -0.0284971447641092
-0.938994012659711 to -0.935936376520252
-0.154230676594486 to -0.151173040455027
-0.284479282799473 to -0.281421646660014
-0.614664129915942 to -0.611606493776483
-0.156151257458439 to -0.15309362131898
-0.487847231331577 to -0.484789595192118
-0.0150173530013471 to -0.0119597168618882
Changing layer 4's weights from 
-0.114291153851261 to -0.111233517711802
-0.323123954239597 to -0.320066318100138
-0.523601554337253 to -0.520543918197794
-0.236666820946445 to -0.233609184806986
-0.475919626656284 to -0.472861990516825
-0.244454406204929 to -0.24139677006547
-0.248812936249485 to -0.245755300110026
-0.710710458460559 to -0.7076528223211
-0.117014370861759 to -0.1139567347223
-0.549316309395542 to -0.546258673256083
Changing layer 5's weights from 
0.0384886637299158 to 0.0415462998693747
-0.403584860268345 to -0.400527224128886
0.0216035023300749 to 0.0246611384695338
-0.913908679041376 to -0.910851042901917
-0.376639865341892 to -0.373582229202433
-0.775107808414211 to -0.772050172274752
-0.475142501297702 to -0.472084865158243
-0.402696274224033 to -0.399638638084574
-0.0130942687423132 to -0.0100366326028543
-0.0196023806960492 to -0.0165447445565903
Trying to learn from memory 16, 0, 0.3733333
sum 0.0386788603352959 distri -0.0201314353520458
Using diff 0.0491405806035177 and condRate 0.166666666666667
Changed category 0 weights from 
0.16832548961669 to 0.171383125756149
-0.231709474467935 to -0.228651838328476
-0.514230170988741 to -0.511172534849282
-0.36272480502099 to -0.359667168881531
Changing layer 0's weights from 
-0.203633174484415 to -0.200575538344956
-0.575559631174249 to -0.57250199503479
-0.809908896868868 to -0.806851260729409
-0.775349646990937 to -0.772292010851478
-0.0274820776635842 to -0.0244244415241253
-0.710124030893488 to -0.707066394754029
-0.556294307296915 to -0.553236671157456
-0.58374871331994 to -0.580691077180481
-0.821240574305696 to -0.818182938166237
-0.864990629439992 to -0.861932993300533
Changing layer 1's weights from 
-0.859893717055959 to -0.8568360809165
-0.258725032394572 to -0.255667396255113
-0.384427771156474 to -0.381370135017015
-0.394333109443827 to -0.391275473304368
-0.579385891740961 to -0.576328255601502
-0.717623725717707 to -0.714566089578248
-0.446905002182169 to -0.44384736604271
-0.832708031123324 to -0.829650394983865
-0.421045050209208 to -0.417987414069749
-0.783691913073701 to -0.780634276934242
Changing layer 2's weights from 
0.0254963187521257 to 0.0285539548915846
-0.528557166641398 to -0.525499530501939
-0.243967876022501 to -0.240910239883042
-0.0727677794152929 to -0.069710143275834
-0.250020489280863 to -0.246962853141404
-0.456030115669413 to -0.452972479529954
0.0627826003378196 to 0.0658402364772785
-0.00963942725007129 to -0.00658179111061238
-0.741637721841974 to -0.738580085702515
-0.696680203264399 to -0.69362256712494
Changing layer 3's weights from 
-0.0075539799386703 to -0.00449634379921138
-0.271107062881632 to -0.268049426742173
-0.0284971447641092 to -0.0254395086246503
-0.935936376520252 to -0.932878740380793
-0.151173040455027 to -0.148115404315568
-0.281421646660014 to -0.278364010520555
-0.611606493776483 to -0.608548857637024
-0.15309362131898 to -0.150035985179521
-0.484789595192118 to -0.481731959052659
-0.0119597168618882 to -0.00890208072242928
Changing layer 4's weights from 
-0.111233517711802 to -0.108175881572343
-0.320066318100138 to -0.317008681960679
-0.520543918197794 to -0.517486282058335
-0.233609184806986 to -0.230551548667527
-0.472861990516825 to -0.469804354377366
-0.24139677006547 to -0.238339133926011
-0.245755300110026 to -0.242697663970567
-0.7076528223211 to -0.704595186181641
-0.1139567347223 to -0.110899098582841
-0.546258673256083 to -0.543201037116624
Changing layer 5's weights from 
0.0415462998693747 to 0.0446039360088336
-0.400527224128886 to -0.397469587989427
0.0246611384695338 to 0.0277187746089927
-0.910851042901917 to -0.907793406762458
-0.373582229202433 to -0.370524593062974
-0.772050172274752 to -0.768992536135293
-0.472084865158243 to -0.469027229018784
-0.399638638084574 to -0.396581001945115
-0.0100366326028543 to -0.00697899646339537
-0.0165447445565903 to -0.0134871084171314
Trying to learn from memory 16, 0, 0.3733333
sum 0.0386788603352959 distri -0.0201314353520458
Using diff 0.0491405806035177 and condRate 0.166666666666667
Changed category 0 weights from 
0.171383125756149 to 0.174440761895608
-0.228651838328476 to -0.225594202189017
-0.511172534849282 to -0.508114898709823
-0.359667168881531 to -0.356609532742072
Changing layer 0's weights from 
-0.200575538344956 to -0.197517902205497
-0.57250199503479 to -0.569444358895331
-0.806851260729409 to -0.80379362458995
-0.772292010851478 to -0.769234374712019
-0.0244244415241253 to -0.0213668053846664
-0.707066394754029 to -0.70400875861457
-0.553236671157456 to -0.550179035017997
-0.580691077180481 to -0.577633441041022
-0.818182938166237 to -0.815125302026778
-0.861932993300533 to -0.858875357161074
Changing layer 1's weights from 
-0.8568360809165 to -0.853778444777041
-0.255667396255113 to -0.252609760115654
-0.381370135017015 to -0.378312498877556
-0.391275473304368 to -0.388217837164909
-0.576328255601502 to -0.573270619462043
-0.714566089578248 to -0.711508453438789
-0.44384736604271 to -0.440789729903251
-0.829650394983865 to -0.826592758844406
-0.417987414069749 to -0.41492977793029
-0.780634276934242 to -0.777576640794783
Changing layer 2's weights from 
0.0285539548915846 to 0.0316115910310435
-0.525499530501939 to -0.52244189436248
-0.240910239883042 to -0.237852603743583
-0.069710143275834 to -0.0666525071363751
-0.246962853141404 to -0.243905217001945
-0.452972479529954 to -0.449914843390495
0.0658402364772785 to 0.0688978726167375
-0.00658179111061238 to -0.00352415497115346
-0.738580085702515 to -0.735522449563056
-0.69362256712494 to -0.690564930985481
Changing layer 3's weights from 
-0.00449634379921138 to -0.00143870765975247
-0.268049426742173 to -0.264991790602714
-0.0254395086246503 to -0.0223818724851914
-0.932878740380793 to -0.929821104241334
-0.148115404315568 to -0.145057768176109
-0.278364010520555 to -0.275306374381096
-0.608548857637024 to -0.605491221497565
-0.150035985179521 to -0.146978349040062
-0.481731959052659 to -0.4786743229132
-0.00890208072242928 to -0.00584444458297037
Changing layer 4's weights from 
-0.108175881572343 to -0.105118245432884
-0.317008681960679 to -0.31395104582122
-0.517486282058335 to -0.514428645918876
-0.230551548667527 to -0.227493912528068
-0.469804354377366 to -0.466746718237907
-0.238339133926011 to -0.235281497786552
-0.242697663970567 to -0.239640027831108
-0.704595186181641 to -0.701537550042182
-0.110899098582841 to -0.107841462443382
-0.543201037116624 to -0.540143400977165
Changing layer 5's weights from 
0.0446039360088336 to 0.0476615721482925
-0.397469587989427 to -0.394411951849968
0.0277187746089927 to 0.0307764107484516
-0.907793406762458 to -0.904735770622999
-0.370524593062974 to -0.367466956923515
-0.768992536135293 to -0.765934899995834
-0.469027229018784 to -0.465969592879325
-0.396581001945115 to -0.393523365805656
-0.00697899646339537 to -0.00392136032393646
-0.0134871084171314 to -0.0104294722776725
Trying to learn from memory 16, 1, 0.3733333
sum 0.0386788603352959 distri 0.0209404227840417
Using diff 0.00806872246743025 and condRate 0.166666666666667
Changed category 1 weights from 
0.292547603083252 to 0.293049656927807
0.525863190127014 to 0.526365243971569
0.100937952232956 to 0.101440006077511
0.105030764771103 to 0.105532818615658
Changing layer 0's weights from 
-0.197517902205497 to -0.197015848360942
-0.569444358895331 to -0.568942305050776
-0.80379362458995 to -0.803291570745395
-0.769234374712019 to -0.768732320867464
-0.0213668053846664 to -0.020864751540111
-0.70400875861457 to -0.703506704770015
-0.550179035017997 to -0.549676981173442
-0.577633441041022 to -0.577131387196467
-0.815125302026778 to -0.814623248182223
-0.858875357161074 to -0.858373303316519
Changing layer 1's weights from 
-0.853778444777041 to -0.853276390932486
-0.252609760115654 to -0.252107706271099
-0.378312498877556 to -0.377810445033001
-0.388217837164909 to -0.387715783320354
-0.573270619462043 to -0.572768565617488
-0.711508453438789 to -0.711006399594234
-0.440789729903251 to -0.440287676058696
-0.826592758844406 to -0.826090704999851
-0.41492977793029 to -0.414427724085735
-0.777576640794783 to -0.777074586950228
Changing layer 2's weights from 
0.0316115910310435 to 0.0321136448755989
-0.52244189436248 to -0.521939840517925
-0.237852603743583 to -0.237350549899028
-0.0666525071363751 to -0.0661504532918197
-0.243905217001945 to -0.24340316315739
-0.449914843390495 to -0.44941278954594
0.0688978726167375 to 0.0693999264612928
-0.00352415497115346 to -0.0030221011265981
-0.735522449563056 to -0.735020395718501
-0.690564930985481 to -0.690062877140926
Changing layer 3's weights from 
-0.00143870765975247 to -0.000936653815197109
-0.264991790602714 to -0.264489736758159
-0.0223818724851914 to -0.021879818640636
-0.929821104241334 to -0.929319050396779
-0.145057768176109 to -0.144555714331554
-0.275306374381096 to -0.274804320536541
-0.605491221497565 to -0.60498916765301
-0.146978349040062 to -0.146476295195507
-0.4786743229132 to -0.478172269068644
-0.00584444458297037 to -0.00534239073841501
Changing layer 4's weights from 
-0.105118245432884 to -0.104616191588329
-0.31395104582122 to -0.313448991976665
-0.514428645918876 to -0.513926592074321
-0.227493912528068 to -0.226991858683513
-0.466746718237907 to -0.466244664393351
-0.235281497786552 to -0.234779443941997
-0.239640027831108 to -0.239137973986553
-0.701537550042182 to -0.701035496197627
-0.107841462443382 to -0.107339408598827
-0.540143400977165 to -0.53964134713261
Changing layer 5's weights from 
0.0476615721482925 to 0.0481636259928479
-0.394411951849968 to -0.393909898005413
0.0307764107484516 to 0.031278464593007
-0.904735770622999 to -0.904233716778444
-0.367466956923515 to -0.36696490307896
-0.765934899995834 to -0.765432846151279
-0.465969592879325 to -0.46546753903477
-0.393523365805656 to -0.393021311961101
-0.00392136032393646 to -0.0034193064793811
-0.0104294722776725 to -0.0099274184331171
10/5/2016 1:38:45 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 16, 0, 0.3733333
sum 0.0386788603352959 distri -0.0201314353520458
Using diff 0.0491405806035177 and condRate 0.166666666666667
Changed category 0 weights from 
0.174440761895608 to 0.177498398035067
-0.225594202189017 to -0.222536566049558
-0.508114898709823 to -0.505057262570364
-0.356609532742072 to -0.353551896602613
Changing layer 0's weights from 
-0.197015848360942 to -0.193958212221483
-0.568942305050776 to -0.565884668911317
-0.803291570745395 to -0.800233934605936
-0.768732320867464 to -0.765674684728005
-0.020864751540111 to -0.0178071154006521
-0.703506704770015 to -0.700449068630556
-0.549676981173442 to -0.546619345033983
-0.577131387196467 to -0.574073751057008
-0.814623248182223 to -0.811565612042764
-0.858373303316519 to -0.85531566717706
Changing layer 1's weights from 
-0.853276390932486 to -0.850218754793027
-0.252107706271099 to -0.24905007013164
-0.377810445033001 to -0.374752808893542
-0.387715783320354 to -0.384658147180895
-0.572768565617488 to -0.569710929478029
-0.711006399594234 to -0.707948763454775
-0.440287676058696 to -0.437230039919237
-0.826090704999851 to -0.823033068860392
-0.414427724085735 to -0.411370087946276
-0.777074586950228 to -0.774016950810769
Changing layer 2's weights from 
0.0321136448755989 to 0.0351712810150578
-0.521939840517925 to -0.518882204378466
-0.237350549899028 to -0.234292913759569
-0.0661504532918197 to -0.0630928171523608
-0.24340316315739 to -0.240345527017931
-0.44941278954594 to -0.446355153406481
0.0693999264612928 to 0.0724575626007517
-0.0030221011265981 to 3.55350128608111E-05
-0.735020395718501 to -0.731962759579042
-0.690062877140926 to -0.687005241001467
Changing layer 3's weights from 
-0.000936653815197109 to 0.0021209823242618
-0.264489736758159 to -0.2614321006187
-0.021879818640636 to -0.0188221825011771
-0.929319050396779 to -0.92626141425732
-0.144555714331554 to -0.141498078192095
-0.274804320536541 to -0.271746684397082
-0.60498916765301 to -0.601931531513551
-0.146476295195507 to -0.143418659056048
-0.478172269068644 to -0.475114632929185
-0.00534239073841501 to -0.00228475459895609
Changing layer 4's weights from 
-0.104616191588329 to -0.10155855544887
-0.313448991976665 to -0.310391355837206
-0.513926592074321 to -0.510868955934862
-0.226991858683513 to -0.223934222544054
-0.466244664393351 to -0.463187028253893
-0.234779443941997 to -0.231721807802538
-0.239137973986553 to -0.236080337847094
-0.701035496197627 to -0.697977860058168
-0.107339408598827 to -0.104281772459368
-0.53964134713261 to -0.536583710993151
Changing layer 5's weights from 
0.0481636259928479 to 0.0512212621323068
-0.393909898005413 to -0.390852261865954
0.031278464593007 to 0.0343361007324659
-0.904233716778444 to -0.901176080638985
-0.36696490307896 to -0.363907266939501
-0.765432846151279 to -0.76237521001182
-0.46546753903477 to -0.462409902895311
-0.393021311961101 to -0.389963675821642
-0.0034193064793811 to -0.000361670339922184
-0.0099274184331171 to -0.00686978229365819
Trying to learn from memory 16, 0, 0.3733333
sum 0.0386788603352959 distri -0.0201314353520458
Using diff 0.0491405806035177 and condRate 0.166666666666667
Changed category 0 weights from 
0.177498398035067 to 0.180556034174526
-0.222536566049558 to -0.219478929910099
-0.505057262570364 to -0.501999626430905
-0.353551896602613 to -0.350494260463154
Changing layer 0's weights from 
-0.193958212221483 to -0.190900576082024
-0.565884668911317 to -0.562827032771858
-0.800233934605936 to -0.797176298466477
-0.765674684728005 to -0.762617048588546
-0.0178071154006521 to -0.0147494792611932
-0.700449068630556 to -0.697391432491097
-0.546619345033983 to -0.543561708894524
-0.574073751057008 to -0.571016114917549
-0.811565612042764 to -0.808507975903305
-0.85531566717706 to -0.852258031037601
Changing layer 1's weights from 
-0.850218754793027 to -0.847161118653568
-0.24905007013164 to -0.245992433992181
-0.374752808893542 to -0.371695172754083
-0.384658147180895 to -0.381600511041436
-0.569710929478029 to -0.56665329333857
-0.707948763454775 to -0.704891127315316
-0.437230039919237 to -0.434172403779778
-0.823033068860392 to -0.819975432720933
-0.411370087946276 to -0.408312451806817
-0.774016950810769 to -0.77095931467131
Changing layer 2's weights from 
0.0351712810150578 to 0.0382289171545167
-0.518882204378466 to -0.515824568239007
-0.234292913759569 to -0.23123527762011
-0.0630928171523608 to -0.0600351810129019
-0.240345527017931 to -0.237287890878472
-0.446355153406481 to -0.443297517267022
0.0724575626007517 to 0.0755151987402106
3.55350128608111E-05 to 0.00309317115231972
-0.731962759579042 to -0.728905123439583
-0.687005241001467 to -0.683947604862008
Changing layer 3's weights from 
0.0021209823242618 to 0.00517861846372072
-0.2614321006187 to -0.258374464479241
-0.0188221825011771 to -0.0157645463617182
-0.92626141425732 to -0.923203778117861
-0.141498078192095 to -0.138440442052636
-0.271746684397082 to -0.268689048257623
-0.601931531513551 to -0.598873895374092
-0.143418659056048 to -0.140361022916589
-0.475114632929185 to -0.472056996789727
-0.00228475459895609 to 0.000772881540502819
Changing layer 4's weights from 
-0.10155855544887 to -0.0985009193094108
-0.310391355837206 to -0.307333719697747
-0.510868955934862 to -0.507811319795403
-0.223934222544054 to -0.220876586404595
-0.463187028253893 to -0.460129392114434
-0.231721807802538 to -0.228664171663079
-0.236080337847094 to -0.233022701707635
-0.697977860058168 to -0.694920223918709
-0.104281772459368 to -0.101224136319909
-0.536583710993151 to -0.533526074853692
Changing layer 5's weights from 
0.0512212621323068 to 0.0542788982717657
-0.390852261865954 to -0.387794625726495
0.0343361007324659 to 0.0373937368719248
-0.901176080638985 to -0.898118444499526
-0.363907266939501 to -0.360849630800042
-0.76237521001182 to -0.759317573872361
-0.462409902895311 to -0.459352266755852
-0.389963675821642 to -0.386906039682183
-0.000361670339922184 to 0.00269596579953673
-0.00686978229365819 to -0.00381214615419928
Trying to learn from memory 16, 1, 0.3733333
sum 0.0386788603352959 distri 0.0209404227840417
Using diff 0.00806872246743025 and condRate 0.166666666666667
Changed category 1 weights from 
0.293049656927807 to 0.293551710772363
0.526365243971569 to 0.526867297816125
0.101440006077511 to 0.101942059922066
0.105532818615658 to 0.106034872460213
Changing layer 0's weights from 
-0.190900576082024 to -0.190398522237469
-0.562827032771858 to -0.562324978927303
-0.797176298466477 to -0.796674244621922
-0.762617048588546 to -0.762114994743991
-0.0147494792611932 to -0.0142474254166378
-0.697391432491097 to -0.696889378646542
-0.543561708894524 to -0.543059655049969
-0.571016114917549 to -0.570514061072994
-0.808507975903305 to -0.80800592205875
-0.852258031037601 to -0.851755977193046
Changing layer 1's weights from 
-0.847161118653568 to -0.846659064809013
-0.245992433992181 to -0.245490380147625
-0.371695172754083 to -0.371193118909528
-0.381600511041436 to -0.38109845719688
-0.56665329333857 to -0.566151239494015
-0.704891127315316 to -0.704389073470761
-0.434172403779778 to -0.433670349935223
-0.819975432720933 to -0.819473378876378
-0.408312451806817 to -0.407810397962261
-0.77095931467131 to -0.770457260826755
Changing layer 2's weights from 
0.0382289171545167 to 0.0387309709990721
-0.515824568239007 to -0.515322514394452
-0.23123527762011 to -0.230733223775555
-0.0600351810129019 to -0.0595331271683466
-0.237287890878472 to -0.236785837033916
-0.443297517267022 to -0.442795463422467
0.0755151987402106 to 0.076017252584766
0.00309317115231972 to 0.00359522499687509
-0.728905123439583 to -0.728403069595028
-0.683947604862008 to -0.683445551017453
Changing layer 3's weights from 
0.00517861846372072 to 0.00568067230827608
-0.258374464479241 to -0.257872410634685
-0.0157645463617182 to -0.0152624925171628
-0.923203778117861 to -0.922701724273306
-0.138440442052636 to -0.13793838820808
-0.268689048257623 to -0.268186994413067
-0.598873895374092 to -0.598371841529537
-0.140361022916589 to -0.139858969072033
-0.472056996789727 to -0.471554942945171
0.000772881540502819 to 0.00127493538505818
Changing layer 4's weights from 
-0.0985009193094108 to -0.0979988654648554
-0.307333719697747 to -0.306831665853191
-0.507811319795403 to -0.507309265950848
-0.220876586404595 to -0.22037453256004
-0.460129392114434 to -0.459627338269878
-0.228664171663079 to -0.228162117818524
-0.233022701707635 to -0.232520647863079
-0.694920223918709 to -0.694418170074154
-0.101224136319909 to -0.100722082475353
-0.533526074853692 to -0.533024021009137
Changing layer 5's weights from 
0.0542788982717657 to 0.0547809521163211
-0.387794625726495 to -0.38729257188194
0.0373937368719248 to 0.0378957907164802
-0.898118444499526 to -0.897616390654971
-0.360849630800042 to -0.360347576955486
-0.759317573872361 to -0.758815520027806
-0.459352266755852 to -0.458850212911296
-0.386906039682183 to -0.386403985837627
0.00269596579953673 to 0.00319801964409209
-0.00381214615419928 to -0.00331009230964391
Trying to learn from memory 17, 0, 0.3733333
sum 0.0386788603352959 distri -0.0201314353520458
Using diff 0.0491405806035177 and condRate 0.166666666666667
Changed category 0 weights from 
0.180556034174526 to 0.183613670313985
-0.219478929910099 to -0.21642129377064
-0.501999626430905 to -0.498941990291446
-0.350494260463154 to -0.347436624323695
Changing layer 0's weights from 
-0.190398522237469 to -0.18734088609801
-0.562324978927303 to -0.559267342787844
-0.796674244621922 to -0.793616608482463
-0.762114994743991 to -0.759057358604532
-0.0142474254166378 to -0.0111897892771789
-0.696889378646542 to -0.693831742507083
-0.543059655049969 to -0.54000201891051
-0.570514061072994 to -0.567456424933535
-0.80800592205875 to -0.804948285919291
-0.851755977193046 to -0.848698341053587
Changing layer 1's weights from 
-0.846659064809013 to -0.843601428669554
-0.245490380147625 to -0.242432744008167
-0.371193118909528 to -0.368135482770069
-0.38109845719688 to -0.378040821057422
-0.566151239494015 to -0.563093603354556
-0.704389073470761 to -0.701331437331302
-0.433670349935223 to -0.430612713795764
-0.819473378876378 to -0.816415742736919
-0.407810397962261 to -0.404752761822803
-0.770457260826755 to -0.767399624687296
Changing layer 2's weights from 
0.0387309709990721 to 0.041788607138531
-0.515322514394452 to -0.512264878254993
-0.230733223775555 to -0.227675587636096
-0.0595331271683466 to -0.0564754910288877
-0.236785837033916 to -0.233728200894458
-0.442795463422467 to -0.439737827283008
0.076017252584766 to 0.0790748887242249
0.00359522499687509 to 0.006652861136334
-0.728403069595028 to -0.725345433455569
-0.683445551017453 to -0.680387914877994
Changing layer 3's weights from 
0.00568067230827608 to 0.00873830844773499
-0.257872410634685 to -0.254814774495227
-0.0152624925171628 to -0.0122048563777039
-0.922701724273306 to -0.919644088133847
-0.13793838820808 to -0.134880752068621
-0.268186994413067 to -0.265129358273609
-0.598371841529537 to -0.595314205390078
-0.139858969072033 to -0.136801332932574
-0.471554942945171 to -0.468497306805712
0.00127493538505818 to 0.0043325715245171
Changing layer 4's weights from 
-0.0979988654648554 to -0.0949412293253965
-0.306831665853191 to -0.303774029713733
-0.507309265950848 to -0.504251629811389
-0.22037453256004 to -0.217316896420581
-0.459627338269878 to -0.456569702130419
-0.228162117818524 to -0.225104481679065
-0.232520647863079 to -0.229463011723621
-0.694418170074154 to -0.691360533934695
-0.100722082475353 to -0.0976644463358945
-0.533024021009137 to -0.529966384869678
Changing layer 5's weights from 
0.0547809521163211 to 0.05783858825578
-0.38729257188194 to -0.384234935742481
0.0378957907164802 to 0.0409534268559391
-0.897616390654971 to -0.894558754515512
-0.360347576955486 to -0.357289940816028
-0.758815520027806 to -0.755757883888347
-0.458850212911296 to -0.455792576771837
-0.386403985837627 to -0.383346349698169
0.00319801964409209 to 0.006255655783551
-0.00331009230964391 to -0.000252456170185
Trying to learn from memory 18, 1, 0.3733333
sum 0.0386788603352959 distri 0.0209404227840417
Using diff 0.00806872246743025 and condRate 0.166666666666667
Changed category 1 weights from 
0.293551710772363 to 0.294053764616918
0.526867297816125 to 0.52736935166068
0.101942059922066 to 0.102444113766622
0.106034872460213 to 0.106536926304769
Changing layer 0's weights from 
-0.18734088609801 to -0.186838832253454
-0.559267342787844 to -0.558765288943288
-0.793616608482463 to -0.793114554637907
-0.759057358604532 to -0.758555304759976
-0.0111897892771789 to -0.0106877354326235
-0.693831742507083 to -0.693329688662527
-0.54000201891051 to -0.539499965065954
-0.567456424933535 to -0.566954371088979
-0.804948285919291 to -0.804446232074735
-0.848698341053587 to -0.848196287209031
Changing layer 1's weights from 
-0.843601428669554 to -0.843099374824998
-0.242432744008167 to -0.241930690163611
-0.368135482770069 to -0.367633428925513
-0.378040821057422 to -0.377538767212866
-0.563093603354556 to -0.56259154951
-0.701331437331302 to -0.700829383486746
-0.430612713795764 to -0.430110659951208
-0.816415742736919 to -0.815913688892363
-0.404752761822803 to -0.404250707978247
-0.767399624687296 to -0.76689757084274
Changing layer 2's weights from 
0.041788607138531 to 0.0422906609830864
-0.512264878254993 to -0.511762824410437
-0.227675587636096 to -0.22717353379154
-0.0564754910288877 to -0.0559734371843323
-0.233728200894458 to -0.233226147049902
-0.439737827283008 to -0.439235773438452
0.0790748887242249 to 0.0795769425687803
0.006652861136334 to 0.00715491498088936
-0.725345433455569 to -0.724843379611013
-0.680387914877994 to -0.679885861033438
Changing layer 3's weights from 
0.00873830844773499 to 0.00924036229229036
-0.254814774495227 to -0.254312720650671
-0.0122048563777039 to -0.0117028025331485
-0.919644088133847 to -0.919142034289291
-0.134880752068621 to -0.134378698224066
-0.265129358273609 to -0.264627304429053
-0.595314205390078 to -0.594812151545522
-0.136801332932574 to -0.136299279088019
-0.468497306805712 to -0.467995252961157
0.0043325715245171 to 0.00483462536907246
Changing layer 4's weights from 
-0.0949412293253965 to -0.0944391754808411
-0.303774029713733 to -0.303271975869177
-0.504251629811389 to -0.503749575966833
-0.217316896420581 to -0.216814842576025
-0.456569702130419 to -0.456067648285864
-0.225104481679065 to -0.224602427834509
-0.229463011723621 to -0.228960957879065
-0.691360533934695 to -0.690858480090139
-0.0976644463358945 to -0.0971623924913391
-0.529966384869678 to -0.529464331025122
Changing layer 5's weights from 
0.05783858825578 to 0.0583406421003354
-0.384234935742481 to -0.383732881897925
0.0409534268559391 to 0.0414554807004944
-0.894558754515512 to -0.894056700670956
-0.357289940816028 to -0.356787886971472
-0.755757883888347 to -0.755255830043791
-0.455792576771837 to -0.455290522927282
-0.383346349698169 to -0.382844295853613
0.006255655783551 to 0.00675770962810637
-0.000252456170185 to 0.000249597674370364
10/5/2016 1:38:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:38:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:15 PMStarting AI
Reading weights from 10/5/2016 1:39:15 PMStarting AI
Weights.txt
Layer 0's weights: -0.186838832253454 -0.558765288943288 -0.793114554637907 -0.758555304759976 -0.0106877354326235 -0.693329688662527 -0.539499965065954 -0.566954371088979 -0.804446232074735 -0.848196287209031 14 11 
Layer 1's weights: -0.843099374824998 -0.241930690163611 -0.367633428925513 -0.377538767212866 -0.56259154951 -0.700829383486746 -0.430110659951208 -0.815913688892363 -0.404250707978247 -0.76689757084274 12 9 
Layer 2's weights: 0.0422906609830864 -0.511762824410437 -0.22717353379154 -0.0559734371843323 -0.233226147049902 -0.439235773438452 0.0795769425687803 0.00715491498088936 -0.724843379611013 -0.679885861033438 10 7 
Layer 3's weights: 0.00924036229229036 -0.254312720650671 -0.0117028025331485 -0.919142034289291 -0.134378698224066 -0.264627304429053 -0.594812151545522 -0.136299279088019 -0.467995252961157 0.00483462536907246 8 5 
Layer 4's weights: -0.0944391754808411 -0.303271975869177 -0.503749575966833 -0.216814842576025 -0.456067648285864 -0.224602427834509 -0.228960957879065 -0.690858480090139 -0.0971623924913391 -0.529464331025122 6 3 
Layer 5's weights: 0.0583406421003354 -0.383732881897925 0.0414554807004944 -0.894056700670956 -0.356787886971472 -0.755255830043791 -0.455290522927282 -0.382844295853613 0.00675770962810637 0.000249597674370364 4 1 
Layer 6's weights: 0.183613670313985 -0.21642129377064 -0.498941990291446 -0.347436624323695 
Layer 7's weights: 0.294053764616918 0.52736935166068 0.102444113766622 0.106536926304769 
Layer 8's weights: 0.457811464196105 0.736253429776094 0.649213065033814 -0.00535863953266557 
10/5/2016 1:39:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:55 PMStarting learning phase with deltaScore: 4.8
Modified index 0's learning in memoryPool to 0.96
Modified index 1's learning in memoryPool to 0.96
Modified index 2's learning in memoryPool to 0.96
Modified index 3's learning in memoryPool to 0.96
Modified index 4's learning in memoryPool to 0.96
10/5/2016 1:39:55 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 0, 1, 0.96
sum 0.0388984973993258 distri 0.0202819256425719
Using diff 0.00889194740692239 and condRate 0.166666666666667
Changed category 1 weights from 
0.294053764616918 to 0.295476476258559
0.52736935166068 to 0.528792063302321
0.102444113766622 to 0.103866825408263
0.106536926304769 to 0.10795963794641
Changing layer 0's weights from 
-0.186838832253454 to -0.185416120611813
-0.558765288943288 to -0.557342577301647
-0.793114554637907 to -0.791691842996266
-0.758555304759976 to -0.757132593118335
-0.0106877354326235 to -0.00926502379098244
-0.693329688662527 to -0.691906977020886
-0.539499965065954 to -0.538077253424313
-0.566954371088979 to -0.565531659447338
-0.804446232074735 to -0.803023520433094
-0.848196287209031 to -0.84677357556739
Changing layer 1's weights from 
-0.843099374824998 to -0.841676663183357
-0.241930690163611 to -0.24050797852197
-0.367633428925513 to -0.366210717283872
-0.377538767212866 to -0.376116055571225
-0.56259154951 to -0.561168837868359
-0.700829383486746 to -0.699406671845105
-0.430110659951208 to -0.428687948309567
-0.815913688892363 to -0.814490977250722
-0.404250707978247 to -0.402827996336606
-0.76689757084274 to -0.765474859201099
Changing layer 2's weights from 
0.0422906609830864 to 0.0437133726247275
-0.511762824410437 to -0.510340112768796
-0.22717353379154 to -0.225750822149899
-0.0559734371843323 to -0.0545507255426912
-0.233226147049902 to -0.231803435408261
-0.439235773438452 to -0.437813061796811
0.0795769425687803 to 0.0809996542104214
0.00715491498088936 to 0.00857762662253042
-0.724843379611013 to -0.723420667969372
-0.679885861033438 to -0.678463149391797
Changing layer 3's weights from 
0.00924036229229036 to 0.0106630739339314
-0.254312720650671 to -0.25289000900903
-0.0117028025331485 to -0.0102800908915074
-0.919142034289291 to -0.91771932264765
-0.134378698224066 to -0.132955986582425
-0.264627304429053 to -0.263204592787412
-0.594812151545522 to -0.593389439903881
-0.136299279088019 to -0.134876567446378
-0.467995252961157 to -0.466572541319516
0.00483462536907246 to 0.00625733701071352
Changing layer 4's weights from 
-0.0944391754808411 to -0.0930164638392
-0.303271975869177 to -0.301849264227536
-0.503749575966833 to -0.502326864325192
-0.216814842576025 to -0.215392130934384
-0.456067648285864 to -0.454644936644223
-0.224602427834509 to -0.223179716192868
-0.228960957879065 to -0.227538246237424
-0.690858480090139 to -0.689435768448498
-0.0971623924913391 to -0.095739680849698
-0.529464331025122 to -0.528041619383481
Changing layer 5's weights from 
0.0583406421003354 to 0.0597633537419765
-0.383732881897925 to -0.382310170256284
0.0414554807004944 to 0.0428781923421355
-0.894056700670956 to -0.892633989029315
-0.356787886971472 to -0.355365175329831
-0.755255830043791 to -0.75383311840215
-0.455290522927282 to -0.453867811285641
-0.382844295853613 to -0.381421584211972
0.00675770962810637 to 0.00818042126974743
0.000249597674370364 to 0.00167230931601143
Trying to learn from memory 1, 1, 0.96
sum 0.0388984973993258 distri 0.0202819256425719
Using diff 0.00889194740692239 and condRate 0.166666666666667
Changed category 1 weights from 
0.295476476258559 to 0.2968991879002
0.528792063302321 to 0.530214774943962
0.103866825408263 to 0.105289537049904
0.10795963794641 to 0.109382349588051
Changing layer 0's weights from 
-0.185416120611813 to -0.183993408970172
-0.557342577301647 to -0.555919865660006
-0.791691842996266 to -0.790269131354625
-0.757132593118335 to -0.755709881476694
-0.00926502379098244 to -0.00784231214934138
-0.691906977020886 to -0.690484265379245
-0.538077253424313 to -0.536654541782672
-0.565531659447338 to -0.564108947805697
-0.803023520433094 to -0.801600808791453
-0.84677357556739 to -0.845350863925749
Changing layer 1's weights from 
-0.841676663183357 to -0.840253951541716
-0.24050797852197 to -0.239085266880329
-0.366210717283872 to -0.364788005642231
-0.376116055571225 to -0.374693343929584
-0.561168837868359 to -0.559746126226718
-0.699406671845105 to -0.697983960203464
-0.428687948309567 to -0.427265236667926
-0.814490977250722 to -0.813068265609081
-0.402827996336606 to -0.401405284694965
-0.765474859201099 to -0.764052147559458
Changing layer 2's weights from 
0.0437133726247275 to 0.0451360842663685
-0.510340112768796 to -0.508917401127155
-0.225750822149899 to -0.224328110508258
-0.0545507255426912 to -0.0531280139010502
-0.231803435408261 to -0.23038072376662
-0.437813061796811 to -0.43639035015517
0.0809996542104214 to 0.0824223658520624
0.00857762662253042 to 0.0100003382641715
-0.723420667969372 to -0.721997956327731
-0.678463149391797 to -0.677040437750156
Changing layer 3's weights from 
0.0106630739339314 to 0.0120857855755725
-0.25289000900903 to -0.251467297367389
-0.0102800908915074 to -0.00885737924986638
-0.91771932264765 to -0.916296611006009
-0.132955986582425 to -0.131533274940784
-0.263204592787412 to -0.261781881145771
-0.593389439903881 to -0.59196672826224
-0.134876567446378 to -0.133453855804737
-0.466572541319516 to -0.465149829677875
0.00625733701071352 to 0.00768004865235458
Changing layer 4's weights from 
-0.0930164638392 to -0.091593752197559
-0.301849264227536 to -0.300426552585895
-0.502326864325192 to -0.500904152683551
-0.215392130934384 to -0.213969419292743
-0.454644936644223 to -0.453222225002582
-0.223179716192868 to -0.221757004551227
-0.227538246237424 to -0.226115534595783
-0.689435768448498 to -0.688013056806857
-0.095739680849698 to -0.094316969208057
-0.528041619383481 to -0.52661890774184
Changing layer 5's weights from 
0.0597633537419765 to 0.0611860653836175
-0.382310170256284 to -0.380887458614643
0.0428781923421355 to 0.0443009039837765
-0.892633989029315 to -0.891211277387674
-0.355365175329831 to -0.35394246368819
-0.75383311840215 to -0.752410406760509
-0.453867811285641 to -0.452445099644
-0.381421584211972 to -0.379998872570331
0.00818042126974743 to 0.00960313291138849
0.00167230931601143 to 0.00309502095765249
Trying to learn from memory 2, 0, 0.96
sum 0.0388984973993258 distri -0.0175288092936742
Using diff 0.0467026823431685 and condRate 0.166666666666667
Changed category 0 weights from 
0.183613670313985 to 0.19108609978582
-0.21642129377064 to -0.208948864298805
-0.498941990291446 to -0.491469560819611
-0.347436624323695 to -0.33996419485186
Changing layer 0's weights from 
-0.183993408970172 to -0.176520979498337
-0.555919865660006 to -0.548447436188171
-0.790269131354625 to -0.78279670188279
-0.755709881476694 to -0.748237452004859
-0.00784231214934138 to -0.000369882677506757
-0.690484265379245 to -0.68301183590741
-0.536654541782672 to -0.529182112310837
-0.564108947805697 to -0.556636518333862
-0.801600808791453 to -0.794128379319618
-0.845350863925749 to -0.837878434453914
Changing layer 1's weights from 
-0.840253951541716 to -0.832781522069881
-0.239085266880329 to -0.231612837408494
-0.364788005642231 to -0.357315576170396
-0.374693343929584 to -0.367220914457749
-0.559746126226718 to -0.552273696754883
-0.697983960203464 to -0.690511530731629
-0.427265236667926 to -0.419792807196091
-0.813068265609081 to -0.805595836137246
-0.401405284694965 to -0.39393285522313
-0.764052147559458 to -0.756579718087623
Changing layer 2's weights from 
0.0451360842663685 to 0.0526085137382031
-0.508917401127155 to -0.50144497165532
-0.224328110508258 to -0.216855681036423
-0.0531280139010502 to -0.0456555844292156
-0.23038072376662 to -0.222908294294785
-0.43639035015517 to -0.428917920683335
0.0824223658520624 to 0.089894795323897
0.0100003382641715 to 0.0174727677360061
-0.721997956327731 to -0.714525526855896
-0.677040437750156 to -0.669568008278321
Changing layer 3's weights from 
0.0120857855755725 to 0.0195582150474071
-0.251467297367389 to -0.243994867895554
-0.00885737924986638 to -0.00138494977803176
-0.916296611006009 to -0.908824181534174
-0.131533274940784 to -0.124060845468949
-0.261781881145771 to -0.254309451673936
-0.59196672826224 to -0.584494298790405
-0.133453855804737 to -0.125981426332902
-0.465149829677875 to -0.45767740020604
0.00768004865235458 to 0.0151524781241892
Changing layer 4's weights from 
-0.091593752197559 to -0.0841213227257244
-0.300426552585895 to -0.29295412311406
-0.500904152683551 to -0.493431723211716
-0.213969419292743 to -0.206496989820908
-0.453222225002582 to -0.445749795530747
-0.221757004551227 to -0.214284575079392
-0.226115534595783 to -0.218643105123948
-0.688013056806857 to -0.680540627335022
-0.094316969208057 to -0.0868445397362223
-0.52661890774184 to -0.519146478270005
Changing layer 5's weights from 
0.0611860653836175 to 0.0686584948554521
-0.380887458614643 to -0.373415029142808
0.0443009039837765 to 0.0517733334556111
-0.891211277387674 to -0.883738847915839
-0.35394246368819 to -0.346470034216355
-0.752410406760509 to -0.744937977288674
-0.452445099644 to -0.444972670172165
-0.379998872570331 to -0.372526443098496
0.00960313291138849 to 0.0170755623832231
0.00309502095765249 to 0.0105674504294871
Trying to learn from memory 3, 1, 0.96
sum 0.0388984973993258 distri 0.0202819256425719
Using diff 0.00889194740692239 and condRate 0.166666666666667
Changed category 1 weights from 
0.2968991879002 to 0.298321899541841
0.530214774943962 to 0.531637486585603
0.105289537049904 to 0.106712248691545
0.109382349588051 to 0.110805061229692
Changing layer 0's weights from 
-0.176520979498337 to -0.175098267856696
-0.548447436188171 to -0.54702472454653
-0.78279670188279 to -0.781373990241149
-0.748237452004859 to -0.746814740363218
-0.000369882677506757 to 0.00105282896413431
-0.68301183590741 to -0.681589124265769
-0.529182112310837 to -0.527759400669196
-0.556636518333862 to -0.555213806692221
-0.794128379319618 to -0.792705667677977
-0.837878434453914 to -0.836455722812273
Changing layer 1's weights from 
-0.832781522069881 to -0.83135881042824
-0.231612837408494 to -0.230190125766853
-0.357315576170396 to -0.355892864528755
-0.367220914457749 to -0.365798202816108
-0.552273696754883 to -0.550850985113242
-0.690511530731629 to -0.689088819089988
-0.419792807196091 to -0.41837009555445
-0.805595836137246 to -0.804173124495605
-0.39393285522313 to -0.392510143581489
-0.756579718087623 to -0.755157006445982
Changing layer 2's weights from 
0.0526085137382031 to 0.0540312253798442
-0.50144497165532 to -0.500022260013679
-0.216855681036423 to -0.215432969394782
-0.0456555844292156 to -0.0442328727875745
-0.222908294294785 to -0.221485582653144
-0.428917920683335 to -0.427495209041694
0.089894795323897 to 0.0913175069655381
0.0174727677360061 to 0.0188954793776472
-0.714525526855896 to -0.713102815214255
-0.669568008278321 to -0.66814529663668
Changing layer 3's weights from 
0.0195582150474071 to 0.0209809266890482
-0.243994867895554 to -0.242572156253913
-0.00138494977803176 to 3.77618636093057E-05
-0.908824181534174 to -0.907401469892533
-0.124060845468949 to -0.122638133827308
-0.254309451673936 to -0.252886740032295
-0.584494298790405 to -0.583071587148764
-0.125981426332902 to -0.124558714691261
-0.45767740020604 to -0.456254688564399
0.0151524781241892 to 0.0165751897658303
Changing layer 4's weights from 
-0.0841213227257244 to -0.0826986110840833
-0.29295412311406 to -0.291531411472419
-0.493431723211716 to -0.492009011570075
-0.206496989820908 to -0.205074278179267
-0.445749795530747 to -0.444327083889106
-0.214284575079392 to -0.212861863437751
-0.218643105123948 to -0.217220393482307
-0.680540627335022 to -0.679117915693381
-0.0868445397362223 to -0.0854218280945813
-0.519146478270005 to -0.517723766628364
Changing layer 5's weights from 
0.0686584948554521 to 0.0700812064970932
-0.373415029142808 to -0.371992317501167
0.0517733334556111 to 0.0531960450972522
-0.883738847915839 to -0.882316136274198
-0.346470034216355 to -0.345047322574714
-0.744937977288674 to -0.743515265647033
-0.444972670172165 to -0.443549958530524
-0.372526443098496 to -0.371103731456855
0.0170755623832231 to 0.0184982740248642
0.0105674504294871 to 0.0119901620711282
Trying to learn from memory 4, 1, 0.96
sum 0.0388984973993258 distri 0.0202819256425719
Using diff 0.00889194740692239 and condRate 0.166666666666667
Changed category 1 weights from 
0.298321899541841 to 0.299744611183482
0.531637486585603 to 0.533060198227244
0.106712248691545 to 0.108134960333186
0.110805061229692 to 0.112227772871333
Changing layer 0's weights from 
-0.175098267856696 to -0.173675556215055
-0.54702472454653 to -0.545602012904889
-0.781373990241149 to -0.779951278599508
-0.746814740363218 to -0.745392028721577
0.00105282896413431 to 0.00247554060577537
-0.681589124265769 to -0.680166412624128
-0.527759400669196 to -0.526336689027555
-0.555213806692221 to -0.55379109505058
-0.792705667677977 to -0.791282956036336
-0.836455722812273 to -0.835033011170632
Changing layer 1's weights from 
-0.83135881042824 to -0.829936098786599
-0.230190125766853 to -0.228767414125212
-0.355892864528755 to -0.354470152887114
-0.365798202816108 to -0.364375491174467
-0.550850985113242 to -0.549428273471601
-0.689088819089988 to -0.687666107448347
-0.41837009555445 to -0.416947383912809
-0.804173124495605 to -0.802750412853964
-0.392510143581489 to -0.391087431939848
-0.755157006445982 to -0.753734294804341
Changing layer 2's weights from 
0.0540312253798442 to 0.0554539370214853
-0.500022260013679 to -0.498599548372038
-0.215432969394782 to -0.214010257753141
-0.0442328727875745 to -0.0428101611459334
-0.221485582653144 to -0.220062871011503
-0.427495209041694 to -0.426072497400053
0.0913175069655381 to 0.0927402186071792
0.0188954793776472 to 0.0203181910192882
-0.713102815214255 to -0.711680103572614
-0.66814529663668 to -0.666722584995039
Changing layer 3's weights from 
0.0209809266890482 to 0.0224036383306892
-0.242572156253913 to -0.241149444612272
3.77618636093057E-05 to 0.00146047350525037
-0.907401469892533 to -0.905978758250892
-0.122638133827308 to -0.121215422185667
-0.252886740032295 to -0.251464028390654
-0.583071587148764 to -0.581648875507123
-0.124558714691261 to -0.12313600304962
-0.456254688564399 to -0.454831976922758
0.0165751897658303 to 0.0179979014074713
Changing layer 4's weights from 
-0.0826986110840833 to -0.0812758994424422
-0.291531411472419 to -0.290108699830778
-0.492009011570075 to -0.490586299928434
-0.205074278179267 to -0.203651566537626
-0.444327083889106 to -0.442904372247465
-0.212861863437751 to -0.21143915179611
-0.217220393482307 to -0.215797681840666
-0.679117915693381 to -0.67769520405174
-0.0854218280945813 to -0.0839991164529402
-0.517723766628364 to -0.516301054986723
Changing layer 5's weights from 
0.0700812064970932 to 0.0715039181387343
-0.371992317501167 to -0.370569605859526
0.0531960450972522 to 0.0546187567388933
-0.882316136274198 to -0.880893424632557
-0.345047322574714 to -0.343624610933073
-0.743515265647033 to -0.742092554005392
-0.443549958530524 to -0.442127246888883
-0.371103731456855 to -0.369681019815214
0.0184982740248642 to 0.0199209856665052
0.0119901620711282 to 0.0134128737127692
10/5/2016 1:39:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:39:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:35 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
Modified index 9's learning in memoryPool to -0.2
Modified index 10's learning in memoryPool to -0.2
Modified index 11's learning in memoryPool to -0.2
Modified index 12's learning in memoryPool to -0.2
Modified index 13's learning in memoryPool to -0.2
Modified index 14's learning in memoryPool to -0.2
Modified index 15's learning in memoryPool to -0.2
Modified index 16's learning in memoryPool to -0.2
Modified index 17's learning in memoryPool to -0.2
Modified index 18's learning in memoryPool to -0.2
Modified index 19's learning in memoryPool to -0.2
Modified index 20's learning in memoryPool to -0.2
Modified index 21's learning in memoryPool to -0.2
Modified index 22's learning in memoryPool to -0.2
Modified index 23's learning in memoryPool to -0.2
Modified index 24's learning in memoryPool to -0.2
Modified index 25's learning in memoryPool to -0.2
Modified index 26's learning in memoryPool to -0.2
10/5/2016 1:40:35 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 5, 0, -0.2
sum 0.0388979645284537 distri -0.0175499237274994
Using diff 0.0467233971238397 and condRate 0.166666666666667
Changed category 0 weights from 
0.19108609978582 to 0.189528653191817
-0.208948864298805 to -0.210506310892808
-0.491469560819611 to -0.493027007413614
-0.33996419485186 to -0.341521641445863
Changing layer 0's weights from 
-0.173675556215055 to -0.175233002809058
-0.545602012904889 to -0.547159459498891
-0.779951278599508 to -0.78150872519351
-0.745392028721577 to -0.746949475315579
0.00247554060577537 to 0.000918094011772948
-0.680166412624128 to -0.68172385921813
-0.526336689027555 to -0.527894135621557
-0.55379109505058 to -0.555348541644582
-0.791282956036336 to -0.792840402630338
-0.835033011170632 to -0.836590457764634
Changing layer 1's weights from 
-0.829936098786599 to -0.831493545380601
-0.228767414125212 to -0.230324860719215
-0.354470152887114 to -0.356027599481117
-0.364375491174467 to -0.36593293776847
-0.549428273471601 to -0.550985720065603
-0.687666107448347 to -0.689223554042349
-0.416947383912809 to -0.418504830506812
-0.802750412853964 to -0.804307859447966
-0.391087431939848 to -0.392644878533851
-0.753734294804341 to -0.755291741398343
Changing layer 2's weights from 
0.0554539370214853 to 0.0538964904274828
-0.498599548372038 to -0.50015699496604
-0.214010257753141 to -0.215567704347144
-0.0428101611459334 to -0.0443676077399359
-0.220062871011503 to -0.221620317605506
-0.426072497400053 to -0.427629943994056
0.0927402186071792 to 0.0911827720131768
0.0203181910192882 to 0.0187607444252858
-0.711680103572614 to -0.713237550166616
-0.666722584995039 to -0.668280031589041
Changing layer 3's weights from 
0.0224036383306892 to 0.0208461917366868
-0.241149444612272 to -0.242706891206275
0.00146047350525037 to -9.69730887520519E-05
-0.905978758250892 to -0.907536204844894
-0.121215422185667 to -0.12277286877967
-0.251464028390654 to -0.253021474984657
-0.581648875507123 to -0.583206322101125
-0.12313600304962 to -0.124693449643623
-0.454831976922758 to -0.456389423516761
0.0179979014074713 to 0.0164404548134689
Changing layer 4's weights from 
-0.0812758994424422 to -0.0828333460364446
-0.290108699830778 to -0.291666146424781
-0.490586299928434 to -0.492143746522437
-0.203651566537626 to -0.205209013131629
-0.442904372247465 to -0.444461818841468
-0.21143915179611 to -0.212996598390113
-0.215797681840666 to -0.217355128434669
-0.67769520405174 to -0.679252650645742
-0.0839991164529402 to -0.0855565630469426
-0.516301054986723 to -0.517858501580725
Changing layer 5's weights from 
0.0715039181387343 to 0.0699464715447319
-0.370569605859526 to -0.372127052453529
0.0546187567388933 to 0.0530613101448908
-0.880893424632557 to -0.882450871226559
-0.343624610933073 to -0.345182057527076
-0.742092554005392 to -0.743650000599394
-0.442127246888883 to -0.443684693482886
-0.369681019815214 to -0.371238466409217
0.0199209856665052 to 0.0183635390725028
0.0134128737127692 to 0.0118554271187668
Trying to learn from memory 6, 0, -0.2
sum 0.0397115215869907 distri -0.0168064122108915
Using diff 0.0465900534011345 and condRate 0.166666666666667
Changed category 0 weights from 
0.189528653191817 to 0.187975651388638
-0.210506310892808 to -0.212059312695987
-0.493027007413614 to -0.494580009216793
-0.341521641445863 to -0.343074643249042
Changing layer 0's weights from 
-0.175233002809058 to -0.176786004612237
-0.547159459498891 to -0.548712461302071
-0.78150872519351 to -0.78306172699669
-0.746949475315579 to -0.748502477118759
0.000918094011772948 to -0.0006349077914064
-0.68172385921813 to -0.68327686102131
-0.527894135621557 to -0.529447137424737
-0.555348541644582 to -0.556901543447762
-0.792840402630338 to -0.794393404433518
-0.836590457764634 to -0.838143459567814
Changing layer 1's weights from 
-0.831493545380601 to -0.833046547183781
-0.230324860719215 to -0.231877862522394
-0.356027599481117 to -0.357580601284296
-0.36593293776847 to -0.367485939571649
-0.550985720065603 to -0.552538721868783
-0.689223554042349 to -0.690776555845529
-0.418504830506812 to -0.420057832309991
-0.804307859447966 to -0.805860861251146
-0.392644878533851 to -0.39419788033703
-0.755291741398343 to -0.756844743201523
Changing layer 2's weights from 
0.0538964904274828 to 0.0523434886243035
-0.50015699496604 to -0.50170999676922
-0.215567704347144 to -0.217120706150323
-0.0443676077399359 to -0.0459206095431152
-0.221620317605506 to -0.223173319408685
-0.427629943994056 to -0.429182945797235
0.0911827720131768 to 0.0896297702099974
0.0187607444252858 to 0.0172077426221065
-0.713237550166616 to -0.714790551969796
-0.668280031589041 to -0.669833033392221
Changing layer 3's weights from 
0.0208461917366868 to 0.0192931899335075
-0.242706891206275 to -0.244259893009454
-9.69730887520519E-05 to -0.0016499748919314
-0.907536204844894 to -0.909089206648074
-0.12277286877967 to -0.124325870582849
-0.253021474984657 to -0.254574476787836
-0.583206322101125 to -0.584759323904305
-0.124693449643623 to -0.126246451446802
-0.456389423516761 to -0.45794242531994
0.0164404548134689 to 0.0148874530102896
Changing layer 4's weights from 
-0.0828333460364446 to -0.084386347839624
-0.291666146424781 to -0.29321914822796
-0.492143746522437 to -0.493696748325616
-0.205209013131629 to -0.206762014934808
-0.444461818841468 to -0.446014820644647
-0.212996598390113 to -0.214549600193292
-0.217355128434669 to -0.218908130237848
-0.679252650645742 to -0.680805652448922
-0.0855565630469426 to -0.087109564850122
-0.517858501580725 to -0.519411503383905
Changing layer 5's weights from 
0.0699464715447319 to 0.0683934697415525
-0.372127052453529 to -0.373680054256708
0.0530613101448908 to 0.0515083083417115
-0.882450871226559 to -0.884003873029739
-0.345182057527076 to -0.346735059330255
-0.743650000599394 to -0.745203002402574
-0.443684693482886 to -0.445237695286065
-0.371238466409217 to -0.372791468212396
0.0183635390725028 to 0.0168105372693235
0.0118554271187668 to 0.0103024253155875
Trying to learn from memory 7, 1, -0.2
sum 0.0397115215869907 distri 0.0205986260683996
Using diff 0.00918501512184347 and condRate 0.166666666666667
Changed category 1 weights from 
0.299744611183482 to 0.299438444008192
0.533060198227244 to 0.532754031051954
0.108134960333186 to 0.107828793157896
0.112227772871333 to 0.111921605696043
Changing layer 0's weights from 
-0.176786004612237 to -0.177092171787527
-0.548712461302071 to -0.549018628477361
-0.78306172699669 to -0.78336789417198
-0.748502477118759 to -0.748808644294049
-0.0006349077914064 to -0.000941074966696762
-0.68327686102131 to -0.6835830281966
-0.529447137424737 to -0.529753304600027
-0.556901543447762 to -0.557207710623052
-0.794393404433518 to -0.794699571608808
-0.838143459567814 to -0.838449626743104
Changing layer 1's weights from 
-0.833046547183781 to -0.833352714359071
-0.231877862522394 to -0.232184029697684
-0.357580601284296 to -0.357886768459586
-0.367485939571649 to -0.367792106746939
-0.552538721868783 to -0.552844889044073
-0.690776555845529 to -0.691082723020819
-0.420057832309991 to -0.420363999485281
-0.805860861251146 to -0.806167028426436
-0.39419788033703 to -0.39450404751232
-0.756844743201523 to -0.757150910376813
Changing layer 2's weights from 
0.0523434886243035 to 0.0520373214490131
-0.50170999676922 to -0.50201616394451
-0.217120706150323 to -0.217426873325613
-0.0459206095431152 to -0.0462267767184056
-0.223173319408685 to -0.223479486583975
-0.429182945797235 to -0.429489112972525
0.0896297702099974 to 0.089323603034707
0.0172077426221065 to 0.0169015754468161
-0.714790551969796 to -0.715096719145086
-0.669833033392221 to -0.670139200567511
Changing layer 3's weights from 
0.0192931899335075 to 0.0189870227582171
-0.244259893009454 to -0.244566060184744
-0.0016499748919314 to -0.00195614206722176
-0.909089206648074 to -0.909395373823364
-0.124325870582849 to -0.124632037758139
-0.254574476787836 to -0.254880643963126
-0.584759323904305 to -0.585065491079595
-0.126246451446802 to -0.126552618622092
-0.45794242531994 to -0.45824859249523
0.0148874530102896 to 0.0145812858349992
Changing layer 4's weights from 
-0.084386347839624 to -0.0846925150149144
-0.29321914822796 to -0.29352531540325
-0.493696748325616 to -0.494002915500906
-0.206762014934808 to -0.207068182110098
-0.446014820644647 to -0.446320987819937
-0.214549600193292 to -0.214855767368582
-0.218908130237848 to -0.219214297413138
-0.680805652448922 to -0.681111819624212
-0.087109564850122 to -0.0874157320254123
-0.519411503383905 to -0.519717670559195
Changing layer 5's weights from 
0.0683934697415525 to 0.0680873025662621
-0.373680054256708 to -0.373986221431998
0.0515083083417115 to 0.0512021411664211
-0.884003873029739 to -0.884310040205029
-0.346735059330255 to -0.347041226505545
-0.745203002402574 to -0.745509169577864
-0.445237695286065 to -0.445543862461355
-0.372791468212396 to -0.373097635387686
0.0168105372693235 to 0.0165043700940331
0.0103024253155875 to 0.0099962581402971
Trying to learn from memory 8, 1, -0.2
sum 0.0397115215869907 distri 0.0205986260683996
Using diff 0.00918501512184347 and condRate 0.166666666666667
Changed category 1 weights from 
0.299438444008192 to 0.299132276832901
0.532754031051954 to 0.532447863876664
0.107828793157896 to 0.107522625982606
0.111921605696043 to 0.111615438520753
Changing layer 0's weights from 
-0.177092171787527 to -0.177398338962818
-0.549018628477361 to -0.549324795652652
-0.78336789417198 to -0.783674061347271
-0.748808644294049 to -0.74911481146934
-0.000941074966696762 to -0.00124724214198712
-0.6835830281966 to -0.683889195371891
-0.529753304600027 to -0.530059471775318
-0.557207710623052 to -0.557513877798342
-0.794699571608808 to -0.795005738784099
-0.838449626743104 to -0.838755793918395
Changing layer 1's weights from 
-0.833352714359071 to -0.833658881534362
-0.232184029697684 to -0.232490196872975
-0.357886768459586 to -0.358192935634877
-0.367792106746939 to -0.36809827392223
-0.552844889044073 to -0.553151056219364
-0.691082723020819 to -0.69138889019611
-0.420363999485281 to -0.420670166660572
-0.806167028426436 to -0.806473195601727
-0.39450404751232 to -0.394810214687611
-0.757150910376813 to -0.757457077552104
Changing layer 2's weights from 
0.0520373214490131 to 0.0517311542737228
-0.50201616394451 to -0.502322331119801
-0.217426873325613 to -0.217733040500904
-0.0462267767184056 to -0.0465329438936959
-0.223479486583975 to -0.223785653759266
-0.429489112972525 to -0.429795280147816
0.089323603034707 to 0.0890174358594167
0.0169015754468161 to 0.0165954082715257
-0.715096719145086 to -0.715402886320377
-0.670139200567511 to -0.670445367742802
Changing layer 3's weights from 
0.0189870227582171 to 0.0186808555829267
-0.244566060184744 to -0.244872227360035
-0.00195614206722176 to -0.00226230924251212
-0.909395373823364 to -0.909701540998655
-0.124632037758139 to -0.12493820493343
-0.254880643963126 to -0.255186811138417
-0.585065491079595 to -0.585371658254886
-0.126552618622092 to -0.126858785797383
-0.45824859249523 to -0.458554759670521
0.0145812858349992 to 0.0142751186597088
Changing layer 4's weights from 
-0.0846925150149144 to -0.0849986821902047
-0.29352531540325 to -0.293831482578541
-0.494002915500906 to -0.494309082676197
-0.207068182110098 to -0.207374349285389
-0.446320987819937 to -0.446627154995228
-0.214855767368582 to -0.215161934543873
-0.219214297413138 to -0.219520464588429
-0.681111819624212 to -0.681417986799503
-0.0874157320254123 to -0.0877218992007027
-0.519717670559195 to -0.520023837734486
Changing layer 5's weights from 
0.0680873025662621 to 0.0677811353909718
-0.373986221431998 to -0.374292388607289
0.0512021411664211 to 0.0508959739911308
-0.884310040205029 to -0.88461620738032
-0.347041226505545 to -0.347347393680836
-0.745509169577864 to -0.745815336753155
-0.445543862461355 to -0.445850029636646
-0.373097635387686 to -0.373403802562977
0.0165043700940331 to 0.0161982029187427
0.0099962581402971 to 0.00969009096500674
Trying to learn from memory 9, 1, -0.2
sum 0.0397115215869907 distri 0.0205986260683996
Using diff 0.00918501512184347 and condRate 0.166666666666667
Changed category 1 weights from 
0.299132276832901 to 0.298826109657611
0.532447863876664 to 0.532141696701373
0.107522625982606 to 0.107216458807315
0.111615438520753 to 0.111309271345462
Changing layer 0's weights from 
-0.177398338962818 to -0.177704506138108
-0.549324795652652 to -0.549630962827942
-0.783674061347271 to -0.783980228522561
-0.74911481146934 to -0.74942097864463
-0.00124724214198712 to -0.00155340931727749
-0.683889195371891 to -0.684195362547181
-0.530059471775318 to -0.530365638950608
-0.557513877798342 to -0.557820044973633
-0.795005738784099 to -0.795311905959389
-0.838755793918395 to -0.839061961093685
Changing layer 1's weights from 
-0.833658881534362 to -0.833965048709652
-0.232490196872975 to -0.232796364048265
-0.358192935634877 to -0.358499102810167
-0.36809827392223 to -0.36840444109752
-0.553151056219364 to -0.553457223394654
-0.69138889019611 to -0.6916950573714
-0.420670166660572 to -0.420976333835862
-0.806473195601727 to -0.806779362777017
-0.394810214687611 to -0.395116381862901
-0.757457077552104 to -0.757763244727394
Changing layer 2's weights from 
0.0517311542737228 to 0.0514249870984324
-0.502322331119801 to -0.502628498295091
-0.217733040500904 to -0.218039207676194
-0.0465329438936959 to -0.0468391110689863
-0.223785653759266 to -0.224091820934556
-0.429795280147816 to -0.430101447323106
0.0890174358594167 to 0.0887112686841263
0.0165954082715257 to 0.0162892410962354
-0.715402886320377 to -0.715709053495667
-0.670445367742802 to -0.670751534918092
Changing layer 3's weights from 
0.0186808555829267 to 0.0183746884076364
-0.244872227360035 to -0.245178394535325
-0.00226230924251212 to -0.00256847641780249
-0.909701540998655 to -0.910007708173945
-0.12493820493343 to -0.12524437210872
-0.255186811138417 to -0.255492978313707
-0.585371658254886 to -0.585677825430176
-0.126858785797383 to -0.127164952972673
-0.458554759670521 to -0.458860926845811
0.0142751186597088 to 0.0139689514844185
Changing layer 4's weights from 
-0.0849986821902047 to -0.0853048493654951
-0.293831482578541 to -0.294137649753831
-0.494309082676197 to -0.494615249851487
-0.207374349285389 to -0.207680516460679
-0.446627154995228 to -0.446933322170518
-0.215161934543873 to -0.215468101719163
-0.219520464588429 to -0.219826631763719
-0.681417986799503 to -0.681724153974793
-0.0877218992007027 to -0.0880280663759931
-0.520023837734486 to -0.520330004909776
Changing layer 5's weights from 
0.0677811353909718 to 0.0674749682156814
-0.374292388607289 to -0.374598555782579
0.0508959739911308 to 0.0505898068158404
-0.88461620738032 to -0.88492237455561
-0.347347393680836 to -0.347653560856126
-0.745815336753155 to -0.746121503928445
-0.445850029636646 to -0.446156196811936
-0.373403802562977 to -0.373709969738267
0.0161982029187427 to 0.0158920357434524
0.00969009096500674 to 0.00938392378971638
Trying to learn from memory 10, 0, -0.2
sum 0.0397115215869907 distri -0.0168064122108915
Using diff 0.0465900534011345 and condRate 0.166666666666667
Changed category 0 weights from 
0.187975651388638 to 0.186422649585459
-0.212059312695987 to -0.213612314499167
-0.494580009216793 to -0.496133011019972
-0.343074643249042 to -0.344627645052221
Changing layer 0's weights from 
-0.177704506138108 to -0.179257507941287
-0.549630962827942 to -0.551183964631121
-0.783980228522561 to -0.78553323032574
-0.74942097864463 to -0.750973980447809
-0.00155340931727749 to -0.00310641112045683
-0.684195362547181 to -0.68574836435036
-0.530365638950608 to -0.531918640753787
-0.557820044973633 to -0.559373046776812
-0.795311905959389 to -0.796864907762568
-0.839061961093685 to -0.840614962896864
Changing layer 1's weights from 
-0.833965048709652 to -0.835518050512831
-0.232796364048265 to -0.234349365851444
-0.358499102810167 to -0.360052104613346
-0.36840444109752 to -0.369957442900699
-0.553457223394654 to -0.555010225197833
-0.6916950573714 to -0.693248059174579
-0.420976333835862 to -0.422529335639041
-0.806779362777017 to -0.808332364580196
-0.395116381862901 to -0.39666938366608
-0.757763244727394 to -0.759316246530573
Changing layer 2's weights from 
0.0514249870984324 to 0.0498719852952531
-0.502628498295091 to -0.50418150009827
-0.218039207676194 to -0.219592209479373
-0.0468391110689863 to -0.0483921128721656
-0.224091820934556 to -0.225644822737735
-0.430101447323106 to -0.431654449126285
0.0887112686841263 to 0.087158266880947
0.0162892410962354 to 0.014736239293056
-0.715709053495667 to -0.717262055298846
-0.670751534918092 to -0.672304536721271
Changing layer 3's weights from 
0.0183746884076364 to 0.016821686604457
-0.245178394535325 to -0.246731396338504
-0.00256847641780249 to -0.00412147822098183
-0.910007708173945 to -0.911560709977124
-0.12524437210872 to -0.126797373911899
-0.255492978313707 to -0.257045980116886
-0.585677825430176 to -0.587230827233355
-0.127164952972673 to -0.128717954775852
-0.458860926845811 to -0.46041392864899
0.0139689514844185 to 0.0124159496812391
Changing layer 4's weights from 
-0.0853048493654951 to -0.0868578511686744
-0.294137649753831 to -0.29569065155701
-0.494615249851487 to -0.496168251654666
-0.207680516460679 to -0.209233518263858
-0.446933322170518 to -0.448486323973697
-0.215468101719163 to -0.217021103522342
-0.219826631763719 to -0.221379633566898
-0.681724153974793 to -0.683277155777972
-0.0880280663759931 to -0.0895810681791724
-0.520330004909776 to -0.521883006712955
Changing layer 5's weights from 
0.0674749682156814 to 0.0659219664125021
-0.374598555782579 to -0.376151557585758
0.0505898068158404 to 0.0490368050126611
-0.88492237455561 to -0.886475376358789
-0.347653560856126 to -0.349206562659305
-0.746121503928445 to -0.747674505731624
-0.446156196811936 to -0.447709198615115
-0.373709969738267 to -0.375262971541446
0.0158920357434524 to 0.014339033940273
0.00938392378971638 to 0.00783092198653703
Trying to learn from memory 10, 1, -0.2
sum 0.0397115215869907 distri 0.0205986260683996
Using diff 0.00918501512184347 and condRate 0.166666666666667
Changed category 1 weights from 
0.298826109657611 to 0.298519942482321
0.532141696701373 to 0.531835529526083
0.107216458807315 to 0.106910291632025
0.111309271345462 to 0.111003104170172
Changing layer 0's weights from 
-0.179257507941287 to -0.179563675116578
-0.551183964631121 to -0.551490131806412
-0.78553323032574 to -0.785839397501031
-0.750973980447809 to -0.7512801476231
-0.00310641112045683 to -0.0034125782957472
-0.68574836435036 to -0.686054531525651
-0.531918640753787 to -0.532224807929078
-0.559373046776812 to -0.559679213952103
-0.796864907762568 to -0.797171074937859
-0.840614962896864 to -0.840921130072155
Changing layer 1's weights from 
-0.835518050512831 to -0.835824217688122
-0.234349365851444 to -0.234655533026735
-0.360052104613346 to -0.360358271788637
-0.369957442900699 to -0.37026361007599
-0.555010225197833 to -0.555316392373124
-0.693248059174579 to -0.69355422634987
-0.422529335639041 to -0.422835502814332
-0.808332364580196 to -0.808638531755487
-0.39666938366608 to -0.396975550841371
-0.759316246530573 to -0.759622413705864
Changing layer 2's weights from 
0.0498719852952531 to 0.0495658181199627
-0.50418150009827 to -0.504487667273561
-0.219592209479373 to -0.219898376654664
-0.0483921128721656 to -0.048698280047456
-0.225644822737735 to -0.225950989913026
-0.431654449126285 to -0.431960616301576
0.087158266880947 to 0.0868520997056566
0.014736239293056 to 0.0144300721177657
-0.717262055298846 to -0.717568222474137
-0.672304536721271 to -0.672610703896562
Changing layer 3's weights from 
0.016821686604457 to 0.0165155194291667
-0.246731396338504 to -0.247037563513795
-0.00412147822098183 to -0.0044276453962722
-0.911560709977124 to -0.911866877152415
-0.126797373911899 to -0.12710354108719
-0.257045980116886 to -0.257352147292177
-0.587230827233355 to -0.587536994408646
-0.128717954775852 to -0.129024121951143
-0.46041392864899 to -0.460720095824281
0.0124159496812391 to 0.0121097825059488
Changing layer 4's weights from 
-0.0868578511686744 to -0.0871640183439648
-0.29569065155701 to -0.295996818732301
-0.496168251654666 to -0.496474418829957
-0.209233518263858 to -0.209539685439149
-0.448486323973697 to -0.448792491148988
-0.217021103522342 to -0.217327270697633
-0.221379633566898 to -0.221685800742189
-0.683277155777972 to -0.683583322953263
-0.0895810681791724 to -0.0898872353544628
-0.521883006712955 to -0.522189173888246
Changing layer 5's weights from 
0.0659219664125021 to 0.0656157992372117
-0.376151557585758 to -0.376457724761049
0.0490368050126611 to 0.0487306378373707
-0.886475376358789 to -0.88678154353408
-0.349206562659305 to -0.349512729834596
-0.747674505731624 to -0.747980672906915
-0.447709198615115 to -0.448015365790406
-0.375262971541446 to -0.375569138716737
0.014339033940273 to 0.0140328667649827
0.00783092198653703 to 0.00752475481124667
Trying to learn from memory 11, 1, -0.2
sum 0.0397115215869907 distri 0.0205986260683996
Using diff 0.00918501512184347 and condRate 0.166666666666667
Changed category 1 weights from 
0.298519942482321 to 0.29821377530703
0.531835529526083 to 0.531529362350792
0.106910291632025 to 0.106604124456734
0.111003104170172 to 0.110696936994881
Changing layer 0's weights from 
-0.179563675116578 to -0.179869842291868
-0.551490131806412 to -0.551796298981702
-0.785839397501031 to -0.786145564676321
-0.7512801476231 to -0.75158631479839
-0.0034125782957472 to -0.00371874547103756
-0.686054531525651 to -0.686360698700941
-0.532224807929078 to -0.532530975104368
-0.559679213952103 to -0.559985381127393
-0.797171074937859 to -0.797477242113149
-0.840921130072155 to -0.841227297247445
Changing layer 1's weights from 
-0.835824217688122 to -0.836130384863412
-0.234655533026735 to -0.234961700202025
-0.360358271788637 to -0.360664438963927
-0.37026361007599 to -0.37056977725128
-0.555316392373124 to -0.555622559548414
-0.69355422634987 to -0.69386039352516
-0.422835502814332 to -0.423141669989622
-0.808638531755487 to -0.808944698930777
-0.396975550841371 to -0.397281718016661
-0.759622413705864 to -0.759928580881154
Changing layer 2's weights from 
0.0495658181199627 to 0.0492596509446723
-0.504487667273561 to -0.504793834448851
-0.219898376654664 to -0.220204543829954
-0.048698280047456 to -0.0490044472227464
-0.225950989913026 to -0.226257157088316
-0.431960616301576 to -0.432266783476866
0.0868520997056566 to 0.0865459325303662
0.0144300721177657 to 0.0141239049424753
-0.717568222474137 to -0.717874389649427
-0.672610703896562 to -0.672916871071852
Changing layer 3's weights from 
0.0165155194291667 to 0.0162093522538763
-0.247037563513795 to -0.247343730689085
-0.0044276453962722 to -0.00473381257156256
-0.911866877152415 to -0.912173044327705
-0.12710354108719 to -0.12740970826248
-0.257352147292177 to -0.257658314467467
-0.587536994408646 to -0.587843161583936
-0.129024121951143 to -0.129330289126433
-0.460720095824281 to -0.461026262999571
0.0121097825059488 to 0.0118036153306584
Changing layer 4's weights from 
-0.0871640183439648 to -0.0874701855192552
-0.295996818732301 to -0.296302985907591
-0.496474418829957 to -0.496780586005247
-0.209539685439149 to -0.209845852614439
-0.448792491148988 to -0.449098658324278
-0.217327270697633 to -0.217633437872923
-0.221685800742189 to -0.221991967917479
-0.683583322953263 to -0.683889490128553
-0.0898872353544628 to -0.0901934025297532
-0.522189173888246 to -0.522495341063536
Changing layer 5's weights from 
0.0656157992372117 to 0.0653096320619213
-0.376457724761049 to -0.376763891936339
0.0487306378373707 to 0.0484244706620803
-0.88678154353408 to -0.88708771070937
-0.349512729834596 to -0.349818897009886
-0.747980672906915 to -0.748286840082205
-0.448015365790406 to -0.448321532965696
-0.375569138716737 to -0.375875305892027
0.0140328667649827 to 0.0137266995896923
0.00752475481124667 to 0.00721858763595631
Trying to learn from memory 11, 1, -0.2
sum 0.0397115215869907 distri 0.0205986260683996
Using diff 0.00918501512184347 and condRate 0.166666666666667
Changed category 1 weights from 
0.29821377530703 to 0.29790760813174
0.531529362350792 to 0.531223195175502
0.106604124456734 to 0.106297957281444
0.110696936994881 to 0.110390769819591
Changing layer 0's weights from 
-0.179869842291868 to -0.180176009467158
-0.551796298981702 to -0.552102466156993
-0.786145564676321 to -0.786451731851612
-0.75158631479839 to -0.751892481973681
-0.00371874547103756 to -0.00402491264632792
-0.686360698700941 to -0.686666865876231
-0.532530975104368 to -0.532837142279659
-0.559985381127393 to -0.560291548302683
-0.797477242113149 to -0.797783409288439
-0.841227297247445 to -0.841533464422735
Changing layer 1's weights from 
-0.836130384863412 to -0.836436552038702
-0.234961700202025 to -0.235267867377315
-0.360664438963927 to -0.360970606139218
-0.37056977725128 to -0.370875944426571
-0.555622559548414 to -0.555928726723705
-0.69386039352516 to -0.69416656070045
-0.423141669989622 to -0.423447837164913
-0.808944698930777 to -0.809250866106068
-0.397281718016661 to -0.397587885191952
-0.759928580881154 to -0.760234748056445
Changing layer 2's weights from 
0.0492596509446723 to 0.048953483769382
-0.504793834448851 to -0.505100001624142
-0.220204543829954 to -0.220510711005244
-0.0490044472227464 to -0.0493106143980367
-0.226257157088316 to -0.226563324263606
-0.432266783476866 to -0.432572950652157
0.0865459325303662 to 0.0862397653550759
0.0141239049424753 to 0.0138177377671849
-0.717874389649427 to -0.718180556824717
-0.672916871071852 to -0.673223038247142
Changing layer 3's weights from 
0.0162093522538763 to 0.0159031850785859
-0.247343730689085 to -0.247649897864375
-0.00473381257156256 to -0.00503997974685292
-0.912173044327705 to -0.912479211502995
-0.12740970826248 to -0.12771587543777
-0.257658314467467 to -0.257964481642758
-0.587843161583936 to -0.588149328759227
-0.129330289126433 to -0.129636456301723
-0.461026262999571 to -0.461332430174862
0.0118036153306584 to 0.011497448155368
Changing layer 4's weights from 
-0.0874701855192552 to -0.0877763526945455
-0.296302985907591 to -0.296609153082881
-0.496780586005247 to -0.497086753180537
-0.209845852614439 to -0.210152019789729
-0.449098658324278 to -0.449404825499569
-0.217633437872923 to -0.217939605048213
-0.221991967917479 to -0.222298135092769
-0.683889490128553 to -0.684195657303843
-0.0901934025297532 to -0.0904995697050435
-0.522495341063536 to -0.522801508238827
Changing layer 5's weights from 
0.0653096320619213 to 0.065003464886631
-0.376763891936339 to -0.37707005911163
0.0484244706620803 to 0.04811830348679
-0.88708771070937 to -0.887393877884661
-0.349818897009886 to -0.350125064185177
-0.748286840082205 to -0.748593007257496
-0.448321532965696 to -0.448627700140987
-0.375875305892027 to -0.376181473067318
0.0137266995896923 to 0.013420532414402
0.00721858763595631 to 0.00691242046066595
Trying to learn from memory 11, 0, -0.2
sum 0.0397115215869907 distri -0.0168064122108915
Using diff 0.0465900534011345 and condRate 0.166666666666667
Changed category 0 weights from 
0.186422649585459 to 0.184869647782279
-0.213612314499167 to -0.215165316302346
-0.496133011019972 to -0.497686012823152
-0.344627645052221 to -0.346180646855401
Changing layer 0's weights from 
-0.180176009467158 to -0.181729011270338
-0.552102466156993 to -0.553655467960172
-0.786451731851612 to -0.788004733654791
-0.751892481973681 to -0.75344548377686
-0.00402491264632792 to -0.00557791444950727
-0.686666865876231 to -0.688219867679411
-0.532837142279659 to -0.534390144082838
-0.560291548302683 to -0.561844550105863
-0.797783409288439 to -0.799336411091619
-0.841533464422735 to -0.843086466225915
Changing layer 1's weights from 
-0.836436552038702 to -0.837989553841882
-0.235267867377315 to -0.236820869180495
-0.360970606139218 to -0.362523607942397
-0.370875944426571 to -0.37242894622975
-0.555928726723705 to -0.557481728526884
-0.69416656070045 to -0.69571956250363
-0.423447837164913 to -0.425000838968092
-0.809250866106068 to -0.810803867909247
-0.397587885191952 to -0.399140886995131
-0.760234748056445 to -0.761787749859624
Changing layer 2's weights from 
0.048953483769382 to 0.0474004819662026
-0.505100001624142 to -0.506653003427321
-0.220510711005244 to -0.222063712808424
-0.0493106143980367 to -0.0508636162012161
-0.226563324263606 to -0.228116326066786
-0.432572950652157 to -0.434125952455336
0.0862397653550759 to 0.0846867635518965
0.0138177377671849 to 0.0122647359640056
-0.718180556824717 to -0.719733558627897
-0.673223038247142 to -0.674776040050322
Changing layer 3's weights from 
0.0159031850785859 to 0.0143501832754066
-0.247649897864375 to -0.249202899667555
-0.00503997974685292 to -0.00659298155003227
-0.912479211502995 to -0.914032213306175
-0.12771587543777 to -0.12926887724095
-0.257964481642758 to -0.259517483445937
-0.588149328759227 to -0.589702330562406
-0.129636456301723 to -0.131189458104903
-0.461332430174862 to -0.462885431978041
0.011497448155368 to 0.00994444635218869
Changing layer 4's weights from 
-0.0877763526945455 to -0.0893293544977249
-0.296609153082881 to -0.298162154886061
-0.497086753180537 to -0.498639754983717
-0.210152019789729 to -0.211705021592909
-0.449404825499569 to -0.450957827302748
-0.217939605048213 to -0.219492606851393
-0.222298135092769 to -0.223851136895949
-0.684195657303843 to -0.685748659107023
-0.0904995697050435 to -0.0920525715082229
-0.522801508238827 to -0.524354510042006
Changing layer 5's weights from 
0.065003464886631 to 0.0634504630834516
-0.37707005911163 to -0.378623060914809
0.04811830348679 to 0.0465653016836106
-0.887393877884661 to -0.88894687968784
-0.350125064185177 to -0.351678065988356
-0.748593007257496 to -0.750146009060675
-0.448627700140987 to -0.450180701944166
-0.376181473067318 to -0.377734474870497
0.013420532414402 to 0.0118675306112226
0.00691242046066595 to 0.0053594186574866
10/5/2016 1:40:36 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 11, 1, -0.2
sum 0.0397115215869907 distri 0.0205986260683996
Using diff 0.00918501512184347 and condRate 0.166666666666667
Changed category 1 weights from 
0.29790760813174 to 0.29760144095645
0.531223195175502 to 0.530917028000211
0.106297957281444 to 0.105991790106154
0.110390769819591 to 0.110084602644301
Changing layer 0's weights from 
-0.181729011270338 to -0.182035178445628
-0.553655467960172 to -0.553961635135462
-0.788004733654791 to -0.788310900830081
-0.75344548377686 to -0.75375165095215
-0.00557791444950727 to -0.00588408162479763
-0.688219867679411 to -0.688526034854701
-0.534390144082838 to -0.534696311258128
-0.561844550105863 to -0.562150717281153
-0.799336411091619 to -0.799642578266909
-0.843086466225915 to -0.843392633401205
Changing layer 1's weights from 
-0.837989553841882 to -0.838295721017172
-0.236820869180495 to -0.237127036355785
-0.362523607942397 to -0.362829775117687
-0.37242894622975 to -0.37273511340504
-0.557481728526884 to -0.557787895702174
-0.69571956250363 to -0.69602572967892
-0.425000838968092 to -0.425307006143382
-0.810803867909247 to -0.811110035084537
-0.399140886995131 to -0.399447054170421
-0.761787749859624 to -0.762093917034914
Changing layer 2's weights from 
0.0474004819662026 to 0.0470943147909123
-0.506653003427321 to -0.506959170602611
-0.222063712808424 to -0.222369879983714
-0.0508636162012161 to -0.0511697833765064
-0.228116326066786 to -0.228422493242076
-0.434125952455336 to -0.434432119630626
0.0846867635518965 to 0.0843805963766062
0.0122647359640056 to 0.0119585687887152
-0.719733558627897 to -0.720039725803187
-0.674776040050322 to -0.675082207225612
Changing layer 3's weights from 
0.0143501832754066 to 0.0140440161001162
-0.249202899667555 to -0.249509066842845
-0.00659298155003227 to -0.00689914872532263
-0.914032213306175 to -0.914338380481465
-0.12926887724095 to -0.12957504441624
-0.259517483445937 to -0.259823650621227
-0.589702330562406 to -0.590008497737696
-0.131189458104903 to -0.131495625280193
-0.462885431978041 to -0.463191599153331
0.00994444635218869 to 0.00963827917689833
Changing layer 4's weights from 
-0.0893293544977249 to -0.0896355216730152
-0.298162154886061 to -0.298468322061351
-0.498639754983717 to -0.498945922159007
-0.211705021592909 to -0.212011188768199
-0.450957827302748 to -0.451263994478038
-0.219492606851393 to -0.219798774026683
-0.223851136895949 to -0.224157304071239
-0.685748659107023 to -0.686054826282313
-0.0920525715082229 to -0.0923587386835132
-0.524354510042006 to -0.524660677217296
Changing layer 5's weights from 
0.0634504630834516 to 0.0631442959081613
-0.378623060914809 to -0.378929228090099
0.0465653016836106 to 0.0462591345083203
-0.88894687968784 to -0.88925304686313
-0.351678065988356 to -0.351984233163646
-0.750146009060675 to -0.750452176235965
-0.450180701944166 to -0.450486869119456
-0.377734474870497 to -0.378040642045787
0.0118675306112226 to 0.0115613634359322
0.0053594186574866 to 0.00505325148219624
Trying to learn from memory 11, 2, -0.2
sum 0.0397115215869907 distri 0.0359193077294827
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.457811464196105 to 0.424478130366066
0.736253429776094 to 0.702920095946055
0.649213065033814 to 0.615879731203775
-0.00535863953266557 to -0.0386919733627043
Changing layer 0's weights from 
-0.182035178445628 to -0.215368512275667
-0.553961635135462 to -0.587294968965501
-0.788310900830081 to -0.82164423466012
-0.75375165095215 to -0.787084984782189
-0.00588408162479763 to -0.0392174154548363
-0.688526034854701 to -0.72185936868474
-0.534696311258128 to -0.568029645088167
-0.562150717281153 to -0.595484051111192
-0.799642578266909 to -0.832975912096948
-0.843392633401205 to -0.876725967231244
Changing layer 1's weights from 
-0.838295721017172 to -0.871629054847211
-0.237127036355785 to -0.270460370185824
-0.362829775117687 to -0.396163108947726
-0.37273511340504 to -0.406068447235079
-0.557787895702174 to -0.591121229532213
-0.69602572967892 to -0.729359063508959
-0.425307006143382 to -0.458640339973421
-0.811110035084537 to -0.844443368914576
-0.399447054170421 to -0.43278038800046
-0.762093917034914 to -0.795427250864953
Changing layer 2's weights from 
0.0470943147909123 to 0.0137609809608736
-0.506959170602611 to -0.54029250443265
-0.222369879983714 to -0.255703213813753
-0.0511697833765064 to -0.0845031172065451
-0.228422493242076 to -0.261755827072115
-0.434432119630626 to -0.467765453460665
0.0843805963766062 to 0.0510472625465675
0.0119585687887152 to -0.0213747650413235
-0.720039725803187 to -0.753373059633226
-0.675082207225612 to -0.708415541055651
Changing layer 3's weights from 
0.0140440161001162 to -0.0192893177299225
-0.249509066842845 to -0.282842400672884
-0.00689914872532263 to -0.0402324825553613
-0.914338380481465 to -0.947671714311504
-0.12957504441624 to -0.162908378246279
-0.259823650621227 to -0.293156984451266
-0.590008497737696 to -0.623341831567735
-0.131495625280193 to -0.164828959110232
-0.463191599153331 to -0.49652493298337
0.00963827917689833 to -0.0236950546531404
Changing layer 4's weights from 
-0.0896355216730152 to -0.122968855503054
-0.298468322061351 to -0.33180165589139
-0.498945922159007 to -0.532279255989046
-0.212011188768199 to -0.245344522598238
-0.451263994478038 to -0.484597328308077
-0.219798774026683 to -0.253132107856722
-0.224157304071239 to -0.257490637901278
-0.686054826282313 to -0.719388160112352
-0.0923587386835132 to -0.125692072513552
-0.524660677217296 to -0.557994011047335
Changing layer 5's weights from 
0.0631442959081613 to 0.0298109620781226
-0.378929228090099 to -0.412262561920138
0.0462591345083203 to 0.0129258006782816
-0.88925304686313 to -0.922586380693169
-0.351984233163646 to -0.385317566993685
-0.750452176235965 to -0.783785510066004
-0.450486869119456 to -0.483820202949495
-0.378040642045787 to -0.411373975875826
0.0115613634359322 to -0.0217719703941065
0.00505325148219624 to -0.0282800823478425
Trying to learn from memory 11, 0, -0.2
sum 0.0397115215869907 distri -0.0168064122108915
Using diff 0.0465900534011345 and condRate 0.166666666666667
Changed category 0 weights from 
0.184869647782279 to 0.1833166459791
-0.215165316302346 to -0.216718318105525
-0.497686012823152 to -0.499239014626331
-0.346180646855401 to -0.34773364865858
Changing layer 0's weights from 
-0.215368512275667 to -0.216921514078846
-0.587294968965501 to -0.58884797076868
-0.82164423466012 to -0.823197236463299
-0.787084984782189 to -0.788637986585368
-0.0392174154548363 to -0.0407704172580157
-0.72185936868474 to -0.723412370487919
-0.568029645088167 to -0.569582646891346
-0.595484051111192 to -0.597037052914371
-0.832975912096948 to -0.834528913900127
-0.876725967231244 to -0.878278969034423
Changing layer 1's weights from 
-0.871629054847211 to -0.87318205665039
-0.270460370185824 to -0.272013371989003
-0.396163108947726 to -0.397716110750905
-0.406068447235079 to -0.407621449038258
-0.591121229532213 to -0.592674231335392
-0.729359063508959 to -0.730912065312138
-0.458640339973421 to -0.4601933417766
-0.844443368914576 to -0.845996370717755
-0.43278038800046 to -0.434333389803639
-0.795427250864953 to -0.796980252668132
Changing layer 2's weights from 
0.0137609809608736 to 0.0122079791576942
-0.54029250443265 to -0.541845506235829
-0.255703213813753 to -0.257256215616932
-0.0845031172065451 to -0.0860561190097245
-0.261755827072115 to -0.263308828875294
-0.467765453460665 to -0.469318455263844
0.0510472625465675 to 0.0494942607433881
-0.0213747650413235 to -0.0229277668445028
-0.753373059633226 to -0.754926061436405
-0.708415541055651 to -0.70996854285883
Changing layer 3's weights from 
-0.0192893177299225 to -0.0208423195331018
-0.282842400672884 to -0.284395402476063
-0.0402324825553613 to -0.0417854843585407
-0.947671714311504 to -0.949224716114683
-0.162908378246279 to -0.164461380049458
-0.293156984451266 to -0.294709986254445
-0.623341831567735 to -0.624894833370914
-0.164828959110232 to -0.166381960913411
-0.49652493298337 to -0.498077934786549
-0.0236950546531404 to -0.0252480564563197
Changing layer 4's weights from 
-0.122968855503054 to -0.124521857306233
-0.33180165589139 to -0.333354657694569
-0.532279255989046 to -0.533832257792225
-0.245344522598238 to -0.246897524401417
-0.484597328308077 to -0.486150330111256
-0.253132107856722 to -0.254685109659901
-0.257490637901278 to -0.259043639704457
-0.719388160112352 to -0.720941161915531
-0.125692072513552 to -0.127245074316731
-0.557994011047335 to -0.559547012850514
Changing layer 5's weights from 
0.0298109620781226 to 0.0282579602749432
-0.412262561920138 to -0.413815563723317
0.0129258006782816 to 0.0113727988751022
-0.922586380693169 to -0.924139382496348
-0.385317566993685 to -0.386870568796864
-0.783785510066004 to -0.785338511869183
-0.483820202949495 to -0.485373204752674
-0.411373975875826 to -0.412926977679005
-0.0217719703941065 to -0.0233249721972858
-0.0282800823478425 to -0.0298330841510218
Trying to learn from memory 11, 1, -0.2
sum 0.0397115215869907 distri 0.0205986260683996
Using diff 0.00918501512184347 and condRate 0.166666666666667
Changed category 1 weights from 
0.29760144095645 to 0.297295273781159
0.530917028000211 to 0.530610860824921
0.105991790106154 to 0.105685622930863
0.110084602644301 to 0.10977843546901
Changing layer 0's weights from 
-0.216921514078846 to -0.217227681254137
-0.58884797076868 to -0.589154137943971
-0.823197236463299 to -0.82350340363859
-0.788637986585368 to -0.788944153760659
-0.0407704172580157 to -0.041076584433306
-0.723412370487919 to -0.72371853766321
-0.569582646891346 to -0.569888814066637
-0.597037052914371 to -0.597343220089662
-0.834528913900127 to -0.834835081075418
-0.878278969034423 to -0.878585136209714
Changing layer 1's weights from 
-0.87318205665039 to -0.873488223825681
-0.272013371989003 to -0.272319539164294
-0.397716110750905 to -0.398022277926196
-0.407621449038258 to -0.407927616213549
-0.592674231335392 to -0.592980398510683
-0.730912065312138 to -0.731218232487429
-0.4601933417766 to -0.460499508951891
-0.845996370717755 to -0.846302537893046
-0.434333389803639 to -0.43463955697893
-0.796980252668132 to -0.797286419843423
Changing layer 2's weights from 
0.0122079791576942 to 0.0119018119824039
-0.541845506235829 to -0.54215167341112
-0.257256215616932 to -0.257562382792223
-0.0860561190097245 to -0.0863622861850148
-0.263308828875294 to -0.263614996050585
-0.469318455263844 to -0.469624622439135
0.0494942607433881 to 0.0491880935680978
-0.0229277668445028 to -0.0232339340197932
-0.754926061436405 to -0.755232228611696
-0.70996854285883 to -0.710274710034121
Changing layer 3's weights from 
-0.0208423195331018 to -0.0211484867083922
-0.284395402476063 to -0.284701569651354
-0.0417854843585407 to -0.042091651533831
-0.949224716114683 to -0.949530883289974
-0.164461380049458 to -0.164767547224749
-0.294709986254445 to -0.295016153429736
-0.624894833370914 to -0.625201000546205
-0.166381960913411 to -0.166688128088702
-0.498077934786549 to -0.49838410196184
-0.0252480564563197 to -0.0255542236316101
Changing layer 4's weights from 
-0.124521857306233 to -0.124828024481524
-0.333354657694569 to -0.33366082486986
-0.533832257792225 to -0.534138424967516
-0.246897524401417 to -0.247203691576708
-0.486150330111256 to -0.486456497286547
-0.254685109659901 to -0.254991276835192
-0.259043639704457 to -0.259349806879748
-0.720941161915531 to -0.721247329090822
-0.127245074316731 to -0.127551241492022
-0.559547012850514 to -0.559853180025805
Changing layer 5's weights from 
0.0282579602749432 to 0.0279517930996528
-0.413815563723317 to -0.414121730898608
0.0113727988751022 to 0.0110666316998119
-0.924139382496348 to -0.924445549671639
-0.386870568796864 to -0.387176735972155
-0.785338511869183 to -0.785644679044474
-0.485373204752674 to -0.485679371927965
-0.412926977679005 to -0.413233144854296
-0.0233249721972858 to -0.0236311393725762
-0.0298330841510218 to -0.0301392513263122
Trying to learn from memory 11, 0, -0.2
sum 0.0397115215869907 distri -0.0168064122108915
Using diff 0.0465900534011345 and condRate 0.166666666666667
Changed category 0 weights from 
0.1833166459791 to 0.18176364417592
-0.216718318105525 to -0.218271319908705
-0.499239014626331 to -0.500792016429511
-0.34773364865858 to -0.34928665046176
Changing layer 0's weights from 
-0.217227681254137 to -0.218780683057316
-0.589154137943971 to -0.59070713974715
-0.82350340363859 to -0.825056405441769
-0.788944153760659 to -0.790497155563838
-0.041076584433306 to -0.0426295862364854
-0.72371853766321 to -0.725271539466389
-0.569888814066637 to -0.571441815869816
-0.597343220089662 to -0.598896221892841
-0.834835081075418 to -0.836388082878597
-0.878585136209714 to -0.880138138012893
Changing layer 1's weights from 
-0.873488223825681 to -0.87504122562886
-0.272319539164294 to -0.273872540967473
-0.398022277926196 to -0.399575279729375
-0.407927616213549 to -0.409480618016728
-0.592980398510683 to -0.594533400313862
-0.731218232487429 to -0.732771234290608
-0.460499508951891 to -0.46205251075507
-0.846302537893046 to -0.847855539696225
-0.43463955697893 to -0.436192558782109
-0.797286419843423 to -0.798839421646602
Changing layer 2's weights from 
0.0119018119824039 to 0.0103488101792245
-0.54215167341112 to -0.543704675214299
-0.257562382792223 to -0.259115384595402
-0.0863622861850148 to -0.0879152879881942
-0.263614996050585 to -0.265167997853764
-0.469624622439135 to -0.471177624242314
0.0491880935680978 to 0.0476350917649184
-0.0232339340197932 to -0.0247869358229725
-0.755232228611696 to -0.756785230414875
-0.710274710034121 to -0.7118277118373
Changing layer 3's weights from 
-0.0211484867083922 to -0.0227014885115715
-0.284701569651354 to -0.286254571454533
-0.042091651533831 to -0.0436446533370104
-0.949530883289974 to -0.951083885093153
-0.164767547224749 to -0.166320549027928
-0.295016153429736 to -0.296569155232915
-0.625201000546205 to -0.626754002349384
-0.166688128088702 to -0.168241129891881
-0.49838410196184 to -0.499937103765019
-0.0255542236316101 to -0.0271072254347894
Changing layer 4's weights from 
-0.124828024481524 to -0.126381026284703
-0.33366082486986 to -0.335213826673039
-0.534138424967516 to -0.535691426770695
-0.247203691576708 to -0.248756693379887
-0.486456497286547 to -0.488009499089726
-0.254991276835192 to -0.256544278638371
-0.259349806879748 to -0.260902808682927
-0.721247329090822 to -0.722800330894001
-0.127551241492022 to -0.129104243295201
-0.559853180025805 to -0.561406181828984
Changing layer 5's weights from 
0.0279517930996528 to 0.0263987912964735
-0.414121730898608 to -0.415674732701787
0.0110666316998119 to 0.00951362989663251
-0.924445549671639 to -0.925998551474818
-0.387176735972155 to -0.388729737775334
-0.785644679044474 to -0.787197680847653
-0.485679371927965 to -0.487232373731144
-0.413233144854296 to -0.414786146657475
-0.0236311393725762 to -0.0251841411757555
-0.0301392513263122 to -0.0316922531294915
Trying to learn from memory 11, 1, -0.2
sum 0.0397115215869907 distri 0.0205986260683996
Using diff 0.00918501512184347 and condRate 0.166666666666667
Changed category 1 weights from 
0.297295273781159 to 0.296989106605869
0.530610860824921 to 0.530304693649631
0.105685622930863 to 0.105379455755573
0.10977843546901 to 0.10947226829372
Changing layer 0's weights from 
-0.218780683057316 to -0.219086850232606
-0.59070713974715 to -0.59101330692244
-0.825056405441769 to -0.825362572617059
-0.790497155563838 to -0.790803322739128
-0.0426295862364854 to -0.0429357534117758
-0.725271539466389 to -0.725577706641679
-0.571441815869816 to -0.571747983045106
-0.598896221892841 to -0.599202389068131
-0.836388082878597 to -0.836694250053887
-0.880138138012893 to -0.880444305188183
Changing layer 1's weights from 
-0.87504122562886 to -0.87534739280415
-0.273872540967473 to -0.274178708142763
-0.399575279729375 to -0.399881446904665
-0.409480618016728 to -0.409786785192018
-0.594533400313862 to -0.594839567489152
-0.732771234290608 to -0.733077401465898
-0.46205251075507 to -0.46235867793036
-0.847855539696225 to -0.848161706871515
-0.436192558782109 to -0.436498725957399
-0.798839421646602 to -0.799145588821892
Changing layer 2's weights from 
0.0103488101792245 to 0.0100426430039341
-0.543704675214299 to -0.544010842389589
-0.259115384595402 to -0.259421551770692
-0.0879152879881942 to -0.0882214551634846
-0.265167997853764 to -0.265474165029054
-0.471177624242314 to -0.471483791417604
0.0476350917649184 to 0.047328924589628
-0.0247869358229725 to -0.0250931029982629
-0.756785230414875 to -0.757091397590165
-0.7118277118373 to -0.71213387901259
Changing layer 3's weights from 
-0.0227014885115715 to -0.0230076556868619
-0.286254571454533 to -0.286560738629823
-0.0436446533370104 to -0.0439508205123008
-0.951083885093153 to -0.951390052268443
-0.166320549027928 to -0.166626716203218
-0.296569155232915 to -0.296875322408205
-0.626754002349384 to -0.627060169524674
-0.168241129891881 to -0.168547297067171
-0.499937103765019 to -0.500243270940309
-0.0271072254347894 to -0.0274133926100798
Changing layer 4's weights from 
-0.126381026284703 to -0.126687193459993
-0.335213826673039 to -0.335519993848329
-0.535691426770695 to -0.535997593945985
-0.248756693379887 to -0.249062860555177
-0.488009499089726 to -0.488315666265016
-0.256544278638371 to -0.256850445813661
-0.260902808682927 to -0.261208975858217
-0.722800330894001 to -0.723106498069291
-0.129104243295201 to -0.129410410470491
-0.561406181828984 to -0.561712349004274
Changing layer 5's weights from 
0.0263987912964735 to 0.0260926241211831
-0.415674732701787 to -0.415980899877077
0.00951362989663251 to 0.00920746272134215
-0.925998551474818 to -0.926304718650108
-0.388729737775334 to -0.389035904950624
-0.787197680847653 to -0.787503848022943
-0.487232373731144 to -0.487538540906434
-0.414786146657475 to -0.415092313832765
-0.0251841411757555 to -0.0254903083510459
-0.0316922531294915 to -0.0319984203047819
Trying to learn from memory 12, 1, -0.2
sum 0.0397115215869907 distri 0.0205986260683996
Using diff 0.00918501512184347 and condRate 0.166666666666667
Changed category 1 weights from 
0.296989106605869 to 0.296682939430579
0.530304693649631 to 0.52999852647434
0.105379455755573 to 0.105073288580283
0.10947226829372 to 0.10916610111843
Changing layer 0's weights from 
-0.219086850232606 to -0.219393017407897
-0.59101330692244 to -0.591319474097731
-0.825362572617059 to -0.82566873979235
-0.790803322739128 to -0.791109489914419
-0.0429357534117758 to -0.0432419205870661
-0.725577706641679 to -0.72588387381697
-0.571747983045106 to -0.572054150220397
-0.599202389068131 to -0.599508556243422
-0.836694250053887 to -0.837000417229178
-0.880444305188183 to -0.880750472363474
Changing layer 1's weights from 
-0.87534739280415 to -0.875653559979441
-0.274178708142763 to -0.274484875318054
-0.399881446904665 to -0.400187614079956
-0.409786785192018 to -0.410092952367309
-0.594839567489152 to -0.595145734664443
-0.733077401465898 to -0.733383568641189
-0.46235867793036 to -0.462664845105651
-0.848161706871515 to -0.848467874046806
-0.436498725957399 to -0.43680489313269
-0.799145588821892 to -0.799451755997183
Changing layer 2's weights from 
0.0100426430039341 to 0.00973647582864378
-0.544010842389589 to -0.54431700956488
-0.259421551770692 to -0.259727718945983
-0.0882214551634846 to -0.0885276223387749
-0.265474165029054 to -0.265780332204345
-0.471483791417604 to -0.471789958592895
0.047328924589628 to 0.0470227574143377
-0.0250931029982629 to -0.0253992701735533
-0.757091397590165 to -0.757397564765456
-0.71213387901259 to -0.712440046187881
Changing layer 3's weights from 
-0.0230076556868619 to -0.0233138228621523
-0.286560738629823 to -0.286866905805114
-0.0439508205123008 to -0.0442569876875911
-0.951390052268443 to -0.951696219443734
-0.166626716203218 to -0.166932883378509
-0.296875322408205 to -0.297181489583496
-0.627060169524674 to -0.627366336699965
-0.168547297067171 to -0.168853464242462
-0.500243270940309 to -0.5005494381156
-0.0274133926100798 to -0.0277195597853702
Changing layer 4's weights from 
-0.126687193459993 to -0.126993360635284
-0.335519993848329 to -0.33582616102362
-0.535997593945985 to -0.536303761121276
-0.249062860555177 to -0.249369027730468
-0.488315666265016 to -0.488621833440307
-0.256850445813661 to -0.257156612988952
-0.261208975858217 to -0.261515143033508
-0.723106498069291 to -0.723412665244582
-0.129410410470491 to -0.129716577645782
-0.561712349004274 to -0.562018516179565
Changing layer 5's weights from 
0.0260926241211831 to 0.0257864569458928
-0.415980899877077 to -0.416287067052368
0.00920746272134215 to 0.00890129554605178
-0.926304718650108 to -0.926610885825399
-0.389035904950624 to -0.389342072125915
-0.787503848022943 to -0.787810015198234
-0.487538540906434 to -0.487844708081725
-0.415092313832765 to -0.415398481008056
-0.0254903083510459 to -0.0257964755263362
-0.0319984203047819 to -0.0323045874800722
Trying to learn from memory 12, 0, -0.2
sum 0.0397115215869907 distri -0.0168064122108915
Using diff 0.0465900534011345 and condRate 0.166666666666667
Changed category 0 weights from 
0.18176364417592 to 0.180210642372741
-0.218271319908705 to -0.219824321711884
-0.500792016429511 to -0.50234501823269
-0.34928665046176 to -0.350839652264939
Changing layer 0's weights from 
-0.219393017407897 to -0.220946019211076
-0.591319474097731 to -0.59287247590091
-0.82566873979235 to -0.827221741595529
-0.791109489914419 to -0.792662491717598
-0.0432419205870661 to -0.0447949223902455
-0.72588387381697 to -0.727436875620149
-0.572054150220397 to -0.573607152023576
-0.599508556243422 to -0.601061558046601
-0.837000417229178 to -0.838553419032357
-0.880750472363474 to -0.882303474166653
Changing layer 1's weights from 
-0.875653559979441 to -0.87720656178262
-0.274484875318054 to -0.276037877121233
-0.400187614079956 to -0.401740615883135
-0.410092952367309 to -0.411645954170488
-0.595145734664443 to -0.596698736467622
-0.733383568641189 to -0.734936570444368
-0.462664845105651 to -0.46421784690883
-0.848467874046806 to -0.850020875849985
-0.43680489313269 to -0.438357894935869
-0.799451755997183 to -0.801004757800362
Changing layer 2's weights from 
0.00973647582864378 to 0.00818347402546443
-0.54431700956488 to -0.545870011368059
-0.259727718945983 to -0.261280720749162
-0.0885276223387749 to -0.0900806241419543
-0.265780332204345 to -0.267333334007524
-0.471789958592895 to -0.473342960396074
0.0470227574143377 to 0.0454697556111583
-0.0253992701735533 to -0.0269522719767326
-0.757397564765456 to -0.758950566568635
-0.712440046187881 to -0.71399304799106
Changing layer 3's weights from 
-0.0233138228621523 to -0.0248668246653316
-0.286866905805114 to -0.288419907608293
-0.0442569876875911 to -0.0458099894907705
-0.951696219443734 to -0.953249221246913
-0.166932883378509 to -0.168485885181688
-0.297181489583496 to -0.298734491386675
-0.627366336699965 to -0.628919338503144
-0.168853464242462 to -0.170406466045641
-0.5005494381156 to -0.502102439918779
-0.0277195597853702 to -0.0292725615885495
Changing layer 4's weights from 
-0.126993360635284 to -0.128546362438463
-0.33582616102362 to -0.337379162826799
-0.536303761121276 to -0.537856762924455
-0.249369027730468 to -0.250922029533647
-0.488621833440307 to -0.490174835243486
-0.257156612988952 to -0.258709614792131
-0.261515143033508 to -0.263068144836687
-0.723412665244582 to -0.724965667047761
-0.129716577645782 to -0.131269579448961
-0.562018516179565 to -0.563571517982744
Changing layer 5's weights from 
0.0257864569458928 to 0.0242334551427134
-0.416287067052368 to -0.417840068855547
0.00890129554605178 to 0.00734829374287244
-0.926610885825399 to -0.928163887628578
-0.389342072125915 to -0.390895073929094
-0.787810015198234 to -0.789363017001413
-0.487844708081725 to -0.489397709884904
-0.415398481008056 to -0.416951482811235
-0.0257964755263362 to -0.0273494773295156
-0.0323045874800722 to -0.0338575892832516
Trying to learn from memory 12, 2, -0.2
sum 0.0397115215869907 distri 0.0359193077294827
Using diff 1 and condRate 0.166666666666667
Changed category 2 weights from 
0.424478130366066 to 0.391144796536028
0.702920095946055 to 0.669586762116017
0.615879731203775 to 0.582546397373737
-0.0386919733627043 to -0.072025307192743
Changing layer 0's weights from 
-0.220946019211076 to -0.254279353041115
-0.59287247590091 to -0.626205809730949
-0.827221741595529 to -0.860555075425568
-0.792662491717598 to -0.825995825547637
-0.0447949223902455 to -0.0781282562202842
-0.727436875620149 to -0.760770209450188
-0.573607152023576 to -0.606940485853615
-0.601061558046601 to -0.63439489187664
-0.838553419032357 to -0.871886752862396
-0.882303474166653 to -0.915636807996692
Changing layer 1's weights from 
-0.87720656178262 to -0.910539895612659
-0.276037877121233 to -0.309371210951272
-0.401740615883135 to -0.435073949713174
-0.411645954170488 to -0.444979288000527
-0.596698736467622 to -0.630032070297661
-0.734936570444368 to -0.768269904274407
-0.46421784690883 to -0.497551180738869
-0.850020875849985 to -0.883354209680024
-0.438357894935869 to -0.471691228765908
-0.801004757800362 to -0.834338091630401
Changing layer 2's weights from 
0.00818347402546443 to -0.0251498598045743
-0.545870011368059 to -0.579203345198098
-0.261280720749162 to -0.294614054579201
-0.0900806241419543 to -0.123413957971993
-0.267333334007524 to -0.300666667837563
-0.473342960396074 to -0.506676294226113
0.0454697556111583 to 0.0121364217811196
-0.0269522719767326 to -0.0602856058067713
-0.758950566568635 to -0.792283900398674
-0.71399304799106 to -0.747326381821099
Changing layer 3's weights from 
-0.0248668246653316 to -0.0582001584953703
-0.288419907608293 to -0.321753241438332
-0.0458099894907705 to -0.0791433233208092
-0.953249221246913 to -0.986582555076952
-0.168485885181688 to -0.201819219011727
-0.298734491386675 to -0.332067825216714
-0.628919338503144 to -0.662252672333183
-0.170406466045641 to -0.20373979987568
-0.502102439918779 to -0.535435773748818
-0.0292725615885495 to -0.0626058954185882
Changing layer 4's weights from 
-0.128546362438463 to -0.161879696268502
-0.337379162826799 to -0.370712496656838
-0.537856762924455 to -0.571190096754494
-0.250922029533647 to -0.284255363363686
-0.490174835243486 to -0.523508169073525
-0.258709614792131 to -0.29204294862217
-0.263068144836687 to -0.296401478666726
-0.724965667047761 to -0.7582990008778
-0.131269579448961 to -0.164602913279
-0.563571517982744 to -0.596904851812783
Changing layer 5's weights from 
0.0242334551427134 to -0.00909987868732528
-0.417840068855547 to -0.451173402685586
0.00734829374287244 to -0.0259850400871663
-0.928163887628578 to -0.961497221458617
-0.390895073929094 to -0.424228407759133
-0.789363017001413 to -0.822696350831452
-0.489397709884904 to -0.522731043714943
-0.416951482811235 to -0.450284816641274
-0.0273494773295156 to -0.0606828111595543
-0.0338575892832516 to -0.0671909231132903
Trying to learn from memory 12, 1, -0.2
sum 0.0397115215869907 distri 0.0205986260683996
Using diff 0.00918501512184347 and condRate 0.166666666666667
Changed category 1 weights from 
0.296682939430579 to 0.296376772255288
0.52999852647434 to 0.52969235929905
0.105073288580283 to 0.104767121404992
0.10916610111843 to 0.108859933943139
Changing layer 0's weights from 
-0.254279353041115 to -0.254585520216405
-0.626205809730949 to -0.626511976906239
-0.860555075425568 to -0.860861242600858
-0.825995825547637 to -0.826301992722927
-0.0781282562202842 to -0.0784344233955745
-0.760770209450188 to -0.761076376625478
-0.606940485853615 to -0.607246653028905
-0.63439489187664 to -0.63470105905193
-0.871886752862396 to -0.872192920037686
-0.915636807996692 to -0.915942975171982
Changing layer 1's weights from 
-0.910539895612659 to -0.910846062787949
-0.309371210951272 to -0.309677378126562
-0.435073949713174 to -0.435380116888464
-0.444979288000527 to -0.445285455175817
-0.630032070297661 to -0.630338237472951
-0.768269904274407 to -0.768576071449697
-0.497551180738869 to -0.497857347914159
-0.883354209680024 to -0.883660376855314
-0.471691228765908 to -0.471997395941198
-0.834338091630401 to -0.834644258805691
Changing layer 2's weights from 
-0.0251498598045743 to -0.0254560269798646
-0.579203345198098 to -0.579509512373388
-0.294614054579201 to -0.294920221754491
-0.123413957971993 to -0.123720125147283
-0.300666667837563 to -0.300972835012853
-0.506676294226113 to -0.506982461401403
0.0121364217811196 to 0.0118302546058293
-0.0602856058067713 to -0.0605917729820617
-0.792283900398674 to -0.792590067573964
-0.747326381821099 to -0.747632548996389
Changing layer 3's weights from 
-0.0582001584953703 to -0.0585063256706607
-0.321753241438332 to -0.322059408613622
-0.0791433233208092 to -0.0794494904960995
-0.986582555076952 to -0.986888722252242
-0.201819219011727 to -0.202125386187017
-0.332067825216714 to -0.332373992392004
-0.662252672333183 to -0.662558839508473
-0.20373979987568 to -0.20404596705097
-0.535435773748818 to -0.535741940924108
-0.0626058954185882 to -0.0629120625938786
Changing layer 4's weights from 
-0.161879696268502 to -0.162185863443792
-0.370712496656838 to -0.371018663832128
-0.571190096754494 to -0.571496263929784
-0.284255363363686 to -0.284561530538976
-0.523508169073525 to -0.523814336248815
-0.29204294862217 to -0.29234911579746
-0.296401478666726 to -0.296707645842016
-0.7582990008778 to -0.75860516805309
-0.164602913279 to -0.16490908045429
-0.596904851812783 to -0.597211018988073
Changing layer 5's weights from 
-0.00909987868732528 to -0.00940604586261564
-0.451173402685586 to -0.451479569860876
-0.0259850400871663 to -0.0262912072624566
-0.961497221458617 to -0.961803388633907
-0.424228407759133 to -0.424534574934423
-0.822696350831452 to -0.823002518006742
-0.522731043714943 to -0.523037210890233
-0.450284816641274 to -0.450590983816564
-0.0606828111595543 to -0.0609889783348447
-0.0671909231132903 to -0.0674970902885807
10/5/2016 1:40:36 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 12, 1, -0.2
sum 0.0397115215869907 distri 0.0205986260683996
Using diff 0.00918501512184347 and condRate 0.166666666666667
Changed category 1 weights from 
0.296376772255288 to 0.296070605079998
0.52969235929905 to 0.529386192123759
0.104767121404992 to 0.104460954229702
0.108859933943139 to 0.108553766767849
Changing layer 0's weights from 
-0.254585520216405 to -0.254891687391695
-0.626511976906239 to -0.62681814408153
-0.860861242600858 to -0.861167409776149
-0.826301992722927 to -0.826608159898218
-0.0784344233955745 to -0.0787405905708649
-0.761076376625478 to -0.761382543800769
-0.607246653028905 to -0.607552820204196
-0.63470105905193 to -0.635007226227221
-0.872192920037686 to -0.872499087212977
-0.915942975171982 to -0.916249142347273
Changing layer 1's weights from 
-0.910846062787949 to -0.91115222996324
-0.309677378126562 to -0.309983545301852
-0.435380116888464 to -0.435686284063755
-0.445285455175817 to -0.445591622351108
-0.630338237472951 to -0.630644404648242
-0.768576071449697 to -0.768882238624988
-0.497857347914159 to -0.49816351508945
-0.883660376855314 to -0.883966544030605
-0.471997395941198 to -0.472303563116489
-0.834644258805691 to -0.834950425980982
Changing layer 2's weights from 
-0.0254560269798646 to -0.025762194155155
-0.579509512373388 to -0.579815679548679
-0.294920221754491 to -0.295226388929781
-0.123720125147283 to -0.124026292322574
-0.300972835012853 to -0.301279002188143
-0.506982461401403 to -0.507288628576694
0.0118302546058293 to 0.0115240874305389
-0.0605917729820617 to -0.060897940157352
-0.792590067573964 to -0.792896234749255
-0.747632548996389 to -0.74793871617168
Changing layer 3's weights from 
-0.0585063256706607 to -0.058812492845951
-0.322059408613622 to -0.322365575788913
-0.0794494904960995 to -0.0797556576713899
-0.986888722252242 to -0.987194889427533
-0.202125386187017 to -0.202431553362307
-0.332373992392004 to -0.332680159567295
-0.662558839508473 to -0.662865006683764
-0.20404596705097 to -0.20435213422626
-0.535741940924108 to -0.536048108099399
-0.0629120625938786 to -0.0632182297691689
Changing layer 4's weights from 
-0.162185863443792 to -0.162492030619083
-0.371018663832128 to -0.371324831007419
-0.571496263929784 to -0.571802431105074
-0.284561530538976 to -0.284867697714266
-0.523814336248815 to -0.524120503424106
-0.29234911579746 to -0.29265528297275
-0.296707645842016 to -0.297013813017306
-0.75860516805309 to -0.758911335228381
-0.16490908045429 to -0.165215247629581
-0.597211018988073 to -0.597517186163364
Changing layer 5's weights from 
-0.00940604586261564 to -0.009712213037906
-0.451479569860876 to -0.451785737036167
-0.0262912072624566 to -0.026597374437747
-0.961803388633907 to -0.962109555809198
-0.424534574934423 to -0.424840742109714
-0.823002518006742 to -0.823308685182033
-0.523037210890233 to -0.523343378065524
-0.450590983816564 to -0.450897150991855
-0.0609889783348447 to -0.061295145510135
-0.0674970902885807 to -0.067803257463871
Trying to learn from memory 12, 0, -0.2
sum 0.0397115215869907 distri -0.0168064122108915
Using diff 0.0465900534011345 and condRate 0.166666666666667
Changed category 0 weights from 
0.180210642372741 to 0.178657640569562
-0.219824321711884 to -0.221377323515063
-0.50234501823269 to -0.503898020035869
-0.350839652264939 to -0.352392654068118
Changing layer 0's weights from 
-0.254891687391695 to -0.256444689194875
-0.62681814408153 to -0.628371145884709
-0.861167409776149 to -0.862720411579328
-0.826608159898218 to -0.828161161701397
-0.0787405905708649 to -0.0802935923740442
-0.761382543800769 to -0.762935545603948
-0.607552820204196 to -0.609105822007375
-0.635007226227221 to -0.6365602280304
-0.872499087212977 to -0.874052089016156
-0.916249142347273 to -0.917802144150452
Changing layer 1's weights from 
-0.91115222996324 to -0.912705231766419
-0.309983545301852 to -0.311536547105032
-0.435686284063755 to -0.437239285866934
-0.445591622351108 to -0.447144624154287
-0.630644404648242 to -0.632197406451421
-0.768882238624988 to -0.770435240428167
-0.49816351508945 to -0.499716516892629
-0.883966544030605 to -0.885519545833784
-0.472303563116489 to -0.473856564919668
-0.834950425980982 to -0.836503427784161
Changing layer 2's weights from 
-0.025762194155155 to -0.0273151959583343
-0.579815679548679 to -0.581368681351858
-0.295226388929781 to -0.296779390732961
-0.124026292322574 to -0.125579294125753
-0.301279002188143 to -0.302832003991323
-0.507288628576694 to -0.508841630379873
0.0115240874305389 to 0.00997108562735956
-0.060897940157352 to -0.0624509419605314
-0.792896234749255 to -0.794449236552434
-0.74793871617168 to -0.749491717974859
Changing layer 3's weights from 
-0.058812492845951 to -0.0603654946491304
-0.322365575788913 to -0.323918577592092
-0.0797556576713899 to -0.0813086594745693
-0.987194889427533 to -0.988747891230712
-0.202431553362307 to -0.203984555165487
-0.332680159567295 to -0.334233161370474
-0.662865006683764 to -0.664418008486943
-0.20435213422626 to -0.20590513602944
-0.536048108099399 to -0.537601109902578
-0.0632182297691689 to -0.0647712315723483
Changing layer 4's weights from 
-0.162492030619083 to -0.164045032422262
-0.371324831007419 to -0.372877832810598
-0.571802431105074 to -0.573355432908254
-0.284867697714266 to -0.286420699517446
-0.524120503424106 to -0.525673505227285
-0.29265528297275 to -0.29420828477593
-0.297013813017306 to -0.298566814820486
-0.758911335228381 to -0.76046433703156
-0.165215247629581 to -0.16676824943276
-0.597517186163364 to -0.599070187966543
Changing layer 5's weights from 
-0.009712213037906 to -0.0112652148410854
-0.451785737036167 to -0.453338738839346
-0.026597374437747 to -0.0281503762409263
-0.962109555809198 to -0.963662557612377
-0.424840742109714 to -0.426393743912893
-0.823308685182033 to -0.824861686985212
-0.523343378065524 to -0.524896379868703
-0.450897150991855 to -0.452450152795034
-0.061295145510135 to -0.0628481473133144
-0.067803257463871 to -0.0693562592670504
Trying to learn from memory 12, 1, -0.2
sum 0.0397115215869907 distri 0.0205986260683996
Using diff 0.00918501512184347 and condRate 0.166666666666667
Changed category 1 weights from 
0.296070605079998 to 0.295764437904707
0.529386192123759 to 0.529080024948469
0.104460954229702 to 0.104154787054412
0.108553766767849 to 0.108247599592559
Changing layer 0's weights from 
-0.256444689194875 to -0.256750856370165
-0.628371145884709 to -0.628677313059999
-0.862720411579328 to -0.863026578754618
-0.828161161701397 to -0.828467328876687
-0.0802935923740442 to -0.0805997595493346
-0.762935545603948 to -0.763241712779238
-0.609105822007375 to -0.609411989182665
-0.6365602280304 to -0.63686639520569
-0.874052089016156 to -0.874358256191446
-0.917802144150452 to -0.918108311325742
Changing layer 1's weights from 
-0.912705231766419 to -0.913011398941709
-0.311536547105032 to -0.311842714280322
-0.437239285866934 to -0.437545453042224
-0.447144624154287 to -0.447450791329577
-0.632197406451421 to -0.632503573626711
-0.770435240428167 to -0.770741407603457
-0.499716516892629 to -0.500022684067919
-0.885519545833784 to -0.885825713009074
-0.473856564919668 to -0.474162732094958
-0.836503427784161 to -0.836809594959451
Changing layer 2's weights from 
-0.0273151959583343 to -0.0276213631336247
-0.581368681351858 to -0.581674848527148
-0.296779390732961 to -0.297085557908251
-0.125579294125753 to -0.125885461301043
-0.302832003991323 to -0.303138171166613
-0.508841630379873 to -0.509147797555163
0.00997108562735956 to 0.0096649184520692
-0.0624509419605314 to -0.0627571091358217
-0.794449236552434 to -0.794755403727724
-0.749491717974859 to -0.749797885150149
Changing layer 3's weights from 
-0.0603654946491304 to -0.0606716618244207
-0.323918577592092 to -0.324224744767382
-0.0813086594745693 to -0.0816148266498596
-0.988747891230712 to -0.989054058406002
-0.203984555165487 to -0.204290722340777
-0.334233161370474 to -0.334539328545764
-0.664418008486943 to -0.664724175662233
-0.20590513602944 to -0.20621130320473
-0.537601109902578 to -0.537907277077868
-0.0647712315723483 to -0.0650773987476386
Changing layer 4's weights from 
-0.164045032422262 to -0.164351199597552
-0.372877832810598 to -0.373183999985888
-0.573355432908254 to -0.573661600083544
-0.286420699517446 to -0.286726866692736
-0.525673505227285 to -0.525979672402575
-0.29420828477593 to -0.29451445195122
-0.298566814820486 to -0.298872981995776
-0.76046433703156 to -0.76077050420685
-0.16676824943276 to -0.16707441660805
-0.599070187966543 to -0.599376355141833
Changing layer 5's weights from 
-0.0112652148410854 to -0.0115713820163757
-0.453338738839346 to -0.453644906014636
-0.0281503762409263 to -0.0284565434162167
-0.963662557612377 to -0.963968724787667
-0.426393743912893 to -0.426699911088183
-0.824861686985212 to -0.825167854160502
-0.524896379868703 to -0.525202547043993
-0.452450152795034 to -0.452756319970324
-0.0628481473133144 to -0.0631543144886047
-0.0693562592670504 to -0.0696624264423407
Trying to learn from memory 13, 0, -0.2
sum 0.0397115152605874 distri -0.0168064188650662
Using diff 0.0465900553105067 and condRate 0.166666666666667
Changed category 0 weights from 
0.178657640569562 to 0.177104638702737
-0.221377323515063 to -0.222930325381888
-0.503898020035869 to -0.505451021902694
-0.352392654068118 to -0.353945655934943
Changing layer 0's weights from 
-0.256750856370165 to -0.25830385823699
-0.628677313059999 to -0.630230314926824
-0.863026578754618 to -0.864579580621443
-0.828467328876687 to -0.830020330743512
-0.0805997595493346 to -0.0821527614161597
-0.763241712779238 to -0.764794714646063
-0.609411989182665 to -0.61096499104949
-0.63686639520569 to -0.638419397072515
-0.874358256191446 to -0.875911258058271
-0.918108311325742 to -0.919661313192567
Changing layer 1's weights from 
-0.913011398941709 to -0.914564400808534
-0.311842714280322 to -0.313395716147147
-0.437545453042224 to -0.439098454909049
-0.447450791329577 to -0.449003793196402
-0.632503573626711 to -0.634056575493536
-0.770741407603457 to -0.772294409470282
-0.500022684067919 to -0.501575685934744
-0.885825713009074 to -0.887378714875899
-0.474162732094958 to -0.475715733961783
-0.836809594959451 to -0.838362596826276
Changing layer 2's weights from 
-0.0276213631336247 to -0.0291743650004498
-0.581674848527148 to -0.583227850393973
-0.297085557908251 to -0.298638559775076
-0.125885461301043 to -0.127438463167869
-0.303138171166613 to -0.304691173033438
-0.509147797555163 to -0.510700799421988
0.0096649184520692 to 0.00811191658524411
-0.0627571091358217 to -0.0643101110026468
-0.794755403727724 to -0.796308405594549
-0.749797885150149 to -0.751350887016974
Changing layer 3's weights from 
-0.0606716618244207 to -0.0622246636912458
-0.324224744767382 to -0.325777746634207
-0.0816148266498596 to -0.0831678285166847
-0.989054058406002 to -0.990607060272827
-0.204290722340777 to -0.205843724207602
-0.334539328545764 to -0.336092330412589
-0.664724175662233 to -0.666277177529058
-0.20621130320473 to -0.207764305071555
-0.537907277077868 to -0.539460278944693
-0.0650773987476386 to -0.0666304006144637
Changing layer 4's weights from 
-0.164351199597552 to -0.165904201464377
-0.373183999985888 to -0.374737001852713
-0.573661600083544 to -0.575214601950369
-0.286726866692736 to -0.288279868559561
-0.525979672402575 to -0.5275326742694
-0.29451445195122 to -0.296067453818045
-0.298872981995776 to -0.300425983862601
-0.76077050420685 to -0.762323506073675
-0.16707441660805 to -0.168627418474875
-0.599376355141833 to -0.600929357008658
Changing layer 5's weights from 
-0.0115713820163757 to -0.0131243838832008
-0.453644906014636 to -0.455197907881461
-0.0284565434162167 to -0.0300095452830418
-0.963968724787667 to -0.965521726654492
-0.426699911088183 to -0.428252912955008
-0.825167854160502 to -0.826720856027327
-0.525202547043993 to -0.526755548910818
-0.452756319970324 to -0.454309321837149
-0.0631543144886047 to -0.0647073163554298
-0.0696624264423407 to -0.0712154283091658
Trying to learn from memory 14, 1, -0.2
sum 0.0397115054179654 distri 0.020599194966568
Using diff 0.00918443409690602 and condRate 0.166666666666667
Changed category 1 weights from 
0.295764437904707 to 0.295458290096915
0.529080024948469 to 0.528773877140677
0.104154787054412 to 0.103848639246619
0.108247599592559 to 0.107941451784766
Changing layer 0's weights from 
-0.25830385823699 to -0.258610006044782
-0.630230314926824 to -0.630536462734617
-0.864579580621443 to -0.864885728429236
-0.830020330743512 to -0.830326478551304
-0.0821527614161597 to -0.0824589092239518
-0.764794714646063 to -0.765100862453855
-0.61096499104949 to -0.611271138857282
-0.638419397072515 to -0.638725544880307
-0.875911258058271 to -0.876217405866063
-0.919661313192567 to -0.919967461000359
Changing layer 1's weights from 
-0.914564400808534 to -0.914870548616326
-0.313395716147147 to -0.313701863954939
-0.439098454909049 to -0.439404602716842
-0.449003793196402 to -0.449309941004195
-0.634056575493536 to -0.634362723301328
-0.772294409470282 to -0.772600557278074
-0.501575685934744 to -0.501881833742536
-0.887378714875899 to -0.887684862683692
-0.475715733961783 to -0.476021881769576
-0.838362596826276 to -0.838668744634068
Changing layer 2's weights from 
-0.0291743650004498 to -0.029480512808242
-0.583227850393973 to -0.583533998201766
-0.298638559775076 to -0.298944707582868
-0.127438463167869 to -0.127744610975661
-0.304691173033438 to -0.30499732084123
-0.510700799421988 to -0.511006947229781
0.00811191658524411 to 0.00780576877745195
-0.0643101110026468 to -0.064616258810439
-0.796308405594549 to -0.796614553402341
-0.751350887016974 to -0.751657034824766
Changing layer 3's weights from 
-0.0622246636912458 to -0.062530811499038
-0.325777746634207 to -0.326083894441999
-0.0831678285166847 to -0.0834739763244769
-0.990607060272827 to -0.990913208080619
-0.205843724207602 to -0.206149872015394
-0.336092330412589 to -0.336398478220382
-0.666277177529058 to -0.666583325336851
-0.207764305071555 to -0.208070452879347
-0.539460278944693 to -0.539766426752486
-0.0666304006144637 to -0.0669365484222559
Changing layer 4's weights from 
-0.165904201464377 to -0.166210349272169
-0.374737001852713 to -0.375043149660506
-0.575214601950369 to -0.575520749758161
-0.288279868559561 to -0.288586016367353
-0.5275326742694 to -0.527838822077193
-0.296067453818045 to -0.296373601625837
-0.300425983862601 to -0.300732131670393
-0.762323506073675 to -0.762629653881467
-0.168627418474875 to -0.168933566282667
-0.600929357008658 to -0.60123550481645
Changing layer 5's weights from 
-0.0131243838832008 to -0.013430531690993
-0.455197907881461 to -0.455504055689254
-0.0300095452830418 to -0.0303156930908339
-0.965521726654492 to -0.965827874462284
-0.428252912955008 to -0.428559060762801
-0.826720856027327 to -0.82702700383512
-0.526755548910818 to -0.527061696718611
-0.454309321837149 to -0.454615469644942
-0.0647073163554298 to -0.065013464163222
-0.0712154283091658 to -0.071521576116958
Trying to learn from memory 15, 1, -0.2
sum 0.0397125830486292 distri 0.0206001377383802
Using diff 0.00918429954809175 and condRate 0.166666666666667
Changed category 1 weights from 
0.295458290096915 to 0.295152146774084
0.528773877140677 to 0.528467733817845
0.103848639246619 to 0.103542495923788
0.107941451784766 to 0.107635308461935
Changing layer 0's weights from 
-0.258610006044782 to -0.258916149367614
-0.630536462734617 to -0.630842606057448
-0.864885728429236 to -0.865191871752067
-0.830326478551304 to -0.830632621874136
-0.0824589092239518 to -0.0827650525467835
-0.765100862453855 to -0.765407005776687
-0.611271138857282 to -0.611577282180114
-0.638725544880307 to -0.639031688203139
-0.876217405866063 to -0.876523549188895
-0.919967461000359 to -0.920273604323191
Changing layer 1's weights from 
-0.914870548616326 to -0.915176691939158
-0.313701863954939 to -0.314008007277771
-0.439404602716842 to -0.439710746039673
-0.449309941004195 to -0.449616084327026
-0.634362723301328 to -0.63466886662416
-0.772600557278074 to -0.772906700600906
-0.501881833742536 to -0.502187977065368
-0.887684862683692 to -0.887991006006523
-0.476021881769576 to -0.476328025092407
-0.838668744634068 to -0.8389748879569
Changing layer 2's weights from 
-0.029480512808242 to -0.0297866561310736
-0.583533998201766 to -0.583840141524597
-0.298944707582868 to -0.2992508509057
-0.127744610975661 to -0.128050754298492
-0.30499732084123 to -0.305303464164062
-0.511006947229781 to -0.511313090552612
0.00780576877745195 to 0.00749962545462033
-0.064616258810439 to -0.0649224021332706
-0.796614553402341 to -0.796920696725173
-0.751657034824766 to -0.751963178147598
Changing layer 3's weights from 
-0.062530811499038 to -0.0628369548218696
-0.326083894441999 to -0.326390037764831
-0.0834739763244769 to -0.0837801196473085
-0.990913208080619 to -0.991219351403451
-0.206149872015394 to -0.206456015338226
-0.336398478220382 to -0.336704621543213
-0.666583325336851 to -0.666889468659682
-0.208070452879347 to -0.208376596202179
-0.539766426752486 to -0.540072570075317
-0.0669365484222559 to -0.0672426917450875
Changing layer 4's weights from 
-0.166210349272169 to -0.166516492595001
-0.375043149660506 to -0.375349292983337
-0.575520749758161 to -0.575826893080993
-0.288586016367353 to -0.288892159690185
-0.527838822077193 to -0.528144965400024
-0.296373601625837 to -0.296679744948669
-0.300732131670393 to -0.301038274993225
-0.762629653881467 to -0.762935797204299
-0.168933566282667 to -0.169239709605499
-0.60123550481645 to -0.601541648139282
Changing layer 5's weights from 
-0.013430531690993 to -0.0137366750138246
-0.455504055689254 to -0.455810199012085
-0.0303156930908339 to -0.0306218364136656
-0.965827874462284 to -0.966134017785116
-0.428559060762801 to -0.428865204085632
-0.82702700383512 to -0.827333147157951
-0.527061696718611 to -0.527367840041442
-0.454615469644942 to -0.454921612967773
-0.065013464163222 to -0.0653196074860536
-0.071521576116958 to -0.0718277194397896
Trying to learn from memory 16, 1, -0.2
sum 0.0397125735565434 distri 0.020599595514648
Using diff 0.00918483465275957 and condRate 0.166666666666667
Changed category 1 weights from 
0.295152146774084 to 0.29484598561443
0.528467733817845 to 0.528161572658191
0.103542495923788 to 0.103236334764134
0.107635308461935 to 0.107329147302281
Changing layer 0's weights from 
-0.258916149367614 to -0.259222310527268
-0.630842606057448 to -0.631148767217102
-0.865191871752067 to -0.865498032911721
-0.830632621874136 to -0.83093878303379
-0.0827650525467835 to -0.0830712137064376
-0.765407005776687 to -0.765713166936341
-0.611577282180114 to -0.611883443339768
-0.639031688203139 to -0.639337849362793
-0.876523549188895 to -0.876829710348549
-0.920273604323191 to -0.920579765482845
Changing layer 1's weights from 
-0.915176691939158 to -0.915482853098812
-0.314008007277771 to -0.314314168437425
-0.439710746039673 to -0.440016907199327
-0.449616084327026 to -0.44992224548668
-0.63466886662416 to -0.634975027783814
-0.772906700600906 to -0.77321286176056
-0.502187977065368 to -0.502494138225022
-0.887991006006523 to -0.888297167166177
-0.476328025092407 to -0.476634186252061
-0.8389748879569 to -0.839281049116554
Changing layer 2's weights from 
-0.0297866561310736 to -0.0300928172907277
-0.583840141524597 to -0.584146302684251
-0.2992508509057 to -0.299557012065354
-0.128050754298492 to -0.128356915458146
-0.305303464164062 to -0.305609625323716
-0.511313090552612 to -0.511619251712266
0.00749962545462033 to 0.00719346429496619
-0.0649224021332706 to -0.0652285632929247
-0.796920696725173 to -0.797226857884827
-0.751963178147598 to -0.752269339307252
Changing layer 3's weights from 
-0.0628369548218696 to -0.0631431159815237
-0.326390037764831 to -0.326696198924485
-0.0837801196473085 to -0.0840862808069626
-0.991219351403451 to -0.991525512563105
-0.206456015338226 to -0.20676217649788
-0.336704621543213 to -0.337010782702867
-0.666889468659682 to -0.667195629819336
-0.208376596202179 to -0.208682757361833
-0.540072570075317 to -0.540378731234971
-0.0672426917450875 to -0.0675488529047416
Changing layer 4's weights from 
-0.166516492595001 to -0.166822653754655
-0.375349292983337 to -0.375655454142991
-0.575826893080993 to -0.576133054240647
-0.288892159690185 to -0.289198320849839
-0.528144965400024 to -0.528451126559678
-0.296679744948669 to -0.296985906108323
-0.301038274993225 to -0.301344436152879
-0.762935797204299 to -0.763241958363953
-0.169239709605499 to -0.169545870765153
-0.601541648139282 to -0.601847809298936
Changing layer 5's weights from 
-0.0137366750138246 to -0.0140428361734787
-0.455810199012085 to -0.456116360171739
-0.0306218364136656 to -0.0309279975733197
-0.966134017785116 to -0.96644017894477
-0.428865204085632 to -0.429171365245286
-0.827333147157951 to -0.827639308317605
-0.527367840041442 to -0.527674001201096
-0.454921612967773 to -0.455227774127427
-0.0653196074860536 to -0.0656257686457077
-0.0718277194397896 to -0.0721338805994437
10/5/2016 1:40:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:40:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:16 PMStarting learning phase with deltaScore: 4.4
Modified index 0's learning in memoryPool to 0.88
Modified index 1's learning in memoryPool to 0.88
Modified index 2's learning in memoryPool to 0.88
Modified index 3's learning in memoryPool to 0.88
Modified index 4's learning in memoryPool to 0.88
Modified index 5's learning in memoryPool to 0.88
Modified index 6's learning in memoryPool to 0.88
Modified index 7's learning in memoryPool to 0.88
Modified index 8's learning in memoryPool to 0.88
10/5/2016 1:41:16 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 17, 0, 0.88
sum 0.0397115215869907 distri -0.0168064122108915
Using diff 0.0465900534011345 and condRate 0.166666666666667
Changed category 0 weights from 
0.177104638702737 to 0.183937846497877
-0.222930325381888 to -0.216097117586748
-0.505451021902694 to -0.498617814107554
-0.353945655934943 to -0.347112448139803
Changing layer 0's weights from 
-0.259222310527268 to -0.252389102732128
-0.631148767217102 to -0.624315559421962
-0.865498032911721 to -0.858664825116581
-0.83093878303379 to -0.82410557523865
-0.0830712137064376 to -0.0762380059112976
-0.765713166936341 to -0.758879959141201
-0.611883443339768 to -0.605050235544628
-0.639337849362793 to -0.632504641567653
-0.876829710348549 to -0.869996502553409
-0.920579765482845 to -0.913746557687705
Changing layer 1's weights from 
-0.915482853098812 to -0.908649645303672
-0.314314168437425 to -0.307480960642285
-0.440016907199327 to -0.433183699404187
-0.44992224548668 to -0.44308903769154
-0.634975027783814 to -0.628141819988674
-0.77321286176056 to -0.76637965396542
-0.502494138225022 to -0.495660930429882
-0.888297167166177 to -0.881463959371037
-0.476634186252061 to -0.469800978456921
-0.839281049116554 to -0.832447841321414
Changing layer 2's weights from 
-0.0300928172907277 to -0.0232596094955878
-0.584146302684251 to -0.577313094889111
-0.299557012065354 to -0.292723804270214
-0.128356915458146 to -0.121523707663006
-0.305609625323716 to -0.298776417528576
-0.511619251712266 to -0.504786043917126
0.00719346429496619 to 0.0140266720901061
-0.0652285632929247 to -0.0583953554977848
-0.797226857884827 to -0.790393650089687
-0.752269339307252 to -0.745436131512112
Changing layer 3's weights from 
-0.0631431159815237 to -0.0563099081863838
-0.326696198924485 to -0.319862991129345
-0.0840862808069626 to -0.0772530730118227
-0.991525512563105 to -0.984692304767965
-0.20676217649788 to -0.19992896870274
-0.337010782702867 to -0.330177574907727
-0.667195629819336 to -0.660362422024196
-0.208682757361833 to -0.201849549566693
-0.540378731234971 to -0.533545523439831
-0.0675488529047416 to -0.0607156451096017
Changing layer 4's weights from 
-0.166822653754655 to -0.159989445959515
-0.375655454142991 to -0.368822246347851
-0.576133054240647 to -0.569299846445507
-0.289198320849839 to -0.282365113054699
-0.528451126559678 to -0.521617918764538
-0.296985906108323 to -0.290152698313183
-0.301344436152879 to -0.294511228357739
-0.763241958363953 to -0.756408750568813
-0.169545870765153 to -0.162712662970013
-0.601847809298936 to -0.595014601503796
Changing layer 5's weights from 
-0.0140428361734787 to -0.00720962837833876
-0.456116360171739 to -0.449283152376599
-0.0309279975733197 to -0.0240947897781798
-0.96644017894477 to -0.95960697114963
-0.429171365245286 to -0.422338157450146
-0.827639308317605 to -0.820806100522465
-0.527674001201096 to -0.520840793405956
-0.455227774127427 to -0.448394566332287
-0.0656257686457077 to -0.0587925608505678
-0.0721338805994437 to -0.0653006728043038
Trying to learn from memory 18, 1, 0.88
sum 0.0375985283489274 distri 0.0233439856539814
Using diff 0.00485491060771422 and condRate 0.166666666666667
Changed category 1 weights from 
0.29484598561443 to 0.295558039166369
0.528161572658191 to 0.528873626210131
0.103236334764134 to 0.103948388316073
0.107329147302281 to 0.10804120085422
Changing layer 0's weights from 
-0.252389102732128 to -0.251677049180188
-0.624315559421962 to -0.623603505870023
-0.858664825116581 to -0.857952771564642
-0.82410557523865 to -0.823393521686711
-0.0762380059112976 to -0.0755259523593579
-0.758879959141201 to -0.758167905589262
-0.605050235544628 to -0.604338181992689
-0.632504641567653 to -0.631792588015713
-0.869996502553409 to -0.86928444900147
-0.913746557687705 to -0.913034504135765
Changing layer 1's weights from 
-0.908649645303672 to -0.907937591751733
-0.307480960642285 to -0.306768907090345
-0.433183699404187 to -0.432471645852248
-0.44308903769154 to -0.442376984139601
-0.628141819988674 to -0.627429766436735
-0.76637965396542 to -0.76566760041348
-0.495660930429882 to -0.494948876877942
-0.881463959371037 to -0.880751905819098
-0.469800978456921 to -0.469088924904982
-0.832447841321414 to -0.831735787769475
Changing layer 2's weights from 
-0.0232596094955878 to -0.022547555943648
-0.577313094889111 to -0.576601041337172
-0.292723804270214 to -0.292011750718274
-0.121523707663006 to -0.120811654111067
-0.298776417528576 to -0.298064363976636
-0.504786043917126 to -0.504073990365187
0.0140266720901061 to 0.0147387256420459
-0.0583953554977848 to -0.057683301945845
-0.790393650089687 to -0.789681596537748
-0.745436131512112 to -0.744724077960172
Changing layer 3's weights from 
-0.0563099081863838 to -0.055597854634444
-0.319862991129345 to -0.319150937577406
-0.0772530730118227 to -0.0765410194598829
-0.984692304767965 to -0.983980251216026
-0.19992896870274 to -0.1992169151508
-0.330177574907727 to -0.329465521355788
-0.660362422024196 to -0.659650368472257
-0.201849549566693 to -0.201137496014753
-0.533545523439831 to -0.532833469887892
-0.0607156451096017 to -0.0600035915576619
Changing layer 4's weights from 
-0.159989445959515 to -0.159277392407575
-0.368822246347851 to -0.368110192795912
-0.569299846445507 to -0.568587792893567
-0.282365113054699 to -0.281653059502759
-0.521617918764538 to -0.520905865212599
-0.290152698313183 to -0.289440644761243
-0.294511228357739 to -0.293799174805799
-0.756408750568813 to -0.755696697016873
-0.162712662970013 to -0.162000609418073
-0.595014601503796 to -0.594302547951857
Changing layer 5's weights from 
-0.00720962837833876 to -0.00649757482639901
-0.449283152376599 to -0.44857109882466
-0.0240947897781798 to -0.02338273622624
-0.95960697114963 to -0.958894917597691
-0.422338157450146 to -0.421626103898207
-0.820806100522465 to -0.820094046970526
-0.520840793405956 to -0.520128739854017
-0.448394566332287 to -0.447682512780348
-0.0587925608505678 to -0.058080507298628
-0.0653006728043038 to -0.064588619252364
Trying to learn from memory 19, 0, 0.88
sum 0.0375985283489274 distri -0.0210054169879366
Using diff 0.0492043132496322 and condRate 0.166666666666667
Changed category 0 weights from 
0.183937846497877 to 0.191154479068719
-0.216097117586748 to -0.208880485015906
-0.498617814107554 to -0.491401181536712
-0.347112448139803 to -0.339895815568961
Changing layer 0's weights from 
-0.251677049180188 to -0.244460416609346
-0.623603505870023 to -0.616386873299181
-0.857952771564642 to -0.8507361389938
-0.823393521686711 to -0.816176889115869
-0.0755259523593579 to -0.0683093197885159
-0.758167905589262 to -0.75095127301842
-0.604338181992689 to -0.597121549421847
-0.631792588015713 to -0.624575955444872
-0.86928444900147 to -0.862067816430628
-0.913034504135765 to -0.905817871564924
Changing layer 1's weights from 
-0.907937591751733 to -0.900720959180891
-0.306768907090345 to -0.299552274519503
-0.432471645852248 to -0.425255013281406
-0.442376984139601 to -0.435160351568759
-0.627429766436735 to -0.620213133865893
-0.76566760041348 to -0.758450967842639
-0.494948876877942 to -0.4877322443071
-0.880751905819098 to -0.873535273248256
-0.469088924904982 to -0.46187229233414
-0.831735787769475 to -0.824519155198633
Changing layer 2's weights from 
-0.022547555943648 to -0.015330923372806
-0.576601041337172 to -0.56938440876633
-0.292011750718274 to -0.284795118147432
-0.120811654111067 to -0.113595021540225
-0.298064363976636 to -0.290847731405794
-0.504073990365187 to -0.496857357794345
0.0147387256420459 to 0.0219553582128879
-0.057683301945845 to -0.050466669375003
-0.789681596537748 to -0.782464963966906
-0.744724077960172 to -0.737507445389331
Changing layer 3's weights from 
-0.055597854634444 to -0.048381222063602
-0.319150937577406 to -0.311934305006564
-0.0765410194598829 to -0.0693243868890409
-0.983980251216026 to -0.976763618645184
-0.1992169151508 to -0.192000282579958
-0.329465521355788 to -0.322248888784946
-0.659650368472257 to -0.652433735901415
-0.201137496014753 to -0.193920863443911
-0.532833469887892 to -0.52561683731705
-0.0600035915576619 to -0.0527869589868199
Changing layer 4's weights from 
-0.159277392407575 to -0.152060759836734
-0.368110192795912 to -0.36089356022507
-0.568587792893567 to -0.561371160322725
-0.281653059502759 to -0.274436426931917
-0.520905865212599 to -0.513689232641757
-0.289440644761243 to -0.282224012190402
-0.293799174805799 to -0.286582542234957
-0.755696697016873 to -0.748480064446032
-0.162000609418073 to -0.154783976847232
-0.594302547951857 to -0.587085915381015
Changing layer 5's weights from 
-0.00649757482639901 to 0.000719057744442969
-0.44857109882466 to -0.441354466253818
-0.02338273622624 to -0.016166103655398
-0.958894917597691 to -0.951678285026849
-0.421626103898207 to -0.414409471327365
-0.820094046970526 to -0.812877414399684
-0.520128739854017 to -0.512912107283175
-0.447682512780348 to -0.440465880209506
-0.058080507298628 to -0.050863874727786
-0.064588619252364 to -0.0573719866815221
Trying to learn from memory 20, 0, 0.88
sum 0.0375985283489274 distri -0.0210054169879366
Using diff 0.0492043132496322 and condRate 0.166666666666667
Changed category 0 weights from 
0.191154479068719 to 0.198371111639561
-0.208880485015906 to -0.201663852445064
-0.491401181536712 to -0.48418454896587
-0.339895815568961 to -0.332679182998119
Changing layer 0's weights from 
-0.244460416609346 to -0.237243784038504
-0.616386873299181 to -0.609170240728339
-0.8507361389938 to -0.843519506422958
-0.816176889115869 to -0.808960256545027
-0.0683093197885159 to -0.0610926872176739
-0.75095127301842 to -0.743734640447578
-0.597121549421847 to -0.589904916851005
-0.624575955444872 to -0.61735932287403
-0.862067816430628 to -0.854851183859786
-0.905817871564924 to -0.898601238994082
Changing layer 1's weights from 
-0.900720959180891 to -0.893504326610049
-0.299552274519503 to -0.292335641948661
-0.425255013281406 to -0.418038380710564
-0.435160351568759 to -0.427943718997917
-0.620213133865893 to -0.612996501295051
-0.758450967842639 to -0.751234335271797
-0.4877322443071 to -0.480515611736259
-0.873535273248256 to -0.866318640677414
-0.46187229233414 to -0.454655659763298
-0.824519155198633 to -0.817302522627791
Changing layer 2's weights from 
-0.015330923372806 to -0.00811429080196405
-0.56938440876633 to -0.562167776195488
-0.284795118147432 to -0.27757848557659
-0.113595021540225 to -0.106378388969383
-0.290847731405794 to -0.283631098834952
-0.496857357794345 to -0.489640725223503
0.0219553582128879 to 0.0291719907837299
-0.050466669375003 to -0.0432500368041611
-0.782464963966906 to -0.775248331396064
-0.737507445389331 to -0.730290812818489
Changing layer 3's weights from 
-0.048381222063602 to -0.0411645894927601
-0.311934305006564 to -0.304717672435722
-0.0693243868890409 to -0.0621077543181989
-0.976763618645184 to -0.969546986074342
-0.192000282579958 to -0.184783650009116
-0.322248888784946 to -0.315032256214104
-0.652433735901415 to -0.645217103330573
-0.193920863443911 to -0.186704230873069
-0.52561683731705 to -0.518400204746208
-0.0527869589868199 to -0.045570326415978
Changing layer 4's weights from 
-0.152060759836734 to -0.144844127265892
-0.36089356022507 to -0.353676927654228
-0.561371160322725 to -0.554154527751884
-0.274436426931917 to -0.267219794361075
-0.513689232641757 to -0.506472600070915
-0.282224012190402 to -0.27500737961956
-0.286582542234957 to -0.279365909664116
-0.748480064446032 to -0.74126343187519
-0.154783976847232 to -0.14756734427639
-0.587085915381015 to -0.579869282810173
Changing layer 5's weights from 
0.000719057744442969 to 0.00793569031528495
-0.441354466253818 to -0.434137833682976
-0.016166103655398 to -0.00894947108455604
-0.951678285026849 to -0.944461652456007
-0.414409471327365 to -0.407192838756523
-0.812877414399684 to -0.805660781828842
-0.512912107283175 to -0.505695474712333
-0.440465880209506 to -0.433249247638664
-0.050863874727786 to -0.0436472421569441
-0.0573719866815221 to -0.0501553541106801
Trying to learn from memory 21, 1, 0.88
sum 0.0375985283489274 distri 0.0233439856539814
Using diff 0.00485491060771422 and condRate 0.166666666666667
Changed category 1 weights from 
0.295558039166369 to 0.296270092718309
0.528873626210131 to 0.52958567976207
0.103948388316073 to 0.104660441868013
0.10804120085422 to 0.10875325440616
Changing layer 0's weights from 
-0.237243784038504 to -0.236531730486565
-0.609170240728339 to -0.608458187176399
-0.843519506422958 to -0.842807452871018
-0.808960256545027 to -0.808248202993087
-0.0610926872176739 to -0.0603806336657342
-0.743734640447578 to -0.743022586895638
-0.589904916851005 to -0.589192863299065
-0.61735932287403 to -0.61664726932209
-0.854851183859786 to -0.854139130307846
-0.898601238994082 to -0.897889185442142
Changing layer 1's weights from 
-0.893504326610049 to -0.892792273058109
-0.292335641948661 to -0.291623588396722
-0.418038380710564 to -0.417326327158624
-0.427943718997917 to -0.427231665445977
-0.612996501295051 to -0.612284447743111
-0.751234335271797 to -0.750522281719857
-0.480515611736259 to -0.479803558184319
-0.866318640677414 to -0.865606587125474
-0.454655659763298 to -0.453943606211358
-0.817302522627791 to -0.816590469075851
Changing layer 2's weights from 
-0.00811429080196405 to -0.0074022372500243
-0.562167776195488 to -0.561455722643548
-0.27757848557659 to -0.276866432024651
-0.106378388969383 to -0.105666335417443
-0.283631098834952 to -0.282919045283013
-0.489640725223503 to -0.488928671671563
0.0291719907837299 to 0.0298840443356696
-0.0432500368041611 to -0.0425379832522213
-0.775248331396064 to -0.774536277844124
-0.730290812818489 to -0.729578759266549
Changing layer 3's weights from 
-0.0411645894927601 to -0.0404525359408203
-0.304717672435722 to -0.304005618883782
-0.0621077543181989 to -0.0613957007662592
-0.969546986074342 to -0.968834932522402
-0.184783650009116 to -0.184071596457177
-0.315032256214104 to -0.314320202662164
-0.645217103330573 to -0.644505049778633
-0.186704230873069 to -0.18599217732113
-0.518400204746208 to -0.517688151194268
-0.045570326415978 to -0.0448582728640382
Changing layer 4's weights from 
-0.144844127265892 to -0.144132073713952
-0.353676927654228 to -0.352964874102288
-0.554154527751884 to -0.553442474199944
-0.267219794361075 to -0.266507740809136
-0.506472600070915 to -0.505760546518975
-0.27500737961956 to -0.27429532606762
-0.279365909664116 to -0.278653856112176
-0.74126343187519 to -0.74055137832325
-0.14756734427639 to -0.14685529072445
-0.579869282810173 to -0.579157229258233
Changing layer 5's weights from 
0.00793569031528495 to 0.0086477438672247
-0.434137833682976 to -0.433425780131036
-0.00894947108455604 to -0.00823741753261629
-0.944461652456007 to -0.943749598904067
-0.407192838756523 to -0.406480785204583
-0.805660781828842 to -0.804948728276902
-0.505695474712333 to -0.504983421160393
-0.433249247638664 to -0.432537194086724
-0.0436472421569441 to -0.0429351886050043
-0.0501553541106801 to -0.0494433005587403
Trying to learn from memory 22, 1, 0.88
sum 0.0375985283489274 distri 0.0233439856539814
Using diff 0.00485491060771422 and condRate 0.166666666666667
Changed category 1 weights from 
0.296270092718309 to 0.296982146270249
0.52958567976207 to 0.53029773331401
0.104660441868013 to 0.105372495419953
0.10875325440616 to 0.1094653079581
Changing layer 0's weights from 
-0.236531730486565 to -0.235819676934625
-0.608458187176399 to -0.607746133624459
-0.842807452871018 to -0.842095399319078
-0.808248202993087 to -0.807536149441147
-0.0603806336657342 to -0.0596685801137944
-0.743022586895638 to -0.742310533343698
-0.589192863299065 to -0.588480809747125
-0.61664726932209 to -0.61593521577015
-0.854139130307846 to -0.853427076755906
-0.897889185442142 to -0.897177131890202
Changing layer 1's weights from 
-0.892792273058109 to -0.892080219506169
-0.291623588396722 to -0.290911534844782
-0.417326327158624 to -0.416614273606684
-0.427231665445977 to -0.426519611894037
-0.612284447743111 to -0.611572394191171
-0.750522281719857 to -0.749810228167917
-0.479803558184319 to -0.479091504632379
-0.865606587125474 to -0.864894533573534
-0.453943606211358 to -0.453231552659418
-0.816590469075851 to -0.815878415523911
Changing layer 2's weights from 
-0.0074022372500243 to -0.00669018369808455
-0.561455722643548 to -0.560743669091608
-0.276866432024651 to -0.276154378472711
-0.105666335417443 to -0.104954281865503
-0.282919045283013 to -0.282206991731073
-0.488928671671563 to -0.488216618119623
0.0298840443356696 to 0.0305960978876094
-0.0425379832522213 to -0.0418259297002816
-0.774536277844124 to -0.773824224292184
-0.729578759266549 to -0.728866705714609
Changing layer 3's weights from 
-0.0404525359408203 to -0.0397404823888806
-0.304005618883782 to -0.303293565331842
-0.0613957007662592 to -0.0606836472143194
-0.968834932522402 to -0.968122878970462
-0.184071596457177 to -0.183359542905237
-0.314320202662164 to -0.313608149110224
-0.644505049778633 to -0.643792996226693
-0.18599217732113 to -0.18528012376919
-0.517688151194268 to -0.516976097642328
-0.0448582728640382 to -0.0441462193120985
Changing layer 4's weights from 
-0.144132073713952 to -0.143420020162012
-0.352964874102288 to -0.352252820550348
-0.553442474199944 to -0.552730420648004
-0.266507740809136 to -0.265795687257196
-0.505760546518975 to -0.505048492967035
-0.27429532606762 to -0.27358327251568
-0.278653856112176 to -0.277941802560236
-0.74055137832325 to -0.73983932477131
-0.14685529072445 to -0.14614323717251
-0.579157229258233 to -0.578445175706293
Changing layer 5's weights from 
0.0086477438672247 to 0.00935979741916445
-0.433425780131036 to -0.432713726579096
-0.00823741753261629 to -0.00752536398067654
-0.943749598904067 to -0.943037545352127
-0.406480785204583 to -0.405768731652643
-0.804948728276902 to -0.804236674724962
-0.504983421160393 to -0.504271367608453
-0.432537194086724 to -0.431825140534784
-0.0429351886050043 to -0.0422231350530646
-0.0494433005587403 to -0.0487312470068006
Trying to learn from memory 23, 0, 0.88
sum 0.0375985283489274 distri -0.0210054169879366
Using diff 0.0492043132496322 and condRate 0.166666666666667
Changed category 0 weights from 
0.198371111639561 to 0.205587744210403
-0.201663852445064 to -0.194447219874222
-0.48418454896587 to -0.476967916395028
-0.332679182998119 to -0.325462550427277
Changing layer 0's weights from 
-0.235819676934625 to -0.228603044363783
-0.607746133624459 to -0.600529501053617
-0.842095399319078 to -0.834878766748236
-0.807536149441147 to -0.800319516870305
-0.0596685801137944 to -0.0524519475429524
-0.742310533343698 to -0.735093900772856
-0.588480809747125 to -0.581264177176283
-0.61593521577015 to -0.608718583199308
-0.853427076755906 to -0.846210444185064
-0.897177131890202 to -0.88996049931936
Changing layer 1's weights from 
-0.892080219506169 to -0.884863586935327
-0.290911534844782 to -0.28369490227394
-0.416614273606684 to -0.409397641035842
-0.426519611894037 to -0.419302979323195
-0.611572394191171 to -0.604355761620329
-0.749810228167917 to -0.742593595597075
-0.479091504632379 to -0.471874872061537
-0.864894533573534 to -0.857677901002692
-0.453231552659418 to -0.446014920088576
-0.815878415523911 to -0.808661782953069
Changing layer 2's weights from 
-0.00669018369808455 to 0.000526448872757435
-0.560743669091608 to -0.553527036520766
-0.276154378472711 to -0.268937745901869
-0.104954281865503 to -0.0977376492946613
-0.282206991731073 to -0.274990359160231
-0.488216618119623 to -0.480999985548781
0.0305960978876094 to 0.0378127304584513
-0.0418259297002816 to -0.0346092971294396
-0.773824224292184 to -0.766607591721342
-0.728866705714609 to -0.721650073143767
Changing layer 3's weights from 
-0.0397404823888806 to -0.0325238498180386
-0.303293565331842 to -0.296076932761
-0.0606836472143194 to -0.0534670146434775
-0.968122878970462 to -0.96090624639962
-0.183359542905237 to -0.176142910334395
-0.313608149110224 to -0.306391516539382
-0.643792996226693 to -0.636576363655851
-0.18528012376919 to -0.178063491198348
-0.516976097642328 to -0.509759465071486
-0.0441462193120985 to -0.0369295867412565
Changing layer 4's weights from 
-0.143420020162012 to -0.13620338759117
-0.352252820550348 to -0.345036187979506
-0.552730420648004 to -0.545513788077162
-0.265795687257196 to -0.258579054686354
-0.505048492967035 to -0.497831860396193
-0.27358327251568 to -0.266366639944838
-0.277941802560236 to -0.270725169989394
-0.73983932477131 to -0.732622692200468
-0.14614323717251 to -0.138926604601668
-0.578445175706293 to -0.571228543135451
Changing layer 5's weights from 
0.00935979741916445 to 0.0165764299900064
-0.432713726579096 to -0.425497094008254
-0.00752536398067654 to -0.000308731409834561
-0.943037545352127 to -0.935820912781285
-0.405768731652643 to -0.398552099081801
-0.804236674724962 to -0.79702004215412
-0.504271367608453 to -0.497054735037611
-0.431825140534784 to -0.424608507963942
-0.0422231350530646 to -0.0350065024822226
-0.0487312470068006 to -0.0415146144359586
Trying to learn from memory 23, 1, 0.88
sum 0.0375985283489274 distri 0.0233439856539814
Using diff 0.00485491060771422 and condRate 0.166666666666667
Changed category 1 weights from 
0.296982146270249 to 0.297694199822189
0.53029773331401 to 0.53100978686595
0.105372495419953 to 0.106084548971893
0.1094653079581 to 0.11017736151004
Changing layer 0's weights from 
-0.228603044363783 to -0.227890990811843
-0.600529501053617 to -0.599817447501678
-0.834878766748236 to -0.834166713196297
-0.800319516870305 to -0.799607463318366
-0.0524519475429524 to -0.0517398939910127
-0.735093900772856 to -0.734381847220917
-0.581264177176283 to -0.580552123624344
-0.608718583199308 to -0.608006529647369
-0.846210444185064 to -0.845498390633125
-0.88996049931936 to -0.889248445767421
Changing layer 1's weights from 
-0.884863586935327 to -0.884151533383388
-0.28369490227394 to -0.282982848722
-0.409397641035842 to -0.408685587483902
-0.419302979323195 to -0.418590925771255
-0.604355761620329 to -0.60364370806839
-0.742593595597075 to -0.741881542045136
-0.471874872061537 to -0.471162818509597
-0.857677901002692 to -0.856965847450753
-0.446014920088576 to -0.445302866536636
-0.808661782953069 to -0.80794972940113
Changing layer 2's weights from 
0.000526448872757435 to 0.00123850242469719
-0.553527036520766 to -0.552814982968827
-0.268937745901869 to -0.268225692349929
-0.0977376492946613 to -0.0970255957427215
-0.274990359160231 to -0.274278305608291
-0.480999985548781 to -0.480287931996841
0.0378127304584513 to 0.0385247840103911
-0.0346092971294396 to -0.0338972435774998
-0.766607591721342 to -0.765895538169403
-0.721650073143767 to -0.720938019591828
Changing layer 3's weights from 
-0.0325238498180386 to -0.0318117962660988
-0.296076932761 to -0.29536487920906
-0.0534670146434775 to -0.0527549610915377
-0.96090624639962 to -0.960194192847681
-0.176142910334395 to -0.175430856782455
-0.306391516539382 to -0.305679462987442
-0.636576363655851 to -0.635864310103912
-0.178063491198348 to -0.177351437646408
-0.509759465071486 to -0.509047411519547
-0.0369295867412565 to -0.0362175331893167
Changing layer 4's weights from 
-0.13620338759117 to -0.13549133403923
-0.345036187979506 to -0.344324134427566
-0.545513788077162 to -0.544801734525223
-0.258579054686354 to -0.257867001134414
-0.497831860396193 to -0.497119806844254
-0.266366639944838 to -0.265654586392898
-0.270725169989394 to -0.270013116437454
-0.732622692200468 to -0.731910638648529
-0.138926604601668 to -0.138214551049728
-0.571228543135451 to -0.570516489583512
Changing layer 5's weights from 
0.0165764299900064 to 0.0172884835419462
-0.425497094008254 to -0.424785040456314
-0.000308731409834561 to 0.000403322142105189
-0.935820912781285 to -0.935108859229346
-0.398552099081801 to -0.397840045529861
-0.79702004215412 to -0.796307988602181
-0.497054735037611 to -0.496342681485672
-0.424608507963942 to -0.423896454412002
-0.0350065024822226 to -0.0342944489302828
-0.0415146144359586 to -0.0408025608840188
Trying to learn from memory 24, 0, 0.88
sum 0.0375681585214322 distri -0.0211960088598694
Using diff 0.0493721277509436 and condRate 0.166666666666667
Changed category 0 weights from 
0.205587744210403 to 0.212828989574637
-0.194447219874222 to -0.187205974509988
-0.476967916395028 to -0.469726671030794
-0.325462550427277 to -0.318221305063043
Changing layer 0's weights from 
-0.227890990811843 to -0.220649745447609
-0.599817447501678 to -0.592576202137443
-0.834166713196297 to -0.826925467832062
-0.799607463318366 to -0.792366217954131
-0.0517398939910127 to -0.0444986486267784
-0.734381847220917 to -0.727140601856682
-0.580552123624344 to -0.573310878260109
-0.608006529647369 to -0.600765284283134
-0.845498390633125 to -0.83825714526889
-0.889248445767421 to -0.882007200403186
Changing layer 1's weights from 
-0.884151533383388 to -0.876910288019153
-0.282982848722 to -0.275741603357766
-0.408685587483902 to -0.401444342119668
-0.418590925771255 to -0.411349680407021
-0.60364370806839 to -0.596402462704155
-0.741881542045136 to -0.734640296680901
-0.471162818509597 to -0.463921573145363
-0.856965847450753 to -0.849724602086518
-0.445302866536636 to -0.438061621172402
-0.80794972940113 to -0.800708484036895
Changing layer 2's weights from 
0.00123850242469719 to 0.00847974778893147
-0.552814982968827 to -0.545573737604592
-0.268225692349929 to -0.260984446985695
-0.0970255957427215 to -0.0897843503784872
-0.274278305608291 to -0.267037060244057
-0.480287931996841 to -0.473046686632607
0.0385247840103911 to 0.0457660293746254
-0.0338972435774998 to -0.0266559982132655
-0.765895538169403 to -0.758654292805168
-0.720938019591828 to -0.713696774227593
Changing layer 3's weights from 
-0.0318117962660988 to -0.0245705509018646
-0.29536487920906 to -0.288123633844826
-0.0527549610915377 to -0.0455137157273034
-0.960194192847681 to -0.952952947483446
-0.175430856782455 to -0.168189611418221
-0.305679462987442 to -0.298438217623208
-0.635864310103912 to -0.628623064739677
-0.177351437646408 to -0.170110192282174
-0.509047411519547 to -0.501806166155312
-0.0362175331893167 to -0.0289762878250825
Changing layer 4's weights from 
-0.13549133403923 to -0.128250088674996
-0.344324134427566 to -0.337082889063332
-0.544801734525223 to -0.537560489160988
-0.257867001134414 to -0.25062575577018
-0.497119806844254 to -0.489878561480019
-0.265654586392898 to -0.258413341028664
-0.270013116437454 to -0.26277187107322
-0.731910638648529 to -0.724669393284294
-0.138214551049728 to -0.130973305685494
-0.570516489583512 to -0.563275244219277
Changing layer 5's weights from 
0.0172884835419462 to 0.0245297289061805
-0.424785040456314 to -0.41754379509208
0.000403322142105189 to 0.00764456750633947
-0.935108859229346 to -0.927867613865111
-0.397840045529861 to -0.390598800165627
-0.796307988602181 to -0.789066743237946
-0.496342681485672 to -0.489101436121437
-0.423896454412002 to -0.416655209047768
-0.0342944489302828 to -0.0270532035660485
-0.0408025608840188 to -0.0335613155197846
10/5/2016 1:41:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:31 PMStarting learning phase with deltaScore: 0.06666667
Modified index 0's learning in memoryPool to 0.01333333
Modified index 1's learning in memoryPool to 0.01333333
Modified index 2's learning in memoryPool to 0.01333333
Modified index 3's learning in memoryPool to 0.01333333
Modified index 4's learning in memoryPool to 0.01333333
Modified index 5's learning in memoryPool to 0.01333333
Modified index 6's learning in memoryPool to 0.01333333
Modified index 7's learning in memoryPool to 0.01333333
Modified index 8's learning in memoryPool to 0.01333333
Modified index 9's learning in memoryPool to 0.01333333
Modified index 10's learning in memoryPool to 0.01333333
Modified index 11's learning in memoryPool to 0.01333333
Modified index 12's learning in memoryPool to 0.01333333
10/5/2016 1:41:31 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 25, 0, 0.01333333
sum 0.0376060496381868 distri -0.0160846993847038
Using diff 0.0442892366133438 and condRate 0.166666666666667
Changed category 0 weights from 
0.212828989574637 to 0.212927410102827
-0.187205974509988 to -0.187107553981798
-0.469726671030794 to -0.469628250502603
-0.318221305063043 to -0.318122884534853
Changing layer 0's weights from 
-0.220649745447609 to -0.220551324919418
-0.592576202137443 to -0.592477781609253
-0.826925467832062 to -0.826827047303872
-0.792366217954131 to -0.792267797425941
-0.0444986486267784 to -0.0444002280985878
-0.727140601856682 to -0.727042181328492
-0.573310878260109 to -0.573212457731919
-0.600765284283134 to -0.600666863754944
-0.83825714526889 to -0.8381587247407
-0.882007200403186 to -0.881908779874996
Changing layer 1's weights from 
-0.876910288019153 to -0.876811867490963
-0.275741603357766 to -0.275643182829575
-0.401444342119668 to -0.401345921591477
-0.411349680407021 to -0.41125125987883
-0.596402462704155 to -0.596304042175965
-0.734640296680901 to -0.734541876152711
-0.463921573145363 to -0.463823152617172
-0.849724602086518 to -0.849626181558328
-0.438061621172402 to -0.437963200644211
-0.800708484036895 to -0.800610063508705
Changing layer 2's weights from 
0.00847974778893147 to 0.00857816831712209
-0.545573737604592 to -0.545475317076402
-0.260984446985695 to -0.260886026457504
-0.0897843503784872 to -0.0896859298502966
-0.267037060244057 to -0.266938639715866
-0.473046686632607 to -0.472948266104416
0.0457660293746254 to 0.045864449902816
-0.0266559982132655 to -0.0265575776850749
-0.758654292805168 to -0.758555872276978
-0.713696774227593 to -0.713598353699403
Changing layer 3's weights from 
-0.0245705509018646 to -0.0244721303736739
-0.288123633844826 to -0.288025213316635
-0.0455137157273034 to -0.0454152951991128
-0.952952947483446 to -0.952854526955256
-0.168189611418221 to -0.16809119089003
-0.298438217623208 to -0.298339797095017
-0.628623064739677 to -0.628524644211487
-0.170110192282174 to -0.170011771753983
-0.501806166155312 to -0.501707745627122
-0.0289762878250825 to -0.0288778672968918
Changing layer 4's weights from 
-0.128250088674996 to -0.128151668146805
-0.337082889063332 to -0.336984468535141
-0.537560489160988 to -0.537462068632798
-0.25062575577018 to -0.250527335241989
-0.489878561480019 to -0.489780140951829
-0.258413341028664 to -0.258314920500473
-0.26277187107322 to -0.262673450545029
-0.724669393284294 to -0.724570972756104
-0.130973305685494 to -0.130874885157303
-0.563275244219277 to -0.563176823691087
Changing layer 5's weights from 
0.0245297289061805 to 0.0246281494343711
-0.41754379509208 to -0.417445374563889
0.00764456750633947 to 0.0077429880345301
-0.927867613865111 to -0.927769193336921
-0.390598800165627 to -0.390500379637436
-0.789066743237946 to -0.788968322709756
-0.489101436121437 to -0.489003015593247
-0.416655209047768 to -0.416556788519577
-0.0270532035660485 to -0.0269547830378579
-0.0335613155197846 to -0.0334628949915939
Trying to learn from memory 26, 1, 0.01333333
sum 0.0376060496381868 distri 0.0215015396451152
Using diff 0.00670299758352485 and condRate 0.166666666666667
Changed category 1 weights from 
0.297694199822189 to 0.297709095372735
0.53100978686595 to 0.531024682416496
0.106084548971893 to 0.106099444522439
0.11017736151004 to 0.110192257060586
Changing layer 0's weights from 
-0.220551324919418 to -0.220536429368872
-0.592477781609253 to -0.592462886058706
-0.826827047303872 to -0.826812151753325
-0.792267797425941 to -0.792252901875394
-0.0444002280985878 to -0.0443853325480415
-0.727042181328492 to -0.727027285777945
-0.573212457731919 to -0.573197562181372
-0.600666863754944 to -0.600651968204397
-0.8381587247407 to -0.838143829190153
-0.881908779874996 to -0.881893884324449
Changing layer 1's weights from 
-0.876811867490963 to -0.876796971940416
-0.275643182829575 to -0.275628287279029
-0.401345921591477 to -0.401331026040931
-0.41125125987883 to -0.411236364328284
-0.596304042175965 to -0.596289146625418
-0.734541876152711 to -0.734526980602164
-0.463823152617172 to -0.463808257066626
-0.849626181558328 to -0.849611286007781
-0.437963200644211 to -0.437948305093665
-0.800610063508705 to -0.800595167958158
Changing layer 2's weights from 
0.00857816831712209 to 0.00859306386766839
-0.545475317076402 to -0.545460421525855
-0.260886026457504 to -0.260871130906958
-0.0896859298502966 to -0.0896710342997503
-0.266938639715866 to -0.26692374416532
-0.472948266104416 to -0.47293337055387
0.045864449902816 to 0.0458793454533623
-0.0265575776850749 to -0.0265426821345286
-0.758555872276978 to -0.758540976726431
-0.713598353699403 to -0.713583458148856
Changing layer 3's weights from 
-0.0244721303736739 to -0.0244572348231276
-0.288025213316635 to -0.288010317766089
-0.0454152951991128 to -0.0454003996485665
-0.952854526955256 to -0.952839631404709
-0.16809119089003 to -0.168076295339484
-0.298339797095017 to -0.298324901544471
-0.628524644211487 to -0.62850974866094
-0.170011771753983 to -0.169996876203437
-0.501707745627122 to -0.501692850076575
-0.0288778672968918 to -0.0288629717463455
Changing layer 4's weights from 
-0.128151668146805 to -0.128136772596259
-0.336984468535141 to -0.336969572984595
-0.537462068632798 to -0.537447173082251
-0.250527335241989 to -0.250512439691443
-0.489780140951829 to -0.489765245401282
-0.258314920500473 to -0.258300024949927
-0.262673450545029 to -0.262658554994483
-0.724570972756104 to -0.724556077205557
-0.130874885157303 to -0.130859989606757
-0.563176823691087 to -0.56316192814054
Changing layer 5's weights from 
0.0246281494343711 to 0.0246430449849174
-0.417445374563889 to -0.417430479013343
0.0077429880345301 to 0.00775788358507639
-0.927769193336921 to -0.927754297786374
-0.390500379637436 to -0.39048548408689
-0.788968322709756 to -0.788953427159209
-0.489003015593247 to -0.4889881200427
-0.416556788519577 to -0.416541892969031
-0.0269547830378579 to -0.0269398874873116
-0.0334628949915939 to -0.0334479994410476
Trying to learn from memory 27, 0, 0.01333333
sum 0.0376060496381868 distri -0.0160846993847038
Using diff 0.0442892366133438 and condRate 0.166666666666667
Changed category 0 weights from 
0.212927410102827 to 0.213025830631018
-0.187107553981798 to -0.187009133453607
-0.469628250502603 to -0.469529829974413
-0.318122884534853 to -0.318024464006662
Changing layer 0's weights from 
-0.220536429368872 to -0.220438008840681
-0.592462886058706 to -0.592364465530516
-0.826812151753325 to -0.826713731225135
-0.792252901875394 to -0.792154481347204
-0.0443853325480415 to -0.0442869120198509
-0.727027285777945 to -0.726928865249755
-0.573197562181372 to -0.573099141653182
-0.600651968204397 to -0.600553547676207
-0.838143829190153 to -0.838045408661963
-0.881893884324449 to -0.881795463796259
Changing layer 1's weights from 
-0.876796971940416 to -0.876698551412226
-0.275628287279029 to -0.275529866750838
-0.401331026040931 to -0.40123260551274
-0.411236364328284 to -0.411137943800094
-0.596289146625418 to -0.596190726097228
-0.734526980602164 to -0.734428560073974
-0.463808257066626 to -0.463709836538435
-0.849611286007781 to -0.849512865479591
-0.437948305093665 to -0.437849884565475
-0.800595167958158 to -0.800496747429968
Changing layer 2's weights from 
0.00859306386766839 to 0.00869148439585901
-0.545460421525855 to -0.545362000997665
-0.260871130906958 to -0.260772710378767
-0.0896710342997503 to -0.0895726137715597
-0.26692374416532 to -0.266825323637129
-0.47293337055387 to -0.47283495002568
0.0458793454533623 to 0.0459777659815529
-0.0265426821345286 to -0.026444261606338
-0.758540976726431 to -0.758442556198241
-0.713583458148856 to -0.713485037620666
Changing layer 3's weights from 
-0.0244572348231276 to -0.024358814294937
-0.288010317766089 to -0.287911897237898
-0.0454003996485665 to -0.0453019791203759
-0.952839631404709 to -0.952741210876519
-0.168076295339484 to -0.167977874811293
-0.298324901544471 to -0.298226481016281
-0.62850974866094 to -0.62841132813275
-0.169996876203437 to -0.169898455675246
-0.501692850076575 to -0.501594429548385
-0.0288629717463455 to -0.0287645512181549
Changing layer 4's weights from 
-0.128136772596259 to -0.128038352068068
-0.336969572984595 to -0.336871152456404
-0.537447173082251 to -0.537348752554061
-0.250512439691443 to -0.250414019163252
-0.489765245401282 to -0.489666824873092
-0.258300024949927 to -0.258201604421736
-0.262658554994483 to -0.262560134466292
-0.724556077205557 to -0.724457656677367
-0.130859989606757 to -0.130761569078566
-0.56316192814054 to -0.56306350761235
Changing layer 5's weights from 
0.0246430449849174 to 0.024741465513108
-0.417430479013343 to -0.417332058485152
0.00775788358507639 to 0.00785630411326702
-0.927754297786374 to -0.927655877258184
-0.39048548408689 to -0.3903870635587
-0.788953427159209 to -0.788855006631019
-0.4889881200427 to -0.48888969951451
-0.416541892969031 to -0.416443472440841
-0.0269398874873116 to -0.026841466959121
-0.0334479994410476 to -0.033349578912857
Trying to learn from memory 28, 1, 0.01333333
sum 0.0376060496381868 distri 0.0215015396451152
Using diff 0.00670299758352485 and condRate 0.166666666666667
Changed category 1 weights from 
0.297709095372735 to 0.297723990923281
0.531024682416496 to 0.531039577967043
0.106099444522439 to 0.106114340072985
0.110192257060586 to 0.110207152611132
Changing layer 0's weights from 
-0.220438008840681 to -0.220423113290135
-0.592364465530516 to -0.592349569979969
-0.826713731225135 to -0.826698835674588
-0.792154481347204 to -0.792139585796657
-0.0442869120198509 to -0.0442720164693046
-0.726928865249755 to -0.726913969699208
-0.573099141653182 to -0.573084246102635
-0.600553547676207 to -0.60053865212566
-0.838045408661963 to -0.838030513111416
-0.881795463796259 to -0.881780568245712
Changing layer 1's weights from 
-0.876698551412226 to -0.876683655861679
-0.275529866750838 to -0.275514971200292
-0.40123260551274 to -0.401217709962194
-0.411137943800094 to -0.411123048249547
-0.596190726097228 to -0.596175830546681
-0.734428560073974 to -0.734413664523427
-0.463709836538435 to -0.463694940987889
-0.849512865479591 to -0.849497969929044
-0.437849884565475 to -0.437834989014928
-0.800496747429968 to -0.800481851879421
Changing layer 2's weights from 
0.00869148439585901 to 0.00870637994640531
-0.545362000997665 to -0.545347105447118
-0.260772710378767 to -0.260757814828221
-0.0895726137715597 to -0.0895577182210134
-0.266825323637129 to -0.266810428086583
-0.47283495002568 to -0.472820054475133
0.0459777659815529 to 0.0459926615320992
-0.026444261606338 to -0.0264293660557917
-0.758442556198241 to -0.758427660647694
-0.713485037620666 to -0.713470142070119
Changing layer 3's weights from 
-0.024358814294937 to -0.0243439187443907
-0.287911897237898 to -0.287897001687352
-0.0453019791203759 to -0.0452870835698296
-0.952741210876519 to -0.952726315325972
-0.167977874811293 to -0.167962979260747
-0.298226481016281 to -0.298211585465734
-0.62841132813275 to -0.628396432582203
-0.169898455675246 to -0.1698835601247
-0.501594429548385 to -0.501579533997838
-0.0287645512181549 to -0.0287496556676086
Changing layer 4's weights from 
-0.128038352068068 to -0.128023456517522
-0.336871152456404 to -0.336856256905858
-0.537348752554061 to -0.537333857003514
-0.250414019163252 to -0.250399123612706
-0.489666824873092 to -0.489651929322545
-0.258201604421736 to -0.25818670887119
-0.262560134466292 to -0.262545238915746
-0.724457656677367 to -0.72444276112682
-0.130761569078566 to -0.13074667352802
-0.56306350761235 to -0.563048612061803
Changing layer 5's weights from 
0.024741465513108 to 0.0247563610636543
-0.417332058485152 to -0.417317162934606
0.00785630411326702 to 0.00787119966381331
-0.927655877258184 to -0.927640981707637
-0.3903870635587 to -0.390372168008153
-0.788855006631019 to -0.788840111080472
-0.48888969951451 to -0.488874803963963
-0.416443472440841 to -0.416428576890294
-0.026841466959121 to -0.0268265714085747
-0.033349578912857 to -0.0333346833623107
Trying to learn from memory 29, 0, 0.01333333
sum 0.0376060496381868 distri -0.0160846993847038
Using diff 0.0442892366133438 and condRate 0.166666666666667
Changed category 0 weights from 
0.213025830631018 to 0.213124251159209
-0.187009133453607 to -0.186910712925416
-0.469529829974413 to -0.469431409446222
-0.318024464006662 to -0.317926043478471
Changing layer 0's weights from 
-0.220423113290135 to -0.220324692761944
-0.592349569979969 to -0.592251149451779
-0.826698835674588 to -0.826600415146398
-0.792139585796657 to -0.792041165268467
-0.0442720164693046 to -0.0441735959411139
-0.726913969699208 to -0.726815549171018
-0.573084246102635 to -0.572985825574445
-0.60053865212566 to -0.60044023159747
-0.838030513111416 to -0.837932092583226
-0.881780568245712 to -0.881682147717522
Changing layer 1's weights from 
-0.876683655861679 to -0.876585235333489
-0.275514971200292 to -0.275416550672101
-0.401217709962194 to -0.401119289434004
-0.411123048249547 to -0.411024627721357
-0.596175830546681 to -0.596077410018491
-0.734413664523427 to -0.734315243995237
-0.463694940987889 to -0.463596520459698
-0.849497969929044 to -0.849399549400854
-0.437834989014928 to -0.437736568486738
-0.800481851879421 to -0.800383431351231
Changing layer 2's weights from 
0.00870637994640531 to 0.00880480047459593
-0.545347105447118 to -0.545248684918928
-0.260757814828221 to -0.26065939430003
-0.0895577182210134 to -0.0894592976928228
-0.266810428086583 to -0.266712007558392
-0.472820054475133 to -0.472721633946943
0.0459926615320992 to 0.0460910820602898
-0.0264293660557917 to -0.0263309455276011
-0.758427660647694 to -0.758329240119504
-0.713470142070119 to -0.713371721541929
Changing layer 3's weights from 
-0.0243439187443907 to -0.0242454982162001
-0.287897001687352 to -0.287798581159162
-0.0452870835698296 to -0.045188663041639
-0.952726315325972 to -0.952627894797782
-0.167962979260747 to -0.167864558732556
-0.298211585465734 to -0.298113164937544
-0.628396432582203 to -0.628298012054013
-0.1698835601247 to -0.169785139596509
-0.501579533997838 to -0.501481113469648
-0.0287496556676086 to -0.028651235139418
Changing layer 4's weights from 
-0.128023456517522 to -0.127925035989332
-0.336856256905858 to -0.336757836377668
-0.537333857003514 to -0.537235436475324
-0.250399123612706 to -0.250300703084515
-0.489651929322545 to -0.489553508794355
-0.25818670887119 to -0.258088288342999
-0.262545238915746 to -0.262446818387555
-0.72444276112682 to -0.72434434059863
-0.13074667352802 to -0.13064825299983
-0.563048612061803 to -0.562950191533613
Changing layer 5's weights from 
0.0247563610636543 to 0.0248547815918449
-0.417317162934606 to -0.417218742406416
0.00787119966381331 to 0.00796962019200394
-0.927640981707637 to -0.927542561179447
-0.390372168008153 to -0.390273747479963
-0.788840111080472 to -0.788741690552282
-0.488874803963963 to -0.488776383435773
-0.416428576890294 to -0.416330156362104
-0.0268265714085747 to -0.0267281508803841
-0.0333346833623107 to -0.0332362628341201
Trying to learn from memory 30, 1, 0.01333333
sum 0.0376066194492772 distri 0.0214992661429515
Using diff 0.00670569844400641 and condRate 0.166666666666667
Changed category 1 weights from 
0.297723990923281 to 0.29773889247574
0.531039577967043 to 0.531054479519501
0.106114340072985 to 0.106129241625444
0.110207152611132 to 0.110222054163591
Changing layer 0's weights from 
-0.220324692761944 to -0.220309791209486
-0.592251149451779 to -0.59223624789932
-0.826600415146398 to -0.826585513593939
-0.792041165268467 to -0.792026263716008
-0.0441735959411139 to -0.0441586943886553
-0.726815549171018 to -0.726800647618559
-0.572985825574445 to -0.572970924021986
-0.60044023159747 to -0.600425330045011
-0.837932092583226 to -0.837917191030767
-0.881682147717522 to -0.881667246165063
Changing layer 1's weights from 
-0.876585235333489 to -0.87657033378103
-0.275416550672101 to -0.275401649119643
-0.401119289434004 to -0.401104387881545
-0.411024627721357 to -0.411009726168898
-0.596077410018491 to -0.596062508466032
-0.734315243995237 to -0.734300342442778
-0.463596520459698 to -0.46358161890724
-0.849399549400854 to -0.849384647848395
-0.437736568486738 to -0.437721666934279
-0.800383431351231 to -0.800368529798772
Changing layer 2's weights from 
0.00880480047459593 to 0.00881970202705456
-0.545248684918928 to -0.545233783366469
-0.26065939430003 to -0.260644492747572
-0.0894592976928228 to -0.0894443961403641
-0.266712007558392 to -0.266697106005934
-0.472721633946943 to -0.472706732394484
0.0460910820602898 to 0.0461059836127485
-0.0263309455276011 to -0.0263160439751425
-0.758329240119504 to -0.758314338567045
-0.713371721541929 to -0.71335681998947
Changing layer 3's weights from 
-0.0242454982162001 to -0.0242305966637415
-0.287798581159162 to -0.287783679606703
-0.045188663041639 to -0.0451737614891803
-0.952627894797782 to -0.952612993245323
-0.167864558732556 to -0.167849657180098
-0.298113164937544 to -0.298098263385085
-0.628298012054013 to -0.628283110501554
-0.169785139596509 to -0.169770238044051
-0.501481113469648 to -0.501466211917189
-0.028651235139418 to -0.0286363335869594
Changing layer 4's weights from 
-0.127925035989332 to -0.127910134436873
-0.336757836377668 to -0.336742934825209
-0.537235436475324 to -0.537220534922865
-0.250300703084515 to -0.250285801532057
-0.489553508794355 to -0.489538607241896
-0.258088288342999 to -0.258073386790541
-0.262446818387555 to -0.262431916835097
-0.72434434059863 to -0.724329439046171
-0.13064825299983 to -0.130633351447371
-0.562950191533613 to -0.562935289981154
Changing layer 5's weights from 
0.0248547815918449 to 0.0248696831443036
-0.417218742406416 to -0.417203840853957
0.00796962019200394 to 0.00798452174446256
-0.927542561179447 to -0.927527659626988
-0.390273747479963 to -0.390258845927504
-0.788741690552282 to -0.788726788999823
-0.488776383435773 to -0.488761481883314
-0.416330156362104 to -0.416315254809645
-0.0267281508803841 to -0.0267132493279254
-0.0332362628341201 to -0.0332213612816615
Trying to learn from memory 31, 1, 0.01333333
sum 0.0376024044251433 distri 0.0215013142205263
Using diff 0.00670048909833113 and condRate 0.166666666666667
Changed category 1 weights from 
0.29773889247574 to 0.297753782451874
0.531054479519501 to 0.531069369495636
0.106129241625444 to 0.106144131601579
0.110222054163591 to 0.110236944139726
Changing layer 0's weights from 
-0.220309791209486 to -0.220294901233351
-0.59223624789932 to -0.592221357923185
-0.826585513593939 to -0.826570623617804
-0.792026263716008 to -0.792011373739873
-0.0441586943886553 to -0.0441438044125207
-0.726800647618559 to -0.726785757642424
-0.572970924021986 to -0.572956034045851
-0.600425330045011 to -0.600410440068876
-0.837917191030767 to -0.837902301054632
-0.881667246165063 to -0.881652356188928
Changing layer 1's weights from 
-0.87657033378103 to -0.876555443804895
-0.275401649119643 to -0.275386759143508
-0.401104387881545 to -0.40108949790541
-0.411009726168898 to -0.410994836192763
-0.596062508466032 to -0.596047618489897
-0.734300342442778 to -0.734285452466643
-0.46358161890724 to -0.463566728931105
-0.849384647848395 to -0.84936975787226
-0.437721666934279 to -0.437706776958144
-0.800368529798772 to -0.800353639822637
Changing layer 2's weights from 
0.00881970202705456 to 0.00883459200318918
-0.545233783366469 to -0.545218893390334
-0.260644492747572 to -0.260629602771437
-0.0894443961403641 to -0.0894295061642295
-0.266697106005934 to -0.266682216029799
-0.472706732394484 to -0.472691842418349
0.0461059836127485 to 0.0461208735888831
-0.0263160439751425 to -0.0263011539990078
-0.758314338567045 to -0.75829944859091
-0.71335681998947 to -0.713341930013335
Changing layer 3's weights from 
-0.0242305966637415 to -0.0242157066876068
-0.287783679606703 to -0.287768789630568
-0.0451737614891803 to -0.0451588715130457
-0.952612993245323 to -0.952598103269188
-0.167849657180098 to -0.167834767203963
-0.298098263385085 to -0.29808337340895
-0.628283110501554 to -0.628268220525419
-0.169770238044051 to -0.169755348067916
-0.501466211917189 to -0.501451321941054
-0.0286363335869594 to -0.0286214436108247
Changing layer 4's weights from 
-0.127910134436873 to -0.127895244460738
-0.336742934825209 to -0.336728044849074
-0.537220534922865 to -0.53720564494673
-0.250285801532057 to -0.250270911555922
-0.489538607241896 to -0.489523717265762
-0.258073386790541 to -0.258058496814406
-0.262431916835097 to -0.262417026858962
-0.724329439046171 to -0.724314549070036
-0.130633351447371 to -0.130618461471236
-0.562935289981154 to -0.562920400005019
Changing layer 5's weights from 
0.0248696831443036 to 0.0248845731204382
-0.417203840853957 to -0.417188950877822
0.00798452174446256 to 0.00799941172059718
-0.927527659626988 to -0.927512769650853
-0.390258845927504 to -0.390243955951369
-0.788726788999823 to -0.788711899023688
-0.488761481883314 to -0.48874659190718
-0.416315254809645 to -0.41630036483351
-0.0267132493279254 to -0.0266983593517908
-0.0332213612816615 to -0.0332064713055268
Trying to learn from memory 32, 1, 0.01333333
sum 0.0376024044251433 distri 0.0215013142205263
Using diff 0.00670048909833113 and condRate 0.166666666666667
Changed category 1 weights from 
0.297753782451874 to 0.297768672428009
0.531069369495636 to 0.531084259471771
0.106144131601579 to 0.106159021577713
0.110236944139726 to 0.11025183411586
Changing layer 0's weights from 
-0.220294901233351 to -0.220280011257217
-0.592221357923185 to -0.592206467947051
-0.826570623617804 to -0.82655573364167
-0.792011373739873 to -0.791996483763739
-0.0441438044125207 to -0.0441289144363861
-0.726785757642424 to -0.72677086766629
-0.572956034045851 to -0.572941144069717
-0.600410440068876 to -0.600395550092742
-0.837902301054632 to -0.837887411078498
-0.881652356188928 to -0.881637466212794
Changing layer 1's weights from 
-0.876555443804895 to -0.876540553828761
-0.275386759143508 to -0.275371869167374
-0.40108949790541 to -0.401074607929276
-0.410994836192763 to -0.410979946216629
-0.596047618489897 to -0.596032728513763
-0.734285452466643 to -0.734270562490509
-0.463566728931105 to -0.463551838954971
-0.84936975787226 to -0.849354867896126
-0.437706776958144 to -0.43769188698201
-0.800353639822637 to -0.800338749846503
Changing layer 2's weights from 
0.00883459200318918 to 0.0088494819793238
-0.545218893390334 to -0.5452040034142
-0.260629602771437 to -0.260614712795303
-0.0894295061642295 to -0.0894146161880949
-0.266682216029799 to -0.266667326053665
-0.472691842418349 to -0.472676952442215
0.0461208735888831 to 0.0461357635650177
-0.0263011539990078 to -0.0262862640228732
-0.75829944859091 to -0.758284558614776
-0.713341930013335 to -0.713327040037201
Changing layer 3's weights from 
-0.0242157066876068 to -0.0242008167114722
-0.287768789630568 to -0.287753899654434
-0.0451588715130457 to -0.0451439815369111
-0.952598103269188 to -0.952583213293054
-0.167834767203963 to -0.167819877227829
-0.29808337340895 to -0.298068483432816
-0.628268220525419 to -0.628253330549285
-0.169755348067916 to -0.169740458091782
-0.501451321941054 to -0.50143643196492
-0.0286214436108247 to -0.0286065536346901
Changing layer 4's weights from 
-0.127895244460738 to -0.127880354484604
-0.336728044849074 to -0.33671315487294
-0.53720564494673 to -0.537190754970596
-0.250270911555922 to -0.250256021579787
-0.489523717265762 to -0.489508827289627
-0.258058496814406 to -0.258043606838272
-0.262417026858962 to -0.262402136882828
-0.724314549070036 to -0.724299659093902
-0.130618461471236 to -0.130603571495102
-0.562920400005019 to -0.562905510028885
Changing layer 5's weights from 
0.0248845731204382 to 0.0248994630965728
-0.417188950877822 to -0.417174060901688
0.00799941172059718 to 0.0080143016967318
-0.927512769650853 to -0.927497879674719
-0.390243955951369 to -0.390229065975235
-0.788711899023688 to -0.788697009047554
-0.48874659190718 to -0.488731701931045
-0.41630036483351 to -0.416285474857376
-0.0266983593517908 to -0.0266834693756562
-0.0332064713055268 to -0.0331915813293922
Trying to learn from memory 33, 0, 0.01333333
sum 0.0376024044251433 distri -0.0160855036249893
Using diff 0.0442873069438468 and condRate 0.166666666666667
Changed category 0 weights from 
0.213124251159209 to 0.213222667399245
-0.186910712925416 to -0.18681229668538
-0.469431409446222 to -0.469332993206186
-0.317926043478471 to -0.317827627238435
Changing layer 0's weights from 
-0.220280011257217 to -0.220181595017181
-0.592206467947051 to -0.592108051707015
-0.82655573364167 to -0.826457317401634
-0.791996483763739 to -0.791898067523703
-0.0441289144363861 to -0.04403049819635
-0.72677086766629 to -0.726672451426254
-0.572941144069717 to -0.572842727829681
-0.600395550092742 to -0.600297133852706
-0.837887411078498 to -0.837788994838462
-0.881637466212794 to -0.881539049972758
Changing layer 1's weights from 
-0.876540553828761 to -0.876442137588725
-0.275371869167374 to -0.275273452927337
-0.401074607929276 to -0.40097619168924
-0.410979946216629 to -0.410881529976593
-0.596032728513763 to -0.595934312273727
-0.734270562490509 to -0.734172146250473
-0.463551838954971 to -0.463453422714934
-0.849354867896126 to -0.84925645165609
-0.43769188698201 to -0.437593470741974
-0.800338749846503 to -0.800240333606467
Changing layer 2's weights from 
0.0088494819793238 to 0.00894789821935988
-0.5452040034142 to -0.545105587174164
-0.260614712795303 to -0.260516296555266
-0.0894146161880949 to -0.0893161999480588
-0.266667326053665 to -0.266568909813628
-0.472676952442215 to -0.472578536202179
0.0461357635650177 to 0.0462341798050538
-0.0262862640228732 to -0.0261878477828371
-0.758284558614776 to -0.75818614237474
-0.713327040037201 to -0.713228623797165
Changing layer 3's weights from 
-0.0242008167114722 to -0.0241024004714361
-0.287753899654434 to -0.287655483414398
-0.0451439815369111 to -0.045045565296875
-0.952583213293054 to -0.952484797053018
-0.167819877227829 to -0.167721460987793
-0.298068483432816 to -0.29797006719278
-0.628253330549285 to -0.628154914309249
-0.169740458091782 to -0.169642041851745
-0.50143643196492 to -0.501338015724884
-0.0286065536346901 to -0.028508137394654
Changing layer 4's weights from 
-0.127880354484604 to -0.127781938244568
-0.33671315487294 to -0.336614738632904
-0.537190754970596 to -0.53709233873056
-0.250256021579787 to -0.250157605339751
-0.489508827289627 to -0.489410411049591
-0.258043606838272 to -0.257945190598235
-0.262402136882828 to -0.262303720642791
-0.724299659093902 to -0.724201242853866
-0.130603571495102 to -0.130505155255066
-0.562905510028885 to -0.562807093788849
Changing layer 5's weights from 
0.0248994630965728 to 0.0249978793366089
-0.417174060901688 to -0.417075644661652
0.0080143016967318 to 0.00811271793676789
-0.927497879674719 to -0.927399463434683
-0.390229065975235 to -0.390130649735199
-0.788697009047554 to -0.788598592807518
-0.488731701931045 to -0.488633285691009
-0.416285474857376 to -0.41618705861734
-0.0266834693756562 to -0.0265850531356201
-0.0331915813293922 to -0.0330931650893561
Trying to learn from memory 34, 1, 0.01333333
sum 0.0376515453608555 distri 0.0215509284749472
Using diff 0.0066877305456944 and condRate 0.166666666666667
Changed category 1 weights from 
0.297768672428009 to 0.297783534051804
0.531084259471771 to 0.531099121095565
0.106159021577713 to 0.106173883201508
0.11025183411586 to 0.110266695739655
Changing layer 0's weights from 
-0.220181595017181 to -0.220166733393386
-0.592108051707015 to -0.59209319008322
-0.826457317401634 to -0.826442455777839
-0.791898067523703 to -0.791883205899908
-0.04403049819635 to -0.0440156365725553
-0.726672451426254 to -0.726657589802459
-0.572842727829681 to -0.572827866205886
-0.600297133852706 to -0.600282272228911
-0.837788994838462 to -0.837774133214667
-0.881539049972758 to -0.881524188348963
Changing layer 1's weights from 
-0.876442137588725 to -0.87642727596493
-0.275273452927337 to -0.275258591303543
-0.40097619168924 to -0.400961330065445
-0.410881529976593 to -0.410866668352798
-0.595934312273727 to -0.595919450649932
-0.734172146250473 to -0.734157284626678
-0.463453422714934 to -0.46343856109114
-0.84925645165609 to -0.849241590032295
-0.437593470741974 to -0.437578609118179
-0.800240333606467 to -0.800225471982672
Changing layer 2's weights from 
0.00894789821935988 to 0.00896275984315462
-0.545105587174164 to -0.545090725550369
-0.260516296555266 to -0.260501434931472
-0.0893161999480588 to -0.0893013383242641
-0.266568909813628 to -0.266554048189834
-0.472578536202179 to -0.472563674578384
0.0462341798050538 to 0.0462490414288485
-0.0261878477828371 to -0.0261729861590424
-0.75818614237474 to -0.758171280750945
-0.713228623797165 to -0.71321376217337
Changing layer 3's weights from 
-0.0241024004714361 to -0.0240875388476414
-0.287655483414398 to -0.287640621790603
-0.045045565296875 to -0.0450307036730803
-0.952484797053018 to -0.952469935429223
-0.167721460987793 to -0.167706599363998
-0.29797006719278 to -0.297955205568985
-0.628154914309249 to -0.628140052685454
-0.169642041851745 to -0.169627180227951
-0.501338015724884 to -0.501323154101089
-0.028508137394654 to -0.0284932757708593
Changing layer 4's weights from 
-0.127781938244568 to -0.127767076620773
-0.336614738632904 to -0.336599877009109
-0.53709233873056 to -0.537077477106765
-0.250157605339751 to -0.250142743715957
-0.489410411049591 to -0.489395549425796
-0.257945190598235 to -0.257930328974441
-0.262303720642791 to -0.262288859018997
-0.724201242853866 to -0.724186381230071
-0.130505155255066 to -0.130490293631271
-0.562807093788849 to -0.562792232165054
Changing layer 5's weights from 
0.0249978793366089 to 0.0250127409604036
-0.417075644661652 to -0.417060783037857
0.00811271793676789 to 0.00812757956056263
-0.927399463434683 to -0.927384601810888
-0.390130649735199 to -0.390115788111404
-0.788598592807518 to -0.788583731183723
-0.488633285691009 to -0.488618424067214
-0.41618705861734 to -0.416172196993545
-0.0265850531356201 to -0.0265701915118254
-0.0330931650893561 to -0.0330783034655614
10/5/2016 1:41:31 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 35, 0, 0.01333333
sum 0.0377787860907786 distri -0.0162230295019815
Using diff 0.0445571190700655 and condRate 0.166666666666667
Changed category 0 weights from 
0.213222667399245 to 0.213321683221798
-0.18681229668538 to -0.186713280862827
-0.469332993206186 to -0.469233977383633
-0.317827627238435 to -0.317728611415882
Changing layer 0's weights from 
-0.220166733393386 to -0.220067717570832
-0.59209319008322 to -0.591994174260667
-0.826442455777839 to -0.826343439955286
-0.791883205899908 to -0.791784190077355
-0.0440156365725553 to -0.0439166207500019
-0.726657589802459 to -0.726558573979906
-0.572827866205886 to -0.572728850383333
-0.600282272228911 to -0.600183256406358
-0.837774133214667 to -0.837675117392114
-0.881524188348963 to -0.88142517252641
Changing layer 1's weights from 
-0.87642727596493 to -0.876328260142377
-0.275258591303543 to -0.275159575480989
-0.400961330065445 to -0.400862314242892
-0.410866668352798 to -0.410767652530245
-0.595919450649932 to -0.595820434827379
-0.734157284626678 to -0.734058268804125
-0.46343856109114 to -0.463339545268586
-0.849241590032295 to -0.849142574209742
-0.437578609118179 to -0.437479593295626
-0.800225471982672 to -0.800126456160119
Changing layer 2's weights from 
0.00896275984315462 to 0.00906177566570793
-0.545090725550369 to -0.544991709727816
-0.260501434931472 to -0.260402419108918
-0.0893013383242641 to -0.0892023225017108
-0.266554048189834 to -0.26645503236728
-0.472563674578384 to -0.472464658755831
0.0462490414288485 to 0.0463480572514018
-0.0261729861590424 to -0.0260739703364891
-0.758171280750945 to -0.758072264928392
-0.71321376217337 to -0.713114746350817
Changing layer 3's weights from 
-0.0240875388476414 to -0.0239885230250881
-0.287640621790603 to -0.28754160596805
-0.0450307036730803 to -0.044931687850527
-0.952469935429223 to -0.95237091960667
-0.167706599363998 to -0.167607583541444
-0.297955205568985 to -0.297856189746432
-0.628140052685454 to -0.628041036862901
-0.169627180227951 to -0.169528164405397
-0.501323154101089 to -0.501224138278536
-0.0284932757708593 to -0.028394259948306
Changing layer 4's weights from 
-0.127767076620773 to -0.12766806079822
-0.336599877009109 to -0.336500861186556
-0.537077477106765 to -0.536978461284212
-0.250142743715957 to -0.250043727893403
-0.489395549425796 to -0.489296533603243
-0.257930328974441 to -0.257831313151887
-0.262288859018997 to -0.262189843196443
-0.724186381230071 to -0.724087365407518
-0.130490293631271 to -0.130391277808718
-0.562792232165054 to -0.562693216342501
Changing layer 5's weights from 
0.0250127409604036 to 0.0251117567829569
-0.417060783037857 to -0.416961767215304
0.00812757956056263 to 0.00822659538311593
-0.927384601810888 to -0.927285585988335
-0.390115788111404 to -0.390016772288851
-0.788583731183723 to -0.78848471536117
-0.488618424067214 to -0.488519408244661
-0.416172196993545 to -0.416073181170992
-0.0265701915118254 to -0.0264711756892721
-0.0330783034655614 to -0.0329792876430081
Trying to learn from memory 35, 1, 0.01333333
sum 0.0377787860907786 distri 0.021623133520231
Using diff 0.00671095604785295 and condRate 0.166666666666667
Changed category 1 weights from 
0.297783534051804 to 0.297798447287827
0.531099121095565 to 0.531114034331588
0.106173883201508 to 0.106188796437531
0.110266695739655 to 0.110281608975678
Changing layer 0's weights from 
-0.220067717570832 to -0.220052804334809
-0.591994174260667 to -0.591979261024644
-0.826343439955286 to -0.826328526719263
-0.791784190077355 to -0.791769276841332
-0.0439166207500019 to -0.0439017075139789
-0.726558573979906 to -0.726543660743883
-0.572728850383333 to -0.57271393714731
-0.600183256406358 to -0.600168343170335
-0.837675117392114 to -0.837660204156091
-0.88142517252641 to -0.881410259290387
Changing layer 1's weights from 
-0.876328260142377 to -0.876313346906354
-0.275159575480989 to -0.275144662244966
-0.400862314242892 to -0.400847401006869
-0.410767652530245 to -0.410752739294222
-0.595820434827379 to -0.595805521591356
-0.734058268804125 to -0.734043355568102
-0.463339545268586 to -0.463324632032563
-0.849142574209742 to -0.849127660973719
-0.437479593295626 to -0.437464680059603
-0.800126456160119 to -0.800111542924096
Changing layer 2's weights from 
0.00906177566570793 to 0.00907668890173094
-0.544991709727816 to -0.544976796491793
-0.260402419108918 to -0.260387505872895
-0.0892023225017108 to -0.0891874092656877
-0.26645503236728 to -0.266440119131257
-0.472464658755831 to -0.472449745519808
0.0463480572514018 to 0.0463629704874249
-0.0260739703364891 to -0.0260590571004661
-0.758072264928392 to -0.758057351692369
-0.713114746350817 to -0.713099833114794
Changing layer 3's weights from 
-0.0239885230250881 to -0.0239736097890651
-0.28754160596805 to -0.287526692732027
-0.044931687850527 to -0.0449167746145039
-0.95237091960667 to -0.952356006370647
-0.167607583541444 to -0.167592670305421
-0.297856189746432 to -0.297841276510409
-0.628041036862901 to -0.628026123626878
-0.169528164405397 to -0.169513251169374
-0.501224138278536 to -0.501209225042513
-0.028394259948306 to -0.028379346712283
Changing layer 4's weights from 
-0.12766806079822 to -0.127653147562197
-0.336500861186556 to -0.336485947950533
-0.536978461284212 to -0.536963548048189
-0.250043727893403 to -0.25002881465738
-0.489296533603243 to -0.48928162036722
-0.257831313151887 to -0.257816399915864
-0.262189843196443 to -0.26217492996042
-0.724087365407518 to -0.724072452171495
-0.130391277808718 to -0.130376364572695
-0.562693216342501 to -0.562678303106478
Changing layer 5's weights from 
0.0251117567829569 to 0.0251266700189799
-0.416961767215304 to -0.416946853979281
0.00822659538311593 to 0.00824150861913894
-0.927285585988335 to -0.927270672752312
-0.390016772288851 to -0.390001859052828
-0.78848471536117 to -0.788469802125147
-0.488519408244661 to -0.488504495008638
-0.416073181170992 to -0.416058267934969
-0.0264711756892721 to -0.0264562624532491
-0.0329792876430081 to -0.0329643744069851
Trying to learn from memory 36, 0, 0.01333333
sum 0.0377957023865314 distri -0.0164400057623542
Using diff 0.0447867825522528 and condRate 0.166666666666667
Changed category 0 weights from 
0.213321683221798 to 0.213421209407658
-0.186713280862827 to -0.186613754676967
-0.469233977383633 to -0.469134451197773
-0.317728611415882 to -0.317629085230022
Changing layer 0's weights from 
-0.220052804334809 to -0.21995327814895
-0.591979261024644 to -0.591879734838784
-0.826328526719263 to -0.826229000533403
-0.791769276841332 to -0.791669750655472
-0.0439017075139789 to -0.0438021813281195
-0.726543660743883 to -0.726444134558023
-0.57271393714731 to -0.57261441096145
-0.600168343170335 to -0.600068816984475
-0.837660204156091 to -0.837560677970231
-0.881410259290387 to -0.881310733104527
Changing layer 1's weights from 
-0.876313346906354 to -0.876213820720494
-0.275144662244966 to -0.275045136059107
-0.400847401006869 to -0.400747874821009
-0.410752739294222 to -0.410653213108362
-0.595805521591356 to -0.595705995405496
-0.734043355568102 to -0.733943829382242
-0.463324632032563 to -0.463225105846704
-0.849127660973719 to -0.849028134787859
-0.437464680059603 to -0.437365153873743
-0.800111542924096 to -0.800012016738236
Changing layer 2's weights from 
0.00907668890173094 to 0.00917621508759035
-0.544976796491793 to -0.544877270305933
-0.260387505872895 to -0.260287979687036
-0.0891874092656877 to -0.0890878830798283
-0.266440119131257 to -0.266340592945398
-0.472449745519808 to -0.472350219333948
0.0463629704874249 to 0.0464624966732843
-0.0260590571004661 to -0.0259595309146066
-0.758057351692369 to -0.757957825506509
-0.713099833114794 to -0.713000306928934
Changing layer 3's weights from 
-0.0239736097890651 to -0.0238740836032057
-0.287526692732027 to -0.287427166546167
-0.0449167746145039 to -0.0448172484286445
-0.952356006370647 to -0.952256480184787
-0.167592670305421 to -0.167493144119562
-0.297841276510409 to -0.297741750324549
-0.628026123626878 to -0.627926597441018
-0.169513251169374 to -0.169413724983515
-0.501209225042513 to -0.501109698856653
-0.028379346712283 to -0.0282798205264236
Changing layer 4's weights from 
-0.127653147562197 to -0.127553621376337
-0.336485947950533 to -0.336386421764673
-0.536963548048189 to -0.536864021862329
-0.25002881465738 to -0.249929288471521
-0.48928162036722 to -0.48918209418136
-0.257816399915864 to -0.257716873730005
-0.26217492996042 to -0.262075403774561
-0.724072452171495 to -0.723972925985635
-0.130376364572695 to -0.130276838386835
-0.562678303106478 to -0.562578776920618
Changing layer 5's weights from 
0.0251266700189799 to 0.0252261962048394
-0.416946853979281 to -0.416847327793421
0.00824150861913894 to 0.00834103480499836
-0.927270672752312 to -0.927171146566452
-0.390001859052828 to -0.389902332866968
-0.788469802125147 to -0.788370275939287
-0.488504495008638 to -0.488404968822778
-0.416058267934969 to -0.415958741749109
-0.0264562624532491 to -0.0263567362673896
-0.0329643744069851 to -0.0328648482211257
10/5/2016 1:41:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:56 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
Modified index 9's learning in memoryPool to -0.2
Modified index 10's learning in memoryPool to -0.2
Modified index 11's learning in memoryPool to -0.2
Modified index 12's learning in memoryPool to -0.2
10/5/2016 1:41:56 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 37, 1, -0.2
sum 0.0377090177634025 distri 0.0215685649548737
Using diff 0.00671319836767816 and condRate 0.166666666666667
Changed category 1 weights from 
0.297798447287827 to 0.29757467400557
0.531114034331588 to 0.530890261049331
0.106188796437531 to 0.105965023155274
0.110281608975678 to 0.110057835693421
Changing layer 0's weights from 
-0.21995327814895 to -0.220177051431207
-0.591879734838784 to -0.592103508121041
-0.826229000533403 to -0.82645277381566
-0.791669750655472 to -0.791893523937729
-0.0438021813281195 to -0.0440259546103766
-0.726444134558023 to -0.72666790784028
-0.57261441096145 to -0.572838184243707
-0.600068816984475 to -0.600292590266732
-0.837560677970231 to -0.837784451252488
-0.881310733104527 to -0.881534506386784
Changing layer 1's weights from 
-0.876213820720494 to -0.876437594002751
-0.275045136059107 to -0.275268909341364
-0.400747874821009 to -0.400971648103266
-0.410653213108362 to -0.410876986390619
-0.595705995405496 to -0.595929768687753
-0.733943829382242 to -0.734167602664499
-0.463225105846704 to -0.463448879128961
-0.849028134787859 to -0.849251908070116
-0.437365153873743 to -0.437588927156
-0.800012016738236 to -0.800235790020493
Changing layer 2's weights from 
0.00917621508759035 to 0.00895244180533327
-0.544877270305933 to -0.54510104358819
-0.260287979687036 to -0.260511752969293
-0.0890878830798283 to -0.0893116563620854
-0.266340592945398 to -0.266564366227655
-0.472350219333948 to -0.472573992616205
0.0464624966732843 to 0.0462387233910272
-0.0259595309146066 to -0.0261833041968637
-0.757957825506509 to -0.758181598788766
-0.713000306928934 to -0.713224080211191
Changing layer 3's weights from 
-0.0238740836032057 to -0.0240978568854627
-0.287427166546167 to -0.287650939828424
-0.0448172484286445 to -0.0450410217109016
-0.952256480184787 to -0.952480253467044
-0.167493144119562 to -0.167716917401819
-0.297741750324549 to -0.297965523606806
-0.627926597441018 to -0.628150370723275
-0.169413724983515 to -0.169637498265772
-0.501109698856653 to -0.50133347213891
-0.0282798205264236 to -0.0285035938086806
Changing layer 4's weights from 
-0.127553621376337 to -0.127777394658594
-0.336386421764673 to -0.33661019504693
-0.536864021862329 to -0.537087795144586
-0.249929288471521 to -0.250153061753778
-0.48918209418136 to -0.489405867463618
-0.257716873730005 to -0.257940647012262
-0.262075403774561 to -0.262299177056818
-0.723972925985635 to -0.724196699267892
-0.130276838386835 to -0.130500611669092
-0.562578776920618 to -0.562802550202875
Changing layer 5's weights from 
0.0252261962048394 to 0.0250024229225823
-0.416847327793421 to -0.417071101075678
0.00834103480499836 to 0.00811726152274127
-0.927171146566452 to -0.927394919848709
-0.389902332866968 to -0.390126106149225
-0.788370275939287 to -0.788594049221544
-0.488404968822778 to -0.488628742105036
-0.415958741749109 to -0.416182515031366
-0.0263567362673896 to -0.0265805095496467
-0.0328648482211257 to -0.0330886215033828
Trying to learn from memory 38, 1, -0.2
sum 0.037881886814315 distri 0.0218737844062606
Using diff 0.00653763070447558 and condRate 0.166666666666667
Changed category 1 weights from 
0.29757467400557 to 0.29735675297884
0.530890261049331 to 0.530672340022601
0.105965023155274 to 0.105747102128544
0.110057835693421 to 0.109839914666691
Changing layer 0's weights from 
-0.220177051431207 to -0.220394972457937
-0.592103508121041 to -0.592321429147771
-0.82645277381566 to -0.82667069484239
-0.791893523937729 to -0.792111444964459
-0.0440259546103766 to -0.0442438756371064
-0.72666790784028 to -0.72688582886701
-0.572838184243707 to -0.573056105270437
-0.600292590266732 to -0.600510511293462
-0.837784451252488 to -0.838002372279218
-0.881534506386784 to -0.881752427413514
Changing layer 1's weights from 
-0.876437594002751 to -0.876655515029481
-0.275268909341364 to -0.275486830368094
-0.400971648103266 to -0.401189569129996
-0.410876986390619 to -0.411094907417349
-0.595929768687753 to -0.596147689714483
-0.734167602664499 to -0.734385523691229
-0.463448879128961 to -0.463666800155691
-0.849251908070116 to -0.849469829096846
-0.437588927156 to -0.43780684818273
-0.800235790020493 to -0.800453711047223
Changing layer 2's weights from 
0.00895244180533327 to 0.00873452077860347
-0.54510104358819 to -0.54531896461492
-0.260511752969293 to -0.260729673996023
-0.0893116563620854 to -0.0895295773888152
-0.266564366227655 to -0.266782287254385
-0.472573992616205 to -0.472791913642935
0.0462387233910272 to 0.0460208023642974
-0.0261833041968637 to -0.0264012252235935
-0.758181598788766 to -0.758399519815496
-0.713224080211191 to -0.713442001237921
Changing layer 3's weights from 
-0.0240978568854627 to -0.0243157779121925
-0.287650939828424 to -0.287868860855154
-0.0450410217109016 to -0.0452589427376314
-0.952480253467044 to -0.952698174493774
-0.167716917401819 to -0.167934838428549
-0.297965523606806 to -0.298183444633536
-0.628150370723275 to -0.628368291750005
-0.169637498265772 to -0.169855419292502
-0.50133347213891 to -0.50155139316564
-0.0285035938086806 to -0.0287215148354104
Changing layer 4's weights from 
-0.127777394658594 to -0.127995315685324
-0.33661019504693 to -0.33682811607366
-0.537087795144586 to -0.537305716171316
-0.250153061753778 to -0.250370982780508
-0.489405867463618 to -0.489623788490347
-0.257940647012262 to -0.258158568038992
-0.262299177056818 to -0.262517098083548
-0.724196699267892 to -0.724414620294622
-0.130500611669092 to -0.130718532695822
-0.562802550202875 to -0.563020471229605
Changing layer 5's weights from 
0.0250024229225823 to 0.0247845018958525
-0.417071101075678 to -0.417289022102408
0.00811726152274127 to 0.00789934049601148
-0.927394919848709 to -0.927612840875439
-0.390126106149225 to -0.390344027175955
-0.788594049221544 to -0.788811970248274
-0.488628742105036 to -0.488846663131765
-0.416182515031366 to -0.416400436058096
-0.0265805095496467 to -0.0267984305763765
-0.0330886215033828 to -0.0333065425301126
Trying to learn from memory 39, 0, -0.2
sum 0.0379580748185349 distri -0.0168932935465217
Using diff 0.0453618496604229 and condRate 0.166666666666667
Changed category 0 weights from 
0.213421209407658 to 0.211909147729779
-0.186613754676967 to -0.188125816354846
-0.469134451197773 to -0.470646512875652
-0.317629085230022 to -0.319141146907901
Changing layer 0's weights from 
-0.220394972457937 to -0.221907034135816
-0.592321429147771 to -0.59383349082565
-0.82667069484239 to -0.828182756520269
-0.792111444964459 to -0.793623506642338
-0.0442438756371064 to -0.0457559373149853
-0.72688582886701 to -0.728397890544889
-0.573056105270437 to -0.574568166948316
-0.600510511293462 to -0.602022572971341
-0.838002372279218 to -0.839514433957097
-0.881752427413514 to -0.883264489091393
Changing layer 1's weights from 
-0.876655515029481 to -0.87816757670736
-0.275486830368094 to -0.276998892045973
-0.401189569129996 to -0.402701630807875
-0.411094907417349 to -0.412606969095228
-0.596147689714483 to -0.597659751392362
-0.734385523691229 to -0.735897585369108
-0.463666800155691 to -0.46517886183357
-0.849469829096846 to -0.850981890774725
-0.43780684818273 to -0.439318909860609
-0.800453711047223 to -0.801965772725102
Changing layer 2's weights from 
0.00873452077860347 to 0.00722245910072457
-0.54531896461492 to -0.546831026292799
-0.260729673996023 to -0.262241735673902
-0.0895295773888152 to -0.0910416390666941
-0.266782287254385 to -0.268294348932264
-0.472791913642935 to -0.474303975320814
0.0460208023642974 to 0.0445087406864185
-0.0264012252235935 to -0.0279132869014724
-0.758399519815496 to -0.759911581493375
-0.713442001237921 to -0.7149540629158
Changing layer 3's weights from 
-0.0243157779121925 to -0.0258278395900714
-0.287868860855154 to -0.289380922533033
-0.0452589427376314 to -0.0467710044155103
-0.952698174493774 to -0.954210236171653
-0.167934838428549 to -0.169446900106428
-0.298183444633536 to -0.299695506311415
-0.628368291750005 to -0.629880353427884
-0.169855419292502 to -0.171367480970381
-0.50155139316564 to -0.503063454843519
-0.0287215148354104 to -0.0302335765132893
Changing layer 4's weights from 
-0.127995315685324 to -0.129507377363203
-0.33682811607366 to -0.338340177751539
-0.537305716171316 to -0.538817777849195
-0.250370982780508 to -0.251883044458387
-0.489623788490347 to -0.491135850168226
-0.258158568038992 to -0.259670629716871
-0.262517098083548 to -0.264029159761427
-0.724414620294622 to -0.725926681972501
-0.130718532695822 to -0.132230594373701
-0.563020471229605 to -0.564532532907484
Changing layer 5's weights from 
0.0247845018958525 to 0.0232724402179736
-0.417289022102408 to -0.418801083780287
0.00789934049601148 to 0.00638727881813258
-0.927612840875439 to -0.929124902553318
-0.390344027175955 to -0.391856088853834
-0.788811970248274 to -0.790324031926153
-0.488846663131765 to -0.490358724809644
-0.416400436058096 to -0.417912497735975
-0.0267984305763765 to -0.0283104922542554
-0.0333065425301126 to -0.0348186042079915
Trying to learn from memory 39, 1, -0.2
sum 0.0379580748185349 distri 0.0219820457324139
Using diff 0.00648651038148726 and condRate 0.166666666666667
Changed category 1 weights from 
0.29735675297884 to 0.297140535962902
0.530672340022601 to 0.530456123006663
0.105747102128544 to 0.105530885112606
0.109839914666691 to 0.109623697650753
Changing layer 0's weights from 
-0.221907034135816 to -0.222123251151754
-0.59383349082565 to -0.594049707841588
-0.828182756520269 to -0.828398973536207
-0.793623506642338 to -0.793839723658276
-0.0457559373149853 to -0.0459721543309234
-0.728397890544889 to -0.728614107560827
-0.574568166948316 to -0.574784383964254
-0.602022572971341 to -0.602238789987279
-0.839514433957097 to -0.839730650973035
-0.883264489091393 to -0.883480706107331
Changing layer 1's weights from 
-0.87816757670736 to -0.878383793723298
-0.276998892045973 to -0.277215109061911
-0.402701630807875 to -0.402917847823813
-0.412606969095228 to -0.412823186111166
-0.597659751392362 to -0.5978759684083
-0.735897585369108 to -0.736113802385046
-0.46517886183357 to -0.465395078849508
-0.850981890774725 to -0.851198107790663
-0.439318909860609 to -0.439535126876547
-0.801965772725102 to -0.80218198974104
Changing layer 2's weights from 
0.00722245910072457 to 0.00700624208478644
-0.546831026292799 to -0.547047243308737
-0.262241735673902 to -0.26245795268984
-0.0910416390666941 to -0.0912578560826322
-0.268294348932264 to -0.268510565948202
-0.474303975320814 to -0.474520192336752
0.0445087406864185 to 0.0442925236704804
-0.0279132869014724 to -0.0281295039174106
-0.759911581493375 to -0.760127798509313
-0.7149540629158 to -0.715170279931738
Changing layer 3's weights from 
-0.0258278395900714 to -0.0260440566060096
-0.289380922533033 to -0.289597139548971
-0.0467710044155103 to -0.0469872214314484
-0.954210236171653 to -0.954426453187591
-0.169446900106428 to -0.169663117122366
-0.299695506311415 to -0.299911723327353
-0.629880353427884 to -0.630096570443822
-0.171367480970381 to -0.171583697986319
-0.503063454843519 to -0.503279671859457
-0.0302335765132893 to -0.0304497935292275
Changing layer 4's weights from 
-0.129507377363203 to -0.129723594379141
-0.338340177751539 to -0.338556394767477
-0.538817777849195 to -0.539033994865133
-0.251883044458387 to -0.252099261474325
-0.491135850168226 to -0.491352067184164
-0.259670629716871 to -0.259886846732809
-0.264029159761427 to -0.264245376777365
-0.725926681972501 to -0.726142898988439
-0.132230594373701 to -0.132446811389639
-0.564532532907484 to -0.564748749923422
Changing layer 5's weights from 
0.0232724402179736 to 0.0230562232020355
-0.418801083780287 to -0.419017300796225
0.00638727881813258 to 0.00617106180219445
-0.929124902553318 to -0.929341119569256
-0.391856088853834 to -0.392072305869772
-0.790324031926153 to -0.790540248942091
-0.490358724809644 to -0.490574941825582
-0.417912497735975 to -0.418128714751913
-0.0283104922542554 to -0.0285267092701936
-0.0348186042079915 to -0.0350348212239296
Trying to learn from memory 39, 0, -0.2
sum 0.0379580748185349 distri -0.0168932935465217
Using diff 0.0453618496604229 and condRate 0.166666666666667
Changed category 0 weights from 
0.211909147729779 to 0.2103970860519
-0.188125816354846 to -0.189637878032725
-0.470646512875652 to -0.472158574553531
-0.319141146907901 to -0.32065320858578
Changing layer 0's weights from 
-0.222123251151754 to -0.223635312829633
-0.594049707841588 to -0.595561769519467
-0.828398973536207 to -0.829911035214086
-0.793839723658276 to -0.795351785336155
-0.0459721543309234 to -0.0474842160088023
-0.728614107560827 to -0.730126169238706
-0.574784383964254 to -0.576296445642133
-0.602238789987279 to -0.603750851665158
-0.839730650973035 to -0.841242712650914
-0.883480706107331 to -0.88499276778521
Changing layer 1's weights from 
-0.878383793723298 to -0.879895855401177
-0.277215109061911 to -0.27872717073979
-0.402917847823813 to -0.404429909501692
-0.412823186111166 to -0.414335247789045
-0.5978759684083 to -0.599388030086179
-0.736113802385046 to -0.737625864062925
-0.465395078849508 to -0.466907140527387
-0.851198107790663 to -0.852710169468542
-0.439535126876547 to -0.441047188554426
-0.80218198974104 to -0.803694051418919
Changing layer 2's weights from 
0.00700624208478644 to 0.00549418040690754
-0.547047243308737 to -0.548559304986616
-0.26245795268984 to -0.263970014367719
-0.0912578560826322 to -0.0927699177605111
-0.268510565948202 to -0.270022627626081
-0.474520192336752 to -0.476032254014631
0.0442925236704804 to 0.0427804619926015
-0.0281295039174106 to -0.0296415655952895
-0.760127798509313 to -0.761639860187192
-0.715170279931738 to -0.716682341609617
Changing layer 3's weights from 
-0.0260440566060096 to -0.0275561182838885
-0.289597139548971 to -0.29110920122685
-0.0469872214314484 to -0.0484992831093273
-0.954426453187591 to -0.95593851486547
-0.169663117122366 to -0.171175178800245
-0.299911723327353 to -0.301423785005232
-0.630096570443822 to -0.631608632121701
-0.171583697986319 to -0.173095759664198
-0.503279671859457 to -0.504791733537336
-0.0304497935292275 to -0.0319618552071064
Changing layer 4's weights from 
-0.129723594379141 to -0.13123565605702
-0.338556394767477 to -0.340068456445356
-0.539033994865133 to -0.540546056543012
-0.252099261474325 to -0.253611323152204
-0.491352067184164 to -0.492864128862043
-0.259886846732809 to -0.261398908410688
-0.264245376777365 to -0.265757438455244
-0.726142898988439 to -0.727654960666318
-0.132446811389639 to -0.133958873067518
-0.564748749923422 to -0.566260811601301
Changing layer 5's weights from 
0.0230562232020355 to 0.0215441615241565
-0.419017300796225 to -0.420529362474104
0.00617106180219445 to 0.00465900012431555
-0.929341119569256 to -0.930853181247135
-0.392072305869772 to -0.393584367547651
-0.790540248942091 to -0.79205231061997
-0.490574941825582 to -0.492087003503461
-0.418128714751913 to -0.419640776429792
-0.0285267092701936 to -0.0300387709480725
-0.0350348212239296 to -0.0365468829018085
Trying to learn from memory 39, 0, -0.2
sum 0.0379580748185349 distri -0.0168932935465217
Using diff 0.0453618496604229 and condRate 0.166666666666667
Changed category 0 weights from 
0.2103970860519 to 0.208885024374021
-0.189637878032725 to -0.191149939710604
-0.472158574553531 to -0.47367063623141
-0.32065320858578 to -0.322165270263659
Changing layer 0's weights from 
-0.223635312829633 to -0.225147374507512
-0.595561769519467 to -0.597073831197346
-0.829911035214086 to -0.831423096891965
-0.795351785336155 to -0.796863847014034
-0.0474842160088023 to -0.0489962776866812
-0.730126169238706 to -0.731638230916585
-0.576296445642133 to -0.577808507320012
-0.603750851665158 to -0.605262913343037
-0.841242712650914 to -0.842754774328793
-0.88499276778521 to -0.886504829463089
Changing layer 1's weights from 
-0.879895855401177 to -0.881407917079056
-0.27872717073979 to -0.280239232417669
-0.404429909501692 to -0.405941971179571
-0.414335247789045 to -0.415847309466924
-0.599388030086179 to -0.600900091764058
-0.737625864062925 to -0.739137925740804
-0.466907140527387 to -0.468419202205266
-0.852710169468542 to -0.854222231146421
-0.441047188554426 to -0.442559250232305
-0.803694051418919 to -0.805206113096798
Changing layer 2's weights from 
0.00549418040690754 to 0.00398211872902864
-0.548559304986616 to -0.550071366664495
-0.263970014367719 to -0.265482076045598
-0.0927699177605111 to -0.09428197943839
-0.270022627626081 to -0.27153468930396
-0.476032254014631 to -0.47754431569251
0.0427804619926015 to 0.0412684003147226
-0.0296415655952895 to -0.0311536272731684
-0.761639860187192 to -0.763151921865071
-0.716682341609617 to -0.718194403287496
Changing layer 3's weights from 
-0.0275561182838885 to -0.0290681799617674
-0.29110920122685 to -0.292621262904729
-0.0484992831093273 to -0.0500113447872062
-0.95593851486547 to -0.957450576543349
-0.171175178800245 to -0.172687240478124
-0.301423785005232 to -0.302935846683111
-0.631608632121701 to -0.63312069379958
-0.173095759664198 to -0.174607821342077
-0.504791733537336 to -0.506303795215215
-0.0319618552071064 to -0.0334739168849853
Changing layer 4's weights from 
-0.13123565605702 to -0.132747717734899
-0.340068456445356 to -0.341580518123235
-0.540546056543012 to -0.542058118220891
-0.253611323152204 to -0.255123384830083
-0.492864128862043 to -0.494376190539922
-0.261398908410688 to -0.262910970088567
-0.265757438455244 to -0.267269500133123
-0.727654960666318 to -0.729167022344197
-0.133958873067518 to -0.135470934745397
-0.566260811601301 to -0.56777287327918
Changing layer 5's weights from 
0.0215441615241565 to 0.0200320998462776
-0.420529362474104 to -0.422041424151983
0.00465900012431555 to 0.00314693844643664
-0.930853181247135 to -0.932365242925014
-0.393584367547651 to -0.39509642922553
-0.79205231061997 to -0.793564372297849
-0.492087003503461 to -0.49359906518134
-0.419640776429792 to -0.421152838107671
-0.0300387709480725 to -0.0315508326259514
-0.0365468829018085 to -0.0380589445796874
Trying to learn from memory 39, 1, -0.2
sum 0.0379580748185349 distri 0.0219820457324139
Using diff 0.00648651038148726 and condRate 0.166666666666667
Changed category 1 weights from 
0.297140535962902 to 0.296924318946964
0.530456123006663 to 0.530239905990725
0.105530885112606 to 0.105314668096668
0.109623697650753 to 0.109407480634815
Changing layer 0's weights from 
-0.225147374507512 to -0.22536359152345
-0.597073831197346 to -0.597290048213284
-0.831423096891965 to -0.831639313907903
-0.796863847014034 to -0.797080064029972
-0.0489962776866812 to -0.0492124947026194
-0.731638230916585 to -0.731854447932523
-0.577808507320012 to -0.57802472433595
-0.605262913343037 to -0.605479130358975
-0.842754774328793 to -0.842970991344731
-0.886504829463089 to -0.886721046479027
Changing layer 1's weights from 
-0.881407917079056 to -0.881624134094994
-0.280239232417669 to -0.280455449433607
-0.405941971179571 to -0.406158188195509
-0.415847309466924 to -0.416063526482862
-0.600900091764058 to -0.601116308779996
-0.739137925740804 to -0.739354142756742
-0.468419202205266 to -0.468635419221204
-0.854222231146421 to -0.854438448162359
-0.442559250232305 to -0.442775467248243
-0.805206113096798 to -0.805422330112736
Changing layer 2's weights from 
0.00398211872902864 to 0.00376590171309051
-0.550071366664495 to -0.550287583680433
-0.265482076045598 to -0.265698293061536
-0.09428197943839 to -0.0944981964543282
-0.27153468930396 to -0.271750906319898
-0.47754431569251 to -0.477760532708448
0.0412684003147226 to 0.0410521832987844
-0.0311536272731684 to -0.0313698442891065
-0.763151921865071 to -0.763368138881009
-0.718194403287496 to -0.718410620303434
Changing layer 3's weights from 
-0.0290681799617674 to -0.0292843969777055
-0.292621262904729 to -0.292837479920667
-0.0500113447872062 to -0.0502275618031444
-0.957450576543349 to -0.957666793559287
-0.172687240478124 to -0.172903457494062
-0.302935846683111 to -0.303152063699049
-0.63312069379958 to -0.633336910815518
-0.174607821342077 to -0.174824038358015
-0.506303795215215 to -0.506520012231153
-0.0334739168849853 to -0.0336901339009234
Changing layer 4's weights from 
-0.132747717734899 to -0.132963934750837
-0.341580518123235 to -0.341796735139173
-0.542058118220891 to -0.542274335236829
-0.255123384830083 to -0.255339601846021
-0.494376190539922 to -0.49459240755586
-0.262910970088567 to -0.263127187104505
-0.267269500133123 to -0.267485717149061
-0.729167022344197 to -0.729383239360135
-0.135470934745397 to -0.135687151761335
-0.56777287327918 to -0.567989090295118
Changing layer 5's weights from 
0.0200320998462776 to 0.0198158828303395
-0.422041424151983 to -0.422257641167921
0.00314693844643664 to 0.00293072143049852
-0.932365242925014 to -0.932581459940952
-0.39509642922553 to -0.395312646241468
-0.793564372297849 to -0.793780589313787
-0.49359906518134 to -0.493815282197278
-0.421152838107671 to -0.421369055123609
-0.0315508326259514 to -0.0317670496418895
-0.0380589445796874 to -0.0382751615956255
Trying to learn from memory 40, 1, -0.2
sum 0.0387377713485121 distri 0.0224753740158419
Using diff 0.00657795449554223 and condRate 0.166666666666667
Changed category 1 weights from 
0.296924318946964 to 0.296705053793845
0.530239905990725 to 0.530020640837606
0.105314668096668 to 0.105095402943549
0.109407480634815 to 0.109188215481696
Changing layer 0's weights from 
-0.22536359152345 to -0.225582856676569
-0.597290048213284 to -0.597509313366403
-0.831639313907903 to -0.831858579061022
-0.797080064029972 to -0.797299329183091
-0.0492124947026194 to -0.0494317598557381
-0.731854447932523 to -0.732073713085642
-0.57802472433595 to -0.578243989489069
-0.605479130358975 to -0.605698395512094
-0.842970991344731 to -0.84319025649785
-0.886721046479027 to -0.886940311632146
Changing layer 1's weights from 
-0.881624134094994 to -0.881843399248113
-0.280455449433607 to -0.280674714586726
-0.406158188195509 to -0.406377453348628
-0.416063526482862 to -0.416282791635981
-0.601116308779996 to -0.601335573933115
-0.739354142756742 to -0.739573407909861
-0.468635419221204 to -0.468854684374323
-0.854438448162359 to -0.854657713315478
-0.442775467248243 to -0.442994732401362
-0.805422330112736 to -0.805641595265855
Changing layer 2's weights from 
0.00376590171309051 to 0.0035466365599718
-0.550287583680433 to -0.550506848833552
-0.265698293061536 to -0.265917558214655
-0.0944981964543282 to -0.0947174616074469
-0.271750906319898 to -0.271970171473017
-0.477760532708448 to -0.477979797861567
0.0410521832987844 to 0.0408329181456657
-0.0313698442891065 to -0.0315891094422252
-0.763368138881009 to -0.763587404034128
-0.718410620303434 to -0.718629885456553
Changing layer 3's weights from 
-0.0292843969777055 to -0.0295036621308242
-0.292837479920667 to -0.293056745073786
-0.0502275618031444 to -0.0504468269562631
-0.957666793559287 to -0.957886058712406
-0.172903457494062 to -0.173122722647181
-0.303152063699049 to -0.303371328852168
-0.633336910815518 to -0.633556175968637
-0.174824038358015 to -0.175043303511134
-0.506520012231153 to -0.506739277384272
-0.0336901339009234 to -0.0339093990540421
Changing layer 4's weights from 
-0.132963934750837 to -0.133183199903956
-0.341796735139173 to -0.342016000292292
-0.542274335236829 to -0.542493600389948
-0.255339601846021 to -0.25555886699914
-0.49459240755586 to -0.494811672708979
-0.263127187104505 to -0.263346452257624
-0.267485717149061 to -0.26770498230218
-0.729383239360135 to -0.729602504513254
-0.135687151761335 to -0.135906416914454
-0.567989090295118 to -0.568208355448237
Changing layer 5's weights from 
0.0198158828303395 to 0.0195966176772208
-0.422257641167921 to -0.42247690632104
0.00293072143049852 to 0.0027114562773798
-0.932581459940952 to -0.932800725094071
-0.395312646241468 to -0.395531911394587
-0.793780589313787 to -0.793999854466906
-0.493815282197278 to -0.494034547350397
-0.421369055123609 to -0.421588320276728
-0.0317670496418895 to -0.0319863147950082
-0.0382751615956255 to -0.0384944267487442
Trying to learn from memory 41, 1, -0.2
sum 0.0403404400416513 distri 0.0230809867144471
Using diff 0.00717434331679144 and condRate 0.166666666666667
Changed category 1 weights from 
0.296705053793845 to 0.296465909013055
0.530020640837606 to 0.529781496056816
0.105095402943549 to 0.104856258162759
0.109188215481696 to 0.108949070700906
Changing layer 0's weights from 
-0.225582856676569 to -0.225822001457359
-0.597509313366403 to -0.597748458147193
-0.831858579061022 to -0.832097723841812
-0.797299329183091 to -0.797538473963881
-0.0494317598557381 to -0.049670904636528
-0.732073713085642 to -0.732312857866432
-0.578243989489069 to -0.578483134269859
-0.605698395512094 to -0.605937540292884
-0.84319025649785 to -0.84342940127864
-0.886940311632146 to -0.887179456412936
Changing layer 1's weights from 
-0.881843399248113 to -0.882082544028903
-0.280674714586726 to -0.280913859367516
-0.406377453348628 to -0.406616598129418
-0.416282791635981 to -0.416521936416771
-0.601335573933115 to -0.601574718713905
-0.739573407909861 to -0.739812552690651
-0.468854684374323 to -0.469093829155113
-0.854657713315478 to -0.854896858096268
-0.442994732401362 to -0.443233877182152
-0.805641595265855 to -0.805880740046645
Changing layer 2's weights from 
0.0035466365599718 to 0.00330749177918188
-0.550506848833552 to -0.550745993614342
-0.265917558214655 to -0.266156702995445
-0.0947174616074469 to -0.0949566063882368
-0.271970171473017 to -0.272209316253807
-0.477979797861567 to -0.478218942642357
0.0408329181456657 to 0.0405937733648758
-0.0315891094422252 to -0.0318282542230151
-0.763587404034128 to -0.763826548814918
-0.718629885456553 to -0.718869030237343
Changing layer 3's weights from 
-0.0295036621308242 to -0.0297428069116141
-0.293056745073786 to -0.293295889854576
-0.0504468269562631 to -0.050685971737053
-0.957886058712406 to -0.958125203493196
-0.173122722647181 to -0.173361867427971
-0.303371328852168 to -0.303610473632958
-0.633556175968637 to -0.633795320749427
-0.175043303511134 to -0.175282448291923
-0.506739277384272 to -0.506978422165062
-0.0339093990540421 to -0.034148543834832
Changing layer 4's weights from 
-0.133183199903956 to -0.133422344684746
-0.342016000292292 to -0.342255145073082
-0.542493600389948 to -0.542732745170738
-0.25555886699914 to -0.255798011779929
-0.494811672708979 to -0.495050817489769
-0.263346452257624 to -0.263585597038414
-0.26770498230218 to -0.26794412708297
-0.729602504513254 to -0.729841649294044
-0.135906416914454 to -0.136145561695244
-0.568208355448237 to -0.568447500229027
Changing layer 5's weights from 
0.0195966176772208 to 0.0193574728964309
-0.42247690632104 to -0.42271605110183
0.0027114562773798 to 0.00247231149658989
-0.932800725094071 to -0.933039869874861
-0.395531911394587 to -0.395771056175377
-0.793999854466906 to -0.794238999247696
-0.494034547350397 to -0.494273692131187
-0.421588320276728 to -0.421827465057518
-0.0319863147950082 to -0.0322254595757981
-0.0384944267487442 to -0.0387335715295341
Trying to learn from memory 42, 1, -0.2
sum 0.0384761701223947 distri 0.0219864479951357
Using diff 0.00687067959666033 and condRate 0.166666666666667
Changed category 1 weights from 
0.296465909013055 to 0.29623688635642
0.529781496056816 to 0.529552473400182
0.104856258162759 to 0.104627235506124
0.108949070700906 to 0.108720048044271
Changing layer 0's weights from 
-0.225822001457359 to -0.226051024113993
-0.597748458147193 to -0.597977480803827
-0.832097723841812 to -0.832326746498446
-0.797538473963881 to -0.797767496620515
-0.049670904636528 to -0.0498999272931627
-0.732312857866432 to -0.732541880523066
-0.578483134269859 to -0.578712156926493
-0.605937540292884 to -0.606166562949518
-0.84342940127864 to -0.843658423935274
-0.887179456412936 to -0.88740847906957
Changing layer 1's weights from 
-0.882082544028903 to -0.882311566685537
-0.280913859367516 to -0.28114288202415
-0.406616598129418 to -0.406845620786052
-0.416521936416771 to -0.416750959073405
-0.601574718713905 to -0.601803741370539
-0.739812552690651 to -0.740041575347285
-0.469093829155113 to -0.469322851811747
-0.854896858096268 to -0.855125880752902
-0.443233877182152 to -0.443462899838786
-0.805880740046645 to -0.806109762703279
Changing layer 2's weights from 
0.00330749177918188 to 0.00307846912254717
-0.550745993614342 to -0.550975016270976
-0.266156702995445 to -0.266385725652079
-0.0949566063882368 to -0.0951856290448715
-0.272209316253807 to -0.272438338910441
-0.478218942642357 to -0.478447965298991
0.0405937733648758 to 0.0403647507082411
-0.0318282542230151 to -0.0320572768796498
-0.763826548814918 to -0.764055571471552
-0.718869030237343 to -0.719098052893977
Changing layer 3's weights from 
-0.0297428069116141 to -0.0299718295682488
-0.293295889854576 to -0.29352491251121
-0.050685971737053 to -0.0509149943936877
-0.958125203493196 to -0.95835422614983
-0.173361867427971 to -0.173590890084605
-0.303610473632958 to -0.303839496289592
-0.633795320749427 to -0.634024343406061
-0.175282448291923 to -0.175511470948558
-0.506978422165062 to -0.507207444821696
-0.034148543834832 to -0.0343775664914667
Changing layer 4's weights from 
-0.133422344684746 to -0.13365136734138
-0.342255145073082 to -0.342484167729716
-0.542732745170738 to -0.542961767827372
-0.255798011779929 to -0.256027034436564
-0.495050817489769 to -0.495279840146404
-0.263585597038414 to -0.263814619695048
-0.26794412708297 to -0.268173149739604
-0.729841649294044 to -0.730070671950678
-0.136145561695244 to -0.136374584351878
-0.568447500229027 to -0.568676522885661
Changing layer 5's weights from 
0.0193574728964309 to 0.0191284502397962
-0.42271605110183 to -0.422945073758464
0.00247231149658989 to 0.00224328883995517
-0.933039869874861 to -0.933268892531495
-0.395771056175377 to -0.396000078832011
-0.794238999247696 to -0.79446802190433
-0.494273692131187 to -0.494502714787822
-0.421827465057518 to -0.422056487714152
-0.0322254595757981 to -0.0324544822324328
-0.0387335715295341 to -0.0389625941861688
10/5/2016 1:41:56 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 43, 1, -0.2
sum 0.0378710158903919 distri 0.021766026679481
Using diff 0.0066372352383129 and condRate 0.166666666666667
Changed category 1 weights from 
0.29623688635642 to 0.296015645178513
0.529552473400182 to 0.529331232222275
0.104627235506124 to 0.104405994328217
0.108720048044271 to 0.108498806866364
Changing layer 0's weights from 
-0.226051024113993 to -0.2262722652919
-0.597977480803827 to -0.598198721981735
-0.832326746498446 to -0.832547987676354
-0.797767496620515 to -0.797988737798423
-0.0498999272931627 to -0.0501211684710699
-0.732541880523066 to -0.732763121700974
-0.578712156926493 to -0.578933398104401
-0.606166562949518 to -0.606387804127426
-0.843658423935274 to -0.843879665113182
-0.88740847906957 to -0.887629720247478
Changing layer 1's weights from 
-0.882311566685537 to -0.882532807863445
-0.28114288202415 to -0.281364123202057
-0.406845620786052 to -0.40706686196396
-0.416750959073405 to -0.416972200251313
-0.601803741370539 to -0.602024982548447
-0.740041575347285 to -0.740262816525193
-0.469322851811747 to -0.469544092989654
-0.855125880752902 to -0.85534712193081
-0.443462899838786 to -0.443684141016694
-0.806109762703279 to -0.806331003881187
Changing layer 2's weights from 
0.00307846912254717 to 0.00285722794463999
-0.550975016270976 to -0.551196257448884
-0.266385725652079 to -0.266606966829986
-0.0951856290448715 to -0.0954068702227787
-0.272438338910441 to -0.272659580088348
-0.478447965298991 to -0.478669206476899
0.0403647507082411 to 0.0401435095303339
-0.0320572768796498 to -0.032278518057557
-0.764055571471552 to -0.76427681264946
-0.719098052893977 to -0.719319294071885
Changing layer 3's weights from 
-0.0299718295682488 to -0.030193070746156
-0.29352491251121 to -0.293746153689118
-0.0509149943936877 to -0.0511362355715949
-0.95835422614983 to -0.958575467327738
-0.173590890084605 to -0.173812131262512
-0.303839496289592 to -0.3040607374675
-0.634024343406061 to -0.634245584583969
-0.175511470948558 to -0.175732712126465
-0.507207444821696 to -0.507428685999604
-0.0343775664914667 to -0.0345988076693739
Changing layer 4's weights from 
-0.13365136734138 to -0.133872608519287
-0.342484167729716 to -0.342705408907624
-0.542961767827372 to -0.543183009005279
-0.256027034436564 to -0.256248275614471
-0.495279840146404 to -0.495501081324311
-0.263814619695048 to -0.264035860872955
-0.268173149739604 to -0.268394390917511
-0.730070671950678 to -0.730291913128586
-0.136374584351878 to -0.136595825529785
-0.568676522885661 to -0.568897764063569
Changing layer 5's weights from 
0.0191284502397962 to 0.018907209061889
-0.422945073758464 to -0.423166314936372
0.00224328883995517 to 0.00202204766204799
-0.933268892531495 to -0.933490133709403
-0.396000078832011 to -0.396221320009919
-0.79446802190433 to -0.794689263082238
-0.494502714787822 to -0.494723955965729
-0.422056487714152 to -0.42227772889206
-0.0324544822324328 to -0.03267572341034
-0.0389625941861688 to -0.039183835364076
Trying to learn from memory 44, 0, -0.2
sum 0.0387175919778547 distri -0.0173317502601817
Using diff 0.0463699442435727 and condRate 0.166666666666667
Changed category 0 weights from 
0.208885024374021 to 0.20733935954287
-0.191149939710604 to -0.192695604541755
-0.47367063623141 to -0.475216301062561
-0.322165270263659 to -0.323710935094811
Changing layer 0's weights from 
-0.2262722652919 to -0.227817930123052
-0.598198721981735 to -0.599744386812886
-0.832547987676354 to -0.834093652507505
-0.797988737798423 to -0.799534402629574
-0.0501211684710699 to -0.0516668333022212
-0.732763121700974 to -0.734308786532125
-0.578933398104401 to -0.580479062935552
-0.606387804127426 to -0.607933468958577
-0.843879665113182 to -0.845425329944333
-0.887629720247478 to -0.889175385078629
Changing layer 1's weights from 
-0.882532807863445 to -0.884078472694596
-0.281364123202057 to -0.282909788033209
-0.40706686196396 to -0.408612526795111
-0.416972200251313 to -0.418517865082464
-0.602024982548447 to -0.603570647379598
-0.740262816525193 to -0.741808481356344
-0.469544092989654 to -0.471089757820806
-0.85534712193081 to -0.856892786761961
-0.443684141016694 to -0.445229805847845
-0.806331003881187 to -0.807876668712338
Changing layer 2's weights from 
0.00285722794463999 to 0.0013115631134887
-0.551196257448884 to -0.552741922280035
-0.266606966829986 to -0.268152631661138
-0.0954068702227787 to -0.09695253505393
-0.272659580088348 to -0.2742052449195
-0.478669206476899 to -0.48021487130805
0.0401435095303339 to 0.0385978446991826
-0.032278518057557 to -0.0338241828887083
-0.76427681264946 to -0.765822477480611
-0.719319294071885 to -0.720864958903036
Changing layer 3's weights from 
-0.030193070746156 to -0.0317387355773073
-0.293746153689118 to -0.295291818520269
-0.0511362355715949 to -0.0526819004027462
-0.958575467327738 to -0.960121132158889
-0.173812131262512 to -0.175357796093664
-0.3040607374675 to -0.305606402298651
-0.634245584583969 to -0.63579124941512
-0.175732712126465 to -0.177278376957617
-0.507428685999604 to -0.508974350830755
-0.0345988076693739 to -0.0361444725005252
Changing layer 4's weights from 
-0.133872608519287 to -0.135418273350439
-0.342705408907624 to -0.344251073738775
-0.543183009005279 to -0.544728673836431
-0.256248275614471 to -0.257793940445623
-0.495501081324311 to -0.497046746155462
-0.264035860872955 to -0.265581525704107
-0.268394390917511 to -0.269940055748663
-0.730291913128586 to -0.731837577959737
-0.136595825529785 to -0.138141490360937
-0.568897764063569 to -0.57044342889472
Changing layer 5's weights from 
0.018907209061889 to 0.0173615442307377
-0.423166314936372 to -0.424711979767523
0.00202204766204799 to 0.000476382830896702
-0.933490133709403 to -0.935035798540554
-0.396221320009919 to -0.39776698484107
-0.794689263082238 to -0.796234927913389
-0.494723955965729 to -0.49626962079688
-0.42227772889206 to -0.423823393723211
-0.03267572341034 to -0.0342213882414913
-0.039183835364076 to -0.0407295001952273
Trying to learn from memory 45, 1, -0.2
sum 0.0413114876515286 distri 0.0234504208565817
Using diff 0.00753319488206479 and condRate 0.166666666666667
Changed category 1 weights from 
0.296015645178513 to 0.295764538678703
0.529331232222275 to 0.529080125722464
0.104405994328217 to 0.104154887828407
0.108498806866364 to 0.108247700366554
Changing layer 0's weights from 
-0.227817930123052 to -0.228069036622862
-0.599744386812886 to -0.599995493312697
-0.834093652507505 to -0.834344759007316
-0.799534402629574 to -0.799785509129385
-0.0516668333022212 to -0.0519179398020318
-0.734308786532125 to -0.734559893031935
-0.580479062935552 to -0.580730169435362
-0.607933468958577 to -0.608184575458387
-0.845425329944333 to -0.845676436444143
-0.889175385078629 to -0.889426491578439
Changing layer 1's weights from 
-0.884078472694596 to -0.884329579194406
-0.282909788033209 to -0.283160894533019
-0.408612526795111 to -0.408863633294921
-0.418517865082464 to -0.418768971582274
-0.603570647379598 to -0.603821753879409
-0.741808481356344 to -0.742059587856154
-0.471089757820806 to -0.471340864320616
-0.856892786761961 to -0.857143893261772
-0.445229805847845 to -0.445480912347655
-0.807876668712338 to -0.808127775212149
Changing layer 2's weights from 
0.0013115631134887 to 0.00106045661367809
-0.552741922280035 to -0.552993028779846
-0.268152631661138 to -0.268403738160948
-0.09695253505393 to -0.0972036415537406
-0.2742052449195 to -0.27445635141931
-0.48021487130805 to -0.48046597780786
0.0385978446991826 to 0.038346738199372
-0.0338241828887083 to -0.0340752893885189
-0.765822477480611 to -0.766073583980421
-0.720864958903036 to -0.721116065402846
Changing layer 3's weights from 
-0.0317387355773073 to -0.0319898420771179
-0.295291818520269 to -0.295542925020079
-0.0526819004027462 to -0.0529330069025568
-0.960121132158889 to -0.960372238658699
-0.175357796093664 to -0.175608902593474
-0.305606402298651 to -0.305857508798461
-0.63579124941512 to -0.636042355914931
-0.177278376957617 to -0.177529483457427
-0.508974350830755 to -0.509225457330566
-0.0361444725005252 to -0.0363955790003358
Changing layer 4's weights from 
-0.135418273350439 to -0.135669379850249
-0.344251073738775 to -0.344502180238585
-0.544728673836431 to -0.544979780336241
-0.257793940445623 to -0.258045046945433
-0.497046746155462 to -0.497297852655273
-0.265581525704107 to -0.265832632203917
-0.269940055748663 to -0.270191162248473
-0.731837577959737 to -0.732088684459547
-0.138141490360937 to -0.138392596860747
-0.57044342889472 to -0.570694535394531
Changing layer 5's weights from 
0.0173615442307377 to 0.0171104377309271
-0.424711979767523 to -0.424963086267333
0.000476382830896702 to 0.000225276331086098
-0.935035798540554 to -0.935286905040364
-0.39776698484107 to -0.39801809134088
-0.796234927913389 to -0.7964860344132
-0.49626962079688 to -0.496520727296691
-0.423823393723211 to -0.424074500223021
-0.0342213882414913 to -0.0344724947413019
-0.0407295001952273 to -0.0409806066950379
10/5/2016 1:41:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:41:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:16 PMStarting learning phase with deltaScore: 0.1
Modified index 0's learning in memoryPool to 0.02
Modified index 1's learning in memoryPool to 0.02
Modified index 2's learning in memoryPool to 0.02
Modified index 3's learning in memoryPool to 0.02
Modified index 4's learning in memoryPool to 0.02
Modified index 5's learning in memoryPool to 0.02
Modified index 6's learning in memoryPool to 0.02
Modified index 7's learning in memoryPool to 0.02
Modified index 8's learning in memoryPool to 0.02
Modified index 9's learning in memoryPool to 0.02
Modified index 10's learning in memoryPool to 0.02
Modified index 11's learning in memoryPool to 0.02
Modified index 12's learning in memoryPool to 0.02
Modified index 13's learning in memoryPool to 0.02
Modified index 14's learning in memoryPool to 0.02
Modified index 15's learning in memoryPool to 0.02
Modified index 16's learning in memoryPool to 0.02
10/5/2016 1:42:16 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 46, 0, 0.02
sum 0.0375968276684233 distri -0.0168437907690624
Using diff 0.0450414115203799 and condRate 0.166666666666667
Changed category 0 weights from 
0.20733935954287 to 0.207489497577915
-0.192695604541755 to -0.19254546650671
-0.475216301062561 to -0.475066163027516
-0.323710935094811 to -0.323560797059765
Changing layer 0's weights from 
-0.228069036622862 to -0.227918898587817
-0.599995493312697 to -0.599845355277651
-0.834344759007316 to -0.83419462097227
-0.799785509129385 to -0.799635371094339
-0.0519179398020318 to -0.0517678017669864
-0.734559893031935 to -0.73440975499689
-0.580730169435362 to -0.580580031400317
-0.608184575458387 to -0.608034437423342
-0.845676436444143 to -0.845526298409098
-0.889426491578439 to -0.889276353543394
Changing layer 1's weights from 
-0.884329579194406 to -0.884179441159361
-0.283160894533019 to -0.283010756497974
-0.408863633294921 to -0.408713495259876
-0.418768971582274 to -0.418618833547229
-0.603821753879409 to -0.603671615844363
-0.742059587856154 to -0.741909449821109
-0.471340864320616 to -0.471190726285571
-0.857143893261772 to -0.856993755226726
-0.445480912347655 to -0.44533077431261
-0.808127775212149 to -0.807977637177103
Changing layer 2's weights from 
0.00106045661367809 to 0.00121059464872351
-0.552993028779846 to -0.5528428907448
-0.268403738160948 to -0.268253600125903
-0.0972036415537406 to -0.0970535035186952
-0.27445635141931 to -0.274306213384265
-0.48046597780786 to -0.480315839772815
0.038346738199372 to 0.0384968762344174
-0.0340752893885189 to -0.0339251513534735
-0.766073583980421 to -0.765923445945376
-0.721116065402846 to -0.720965927367801
Changing layer 3's weights from 
-0.0319898420771179 to -0.0318397040420725
-0.295542925020079 to -0.295392786985034
-0.0529330069025568 to -0.0527828688675114
-0.960372238658699 to -0.960222100623654
-0.175608902593474 to -0.175458764558429
-0.305857508798461 to -0.305707370763416
-0.636042355914931 to -0.635892217879885
-0.177529483457427 to -0.177379345422382
-0.509225457330566 to -0.50907531929552
-0.0363955790003358 to -0.0362454409652904
Changing layer 4's weights from 
-0.135669379850249 to -0.135519241815204
-0.344502180238585 to -0.34435204220354
-0.544979780336241 to -0.544829642301196
-0.258045046945433 to -0.257894908910388
-0.497297852655273 to -0.497147714620227
-0.265832632203917 to -0.265682494168872
-0.270191162248473 to -0.270041024213428
-0.732088684459547 to -0.731938546424502
-0.138392596860747 to -0.138242458825702
-0.570694535394531 to -0.570544397359485
Changing layer 5's weights from 
0.0171104377309271 to 0.0172605757659725
-0.424963086267333 to -0.424812948232288
0.000225276331086098 to 0.000375414366131517
-0.935286905040364 to -0.935136767005319
-0.39801809134088 to -0.397867953305835
-0.7964860344132 to -0.796335896378154
-0.496520727296691 to -0.496370589261645
-0.424074500223021 to -0.423924362187976
-0.0344724947413019 to -0.0343223567062565
-0.0409806066950379 to -0.0408304686599925
Trying to learn from memory 47, 0, 0.02
sum 0.0375968276684233 distri -0.0168437907690624
Using diff 0.0450414115203799 and condRate 0.166666666666667
Changed category 0 weights from 
0.207489497577915 to 0.20763963561296
-0.19254546650671 to -0.192395328471665
-0.475066163027516 to -0.474916024992471
-0.323560797059765 to -0.32341065902472
Changing layer 0's weights from 
-0.227918898587817 to -0.227768760552771
-0.599845355277651 to -0.599695217242606
-0.83419462097227 to -0.834044482937225
-0.799635371094339 to -0.799485233059294
-0.0517678017669864 to -0.0516176637319409
-0.73440975499689 to -0.734259616961845
-0.580580031400317 to -0.580429893365272
-0.608034437423342 to -0.607884299388297
-0.845526298409098 to -0.845376160374053
-0.889276353543394 to -0.889126215508349
Changing layer 1's weights from 
-0.884179441159361 to -0.884029303124316
-0.283010756497974 to -0.282860618462928
-0.408713495259876 to -0.408563357224831
-0.418618833547229 to -0.418468695512184
-0.603671615844363 to -0.603521477809318
-0.741909449821109 to -0.741759311786064
-0.471190726285571 to -0.471040588250525
-0.856993755226726 to -0.856843617191681
-0.44533077431261 to -0.445180636277565
-0.807977637177103 to -0.807827499142058
Changing layer 2's weights from 
0.00121059464872351 to 0.00136073268376893
-0.5528428907448 to -0.552692752709755
-0.268253600125903 to -0.268103462090857
-0.0970535035186952 to -0.0969033654836498
-0.274306213384265 to -0.274156075349219
-0.480315839772815 to -0.48016570173777
0.0384968762344174 to 0.0386470142694629
-0.0339251513534735 to -0.0337750133184281
-0.765923445945376 to -0.765773307910331
-0.720965927367801 to -0.720815789332756
Changing layer 3's weights from 
-0.0318397040420725 to -0.0316895660070271
-0.295392786985034 to -0.295242648949989
-0.0527828688675114 to -0.0526327308324659
-0.960222100623654 to -0.960071962588609
-0.175458764558429 to -0.175308626523383
-0.305707370763416 to -0.305557232728371
-0.635892217879885 to -0.63574207984484
-0.177379345422382 to -0.177229207387336
-0.50907531929552 to -0.508925181260475
-0.0362454409652904 to -0.036095302930245
Changing layer 4's weights from 
-0.135519241815204 to -0.135369103780159
-0.34435204220354 to -0.344201904168495
-0.544829642301196 to -0.544679504266151
-0.257894908910388 to -0.257744770875342
-0.497147714620227 to -0.496997576585182
-0.265682494168872 to -0.265532356133826
-0.270041024213428 to -0.269890886178382
-0.731938546424502 to -0.731788408389457
-0.138242458825702 to -0.138092320790657
-0.570544397359485 to -0.57039425932444
Changing layer 5's weights from 
0.0172605757659725 to 0.0174107138010179
-0.424812948232288 to -0.424662810197243
0.000375414366131517 to 0.000525552401176937
-0.935136767005319 to -0.934986628970274
-0.397867953305835 to -0.39771781527079
-0.796335896378154 to -0.796185758343109
-0.496370589261645 to -0.4962204512266
-0.423924362187976 to -0.423774224152931
-0.0343223567062565 to -0.0341722186712111
-0.0408304686599925 to -0.0406803306249471
Trying to learn from memory 48, 1, 0.02
sum 0.0375968276684233 distri 0.0217055326731742
Using diff 0.00649208807814332 and condRate 0.166666666666667
Changed category 1 weights from 
0.295764538678703 to 0.295786178971813
0.529080125722464 to 0.529101766015574
0.104154887828407 to 0.104176528121517
0.108247700366554 to 0.108269340659664
Changing layer 0's weights from 
-0.227768760552771 to -0.227747120259661
-0.599695217242606 to -0.599673576949496
-0.834044482937225 to -0.834022842644115
-0.799485233059294 to -0.799463592766184
-0.0516176637319409 to -0.0515960234388308
-0.734259616961845 to -0.734237976668735
-0.580429893365272 to -0.580408253072162
-0.607884299388297 to -0.607862659095187
-0.845376160374053 to -0.845354520080943
-0.889126215508349 to -0.889104575215239
Changing layer 1's weights from 
-0.884029303124316 to -0.884007662831206
-0.282860618462928 to -0.282838978169818
-0.408563357224831 to -0.40854171693172
-0.418468695512184 to -0.418447055219073
-0.603521477809318 to -0.603499837516208
-0.741759311786064 to -0.741737671492954
-0.471040588250525 to -0.471018947957415
-0.856843617191681 to -0.856821976898571
-0.445180636277565 to -0.445158995984454
-0.807827499142058 to -0.807805858848948
Changing layer 2's weights from 
0.00136073268376893 to 0.00138237297687904
-0.552692752709755 to -0.552671112416645
-0.268103462090857 to -0.268081821797747
-0.0969033654836498 to -0.0968817251905397
-0.274156075349219 to -0.274134435056109
-0.48016570173777 to -0.480144061444659
0.0386470142694629 to 0.038668654562573
-0.0337750133184281 to -0.033753373025318
-0.765773307910331 to -0.765751667617221
-0.720815789332756 to -0.720794149039646
Changing layer 3's weights from 
-0.0316895660070271 to -0.031667925713917
-0.295242648949989 to -0.295221008656878
-0.0526327308324659 to -0.0526110905393558
-0.960071962588609 to -0.960050322295499
-0.175308626523383 to -0.175286986230273
-0.305557232728371 to -0.30553559243526
-0.63574207984484 to -0.63572043955173
-0.177229207387336 to -0.177207567094226
-0.508925181260475 to -0.508903540967365
-0.036095302930245 to -0.0360736626371349
Changing layer 4's weights from 
-0.135369103780159 to -0.135347463487048
-0.344201904168495 to -0.344180263875384
-0.544679504266151 to -0.544657863973041
-0.257744770875342 to -0.257723130582232
-0.496997576585182 to -0.496975936292072
-0.265532356133826 to -0.265510715840716
-0.269890886178382 to -0.269869245885272
-0.731788408389457 to -0.731766768096347
-0.138092320790657 to -0.138070680497546
-0.57039425932444 to -0.57037261903133
Changing layer 5's weights from 
0.0174107138010179 to 0.0174323540941281
-0.424662810197243 to -0.424641169904132
0.000525552401176937 to 0.00054719269428705
-0.934986628970274 to -0.934964988677164
-0.39771781527079 to -0.397696174977679
-0.796185758343109 to -0.796164118049999
-0.4962204512266 to -0.49619881093349
-0.423774224152931 to -0.42375258385982
-0.0341722186712111 to -0.0341505783781009
-0.0406803306249471 to -0.040658690331837
Trying to learn from memory 49, 0, 0.02
sum 0.0375968276684233 distri -0.0168437907690624
Using diff 0.0450414115203799 and condRate 0.166666666666667
Changed category 0 weights from 
0.20763963561296 to 0.207789773648006
-0.192395328471665 to -0.192245190436619
-0.474916024992471 to -0.474765886957425
-0.32341065902472 to -0.323260520989674
Changing layer 0's weights from 
-0.227747120259661 to -0.227596982224616
-0.599673576949496 to -0.59952343891445
-0.834022842644115 to -0.833872704609069
-0.799463592766184 to -0.799313454731138
-0.0515960234388308 to -0.0514458854037854
-0.734237976668735 to -0.734087838633689
-0.580408253072162 to -0.580258115037116
-0.607862659095187 to -0.607712521060141
-0.845354520080943 to -0.845204382045897
-0.889104575215239 to -0.888954437180193
Changing layer 1's weights from 
-0.884007662831206 to -0.88385752479616
-0.282838978169818 to -0.282688840134773
-0.40854171693172 to -0.408391578896675
-0.418447055219073 to -0.418296917184028
-0.603499837516208 to -0.603349699481162
-0.741737671492954 to -0.741587533457908
-0.471018947957415 to -0.47086880992237
-0.856821976898571 to -0.856671838863525
-0.445158995984454 to -0.445008857949409
-0.807805858848948 to -0.807655720813902
Changing layer 2's weights from 
0.00138237297687904 to 0.00153251101192446
-0.552671112416645 to -0.552520974381599
-0.268081821797747 to -0.267931683762702
-0.0968817251905397 to -0.0967315871554942
-0.274134435056109 to -0.273984297021064
-0.480144061444659 to -0.479993923409614
0.038668654562573 to 0.0388187925976184
-0.033753373025318 to -0.0336032349902725
-0.765751667617221 to -0.765601529582175
-0.720794149039646 to -0.7206440110046
Changing layer 3's weights from 
-0.031667925713917 to -0.0315177876788715
-0.295221008656878 to -0.295070870621833
-0.0526110905393558 to -0.0524609525043104
-0.960050322295499 to -0.959900184260453
-0.175286986230273 to -0.175136848195228
-0.30553559243526 to -0.305385454400215
-0.63572043955173 to -0.635570301516684
-0.177207567094226 to -0.177057429059181
-0.508903540967365 to -0.508753402932319
-0.0360736626371349 to -0.0359235246020894
Changing layer 4's weights from 
-0.135347463487048 to -0.135197325452003
-0.344180263875384 to -0.344030125840339
-0.544657863973041 to -0.544507725937995
-0.257723130582232 to -0.257572992547187
-0.496975936292072 to -0.496825798257026
-0.265510715840716 to -0.265360577805671
-0.269869245885272 to -0.269719107850227
-0.731766768096347 to -0.731616630061301
-0.138070680497546 to -0.137920542462501
-0.57037261903133 to -0.570222480996284
Changing layer 5's weights from 
0.0174323540941281 to 0.0175824921291735
-0.424641169904132 to -0.424491031869087
0.00054719269428705 to 0.00069733072933247
-0.934964988677164 to -0.934814850642118
-0.397696174977679 to -0.397546036942634
-0.796164118049999 to -0.796013980014953
-0.49619881093349 to -0.496048672898444
-0.42375258385982 to -0.423602445824775
-0.0341505783781009 to -0.0340004403430555
-0.040658690331837 to -0.0405085522967915
Trying to learn from memory 50, 0, 0.02
sum 0.0375968276684233 distri -0.0168437907690624
Using diff 0.0450414115203799 and condRate 0.166666666666667
Changed category 0 weights from 
0.207789773648006 to 0.207939911683051
-0.192245190436619 to -0.192095052401574
-0.474765886957425 to -0.47461574892238
-0.323260520989674 to -0.323110382954629
Changing layer 0's weights from 
-0.227596982224616 to -0.22744684418957
-0.59952343891445 to -0.599373300879405
-0.833872704609069 to -0.833722566574024
-0.799313454731138 to -0.799163316696093
-0.0514458854037854 to -0.05129574736874
-0.734087838633689 to -0.733937700598644
-0.580258115037116 to -0.580107977002071
-0.607712521060141 to -0.607562383025096
-0.845204382045897 to -0.845054244010852
-0.888954437180193 to -0.888804299145148
Changing layer 1's weights from 
-0.88385752479616 to -0.883707386761115
-0.282688840134773 to -0.282538702099727
-0.408391578896675 to -0.40824144086163
-0.418296917184028 to -0.418146779148983
-0.603349699481162 to -0.603199561446117
-0.741587533457908 to -0.741437395422863
-0.47086880992237 to -0.470718671887324
-0.856671838863525 to -0.85652170082848
-0.445008857949409 to -0.444858719914364
-0.807655720813902 to -0.807505582778857
Changing layer 2's weights from 
0.00153251101192446 to 0.00168264904696988
-0.552520974381599 to -0.552370836346554
-0.267931683762702 to -0.267781545727656
-0.0967315871554942 to -0.0965814491204488
-0.273984297021064 to -0.273834158986018
-0.479993923409614 to -0.479843785374569
0.0388187925976184 to 0.0389689306326638
-0.0336032349902725 to -0.0334530969552271
-0.765601529582175 to -0.76545139154713
-0.7206440110046 to -0.720493872969555
Changing layer 3's weights from 
-0.0315177876788715 to -0.0313676496438261
-0.295070870621833 to -0.294920732586788
-0.0524609525043104 to -0.052310814469265
-0.959900184260453 to -0.959750046225408
-0.175136848195228 to -0.174986710160182
-0.305385454400215 to -0.30523531636517
-0.635570301516684 to -0.635420163481639
-0.177057429059181 to -0.176907291024135
-0.508753402932319 to -0.508603264897274
-0.0359235246020894 to -0.035773386567044
Changing layer 4's weights from 
-0.135197325452003 to -0.135047187416958
-0.344030125840339 to -0.343879987805294
-0.544507725937995 to -0.54435758790295
-0.257572992547187 to -0.257422854512141
-0.496825798257026 to -0.496675660221981
-0.265360577805671 to -0.265210439770625
-0.269719107850227 to -0.269568969815181
-0.731616630061301 to -0.731466492026256
-0.137920542462501 to -0.137770404427456
-0.570222480996284 to -0.570072342961239
Changing layer 5's weights from 
0.0175824921291735 to 0.0177326301642189
-0.424491031869087 to -0.424340893834042
0.00069733072933247 to 0.00084746876437789
-0.934814850642118 to -0.934664712607073
-0.397546036942634 to -0.397395898907589
-0.796013980014953 to -0.795863841979908
-0.496048672898444 to -0.495898534863399
-0.423602445824775 to -0.42345230778973
-0.0340004403430555 to -0.0338503023080101
-0.0405085522967915 to -0.0403584142617461
Trying to learn from memory 51, 1, 0.02
sum 0.0375968276684233 distri 0.0217055326731742
Using diff 0.00649208807814332 and condRate 0.166666666666667
Changed category 1 weights from 
0.295786178971813 to 0.295807819264923
0.529101766015574 to 0.529123406308684
0.104176528121517 to 0.104198168414627
0.108269340659664 to 0.108290980952774
Changing layer 0's weights from 
-0.22744684418957 to -0.22742520389646
-0.599373300879405 to -0.599351660586295
-0.833722566574024 to -0.833700926280914
-0.799163316696093 to -0.799141676402983
-0.05129574736874 to -0.0512741070756299
-0.733937700598644 to -0.733916060305534
-0.580107977002071 to -0.580086336708961
-0.607562383025096 to -0.607540742731986
-0.845054244010852 to -0.845032603717742
-0.888804299145148 to -0.888782658852038
Changing layer 1's weights from 
-0.883707386761115 to -0.883685746468005
-0.282538702099727 to -0.282517061806617
-0.40824144086163 to -0.408219800568519
-0.418146779148983 to -0.418125138855872
-0.603199561446117 to -0.603177921153007
-0.741437395422863 to -0.741415755129753
-0.470718671887324 to -0.470697031594214
-0.85652170082848 to -0.85650006053537
-0.444858719914364 to -0.444837079621253
-0.807505582778857 to -0.807483942485747
Changing layer 2's weights from 
0.00168264904696988 to 0.00170428934008
-0.552370836346554 to -0.552349196053444
-0.267781545727656 to -0.267759905434546
-0.0965814491204488 to -0.0965598088273387
-0.273834158986018 to -0.273812518692908
-0.479843785374569 to -0.479822145081458
0.0389689306326638 to 0.0389905709257739
-0.0334530969552271 to -0.033431456662117
-0.76545139154713 to -0.76542975125402
-0.720493872969555 to -0.720472232676445
Changing layer 3's weights from 
-0.0313676496438261 to -0.031346009350716
-0.294920732586788 to -0.294899092293677
-0.052310814469265 to -0.0522891741761549
-0.959750046225408 to -0.959728405932298
-0.174986710160182 to -0.174965069867072
-0.30523531636517 to -0.305213676072059
-0.635420163481639 to -0.635398523188529
-0.176907291024135 to -0.176885650731025
-0.508603264897274 to -0.508581624604164
-0.035773386567044 to -0.0357517462739339
Changing layer 4's weights from 
-0.135047187416958 to -0.135025547123847
-0.343879987805294 to -0.343858347512183
-0.54435758790295 to -0.54433594760984
-0.257422854512141 to -0.257401214219031
-0.496675660221981 to -0.496654019928871
-0.265210439770625 to -0.265188799477515
-0.269568969815181 to -0.269547329522071
-0.731466492026256 to -0.731444851733146
-0.137770404427456 to -0.137748764134345
-0.570072342961239 to -0.570050702668129
Changing layer 5's weights from 
0.0177326301642189 to 0.017754270457329
-0.424340893834042 to -0.424319253540931
0.00084746876437789 to 0.000869109057488002
-0.934664712607073 to -0.934643072313963
-0.397395898907589 to -0.397374258614478
-0.795863841979908 to -0.795842201686798
-0.495898534863399 to -0.495876894570289
-0.42345230778973 to -0.423430667496619
-0.0338503023080101 to -0.0338286620149
-0.0403584142617461 to -0.040336773968636
Trying to learn from memory 51, 1, 0.02
sum 0.0375968276684233 distri 0.0217055326731742
Using diff 0.00649208807814332 and condRate 0.166666666666667
Changed category 1 weights from 
0.295807819264923 to 0.295829459558033
0.529123406308684 to 0.529145046601794
0.104198168414627 to 0.104219808707737
0.108290980952774 to 0.108312621245884
Changing layer 0's weights from 
-0.22742520389646 to -0.22740356360335
-0.599351660586295 to -0.599330020293185
-0.833700926280914 to -0.833679285987804
-0.799141676402983 to -0.799120036109873
-0.0512741070756299 to -0.0512524667825197
-0.733916060305534 to -0.733894420012424
-0.580086336708961 to -0.580064696415851
-0.607540742731986 to -0.607519102438876
-0.845032603717742 to -0.845010963424632
-0.888782658852038 to -0.888761018558928
Changing layer 1's weights from 
-0.883685746468005 to -0.883664106174895
-0.282517061806617 to -0.282495421513507
-0.408219800568519 to -0.408198160275409
-0.418125138855872 to -0.418103498562762
-0.603177921153007 to -0.603156280859897
-0.741415755129753 to -0.741394114836643
-0.470697031594214 to -0.470675391301104
-0.85650006053537 to -0.85647842024226
-0.444837079621253 to -0.444815439328143
-0.807483942485747 to -0.807462302192637
Changing layer 2's weights from 
0.00170428934008 to 0.00172592963319011
-0.552349196053444 to -0.552327555760334
-0.267759905434546 to -0.267738265141436
-0.0965598088273387 to -0.0965381685342286
-0.273812518692908 to -0.273790878399798
-0.479822145081458 to -0.479800504788348
0.0389905709257739 to 0.039012211218884
-0.033431456662117 to -0.0334098163690069
-0.76542975125402 to -0.76540811096091
-0.720472232676445 to -0.720450592383335
Changing layer 3's weights from 
-0.031346009350716 to -0.0313243690576059
-0.294899092293677 to -0.294877452000567
-0.0522891741761549 to -0.0522675338830448
-0.959728405932298 to -0.959706765639188
-0.174965069867072 to -0.174943429573962
-0.305213676072059 to -0.305192035778949
-0.635398523188529 to -0.635376882895419
-0.176885650731025 to -0.176864010437915
-0.508581624604164 to -0.508559984311054
-0.0357517462739339 to -0.0357301059808238
Changing layer 4's weights from 
-0.135025547123847 to -0.135003906830737
-0.343858347512183 to -0.343836707219073
-0.54433594760984 to -0.544314307316729
-0.257401214219031 to -0.257379573925921
-0.496654019928871 to -0.496632379635761
-0.265188799477515 to -0.265167159184405
-0.269547329522071 to -0.269525689228961
-0.731444851733146 to -0.731423211440036
-0.137748764134345 to -0.137727123841235
-0.570050702668129 to -0.570029062375019
Changing layer 5's weights from 
0.017754270457329 to 0.0177759107504391
-0.424319253540931 to -0.424297613247821
0.000869109057488002 to 0.000890749350598115
-0.934643072313963 to -0.934621432020853
-0.397374258614478 to -0.397352618321368
-0.795842201686798 to -0.795820561393688
-0.495876894570289 to -0.495855254277179
-0.423430667496619 to -0.423409027203509
-0.0338286620149 to -0.0338070217217899
-0.040336773968636 to -0.0403151336755259
Trying to learn from memory 51, 1, 0.02
sum 0.0375968276684233 distri 0.0217055326731742
Using diff 0.00649208807814332 and condRate 0.166666666666667
Changed category 1 weights from 
0.295829459558033 to 0.295851099851143
0.529145046601794 to 0.529166686894904
0.104219808707737 to 0.104241449000847
0.108312621245884 to 0.108334261538994
Changing layer 0's weights from 
-0.22740356360335 to -0.22738192331024
-0.599330020293185 to -0.599308380000074
-0.833679285987804 to -0.833657645694693
-0.799120036109873 to -0.799098395816762
-0.0512524667825197 to -0.0512308264894096
-0.733894420012424 to -0.733872779719313
-0.580064696415851 to -0.58004305612274
-0.607519102438876 to -0.607497462145765
-0.845010963424632 to -0.844989323131521
-0.888761018558928 to -0.888739378265817
Changing layer 1's weights from 
-0.883664106174895 to -0.883642465881784
-0.282495421513507 to -0.282473781220397
-0.408198160275409 to -0.408176519982299
-0.418103498562762 to -0.418081858269652
-0.603156280859897 to -0.603134640566786
-0.741394114836643 to -0.741372474543532
-0.470675391301104 to -0.470653751007994
-0.85647842024226 to -0.856456779949149
-0.444815439328143 to -0.444793799035033
-0.807462302192637 to -0.807440661899526
Changing layer 2's weights from 
0.00172592963319011 to 0.00174756992630022
-0.552327555760334 to -0.552305915467224
-0.267738265141436 to -0.267716624848326
-0.0965381685342286 to -0.0965165282411185
-0.273790878399798 to -0.273769238106688
-0.479800504788348 to -0.479778864495238
0.039012211218884 to 0.0390338515119942
-0.0334098163690069 to -0.0333881760758968
-0.76540811096091 to -0.765386470667799
-0.720450592383335 to -0.720428952090224
Changing layer 3's weights from 
-0.0313243690576059 to -0.0313027287644958
-0.294877452000567 to -0.294855811707457
-0.0522675338830448 to -0.0522458935899346
-0.959706765639188 to -0.959685125346077
-0.174943429573962 to -0.174921789280852
-0.305192035778949 to -0.305170395485839
-0.635376882895419 to -0.635355242602308
-0.176864010437915 to -0.176842370144805
-0.508559984311054 to -0.508538344017944
-0.0357301059808238 to -0.0357084656877137
Changing layer 4's weights from 
-0.135003906830737 to -0.134982266537627
-0.343836707219073 to -0.343815066925963
-0.544314307316729 to -0.544292667023619
-0.257379573925921 to -0.257357933632811
-0.496632379635761 to -0.49661073934265
-0.265167159184405 to -0.265145518891295
-0.269525689228961 to -0.269504048935851
-0.731423211440036 to -0.731401571146925
-0.137727123841235 to -0.137705483548125
-0.570029062375019 to -0.570007422081908
Changing layer 5's weights from 
0.0177759107504391 to 0.0177975510435492
-0.424297613247821 to -0.424275972954711
0.000890749350598115 to 0.000912389643708228
-0.934621432020853 to -0.934599791727742
-0.397352618321368 to -0.397330978028258
-0.795820561393688 to -0.795798921100577
-0.495855254277179 to -0.495833613984068
-0.423409027203509 to -0.423387386910399
-0.0338070217217899 to -0.0337853814286798
-0.0403151336755259 to -0.0402934933824158
Trying to learn from memory 52, 0, 0.02
sum 0.0375968276684233 distri -0.0168437907690624
Using diff 0.0450414115203799 and condRate 0.166666666666667
Changed category 0 weights from 
0.207939911683051 to 0.208090049718097
-0.192095052401574 to -0.191944914366528
-0.47461574892238 to -0.474465610887334
-0.323110382954629 to -0.322960244919583
Changing layer 0's weights from 
-0.22738192331024 to -0.227231785275195
-0.599308380000074 to -0.599158241965029
-0.833657645694693 to -0.833507507659648
-0.799098395816762 to -0.798948257781717
-0.0512308264894096 to -0.0510806884543642
-0.733872779719313 to -0.733722641684268
-0.58004305612274 to -0.579892918087695
-0.607497462145765 to -0.60734732411072
-0.844989323131521 to -0.844839185096476
-0.888739378265817 to -0.888589240230772
Changing layer 1's weights from 
-0.883642465881784 to -0.883492327846739
-0.282473781220397 to -0.282323643185352
-0.408176519982299 to -0.408026381947254
-0.418081858269652 to -0.417931720234607
-0.603134640566786 to -0.602984502531741
-0.741372474543532 to -0.741222336508487
-0.470653751007994 to -0.470503612972949
-0.856456779949149 to -0.856306641914104
-0.444793799035033 to -0.444643660999988
-0.807440661899526 to -0.807290523864481
Changing layer 2's weights from 
0.00174756992630022 to 0.00189770796134564
-0.552305915467224 to -0.552155777432178
-0.267716624848326 to -0.267566486813281
-0.0965165282411185 to -0.0963663902060731
-0.273769238106688 to -0.273619100071643
-0.479778864495238 to -0.479628726460193
0.0390338515119942 to 0.0391839895470396
-0.0333881760758968 to -0.0332380380408513
-0.765386470667799 to -0.765236332632754
-0.720428952090224 to -0.720278814055179
Changing layer 3's weights from 
-0.0313027287644958 to -0.0311525907294504
-0.294855811707457 to -0.294705673672412
-0.0522458935899346 to -0.0520957555548892
-0.959685125346077 to -0.959534987311032
-0.174921789280852 to -0.174771651245807
-0.305170395485839 to -0.305020257450794
-0.635355242602308 to -0.635205104567263
-0.176842370144805 to -0.17669223210976
-0.508538344017944 to -0.508388205982898
-0.0357084656877137 to -0.0355583276526682
Changing layer 4's weights from 
-0.134982266537627 to -0.134832128502582
-0.343815066925963 to -0.343664928890918
-0.544292667023619 to -0.544142528988574
-0.257357933632811 to -0.257207795597766
-0.49661073934265 to -0.496460601307605
-0.265145518891295 to -0.26499538085625
-0.269504048935851 to -0.269353910900806
-0.731401571146925 to -0.73125143311188
-0.137705483548125 to -0.13755534551308
-0.570007422081908 to -0.569857284046863
Changing layer 5's weights from 
0.0177975510435492 to 0.0179476890785946
-0.424275972954711 to -0.424125834919666
0.000912389643708228 to 0.00106252767875365
-0.934599791727742 to -0.934449653692697
-0.397330978028258 to -0.397180839993213
-0.795798921100577 to -0.795648783065532
-0.495833613984068 to -0.495683475949023
-0.423387386910399 to -0.423237248875354
-0.0337853814286798 to -0.0336352433936343
-0.0402934933824158 to -0.0401433553473704
Trying to learn from memory 52, 0, 0.02
sum 0.0375968276684233 distri -0.0168437907690624
Using diff 0.0450414115203799 and condRate 0.166666666666667
Changed category 0 weights from 
0.208090049718097 to 0.208240187753142
-0.191944914366528 to -0.191794776331483
-0.474465610887334 to -0.474315472852289
-0.322960244919583 to -0.322810106884538
Changing layer 0's weights from 
-0.227231785275195 to -0.227081647240149
-0.599158241965029 to -0.599008103929984
-0.833507507659648 to -0.833357369624603
-0.798948257781717 to -0.798798119746672
-0.0510806884543642 to -0.0509305504193188
-0.733722641684268 to -0.733572503649223
-0.579892918087695 to -0.57974278005265
-0.60734732411072 to -0.607197186075675
-0.844839185096476 to -0.844689047061431
-0.888589240230772 to -0.888439102195727
Changing layer 1's weights from 
-0.883492327846739 to -0.883342189811694
-0.282323643185352 to -0.282173505150306
-0.408026381947254 to -0.407876243912208
-0.417931720234607 to -0.417781582199561
-0.602984502531741 to -0.602834364496696
-0.741222336508487 to -0.741072198473442
-0.470503612972949 to -0.470353474937903
-0.856306641914104 to -0.856156503879059
-0.444643660999988 to -0.444493522964942
-0.807290523864481 to -0.807140385829436
Changing layer 2's weights from 
0.00189770796134564 to 0.00204784599639106
-0.552155777432178 to -0.552005639397133
-0.267566486813281 to -0.267416348778235
-0.0963663902060731 to -0.0962162521710277
-0.273619100071643 to -0.273468962036597
-0.479628726460193 to -0.479478588425147
0.0391839895470396 to 0.039334127582085
-0.0332380380408513 to -0.0330879000058059
-0.765236332632754 to -0.765086194597709
-0.720278814055179 to -0.720128676020134
Changing layer 3's weights from 
-0.0311525907294504 to -0.0310024526944049
-0.294705673672412 to -0.294555535637366
-0.0520957555548892 to -0.0519456175198438
-0.959534987311032 to -0.959384849275987
-0.174771651245807 to -0.174621513210761
-0.305020257450794 to -0.304870119415748
-0.635205104567263 to -0.635054966532218
-0.17669223210976 to -0.176542094074714
-0.508388205982898 to -0.508238067947853
-0.0355583276526682 to -0.0354081896176228
Changing layer 4's weights from 
-0.134832128502582 to -0.134681990467536
-0.343664928890918 to -0.343514790855872
-0.544142528988574 to -0.543992390953529
-0.257207795597766 to -0.25705765756272
-0.496460601307605 to -0.49631046327256
-0.26499538085625 to -0.264845242821204
-0.269353910900806 to -0.26920377286576
-0.73125143311188 to -0.731101295076835
-0.13755534551308 to -0.137405207478034
-0.569857284046863 to -0.569707146011818
Changing layer 5's weights from 
0.0179476890785946 to 0.0180978271136401
-0.424125834919666 to -0.42397569688462
0.00106252767875365 to 0.00121266571379907
-0.934449653692697 to -0.934299515657652
-0.397180839993213 to -0.397030701958167
-0.795648783065532 to -0.795498645030487
-0.495683475949023 to -0.495533337913978
-0.423237248875354 to -0.423087110840308
-0.0336352433936343 to -0.0334851053585889
-0.0401433553473704 to -0.0399932173123249
10/5/2016 1:42:16 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 52, 1, 0.02
sum 0.0375968276684233 distri 0.0217055326731742
Using diff 0.00649208807814332 and condRate 0.166666666666667
Changed category 1 weights from 
0.295851099851143 to 0.295872740144253
0.529166686894904 to 0.529188327188015
0.104241449000847 to 0.104263089293957
0.108334261538994 to 0.108355901832104
Changing layer 0's weights from 
-0.227081647240149 to -0.227060006947039
-0.599008103929984 to -0.598986463636874
-0.833357369624603 to -0.833335729331493
-0.798798119746672 to -0.798776479453562
-0.0509305504193188 to -0.0509089101262087
-0.733572503649223 to -0.733550863356113
-0.57974278005265 to -0.57972113975954
-0.607197186075675 to -0.607175545782565
-0.844689047061431 to -0.844667406768321
-0.888439102195727 to -0.888417461902617
Changing layer 1's weights from 
-0.883342189811694 to -0.883320549518584
-0.282173505150306 to -0.282151864857196
-0.407876243912208 to -0.407854603619098
-0.417781582199561 to -0.417759941906451
-0.602834364496696 to -0.602812724203586
-0.741072198473442 to -0.741050558180332
-0.470353474937903 to -0.470331834644793
-0.856156503879059 to -0.856134863585949
-0.444493522964942 to -0.444471882671832
-0.807140385829436 to -0.807118745536326
Changing layer 2's weights from 
0.00204784599639106 to 0.00206948628950118
-0.552005639397133 to -0.551983999104023
-0.267416348778235 to -0.267394708485125
-0.0962162521710277 to -0.0961946118779175
-0.273468962036597 to -0.273447321743487
-0.479478588425147 to -0.479456948132037
0.039334127582085 to 0.0393557678751951
-0.0330879000058059 to -0.0330662597126958
-0.765086194597709 to -0.765064554304599
-0.720128676020134 to -0.720107035727024
Changing layer 3's weights from 
-0.0310024526944049 to -0.0309808124012948
-0.294555535637366 to -0.294533895344256
-0.0519456175198438 to -0.0519239772267337
-0.959384849275987 to -0.959363208982877
-0.174621513210761 to -0.174599872917651
-0.304870119415748 to -0.304848479122638
-0.635054966532218 to -0.635033326239108
-0.176542094074714 to -0.176520453781604
-0.508238067947853 to -0.508216427654743
-0.0354081896176228 to -0.0353865493245127
Changing layer 4's weights from 
-0.134681990467536 to -0.134660350174426
-0.343514790855872 to -0.343493150562762
-0.543992390953529 to -0.543970750660418
-0.25705765756272 to -0.25703601726961
-0.49631046327256 to -0.496288822979449
-0.264845242821204 to -0.264823602528094
-0.26920377286576 to -0.26918213257265
-0.731101295076835 to -0.731079654783725
-0.137405207478034 to -0.137383567184924
-0.569707146011818 to -0.569685505718708
Changing layer 5's weights from 
0.0180978271136401 to 0.0181194674067502
-0.42397569688462 to -0.42395405659151
0.00121266571379907 to 0.00123430600690918
-0.934299515657652 to -0.934277875364542
-0.397030701958167 to -0.397009061665057
-0.795498645030487 to -0.795477004737377
-0.495533337913978 to -0.495511697620867
-0.423087110840308 to -0.423065470547198
-0.0334851053585889 to -0.0334634650654788
-0.0399932173123249 to -0.0399715770192148
Trying to learn from memory 52, 1, 0.02
sum 0.0375968276684233 distri 0.0217055326731742
Using diff 0.00649208807814332 and condRate 0.166666666666667
Changed category 1 weights from 
0.295872740144253 to 0.295894380437363
0.529188327188015 to 0.529209967481125
0.104263089293957 to 0.104284729587067
0.108355901832104 to 0.108377542125214
Changing layer 0's weights from 
-0.227060006947039 to -0.227038366653929
-0.598986463636874 to -0.598964823343763
-0.833335729331493 to -0.833314089038382
-0.798776479453562 to -0.798754839160451
-0.0509089101262087 to -0.0508872698330986
-0.733550863356113 to -0.733529223063002
-0.57972113975954 to -0.579699499466429
-0.607175545782565 to -0.607153905489454
-0.844667406768321 to -0.84464576647521
-0.888417461902617 to -0.888395821609506
Changing layer 1's weights from 
-0.883320549518584 to -0.883298909225473
-0.282151864857196 to -0.282130224564086
-0.407854603619098 to -0.407832963325988
-0.417759941906451 to -0.417738301613341
-0.602812724203586 to -0.602791083910475
-0.741050558180332 to -0.741028917887221
-0.470331834644793 to -0.470310194351683
-0.856134863585949 to -0.856113223292838
-0.444471882671832 to -0.444450242378722
-0.807118745536326 to -0.807097105243215
Changing layer 2's weights from 
0.00206948628950118 to 0.00209112658261129
-0.551983999104023 to -0.551962358810912
-0.267394708485125 to -0.267373068192015
-0.0961946118779175 to -0.0961729715848074
-0.273447321743487 to -0.273425681450377
-0.479456948132037 to -0.479435307838927
0.0393557678751951 to 0.0393774081683052
-0.0330662597126958 to -0.0330446194195857
-0.765064554304599 to -0.765042914011488
-0.720107035727024 to -0.720085395433913
Changing layer 3's weights from 
-0.0309808124012948 to -0.0309591721081847
-0.294533895344256 to -0.294512255051146
-0.0519239772267337 to -0.0519023369336236
-0.959363208982877 to -0.959341568689766
-0.174599872917651 to -0.174578232624541
-0.304848479122638 to -0.304826838829528
-0.635033326239108 to -0.635011685945997
-0.176520453781604 to -0.176498813488494
-0.508216427654743 to -0.508194787361632
-0.0353865493245127 to -0.0353649090314026
Changing layer 4's weights from 
-0.134660350174426 to -0.134638709881316
-0.343493150562762 to -0.343471510269652
-0.543970750660418 to -0.543949110367308
-0.25703601726961 to -0.2570143769765
-0.496288822979449 to -0.496267182686339
-0.264823602528094 to -0.264801962234984
-0.26918213257265 to -0.26916049227954
-0.731079654783725 to -0.731058014490614
-0.137383567184924 to -0.137361926891814
-0.569685505718708 to -0.569663865425597
Changing layer 5's weights from 
0.0181194674067502 to 0.0181411076998603
-0.42395405659151 to -0.4239324162984
0.00123430600690918 to 0.00125594630001929
-0.934277875364542 to -0.934256235071431
-0.397009061665057 to -0.396987421371947
-0.795477004737377 to -0.795455364444266
-0.495511697620867 to -0.495490057327757
-0.423065470547198 to -0.423043830254088
-0.0334634650654788 to -0.0334418247723687
-0.0399715770192148 to -0.0399499367261047
Trying to learn from memory 52, 1, 0.02
sum 0.0375968276684233 distri 0.0217055326731742
Using diff 0.00649208807814332 and condRate 0.166666666666667
Changed category 1 weights from 
0.295894380437363 to 0.295916020730474
0.529209967481125 to 0.529231607774235
0.104284729587067 to 0.104306369880177
0.108377542125214 to 0.108399182418324
Changing layer 0's weights from 
-0.227038366653929 to -0.227016726360819
-0.598964823343763 to -0.598943183050653
-0.833314089038382 to -0.833292448745272
-0.798754839160451 to -0.798733198867341
-0.0508872698330986 to -0.0508656295399884
-0.733529223063002 to -0.733507582769892
-0.579699499466429 to -0.579677859173319
-0.607153905489454 to -0.607132265196344
-0.84464576647521 to -0.8446241261821
-0.888395821609506 to -0.888374181316396
Changing layer 1's weights from 
-0.883298909225473 to -0.883277268932363
-0.282130224564086 to -0.282108584270976
-0.407832963325988 to -0.407811323032878
-0.417738301613341 to -0.417716661320231
-0.602791083910475 to -0.602769443617365
-0.741028917887221 to -0.741007277594111
-0.470310194351683 to -0.470288554058573
-0.856113223292838 to -0.856091582999728
-0.444450242378722 to -0.444428602085612
-0.807097105243215 to -0.807075464950105
Changing layer 2's weights from 
0.00209112658261129 to 0.0021127668757214
-0.551962358810912 to -0.551940718517802
-0.267373068192015 to -0.267351427898905
-0.0961729715848074 to -0.0961513312916973
-0.273425681450377 to -0.273404041157267
-0.479435307838927 to -0.479413667545817
0.0393774081683052 to 0.0393990484614153
-0.0330446194195857 to -0.0330229791264756
-0.765042914011488 to -0.765021273718378
-0.720085395433913 to -0.720063755140803
Changing layer 3's weights from 
-0.0309591721081847 to -0.0309375318150746
-0.294512255051146 to -0.294490614758036
-0.0519023369336236 to -0.0518806966405135
-0.959341568689766 to -0.959319928396656
-0.174578232624541 to -0.174556592331431
-0.304826838829528 to -0.304805198536418
-0.635011685945997 to -0.634990045652887
-0.176498813488494 to -0.176477173195384
-0.508194787361632 to -0.508173147068522
-0.0353649090314026 to -0.0353432687382925
Changing layer 4's weights from 
-0.134638709881316 to -0.134617069588206
-0.343471510269652 to -0.343449869976542
-0.543949110367308 to -0.543927470074198
-0.2570143769765 to -0.25699273668339
-0.496267182686339 to -0.496245542393229
-0.264801962234984 to -0.264780321941874
-0.26916049227954 to -0.26913885198643
-0.731058014490614 to -0.731036374197504
-0.137361926891814 to -0.137340286598704
-0.569663865425597 to -0.569642225132487
Changing layer 5's weights from 
0.0181411076998603 to 0.0181627479929704
-0.4239324162984 to -0.42391077600529
0.00125594630001929 to 0.00127758659312941
-0.934256235071431 to -0.934234594778321
-0.396987421371947 to -0.396965781078837
-0.795455364444266 to -0.795433724151156
-0.495490057327757 to -0.495468417034647
-0.423043830254088 to -0.423022189960978
-0.0334418247723687 to -0.0334201844792586
-0.0399499367261047 to -0.0399282964329946
Trying to learn from memory 53, 1, 0.02
sum 0.0375968234476345 distri 0.0217055342624225
Using diff 0.00649208332330335 and condRate 0.166666666666667
Changed category 1 weights from 
0.295916020730474 to 0.295937661007734
0.529231607774235 to 0.529253248051495
0.104306369880177 to 0.104328010157438
0.108399182418324 to 0.108420822695585
Changing layer 0's weights from 
-0.227016726360819 to -0.226995086083558
-0.598943183050653 to -0.598921542773393
-0.833292448745272 to -0.833270808468012
-0.798733198867341 to -0.798711558590081
-0.0508656295399884 to -0.0508439892627278
-0.733507582769892 to -0.733485942492632
-0.579677859173319 to -0.579656218896059
-0.607132265196344 to -0.607110624919084
-0.8446241261821 to -0.84460248590484
-0.888374181316396 to -0.888352541039136
Changing layer 1's weights from 
-0.883277268932363 to -0.883255628655103
-0.282108584270976 to -0.282086943993715
-0.407811323032878 to -0.407789682755617
-0.417716661320231 to -0.41769502104297
-0.602769443617365 to -0.602747803340105
-0.741007277594111 to -0.740985637316851
-0.470288554058573 to -0.470266913781312
-0.856091582999728 to -0.856069942722468
-0.444428602085612 to -0.444406961808351
-0.807075464950105 to -0.807053824672845
Changing layer 2's weights from 
0.0021127668757214 to 0.00213440715298205
-0.551940718517802 to -0.551919078240542
-0.267351427898905 to -0.267329787621644
-0.0961513312916973 to -0.0961296910144367
-0.273404041157267 to -0.273382400880006
-0.479413667545817 to -0.479392027268556
0.0393990484614153 to 0.039420688738676
-0.0330229791264756 to -0.0330013388492149
-0.765021273718378 to -0.764999633441118
-0.720063755140803 to -0.720042114863543
Changing layer 3's weights from 
-0.0309375318150746 to -0.030915891537814
-0.294490614758036 to -0.294468974480775
-0.0518806966405135 to -0.0518590563632528
-0.959319928396656 to -0.959298288119396
-0.174556592331431 to -0.17453495205417
-0.304805198536418 to -0.304783558259157
-0.634990045652887 to -0.634968405375627
-0.176477173195384 to -0.176455532918123
-0.508173147068522 to -0.508151506791262
-0.0353432687382925 to -0.0353216284610318
Changing layer 4's weights from 
-0.134617069588206 to -0.134595429310945
-0.343449869976542 to -0.343428229699281
-0.543927470074198 to -0.543905829796938
-0.25699273668339 to -0.256971096406129
-0.496245542393229 to -0.496223902115969
-0.264780321941874 to -0.264758681664613
-0.26913885198643 to -0.269117211709169
-0.731036374197504 to -0.731014733920244
-0.137340286598704 to -0.137318646321443
-0.569642225132487 to -0.569620584855227
Changing layer 5's weights from 
0.0181627479929704 to 0.0181843882702311
-0.42391077600529 to -0.423889135728029
0.00127758659312941 to 0.00129922687039005
-0.934234594778321 to -0.934212954501061
-0.396965781078837 to -0.396944140801576
-0.795433724151156 to -0.795412083873896
-0.495468417034647 to -0.495446776757387
-0.423022189960978 to -0.423000549683717
-0.0334201844792586 to -0.0333985442019979
-0.0399282964329946 to -0.0399066561557339
Trying to learn from memory 54, 1, 0.02
sum 0.0375967569766121 distri 0.0217055619881647
Using diff 0.00649200574429432 and condRate 0.166666666666667
Changed category 1 weights from 
0.295937661007734 to 0.295959301026398
0.529253248051495 to 0.529274888070159
0.104328010157438 to 0.104349650176102
0.108420822695585 to 0.108442462714249
Changing layer 0's weights from 
-0.226995086083558 to -0.226973446064894
-0.598921542773393 to -0.598899902754729
-0.833270808468012 to -0.833249168449348
-0.798711558590081 to -0.798689918571417
-0.0508439892627278 to -0.0508223492440638
-0.733485942492632 to -0.733464302473968
-0.579656218896059 to -0.579634578877395
-0.607110624919084 to -0.60708898490042
-0.84460248590484 to -0.844580845886176
-0.888352541039136 to -0.888330901020472
Changing layer 1's weights from 
-0.883255628655103 to -0.883233988636439
-0.282086943993715 to -0.282065303975051
-0.407789682755617 to -0.407768042736953
-0.41769502104297 to -0.417673381024306
-0.602747803340105 to -0.602726163321441
-0.740985637316851 to -0.740963997298187
-0.470266913781312 to -0.470245273762648
-0.856069942722468 to -0.856048302703804
-0.444406961808351 to -0.444385321789687
-0.807053824672845 to -0.807032184654181
Changing layer 2's weights from 
0.00213440715298205 to 0.002156047171646
-0.551919078240542 to -0.551897438221878
-0.267329787621644 to -0.26730814760298
-0.0961296910144367 to -0.0961080509957727
-0.273382400880006 to -0.273360760861342
-0.479392027268556 to -0.479370387249892
0.039420688738676 to 0.0394423287573399
-0.0330013388492149 to -0.032979698830551
-0.764999633441118 to -0.764977993422454
-0.720042114863543 to -0.720020474844879
Changing layer 3's weights from 
-0.030915891537814 to -0.03089425151915
-0.294468974480775 to -0.294447334462111
-0.0518590563632528 to -0.0518374163445889
-0.959298288119396 to -0.959276648100732
-0.17453495205417 to -0.174513312035506
-0.304783558259157 to -0.304761918240493
-0.634968405375627 to -0.634946765356963
-0.176455532918123 to -0.176433892899459
-0.508151506791262 to -0.508129866772598
-0.0353216284610318 to -0.0352999884423679
Changing layer 4's weights from 
-0.134595429310945 to -0.134573789292281
-0.343428229699281 to -0.343406589680617
-0.543905829796938 to -0.543884189778274
-0.256971096406129 to -0.256949456387465
-0.496223902115969 to -0.496202262097305
-0.264758681664613 to -0.264737041645949
-0.269117211709169 to -0.269095571690505
-0.731014733920244 to -0.73099309390158
-0.137318646321443 to -0.137297006302779
-0.569620584855227 to -0.569598944836563
Changing layer 5's weights from 
0.0181843882702311 to 0.018206028288895
-0.423889135728029 to -0.423867495709365
0.00129922687039005 to 0.00132086688905401
-0.934212954501061 to -0.934191314482397
-0.396944140801576 to -0.396922500782912
-0.795412083873896 to -0.795390443855232
-0.495446776757387 to -0.495425136738723
-0.423000549683717 to -0.422978909665053
-0.0333985442019979 to -0.033376904183334
-0.0399066561557339 to -0.03988501613707
Trying to learn from memory 55, 0, 0.02
sum 0.0375967569778095 distri -0.0168438808365123
Using diff 0.0450414485698695 and condRate 0.166666666666667
Changed category 0 weights from 
0.208240187753142 to 0.208390325911686
-0.191794776331483 to -0.191644638172939
-0.474315472852289 to -0.474165334693745
-0.322810106884538 to -0.322659968725994
Changing layer 0's weights from 
-0.226973446064894 to -0.226823307906351
-0.598899902754729 to -0.598749764596185
-0.833249168449348 to -0.833099030290804
-0.798689918571417 to -0.798539780412873
-0.0508223492440638 to -0.0506722110855201
-0.733464302473968 to -0.733314164315424
-0.579634578877395 to -0.579484440718851
-0.60708898490042 to -0.606938846741876
-0.844580845886176 to -0.844430707727632
-0.888330901020472 to -0.888180762861928
Changing layer 1's weights from 
-0.883233988636439 to -0.883083850477895
-0.282065303975051 to -0.281915165816507
-0.407768042736953 to -0.40761790457841
-0.417673381024306 to -0.417523242865763
-0.602726163321441 to -0.602576025162897
-0.740963997298187 to -0.740813859139643
-0.470245273762648 to -0.470095135604104
-0.856048302703804 to -0.85589816454526
-0.444385321789687 to -0.444235183631144
-0.807032184654181 to -0.806882046495637
Changing layer 2's weights from 
0.002156047171646 to 0.00230618533018972
-0.551897438221878 to -0.551747300063334
-0.26730814760298 to -0.267158009444436
-0.0961080509957727 to -0.095957912837229
-0.273360760861342 to -0.273210622702798
-0.479370387249892 to -0.479220249091349
0.0394423287573399 to 0.0395924669158837
-0.032979698830551 to -0.0328295606720073
-0.764977993422454 to -0.76482785526391
-0.720020474844879 to -0.719870336686335
Changing layer 3's weights from 
-0.03089425151915 to -0.0307441133606063
-0.294447334462111 to -0.294297196303568
-0.0518374163445889 to -0.0516872781860451
-0.959276648100732 to -0.959126509942188
-0.174513312035506 to -0.174363173876963
-0.304761918240493 to -0.30461178008195
-0.634946765356963 to -0.634796627198419
-0.176433892899459 to -0.176283754740916
-0.508129866772598 to -0.507979728614054
-0.0352999884423679 to -0.0351498502838242
Changing layer 4's weights from 
-0.134573789292281 to -0.134423651133738
-0.343406589680617 to -0.343256451522074
-0.543884189778274 to -0.54373405161973
-0.256949456387465 to -0.256799318228921
-0.496202262097305 to -0.496052123938761
-0.264737041645949 to -0.264586903487405
-0.269095571690505 to -0.268945433531961
-0.73099309390158 to -0.730842955743036
-0.137297006302779 to -0.137146868144236
-0.569598944836563 to -0.569448806678019
Changing layer 5's weights from 
0.018206028288895 to 0.0183561664474387
-0.423867495709365 to -0.423717357550822
0.00132086688905401 to 0.00147100504759772
-0.934191314482397 to -0.934041176323853
-0.396922500782912 to -0.396772362624369
-0.795390443855232 to -0.795240305696688
-0.495425136738723 to -0.495274998580179
-0.422978909665053 to -0.42282877150651
-0.033376904183334 to -0.0332267660247903
-0.03988501613707 to -0.0397348779785263
Trying to learn from memory 55, 1, 0.02
sum 0.0375967569778095 distri 0.0217055619877138
Using diff 0.00649200574564327 and condRate 0.166666666666667
Changed category 1 weights from 
0.295959301026398 to 0.295980941045067
0.529274888070159 to 0.529296528088828
0.104349650176102 to 0.10437129019477
0.108442462714249 to 0.108464102732917
Changing layer 0's weights from 
-0.226823307906351 to -0.226801667887682
-0.598749764596185 to -0.598728124577517
-0.833099030290804 to -0.833077390272136
-0.798539780412873 to -0.798518140394205
-0.0506722110855201 to -0.0506505710668517
-0.733314164315424 to -0.733292524296756
-0.579484440718851 to -0.579462800700183
-0.606938846741876 to -0.606917206723207
-0.844430707727632 to -0.844409067708964
-0.888180762861928 to -0.888159122843259
Changing layer 1's weights from 
-0.883083850477895 to -0.883062210459227
-0.281915165816507 to -0.281893525797839
-0.40761790457841 to -0.407596264559741
-0.417523242865763 to -0.417501602847094
-0.602576025162897 to -0.602554385144229
-0.740813859139643 to -0.740792219120974
-0.470095135604104 to -0.470073495585436
-0.85589816454526 to -0.855876524526592
-0.444235183631144 to -0.444213543612475
-0.806882046495637 to -0.806860406476969
Changing layer 2's weights from 
0.00230618533018972 to 0.00232782534885817
-0.551747300063334 to -0.551725660044666
-0.267158009444436 to -0.267136369425768
-0.095957912837229 to -0.0959362728185606
-0.273210622702798 to -0.27318898268413
-0.479220249091349 to -0.47919860907268
0.0395924669158837 to 0.0396141069345521
-0.0328295606720073 to -0.0328079206533388
-0.76482785526391 to -0.764806215245242
-0.719870336686335 to -0.719848696667666
Changing layer 3's weights from 
-0.0307441133606063 to -0.0307224733419378
-0.294297196303568 to -0.294275556284899
-0.0516872781860451 to -0.0516656381673767
-0.959126509942188 to -0.95910486992352
-0.174363173876963 to -0.174341533858294
-0.30461178008195 to -0.304590140063281
-0.634796627198419 to -0.634774987179751
-0.176283754740916 to -0.176262114722247
-0.507979728614054 to -0.507958088595386
-0.0351498502838242 to -0.0351282102651557
Changing layer 4's weights from 
-0.134423651133738 to -0.134402011115069
-0.343256451522074 to -0.343234811503405
-0.54373405161973 to -0.543712411601061
-0.256799318228921 to -0.256777678210253
-0.496052123938761 to -0.496030483920092
-0.264586903487405 to -0.264565263468737
-0.268945433531961 to -0.268923793513293
-0.730842955743036 to -0.730821315724367
-0.137146868144236 to -0.137125228125567
-0.569448806678019 to -0.569427166659351
Changing layer 5's weights from 
0.0183561664474387 to 0.0183778064661072
-0.423717357550822 to -0.423695717532153
0.00147100504759772 to 0.00149264506626618
-0.934041176323853 to -0.934019536305185
-0.396772362624369 to -0.3967507226057
-0.795240305696688 to -0.79521866567802
-0.495274998580179 to -0.49525335856151
-0.42282877150651 to -0.422807131487841
-0.0332267660247903 to -0.0332051260061218
-0.0397348779785263 to -0.0397132379598578
10/5/2016 1:42:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:36 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
10/5/2016 1:42:36 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 56, 1, -0.2
sum 0.0376020697099137 distri 0.0216614613489422
Using diff 0.00654009093349305 and condRate 0.166666666666667
Changed category 1 weights from 
0.295980941045067 to 0.295762938010702
0.529296528088828 to 0.529078525054463
0.10437129019477 to 0.104153287160405
0.108464102732917 to 0.108246099698552
Changing layer 0's weights from 
-0.226801667887682 to -0.227019670922047
-0.598728124577517 to -0.598946127611881
-0.833077390272136 to -0.8332953933065
-0.798518140394205 to -0.798736143428569
-0.0506505710668517 to -0.0508685741012166
-0.733292524296756 to -0.73351052733112
-0.579462800700183 to -0.579680803734547
-0.606917206723207 to -0.607135209757572
-0.844409067708964 to -0.844627070743328
-0.888159122843259 to -0.888377125877624
Changing layer 1's weights from 
-0.883062210459227 to -0.883280213493591
-0.281893525797839 to -0.282111528832204
-0.407596264559741 to -0.407814267594106
-0.417501602847094 to -0.417719605881459
-0.602554385144229 to -0.602772388178593
-0.740792219120974 to -0.741010222155339
-0.470073495585436 to -0.470291498619801
-0.855876524526592 to -0.856094527560957
-0.444213543612475 to -0.44443154664684
-0.806860406476969 to -0.807078409511333
Changing layer 2's weights from 
0.00232782534885817 to 0.00210982231449324
-0.551725660044666 to -0.551943663079031
-0.267136369425768 to -0.267354372460133
-0.0959362728185606 to -0.0961542758529255
-0.27318898268413 to -0.273406985718495
-0.47919860907268 to -0.479416612107045
0.0396141069345521 to 0.0393961039001872
-0.0328079206533388 to -0.0330259236877037
-0.764806215245242 to -0.765024218279606
-0.719848696667666 to -0.720066699702031
Changing layer 3's weights from 
-0.0307224733419378 to -0.0309404763763028
-0.294275556284899 to -0.294493559319264
-0.0516656381673767 to -0.0518836412017416
-0.95910486992352 to -0.959322872957884
-0.174341533858294 to -0.174559536892659
-0.304590140063281 to -0.304808143097646
-0.634774987179751 to -0.634992990214115
-0.176262114722247 to -0.176480117756612
-0.507958088595386 to -0.508176091629751
-0.0351282102651557 to -0.0353462132995206
Changing layer 4's weights from 
-0.134402011115069 to -0.134620014149434
-0.343234811503405 to -0.34345281453777
-0.543712411601061 to -0.543930414635426
-0.256777678210253 to -0.256995681244618
-0.496030483920092 to -0.496248486954457
-0.264565263468737 to -0.264783266503102
-0.268923793513293 to -0.269141796547658
-0.730821315724367 to -0.731039318758732
-0.137125228125567 to -0.137343231159932
-0.569427166659351 to -0.569645169693715
Changing layer 5's weights from 
0.0183778064661072 to 0.0181598034317422
-0.423695717532153 to -0.423913720566518
0.00149264506626618 to 0.00127464203190124
-0.934019536305185 to -0.934237539339549
-0.3967507226057 to -0.396968725640065
-0.79521866567802 to -0.795436668712385
-0.49525335856151 to -0.495471361595875
-0.422807131487841 to -0.423025134522206
-0.0332051260061218 to -0.0334231290404867
-0.0397132379598578 to -0.0399312409942228
Trying to learn from memory 57, 0, -0.2
sum 0.0376058080783246 distri -0.0166931466470392
Using diff 0.0448975027057827 and condRate 0.166666666666667
Changed category 0 weights from 
0.208390325911686 to 0.206893742465859
-0.191644638172939 to -0.193141221618766
-0.474165334693745 to -0.475661918139572
-0.322659968725994 to -0.324156552171821
Changing layer 0's weights from 
-0.227019670922047 to -0.228516254367874
-0.598946127611881 to -0.600442711057708
-0.8332953933065 to -0.834791976752327
-0.798736143428569 to -0.800232726874396
-0.0508685741012166 to -0.0523651575470435
-0.73351052733112 to -0.735007110776947
-0.579680803734547 to -0.581177387180374
-0.607135209757572 to -0.608631793203399
-0.844627070743328 to -0.846123654189155
-0.888377125877624 to -0.889873709323451
Changing layer 1's weights from 
-0.883280213493591 to -0.884776796939418
-0.282111528832204 to -0.283608112278031
-0.407814267594106 to -0.409310851039933
-0.417719605881459 to -0.419216189327286
-0.602772388178593 to -0.60426897162442
-0.741010222155339 to -0.742506805601166
-0.470291498619801 to -0.471788082065628
-0.856094527560957 to -0.857591111006783
-0.44443154664684 to -0.445928130092667
-0.807078409511333 to -0.80857499295716
Changing layer 2's weights from 
0.00210982231449324 to 0.000613238868666317
-0.551943663079031 to -0.553440246524857
-0.267354372460133 to -0.26885095590596
-0.0961542758529255 to -0.0976508592987524
-0.273406985718495 to -0.274903569164322
-0.479416612107045 to -0.480913195552872
0.0393961039001872 to 0.0378995204543603
-0.0330259236877037 to -0.0345225071335307
-0.765024218279606 to -0.766520801725433
-0.720066699702031 to -0.721563283147858
Changing layer 3's weights from 
-0.0309404763763028 to -0.0324370598221297
-0.294493559319264 to -0.295990142765091
-0.0518836412017416 to -0.0533802246475685
-0.959322872957884 to -0.960819456403711
-0.174559536892659 to -0.176056120338486
-0.304808143097646 to -0.306304726543473
-0.634992990214115 to -0.636489573659942
-0.176480117756612 to -0.177976701202439
-0.508176091629751 to -0.509672675075577
-0.0353462132995206 to -0.0368427967453476
Changing layer 4's weights from 
-0.134620014149434 to -0.136116597595261
-0.34345281453777 to -0.344949397983597
-0.543930414635426 to -0.545426998081253
-0.256995681244618 to -0.258492264690445
-0.496248486954457 to -0.497745070400284
-0.264783266503102 to -0.266279849948929
-0.269141796547658 to -0.270638379993485
-0.731039318758732 to -0.732535902204559
-0.137343231159932 to -0.138839814605759
-0.569645169693715 to -0.571141753139542
Changing layer 5's weights from 
0.0181598034317422 to 0.0166632199859153
-0.423913720566518 to -0.425410304012345
0.00127464203190124 to -0.000221941413925678
-0.934237539339549 to -0.935734122785376
-0.396968725640065 to -0.398465309085892
-0.795436668712385 to -0.796933252158211
-0.495471361595875 to -0.496967945041702
-0.423025134522206 to -0.424521717968033
-0.0334231290404867 to -0.0349197124863137
-0.0399312409942228 to -0.0414278244400497
Trying to learn from memory 57, 1, -0.2
sum 0.0376058080783246 distri 0.0216598996402984
Using diff 0.0065444564184451 and condRate 0.166666666666667
Changed category 1 weights from 
0.295762938010702 to 0.29554478946017
0.529078525054463 to 0.528860376503931
0.104153287160405 to 0.103935138609873
0.108246099698552 to 0.10802795114802
Changing layer 0's weights from 
-0.228516254367874 to -0.228734402918406
-0.600442711057708 to -0.600660859608241
-0.834791976752327 to -0.83501012530286
-0.800232726874396 to -0.800450875424929
-0.0523651575470435 to -0.0525833060975757
-0.735007110776947 to -0.73522525932748
-0.581177387180374 to -0.581395535730907
-0.608631793203399 to -0.608849941753931
-0.846123654189155 to -0.846341802739688
-0.889873709323451 to -0.890091857873983
Changing layer 1's weights from 
-0.884776796939418 to -0.884994945489951
-0.283608112278031 to -0.283826260828563
-0.409310851039933 to -0.409528999590465
-0.419216189327286 to -0.419434337877818
-0.60426897162442 to -0.604487120174953
-0.742506805601166 to -0.742724954151698
-0.471788082065628 to -0.47200623061616
-0.857591111006783 to -0.857809259557316
-0.445928130092667 to -0.446146278643199
-0.80857499295716 to -0.808793141507693
Changing layer 2's weights from 
0.000613238868666317 to 0.000395090318134147
-0.553440246524857 to -0.55365839507539
-0.26885095590596 to -0.269069104456492
-0.0976508592987524 to -0.0978690078492846
-0.274903569164322 to -0.275121717714854
-0.480913195552872 to -0.481131344103404
0.0378995204543603 to 0.0376813719038281
-0.0345225071335307 to -0.0347406556840628
-0.766520801725433 to -0.766738950275966
-0.721563283147858 to -0.72178143169839
Changing layer 3's weights from 
-0.0324370598221297 to -0.0326552083726619
-0.295990142765091 to -0.296208291315623
-0.0533802246475685 to -0.0535983731981007
-0.960819456403711 to -0.961037604954244
-0.176056120338486 to -0.176274268889018
-0.306304726543473 to -0.306522875094005
-0.636489573659942 to -0.636707722210475
-0.177976701202439 to -0.178194849752971
-0.509672675075577 to -0.50989082362611
-0.0368427967453476 to -0.0370609452958797
Changing layer 4's weights from 
-0.136116597595261 to -0.136334746145793
-0.344949397983597 to -0.345167546534129
-0.545426998081253 to -0.545645146631785
-0.258492264690445 to -0.258710413240977
-0.497745070400284 to -0.497963218950816
-0.266279849948929 to -0.266497998499461
-0.270638379993485 to -0.270856528544017
-0.732535902204559 to -0.732754050755091
-0.138839814605759 to -0.139057963156291
-0.571141753139542 to -0.571359901690075
Changing layer 5's weights from 
0.0166632199859153 to 0.0164450714353832
-0.425410304012345 to -0.425628452562877
-0.000221941413925678 to -0.000440089964457849
-0.935734122785376 to -0.935952271335909
-0.398465309085892 to -0.398683457636424
-0.796933252158211 to -0.797151400708744
-0.496967945041702 to -0.497186093592234
-0.424521717968033 to -0.424739866518565
-0.0349197124863137 to -0.0351378610368458
-0.0414278244400497 to -0.0416459729905818
Trying to learn from memory 58, 1, -0.2
sum 0.0376052006594985 distri 0.0216606015896977
Using diff 0.00654329890492621 and condRate 0.166666666666667
Changed category 1 weights from 
0.29554478946017 to 0.295326679493422
0.528860376503931 to 0.528642266537183
0.103935138609873 to 0.103717028643126
0.10802795114802 to 0.107809841181273
Changing layer 0's weights from 
-0.228734402918406 to -0.228952512885154
-0.600660859608241 to -0.600878969574988
-0.83501012530286 to -0.835228235269607
-0.800450875424929 to -0.800668985391676
-0.0525833060975757 to -0.0528014160643233
-0.73522525932748 to -0.735443369294227
-0.581395535730907 to -0.581613645697654
-0.608849941753931 to -0.609068051720679
-0.846341802739688 to -0.846559912706435
-0.890091857873983 to -0.890309967840731
Changing layer 1's weights from 
-0.884994945489951 to -0.885213055456698
-0.283826260828563 to -0.284044370795311
-0.409528999590465 to -0.409747109557213
-0.419434337877818 to -0.419652447844566
-0.604487120174953 to -0.6047052301417
-0.742724954151698 to -0.742943064118446
-0.47200623061616 to -0.472224340582908
-0.857809259557316 to -0.858027369524063
-0.446146278643199 to -0.446364388609947
-0.808793141507693 to -0.80901125147444
Changing layer 2's weights from 
0.000395090318134147 to 0.000176980351386515
-0.55365839507539 to -0.553876505042137
-0.269069104456492 to -0.26928721442324
-0.0978690078492846 to -0.0980871178160322
-0.275121717714854 to -0.275339827681602
-0.481131344103404 to -0.481349454070152
0.0376813719038281 to 0.0374632619370805
-0.0347406556840628 to -0.0349587656508105
-0.766738950275966 to -0.766957060242713
-0.72178143169839 to -0.721999541665138
Changing layer 3's weights from 
-0.0326552083726619 to -0.0328733183394095
-0.296208291315623 to -0.296426401282371
-0.0535983731981007 to -0.0538164831648483
-0.961037604954244 to -0.961255714920991
-0.176274268889018 to -0.176492378855766
-0.306522875094005 to -0.306740985060753
-0.636707722210475 to -0.636925832177222
-0.178194849752971 to -0.178412959719719
-0.50989082362611 to -0.510108933592857
-0.0370609452958797 to -0.0372790552626274
Changing layer 4's weights from 
-0.136334746145793 to -0.136552856112541
-0.345167546534129 to -0.345385656500877
-0.545645146631785 to -0.545863256598533
-0.258710413240977 to -0.258928523207725
-0.497963218950816 to -0.498181328917564
-0.266497998499461 to -0.266716108466209
-0.270856528544017 to -0.271074638510765
-0.732754050755091 to -0.732972160721839
-0.139057963156291 to -0.139276073123039
-0.571359901690075 to -0.571578011656822
Changing layer 5's weights from 
0.0164450714353832 to 0.0162269614686355
-0.425628452562877 to -0.425846562529625
-0.000440089964457849 to -0.000658199931205481
-0.935952271335909 to -0.936170381302656
-0.398683457636424 to -0.398901567603172
-0.797151400708744 to -0.797369510675491
-0.497186093592234 to -0.497404203558982
-0.424739866518565 to -0.424957976485313
-0.0351378610368458 to -0.0353559710035935
-0.0416459729905818 to -0.0418640829573295
Trying to learn from memory 59, 0, -0.2
sum 0.0376061941938623 distri -0.0166966962467505
Using diff 0.0449013418921472 and condRate 0.166666666666667
Changed category 0 weights from 
0.206893742465859 to 0.205397031047151
-0.193141221618766 to -0.194637933037474
-0.475661918139572 to -0.47715862955828
-0.324156552171821 to -0.325653263590529
Changing layer 0's weights from 
-0.228952512885154 to -0.230449224303861
-0.600878969574988 to -0.602375680993696
-0.835228235269607 to -0.836724946688315
-0.800668985391676 to -0.802165696810384
-0.0528014160643233 to -0.054298127483031
-0.735443369294227 to -0.736940080712935
-0.581613645697654 to -0.583110357116362
-0.609068051720679 to -0.610564763139387
-0.846559912706435 to -0.848056624125143
-0.890309967840731 to -0.891806679259439
Changing layer 1's weights from 
-0.885213055456698 to -0.886709766875406
-0.284044370795311 to -0.285541082214018
-0.409747109557213 to -0.41124382097592
-0.419652447844566 to -0.421149159263273
-0.6047052301417 to -0.606201941560408
-0.742943064118446 to -0.744439775537154
-0.472224340582908 to -0.473721052001615
-0.858027369524063 to -0.859524080942771
-0.446364388609947 to -0.447861100028654
-0.80901125147444 to -0.810507962893148
Changing layer 2's weights from 
0.000176980351386515 to -0.00131973106732113
-0.553876505042137 to -0.555373216460845
-0.26928721442324 to -0.270783925841947
-0.0980871178160322 to -0.0995838292347399
-0.275339827681602 to -0.276836539100309
-0.481349454070152 to -0.48284616548886
0.0374632619370805 to 0.0359665505183728
-0.0349587656508105 to -0.0364554770695181
-0.766957060242713 to -0.768453771661421
-0.721999541665138 to -0.723496253083846
Changing layer 3's weights from 
-0.0328733183394095 to -0.0343700297581171
-0.296426401282371 to -0.297923112701078
-0.0538164831648483 to -0.055313194583556
-0.961255714920991 to -0.962752426339699
-0.176492378855766 to -0.177989090274473
-0.306740985060753 to -0.30823769647946
-0.636925832177222 to -0.63842254359593
-0.178412959719719 to -0.179909671138426
-0.510108933592857 to -0.511605645011565
-0.0372790552626274 to -0.038775766681335
Changing layer 4's weights from 
-0.136552856112541 to -0.138049567531249
-0.345385656500877 to -0.346882367919584
-0.545863256598533 to -0.547359968017241
-0.258928523207725 to -0.260425234626432
-0.498181328917564 to -0.499678040336272
-0.266716108466209 to -0.268212819884916
-0.271074638510765 to -0.272571349929472
-0.732972160721839 to -0.734468872140547
-0.139276073123039 to -0.140772784541747
-0.571578011656822 to -0.57307472307553
Changing layer 5's weights from 
0.0162269614686355 to 0.0147302500499279
-0.425846562529625 to -0.427343273948332
-0.000658199931205481 to -0.00215491134991313
-0.936170381302656 to -0.937667092721364
-0.398901567603172 to -0.400398279021879
-0.797369510675491 to -0.798866222094199
-0.497404203558982 to -0.49890091497769
-0.424957976485313 to -0.42645468790402
-0.0353559710035935 to -0.0368526824223011
-0.0418640829573295 to -0.0433607943760371
Trying to learn from memory 59, 0, -0.2
sum 0.0376061941938623 distri -0.0166966962467505
Using diff 0.0449013418921472 and condRate 0.166666666666667
Changed category 0 weights from 
0.205397031047151 to 0.203900319628444
-0.194637933037474 to -0.196134644456181
-0.47715862955828 to -0.478655340976987
-0.325653263590529 to -0.327149975009237
Changing layer 0's weights from 
-0.230449224303861 to -0.231945935722569
-0.602375680993696 to -0.603872392412404
-0.836724946688315 to -0.838221658107023
-0.802165696810384 to -0.803662408229092
-0.054298127483031 to -0.0557948389017386
-0.736940080712935 to -0.738436792131642
-0.583110357116362 to -0.58460706853507
-0.610564763139387 to -0.612061474558094
-0.848056624125143 to -0.84955333554385
-0.891806679259439 to -0.893303390678146
Changing layer 1's weights from 
-0.886709766875406 to -0.888206478294113
-0.285541082214018 to -0.287037793632726
-0.41124382097592 to -0.412740532394628
-0.421149159263273 to -0.422645870681981
-0.606201941560408 to -0.607698652979116
-0.744439775537154 to -0.745936486955861
-0.473721052001615 to -0.475217763420323
-0.859524080942771 to -0.861020792361479
-0.447861100028654 to -0.449357811447362
-0.810507962893148 to -0.812004674311856
Changing layer 2's weights from 
-0.00131973106732113 to -0.00281644248602878
-0.555373216460845 to -0.556869927879553
-0.270783925841947 to -0.272280637260655
-0.0995838292347399 to -0.101080540653448
-0.276836539100309 to -0.278333250519017
-0.48284616548886 to -0.484342876907567
0.0359665505183728 to 0.0344698390996652
-0.0364554770695181 to -0.0379521884882258
-0.768453771661421 to -0.769950483080128
-0.723496253083846 to -0.724992964502553
Changing layer 3's weights from 
-0.0343700297581171 to -0.0358667411768248
-0.297923112701078 to -0.299419824119786
-0.055313194583556 to -0.0568099060022636
-0.962752426339699 to -0.964249137758406
-0.177989090274473 to -0.179485801693181
-0.30823769647946 to -0.309734407898168
-0.63842254359593 to -0.639919255014638
-0.179909671138426 to -0.181406382557134
-0.511605645011565 to -0.513102356430273
-0.038775766681335 to -0.0402724781000427
Changing layer 4's weights from 
-0.138049567531249 to -0.139546278949956
-0.346882367919584 to -0.348379079338292
-0.547359968017241 to -0.548856679435948
-0.260425234626432 to -0.26192194604514
-0.499678040336272 to -0.501174751754979
-0.268212819884916 to -0.269709531303624
-0.272571349929472 to -0.27406806134818
-0.734468872140547 to -0.735965583559254
-0.140772784541747 to -0.142269495960454
-0.57307472307553 to -0.574571434494238
Changing layer 5's weights from 
0.0147302500499279 to 0.0132335386312202
-0.427343273948332 to -0.42883998536704
-0.00215491134991313 to -0.00365162276862077
-0.937667092721364 to -0.939163804140072
-0.400398279021879 to -0.401894990440587
-0.798866222094199 to -0.800362933512907
-0.49890091497769 to -0.500397626396397
-0.42645468790402 to -0.427951399322728
-0.0368526824223011 to -0.0383493938410088
-0.0433607943760371 to -0.0448575057947448
Trying to learn from memory 59, 0, -0.2
sum 0.0376061941938623 distri -0.0166966962467505
Using diff 0.0449013418921472 and condRate 0.166666666666667
Changed category 0 weights from 
0.203900319628444 to 0.202403608209736
-0.196134644456181 to -0.197631355874889
-0.478655340976987 to -0.480152052395695
-0.327149975009237 to -0.328646686427944
Changing layer 0's weights from 
-0.231945935722569 to -0.233442647141277
-0.603872392412404 to -0.605369103831111
-0.838221658107023 to -0.83971836952573
-0.803662408229092 to -0.805159119647799
-0.0557948389017386 to -0.0572915503204463
-0.738436792131642 to -0.73993350355035
-0.58460706853507 to -0.586103779953777
-0.612061474558094 to -0.613558185976802
-0.84955333554385 to -0.851050046962558
-0.893303390678146 to -0.894800102096854
Changing layer 1's weights from 
-0.888206478294113 to -0.889703189712821
-0.287037793632726 to -0.288534505051434
-0.412740532394628 to -0.414237243813336
-0.422645870681981 to -0.424142582100689
-0.607698652979116 to -0.609195364397823
-0.745936486955861 to -0.747433198374569
-0.475217763420323 to -0.476714474839031
-0.861020792361479 to -0.862517503780186
-0.449357811447362 to -0.45085452286607
-0.812004674311856 to -0.813501385730563
Changing layer 2's weights from 
-0.00281644248602878 to -0.00431315390473642
-0.556869927879553 to -0.55836663929826
-0.272280637260655 to -0.273777348679363
-0.101080540653448 to -0.102577252072155
-0.278333250519017 to -0.279829961937725
-0.484342876907567 to -0.485839588326275
0.0344698390996652 to 0.0329731276809575
-0.0379521884882258 to -0.0394488999069334
-0.769950483080128 to -0.771447194498836
-0.724992964502553 to -0.726489675921261
Changing layer 3's weights from 
-0.0358667411768248 to -0.0373634525955324
-0.299419824119786 to -0.300916535538494
-0.0568099060022636 to -0.0583066174209713
-0.964249137758406 to -0.965745849177114
-0.179485801693181 to -0.180982513111889
-0.309734407898168 to -0.311231119316876
-0.639919255014638 to -0.641415966433345
-0.181406382557134 to -0.182903093975842
-0.513102356430273 to -0.51459906784898
-0.0402724781000427 to -0.0417691895187503
Changing layer 4's weights from 
-0.139546278949956 to -0.141042990368664
-0.348379079338292 to -0.349875790757
-0.548856679435948 to -0.550353390854656
-0.26192194604514 to -0.263418657463848
-0.501174751754979 to -0.502671463173687
-0.269709531303624 to -0.271206242722332
-0.27406806134818 to -0.275564772766888
-0.735965583559254 to -0.737462294977962
-0.142269495960454 to -0.143766207379162
-0.574571434494238 to -0.576068145912945
Changing layer 5's weights from 
0.0132335386312202 to 0.0117368272125126
-0.42883998536704 to -0.430336696785748
-0.00365162276862077 to -0.00514833418732842
-0.939163804140072 to -0.940660515558779
-0.401894990440587 to -0.403391701859295
-0.800362933512907 to -0.801859644931614
-0.500397626396397 to -0.501894337815105
-0.427951399322728 to -0.429448110741436
-0.0383493938410088 to -0.0398461052597164
-0.0448575057947448 to -0.0463542172134524
Trying to learn from memory 59, 1, -0.2
sum 0.0376061941938623 distri 0.0216615470443576
Using diff 0.00654309860103911 and condRate 0.166666666666667
Changed category 1 weights from 
0.295326679493422 to 0.295108576203471
0.528642266537183 to 0.528424163247232
0.103717028643126 to 0.103498925353174
0.107809841181273 to 0.107591737891321
Changing layer 0's weights from 
-0.233442647141277 to -0.233660750431228
-0.605369103831111 to -0.605587207121063
-0.83971836952573 to -0.839936472815682
-0.805159119647799 to -0.80537722293775
-0.0572915503204463 to -0.0575096536103976
-0.73993350355035 to -0.740151606840301
-0.586103779953777 to -0.586321883243728
-0.613558185976802 to -0.613776289266753
-0.851050046962558 to -0.851268150252509
-0.894800102096854 to -0.895018205386805
Changing layer 1's weights from 
-0.889703189712821 to -0.889921293002772
-0.288534505051434 to -0.288752608341385
-0.414237243813336 to -0.414455347103287
-0.424142582100689 to -0.42436068539064
-0.609195364397823 to -0.609413467687774
-0.747433198374569 to -0.74765130166452
-0.476714474839031 to -0.476932578128982
-0.862517503780186 to -0.862735607070138
-0.45085452286607 to -0.451072626156021
-0.813501385730563 to -0.813719489020515
Changing layer 2's weights from 
-0.00431315390473642 to -0.00453125719468772
-0.55836663929826 to -0.558584742588212
-0.273777348679363 to -0.273995451969314
-0.102577252072155 to -0.102795355362106
-0.279829961937725 to -0.280048065227676
-0.485839588326275 to -0.486057691616226
0.0329731276809575 to 0.0327550243910062
-0.0394488999069334 to -0.0396670031968847
-0.771447194498836 to -0.771665297788787
-0.726489675921261 to -0.726707779211212
Changing layer 3's weights from 
-0.0373634525955324 to -0.0375815558854837
-0.300916535538494 to -0.301134638828445
-0.0583066174209713 to -0.0585247207109226
-0.965745849177114 to -0.965963952467065
-0.180982513111889 to -0.18120061640184
-0.311231119316876 to -0.311449222606827
-0.641415966433345 to -0.641634069723297
-0.182903093975842 to -0.183121197265793
-0.51459906784898 to -0.514817171138932
-0.0417691895187503 to -0.0419872928087016
Changing layer 4's weights from 
-0.141042990368664 to -0.141261093658615
-0.349875790757 to -0.350093894046951
-0.550353390854656 to -0.550571494144607
-0.263418657463848 to -0.263636760753799
-0.502671463173687 to -0.502889566463638
-0.271206242722332 to -0.271424346012283
-0.275564772766888 to -0.275782876056839
-0.737462294977962 to -0.737680398267913
-0.143766207379162 to -0.143984310669113
-0.576068145912945 to -0.576286249202897
Changing layer 5's weights from 
0.0117368272125126 to 0.0115187239225613
-0.430336696785748 to -0.430554800075699
-0.00514833418732842 to -0.00536643747727971
-0.940660515558779 to -0.94087861884873
-0.403391701859295 to -0.403609805149246
-0.801859644931614 to -0.802077748221566
-0.501894337815105 to -0.502112441105056
-0.429448110741436 to -0.429666214031387
-0.0398461052597164 to -0.0400642085496677
-0.0463542172134524 to -0.0465723205034037
Trying to learn from memory 60, 0, -0.2
sum 0.037605809961228 distri -0.0166931445054139
Using diff 0.0448975019763349 and condRate 0.166666666666667
Changed category 0 weights from 
0.202403608209736 to 0.200907024788224
-0.197631355874889 to -0.199127939296401
-0.480152052395695 to -0.481648635817207
-0.328646686427944 to -0.330143269849456
Changing layer 0's weights from 
-0.233660750431228 to -0.23515733385274
-0.605587207121063 to -0.607083790542575
-0.839936472815682 to -0.841433056237194
-0.80537722293775 to -0.806873806359263
-0.0575096536103976 to -0.0590062370319096
-0.740151606840301 to -0.741648190261814
-0.586321883243728 to -0.587818466665241
-0.613776289266753 to -0.615272872688265
-0.851268150252509 to -0.852764733674022
-0.895018205386805 to -0.896514788808317
Changing layer 1's weights from 
-0.889921293002772 to -0.891417876424284
-0.288752608341385 to -0.290249191762897
-0.414455347103287 to -0.415951930524799
-0.42436068539064 to -0.425857268812152
-0.609413467687774 to -0.610910051109287
-0.74765130166452 to -0.749147885086032
-0.476932578128982 to -0.478429161550494
-0.862735607070138 to -0.86423219049165
-0.451072626156021 to -0.452569209577533
-0.813719489020515 to -0.815216072442027
Changing layer 2's weights from 
-0.00453125719468772 to -0.00602784061619971
-0.558584742588212 to -0.560081326009724
-0.273995451969314 to -0.275492035390826
-0.102795355362106 to -0.104291938783618
-0.280048065227676 to -0.281544648649188
-0.486057691616226 to -0.487554275037738
0.0327550243910062 to 0.0312584409694942
-0.0396670031968847 to -0.0411635866183967
-0.771665297788787 to -0.7731618812103
-0.726707779211212 to -0.728204362632724
Changing layer 3's weights from 
-0.0375815558854837 to -0.0390781393069957
-0.301134638828445 to -0.302631222249957
-0.0585247207109226 to -0.0600213041324346
-0.965963952467065 to -0.967460535888578
-0.18120061640184 to -0.182697199823352
-0.311449222606827 to -0.312945806028339
-0.641634069723297 to -0.643130653144809
-0.183121197265793 to -0.184617780687305
-0.514817171138932 to -0.516313754560444
-0.0419872928087016 to -0.0434838762302136
Changing layer 4's weights from 
-0.141261093658615 to -0.142757677080127
-0.350093894046951 to -0.351590477468463
-0.550571494144607 to -0.552068077566119
-0.263636760753799 to -0.265133344175311
-0.502889566463638 to -0.50438614988515
-0.271424346012283 to -0.272920929433795
-0.275782876056839 to -0.277279459478351
-0.737680398267913 to -0.739176981689425
-0.143984310669113 to -0.145480894090625
-0.576286249202897 to -0.577782832624409
Changing layer 5's weights from 
0.0115187239225613 to 0.0100221405010493
-0.430554800075699 to -0.432051383497211
-0.00536643747727971 to -0.00686302089879171
-0.94087861884873 to -0.942375202270243
-0.403609805149246 to -0.405106388570758
-0.802077748221566 to -0.803574331643078
-0.502112441105056 to -0.503609024526568
-0.429666214031387 to -0.431162797452899
-0.0400642085496677 to -0.0415607919711797
-0.0465723205034037 to -0.0480689039249157
10/5/2016 1:42:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:42:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:16 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
Modified index 9's learning in memoryPool to -0.2
Modified index 10's learning in memoryPool to -0.2
Modified index 11's learning in memoryPool to -0.2
Modified index 12's learning in memoryPool to -0.2
Modified index 13's learning in memoryPool to -0.2
Modified index 14's learning in memoryPool to -0.2
Modified index 15's learning in memoryPool to -0.2
Modified index 16's learning in memoryPool to -0.2
Modified index 17's learning in memoryPool to -0.2
Modified index 18's learning in memoryPool to -0.2
Modified index 19's learning in memoryPool to -0.2
Modified index 20's learning in memoryPool to -0.2
Modified index 21's learning in memoryPool to -0.2
Modified index 22's learning in memoryPool to -0.2
Modified index 23's learning in memoryPool to -0.2
Modified index 24's learning in memoryPool to -0.2
Modified index 25's learning in memoryPool to -0.2
10/5/2016 1:43:16 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 61, 1, -0.2
sum 0.0375314827906928 distri 0.0219874920605498
Using diff 0.00616112003246983 and condRate 0.166666666666667
Changed category 1 weights from 
0.295108576203471 to 0.294903205532661
0.528424163247232 to 0.528218792576423
0.103498925353174 to 0.103293554682365
0.107591737891321 to 0.107386367220512
Changing layer 0's weights from 
-0.23515733385274 to -0.235362704523549
-0.607083790542575 to -0.607289161213384
-0.841433056237194 to -0.841638426908003
-0.806873806359263 to -0.807079177030072
-0.0590062370319096 to -0.0592116077027188
-0.741648190261814 to -0.741853560932623
-0.587818466665241 to -0.58802383733605
-0.615272872688265 to -0.615478243359075
-0.852764733674022 to -0.852970104344831
-0.896514788808317 to -0.896720159479127
Changing layer 1's weights from 
-0.891417876424284 to -0.891623247095094
-0.290249191762897 to -0.290454562433706
-0.415951930524799 to -0.416157301195608
-0.425857268812152 to -0.426062639482961
-0.610910051109287 to -0.611115421780096
-0.749147885086032 to -0.749353255756842
-0.478429161550494 to -0.478634532221303
-0.86423219049165 to -0.864437561162459
-0.452569209577533 to -0.452774580248342
-0.815216072442027 to -0.815421443112836
Changing layer 2's weights from 
-0.00602784061619971 to -0.00623321128700897
-0.560081326009724 to -0.560286696680533
-0.275492035390826 to -0.275697406061635
-0.104291938783618 to -0.104497309454428
-0.281544648649188 to -0.281750019319997
-0.487554275037738 to -0.487759645708547
0.0312584409694942 to 0.031053070298685
-0.0411635866183967 to -0.0413689572892059
-0.7731618812103 to -0.773367251881109
-0.728204362632724 to -0.728409733303534
Changing layer 3's weights from 
-0.0390781393069957 to -0.039283509977805
-0.302631222249957 to -0.302836592920766
-0.0600213041324346 to -0.0602266748032438
-0.967460535888578 to -0.967665906559387
-0.182697199823352 to -0.182902570494161
-0.312945806028339 to -0.313151176699148
-0.643130653144809 to -0.643336023815618
-0.184617780687305 to -0.184823151358114
-0.516313754560444 to -0.516519125231253
-0.0434838762302136 to -0.0436892469010228
Changing layer 4's weights from 
-0.142757677080127 to -0.142963047750936
-0.351590477468463 to -0.351795848139272
-0.552068077566119 to -0.552273448236929
-0.265133344175311 to -0.26533871484612
-0.50438614988515 to -0.50459152055596
-0.272920929433795 to -0.273126300104604
-0.277279459478351 to -0.27748483014916
-0.739176981689425 to -0.739382352360235
-0.145480894090625 to -0.145686264761434
-0.577782832624409 to -0.577988203295218
Changing layer 5's weights from 
0.0100221405010493 to 0.00981676983024004
-0.432051383497211 to -0.43225675416802
-0.00686302089879171 to -0.00706839156960096
-0.942375202270243 to -0.942580572941052
-0.405106388570758 to -0.405311759241567
-0.803574331643078 to -0.803779702313887
-0.503609024526568 to -0.503814395197378
-0.431162797452899 to -0.431368168123708
-0.0415607919711797 to -0.0417661626419889
-0.0480689039249157 to -0.048274274595725
Trying to learn from memory 62, 0, -0.2
sum 0.0375314827906928 distri -0.0176838653165217
Using diff 0.0458324774095414 and condRate 0.166666666666667
Changed category 0 weights from 
0.200907024788224 to 0.199379275518474
-0.199127939296401 to -0.200655688566151
-0.481648635817207 to -0.483176385086957
-0.330143269849456 to -0.331671019119206
Changing layer 0's weights from 
-0.235362704523549 to -0.236890453793299
-0.607289161213384 to -0.608816910483134
-0.841638426908003 to -0.843166176177753
-0.807079177030072 to -0.808606926299822
-0.0592116077027188 to -0.0607393569724688
-0.741853560932623 to -0.743381310202373
-0.58802383733605 to -0.5895515866058
-0.615478243359075 to -0.617005992628825
-0.852970104344831 to -0.854497853614581
-0.896720159479127 to -0.898247908748877
Changing layer 1's weights from 
-0.891623247095094 to -0.893150996364844
-0.290454562433706 to -0.291982311703456
-0.416157301195608 to -0.417685050465358
-0.426062639482961 to -0.427590388752711
-0.611115421780096 to -0.612643171049846
-0.749353255756842 to -0.750881005026592
-0.478634532221303 to -0.480162281491053
-0.864437561162459 to -0.865965310432209
-0.452774580248342 to -0.454302329518092
-0.815421443112836 to -0.816949192382586
Changing layer 2's weights from 
-0.00623321128700897 to -0.00776096055675891
-0.560286696680533 to -0.561814445950283
-0.275697406061635 to -0.277225155331385
-0.104497309454428 to -0.106025058724178
-0.281750019319997 to -0.283277768589747
-0.487759645708547 to -0.489287394978297
0.031053070298685 to 0.029525321028935
-0.0413689572892059 to -0.0428967065589559
-0.773367251881109 to -0.774895001150859
-0.728409733303534 to -0.729937482573284
Changing layer 3's weights from 
-0.039283509977805 to -0.0408112592475549
-0.302836592920766 to -0.304364342190516
-0.0602266748032438 to -0.0617544240729938
-0.967665906559387 to -0.969193655829137
-0.182902570494161 to -0.184430319763911
-0.313151176699148 to -0.314678925968898
-0.643336023815618 to -0.644863773085368
-0.184823151358114 to -0.186350900627864
-0.516519125231253 to -0.518046874501003
-0.0436892469010228 to -0.0452169961707728
Changing layer 4's weights from 
-0.142963047750936 to -0.144490797020686
-0.351795848139272 to -0.353323597409022
-0.552273448236929 to -0.553801197506679
-0.26533871484612 to -0.26686646411587
-0.50459152055596 to -0.50611926982571
-0.273126300104604 to -0.274654049374354
-0.27748483014916 to -0.27901257941891
-0.739382352360235 to -0.740910101629985
-0.145686264761434 to -0.147214014031184
-0.577988203295218 to -0.579515952564968
Changing layer 5's weights from 
0.00981676983024004 to 0.00828902056049009
-0.43225675416802 to -0.43378450343777
-0.00706839156960096 to -0.00859614083935091
-0.942580572941052 to -0.944108322210802
-0.405311759241567 to -0.406839508511317
-0.803779702313887 to -0.805307451583637
-0.503814395197378 to -0.505342144467128
-0.431368168123708 to -0.432895917393458
-0.0417661626419889 to -0.0432939119117389
-0.048274274595725 to -0.0498020238654749
Trying to learn from memory 63, 0, -0.2
sum 0.0375314827906928 distri -0.0176838653165217
Using diff 0.0458324774095414 and condRate 0.166666666666667
Changed category 0 weights from 
0.199379275518474 to 0.197851526248724
-0.200655688566151 to -0.202183437835901
-0.483176385086957 to -0.484704134356707
-0.331671019119206 to -0.333198768388956
Changing layer 0's weights from 
-0.236890453793299 to -0.238418203063049
-0.608816910483134 to -0.610344659752884
-0.843166176177753 to -0.844693925447503
-0.808606926299822 to -0.810134675569572
-0.0607393569724688 to -0.0622671062422187
-0.743381310202373 to -0.744909059472123
-0.5895515866058 to -0.59107933587555
-0.617005992628825 to -0.618533741898575
-0.854497853614581 to -0.856025602884331
-0.898247908748877 to -0.899775658018627
Changing layer 1's weights from 
-0.893150996364844 to -0.894678745634594
-0.291982311703456 to -0.293510060973206
-0.417685050465358 to -0.419212799735108
-0.427590388752711 to -0.429118138022461
-0.612643171049846 to -0.614170920319596
-0.750881005026592 to -0.752408754296342
-0.480162281491053 to -0.481690030760803
-0.865965310432209 to -0.867493059701959
-0.454302329518092 to -0.455830078787842
-0.816949192382586 to -0.818476941652336
Changing layer 2's weights from 
-0.00776096055675891 to -0.00928870982650886
-0.561814445950283 to -0.563342195220033
-0.277225155331385 to -0.278752904601135
-0.106025058724178 to -0.107552807993928
-0.283277768589747 to -0.284805517859497
-0.489287394978297 to -0.490815144248047
0.029525321028935 to 0.0279975717591851
-0.0428967065589559 to -0.0444244558287058
-0.774895001150859 to -0.776422750420609
-0.729937482573284 to -0.731465231843034
Changing layer 3's weights from 
-0.0408112592475549 to -0.0423390085173049
-0.304364342190516 to -0.305892091460266
-0.0617544240729938 to -0.0632821733427437
-0.969193655829137 to -0.970721405098887
-0.184430319763911 to -0.185958069033661
-0.314678925968898 to -0.316206675238648
-0.644863773085368 to -0.646391522355118
-0.186350900627864 to -0.187878649897614
-0.518046874501003 to -0.519574623770753
-0.0452169961707728 to -0.0467447454405227
Changing layer 4's weights from 
-0.144490797020686 to -0.146018546290436
-0.353323597409022 to -0.354851346678772
-0.553801197506679 to -0.555328946776429
-0.26686646411587 to -0.26839421338562
-0.50611926982571 to -0.50764701909546
-0.274654049374354 to -0.276181798644104
-0.27901257941891 to -0.28054032868866
-0.740910101629985 to -0.742437850899735
-0.147214014031184 to -0.148741763300934
-0.579515952564968 to -0.581043701834718
Changing layer 5's weights from 
0.00828902056049009 to 0.00676127129074014
-0.43378450343777 to -0.43531225270752
-0.00859614083935091 to -0.0101238901091009
-0.944108322210802 to -0.945636071480552
-0.406839508511317 to -0.408367257781067
-0.805307451583637 to -0.806835200853387
-0.505342144467128 to -0.506869893736878
-0.432895917393458 to -0.434423666663208
-0.0432939119117389 to -0.0448216611814888
-0.0498020238654749 to -0.0513297731352249
Trying to learn from memory 64, 1, -0.2
sum 0.0375314827906928 distri 0.0219874920605498
Using diff 0.00616112003246983 and condRate 0.166666666666667
Changed category 1 weights from 
0.294903205532661 to 0.294697834861852
0.528218792576423 to 0.528013421905613
0.103293554682365 to 0.103088184011556
0.107386367220512 to 0.107180996549703
Changing layer 0's weights from 
-0.238418203063049 to -0.238623573733858
-0.610344659752884 to -0.610550030423693
-0.844693925447503 to -0.844899296118312
-0.810134675569572 to -0.810340046240381
-0.0622671062422187 to -0.062472476913028
-0.744909059472123 to -0.745114430142932
-0.59107933587555 to -0.591284706546359
-0.618533741898575 to -0.618739112569384
-0.856025602884331 to -0.85623097355514
-0.899775658018627 to -0.899981028689436
Changing layer 1's weights from 
-0.894678745634594 to -0.894884116305403
-0.293510060973206 to -0.293715431644015
-0.419212799735108 to -0.419418170405918
-0.429118138022461 to -0.429323508693271
-0.614170920319596 to -0.614376290990405
-0.752408754296342 to -0.752614124967151
-0.481690030760803 to -0.481895401431612
-0.867493059701959 to -0.867698430372768
-0.455830078787842 to -0.456035449458652
-0.818476941652336 to -0.818682312323145
Changing layer 2's weights from 
-0.00928870982650886 to -0.00949408049731812
-0.563342195220033 to -0.563547565890842
-0.278752904601135 to -0.278958275271944
-0.107552807993928 to -0.107758178664737
-0.284805517859497 to -0.285010888530306
-0.490815144248047 to -0.491020514918857
0.0279975717591851 to 0.0277922010883758
-0.0444244558287058 to -0.0446298264995151
-0.776422750420609 to -0.776628121091418
-0.731465231843034 to -0.731670602513843
Changing layer 3's weights from 
-0.0423390085173049 to -0.0425443791881141
-0.305892091460266 to -0.306097462131076
-0.0632821733427437 to -0.063487544013553
-0.970721405098887 to -0.970926775769696
-0.185958069033661 to -0.18616343970447
-0.316206675238648 to -0.316412045909458
-0.646391522355118 to -0.646596893025927
-0.187878649897614 to -0.188084020568423
-0.519574623770753 to -0.519779994441562
-0.0467447454405227 to -0.046950116111332
Changing layer 4's weights from 
-0.146018546290436 to -0.146223916961245
-0.354851346678772 to -0.355056717349582
-0.555328946776429 to -0.555534317447238
-0.26839421338562 to -0.268599584056429
-0.50764701909546 to -0.507852389766269
-0.276181798644104 to -0.276387169314913
-0.28054032868866 to -0.280745699359469
-0.742437850899735 to -0.742643221570544
-0.148741763300934 to -0.148947133971743
-0.581043701834718 to -0.581249072505527
Changing layer 5's weights from 
0.00676127129074014 to 0.00655590061993089
-0.43531225270752 to -0.43551762337833
-0.0101238901091009 to -0.0103292607799101
-0.945636071480552 to -0.945841442151361
-0.408367257781067 to -0.408572628451877
-0.806835200853387 to -0.807040571524196
-0.506869893736878 to -0.507075264407687
-0.434423666663208 to -0.434629037334018
-0.0448216611814888 to -0.0450270318522981
-0.0513297731352249 to -0.0515351438060341
Trying to learn from memory 65, 1, -0.2
sum 0.0375314827906928 distri 0.0219874920605498
Using diff 0.00616112003246983 and condRate 0.166666666666667
Changed category 1 weights from 
0.294697834861852 to 0.294492464191043
0.528013421905613 to 0.527808051234804
0.103088184011556 to 0.102882813340747
0.107180996549703 to 0.106975625878894
Changing layer 0's weights from 
-0.238623573733858 to -0.238828944404668
-0.610550030423693 to -0.610755401094502
-0.844899296118312 to -0.845104666789121
-0.810340046240381 to -0.81054541691119
-0.062472476913028 to -0.0626778475838372
-0.745114430142932 to -0.745319800813741
-0.591284706546359 to -0.591490077217168
-0.618739112569384 to -0.618944483240193
-0.85623097355514 to -0.856436344225949
-0.899981028689436 to -0.900186399360245
Changing layer 1's weights from 
-0.894884116305403 to -0.895089486976212
-0.293715431644015 to -0.293920802314825
-0.419418170405918 to -0.419623541076727
-0.429323508693271 to -0.42952887936408
-0.614376290990405 to -0.614581661661214
-0.752614124967151 to -0.75281949563796
-0.481895401431612 to -0.482100772102422
-0.867698430372768 to -0.867903801043577
-0.456035449458652 to -0.456240820129461
-0.818682312323145 to -0.818887682993954
Changing layer 2's weights from 
-0.00949408049731812 to -0.00969945116812737
-0.563547565890842 to -0.563752936561651
-0.278958275271944 to -0.279163645942754
-0.107758178664737 to -0.107963549335546
-0.285010888530306 to -0.285216259201116
-0.491020514918857 to -0.491225885589666
0.0277922010883758 to 0.0275868304175666
-0.0446298264995151 to -0.0448351971703244
-0.776628121091418 to -0.776833491762227
-0.731670602513843 to -0.731875973184652
Changing layer 3's weights from 
-0.0425443791881141 to -0.0427497498589234
-0.306097462131076 to -0.306302832801885
-0.063487544013553 to -0.0636929146843622
-0.970926775769696 to -0.971132146440505
-0.18616343970447 to -0.18636881037528
-0.316412045909458 to -0.316617416580267
-0.646596893025927 to -0.646802263696736
-0.188084020568423 to -0.188289391239233
-0.519779994441562 to -0.519985365112371
-0.046950116111332 to -0.0471554867821413
Changing layer 4's weights from 
-0.146223916961245 to -0.146429287632055
-0.355056717349582 to -0.355262088020391
-0.555534317447238 to -0.555739688118047
-0.268599584056429 to -0.268804954727239
-0.507852389766269 to -0.508057760437078
-0.276387169314913 to -0.276592539985723
-0.280745699359469 to -0.280951070030279
-0.742643221570544 to -0.742848592241353
-0.148947133971743 to -0.149152504642553
-0.581249072505527 to -0.581454443176336
Changing layer 5's weights from 
0.00655590061993089 to 0.00635052994912163
-0.43551762337833 to -0.435722994049139
-0.0103292607799101 to -0.0105346314507194
-0.945841442151361 to -0.94604681282217
-0.408572628451877 to -0.408777999122686
-0.807040571524196 to -0.807245942195005
-0.507075264407687 to -0.507280635078496
-0.434629037334018 to -0.434834408004827
-0.0450270318522981 to -0.0452324025231074
-0.0515351438060341 to -0.0517405144768434
Trying to learn from memory 66, 1, -0.2
sum 0.0375314827906928 distri 0.0219874920605498
Using diff 0.00616112003246983 and condRate 0.166666666666667
Changed category 1 weights from 
0.294492464191043 to 0.294287093520233
0.527808051234804 to 0.527602680563995
0.102882813340747 to 0.102677442669937
0.106975625878894 to 0.106770255208084
Changing layer 0's weights from 
-0.238828944404668 to -0.239034315075477
-0.610755401094502 to -0.610960771765312
-0.845104666789121 to -0.845310037459931
-0.81054541691119 to -0.810750787582
-0.0626778475838372 to -0.0628832182546465
-0.745319800813741 to -0.745525171484551
-0.591490077217168 to -0.591695447887978
-0.618944483240193 to -0.619149853911003
-0.856436344225949 to -0.856641714896759
-0.900186399360245 to -0.900391770031055
Changing layer 1's weights from 
-0.895089486976212 to -0.895294857647022
-0.293920802314825 to -0.294126172985634
-0.419623541076727 to -0.419828911747536
-0.42952887936408 to -0.429734250034889
-0.614581661661214 to -0.614787032332024
-0.75281949563796 to -0.75302486630877
-0.482100772102422 to -0.482306142773231
-0.867903801043577 to -0.868109171714387
-0.456240820129461 to -0.45644619080027
-0.818887682993954 to -0.819093053664764
Changing layer 2's weights from 
-0.00969945116812737 to -0.00990482183893663
-0.563752936561651 to -0.563958307232461
-0.279163645942754 to -0.279369016613563
-0.107963549335546 to -0.108168920006355
-0.285216259201116 to -0.285421629871925
-0.491225885589666 to -0.491431256260475
0.0275868304175666 to 0.0273814597467573
-0.0448351971703244 to -0.0450405678411336
-0.776833491762227 to -0.777038862433037
-0.731875973184652 to -0.732081343855462
Changing layer 3's weights from 
-0.0427497498589234 to -0.0429551205297326
-0.306302832801885 to -0.306508203472694
-0.0636929146843622 to -0.0638982853551715
-0.971132146440505 to -0.971337517111315
-0.18636881037528 to -0.186574181046089
-0.316617416580267 to -0.316822787251076
-0.646802263696736 to -0.647007634367546
-0.188289391239233 to -0.188494761910042
-0.519985365112371 to -0.520190735783181
-0.0471554867821413 to -0.0473608574529505
Changing layer 4's weights from 
-0.146429287632055 to -0.146634658302864
-0.355262088020391 to -0.3554674586912
-0.555739688118047 to -0.555945058788856
-0.268804954727239 to -0.269010325398048
-0.508057760437078 to -0.508263131107887
-0.276592539985723 to -0.276797910656532
-0.280951070030279 to -0.281156440701088
-0.742848592241353 to -0.743053962912163
-0.149152504642553 to -0.149357875313362
-0.581454443176336 to -0.581659813847146
Changing layer 5's weights from 
0.00635052994912163 to 0.00614515927831237
-0.435722994049139 to -0.435928364719948
-0.0105346314507194 to -0.0107400021215286
-0.94604681282217 to -0.94625218349298
-0.408777999122686 to -0.408983369793495
-0.807245942195005 to -0.807451312865815
-0.507280635078496 to -0.507486005749305
-0.434834408004827 to -0.435039778675636
-0.0452324025231074 to -0.0454377731939166
-0.0517405144768434 to -0.0519458851476526
Trying to learn from memory 67, 1, -0.2
sum 0.0375314827906928 distri 0.0219874920605498
Using diff 0.00616112003246983 and condRate 0.166666666666667
Changed category 1 weights from 
0.294287093520233 to 0.294081722849424
0.527602680563995 to 0.527397309893185
0.102677442669937 to 0.102472071999128
0.106770255208084 to 0.106564884537275
Changing layer 0's weights from 
-0.239034315075477 to -0.239239685746286
-0.610960771765312 to -0.611166142436121
-0.845310037459931 to -0.84551540813074
-0.810750787582 to -0.810956158252809
-0.0628832182546465 to -0.0630885889254557
-0.745525171484551 to -0.74573054215536
-0.591695447887978 to -0.591900818558787
-0.619149853911003 to -0.619355224581812
-0.856641714896759 to -0.856847085567568
-0.900391770031055 to -0.900597140701864
Changing layer 1's weights from 
-0.895294857647022 to -0.895500228317831
-0.294126172985634 to -0.294331543656443
-0.419828911747536 to -0.420034282418345
-0.429734250034889 to -0.429939620705698
-0.614787032332024 to -0.614992403002833
-0.75302486630877 to -0.753230236979579
-0.482306142773231 to -0.48251151344404
-0.868109171714387 to -0.868314542385196
-0.45644619080027 to -0.456651561471079
-0.819093053664764 to -0.819298424335573
Changing layer 2's weights from 
-0.00990482183893663 to -0.0101101925097459
-0.563958307232461 to -0.56416367790327
-0.279369016613563 to -0.279574387284372
-0.108168920006355 to -0.108374290677165
-0.285421629871925 to -0.285627000542734
-0.491431256260475 to -0.491636626931284
0.0273814597467573 to 0.0271760890759481
-0.0450405678411336 to -0.0452459385119429
-0.777038862433037 to -0.777244233103846
-0.732081343855462 to -0.732286714526271
Changing layer 3's weights from 
-0.0429551205297326 to -0.0431604912005419
-0.306508203472694 to -0.306713574143503
-0.0638982853551715 to -0.0641036560259807
-0.971337517111315 to -0.971542887782124
-0.186574181046089 to -0.186779551716898
-0.316822787251076 to -0.317028157921885
-0.647007634367546 to -0.647213005038355
-0.188494761910042 to -0.188700132580851
-0.520190735783181 to -0.52039610645399
-0.0473608574529505 to -0.0475662281237598
Changing layer 4's weights from 
-0.146634658302864 to -0.146840028973673
-0.3554674586912 to -0.355672829362009
-0.555945058788856 to -0.556150429459666
-0.269010325398048 to -0.269215696068857
-0.508263131107887 to -0.508468501778697
-0.276797910656532 to -0.277003281327341
-0.281156440701088 to -0.281361811371897
-0.743053962912163 to -0.743259333582972
-0.149357875313362 to -0.149563245984171
-0.581659813847146 to -0.581865184517955
Changing layer 5's weights from 
0.00614515927831237 to 0.00593978860750312
-0.435928364719948 to -0.436133735390757
-0.0107400021215286 to -0.0109453727923379
-0.94625218349298 to -0.946457554163789
-0.408983369793495 to -0.409188740464304
-0.807451312865815 to -0.807656683536624
-0.507486005749305 to -0.507691376420115
-0.435039778675636 to -0.435245149346445
-0.0454377731939166 to -0.0456431438647259
-0.0519458851476526 to -0.0521512558184619
Trying to learn from memory 68, 0, -0.2
sum 0.0375314827906928 distri -0.0176838653165217
Using diff 0.0458324774095414 and condRate 0.166666666666667
Changed category 0 weights from 
0.197851526248724 to 0.196323776978974
-0.202183437835901 to -0.203711187105651
-0.484704134356707 to -0.486231883626457
-0.333198768388956 to -0.334726517658706
Changing layer 0's weights from 
-0.239239685746286 to -0.240767435016036
-0.611166142436121 to -0.612693891705871
-0.84551540813074 to -0.84704315740049
-0.810956158252809 to -0.812483907522559
-0.0630885889254557 to -0.0646163381952057
-0.74573054215536 to -0.74725829142511
-0.591900818558787 to -0.593428567828537
-0.619355224581812 to -0.620882973851562
-0.856847085567568 to -0.858374834837318
-0.900597140701864 to -0.902124889971614
Changing layer 1's weights from 
-0.895500228317831 to -0.897027977587581
-0.294331543656443 to -0.295859292926193
-0.420034282418345 to -0.421562031688095
-0.429939620705698 to -0.431467369975448
-0.614992403002833 to -0.616520152272583
-0.753230236979579 to -0.754757986249329
-0.48251151344404 to -0.48403926271379
-0.868314542385196 to -0.869842291654946
-0.456651561471079 to -0.458179310740829
-0.819298424335573 to -0.820826173605323
Changing layer 2's weights from 
-0.0101101925097459 to -0.0116379417794958
-0.56416367790327 to -0.56569142717302
-0.279574387284372 to -0.281102136554122
-0.108374290677165 to -0.109902039946915
-0.285627000542734 to -0.287154749812484
-0.491636626931284 to -0.493164376201034
0.0271760890759481 to 0.0256483398061981
-0.0452459385119429 to -0.0467736877816928
-0.777244233103846 to -0.778771982373596
-0.732286714526271 to -0.733814463796021
Changing layer 3's weights from 
-0.0431604912005419 to -0.0446882404702918
-0.306713574143503 to -0.308241323413253
-0.0641036560259807 to -0.0656314052957307
-0.971542887782124 to -0.973070637051874
-0.186779551716898 to -0.188307300986648
-0.317028157921885 to -0.318555907191635
-0.647213005038355 to -0.648740754308105
-0.188700132580851 to -0.190227881850601
-0.52039610645399 to -0.52192385572374
-0.0475662281237598 to -0.0490939773935097
Changing layer 4's weights from 
-0.146840028973673 to -0.148367778243423
-0.355672829362009 to -0.357200578631759
-0.556150429459666 to -0.557678178729416
-0.269215696068857 to -0.270743445338607
-0.508468501778697 to -0.509996251048447
-0.277003281327341 to -0.278531030597091
-0.281361811371897 to -0.282889560641647
-0.743259333582972 to -0.744787082852722
-0.149563245984171 to -0.151090995253921
-0.581865184517955 to -0.583392933787705
Changing layer 5's weights from 
0.00593978860750312 to 0.00441203933775317
-0.436133735390757 to -0.437661484660507
-0.0109453727923379 to -0.0124731220620878
-0.946457554163789 to -0.947985303433539
-0.409188740464304 to -0.410716489734054
-0.807656683536624 to -0.809184432806374
-0.507691376420115 to -0.509219125689865
-0.435245149346445 to -0.436772898616195
-0.0456431438647259 to -0.0471708931344758
-0.0521512558184619 to -0.0536790050882118
Trying to learn from memory 68, 0, -0.2
sum 0.0375314827906928 distri -0.0176838653165217
Using diff 0.0458324774095414 and condRate 0.166666666666667
Changed category 0 weights from 
0.196323776978974 to 0.194796027709224
-0.203711187105651 to -0.205238936375401
-0.486231883626457 to -0.487759632896207
-0.334726517658706 to -0.336254266928456
Changing layer 0's weights from 
-0.240767435016036 to -0.242295184285786
-0.612693891705871 to -0.614221640975621
-0.84704315740049 to -0.84857090667024
-0.812483907522559 to -0.814011656792309
-0.0646163381952057 to -0.0661440874649556
-0.74725829142511 to -0.74878604069486
-0.593428567828537 to -0.594956317098287
-0.620882973851562 to -0.622410723121312
-0.858374834837318 to -0.859902584107068
-0.902124889971614 to -0.903652639241364
Changing layer 1's weights from 
-0.897027977587581 to -0.898555726857331
-0.295859292926193 to -0.297387042195943
-0.421562031688095 to -0.423089780957845
-0.431467369975448 to -0.432995119245198
-0.616520152272583 to -0.618047901542333
-0.754757986249329 to -0.756285735519079
-0.48403926271379 to -0.48556701198354
-0.869842291654946 to -0.871370040924696
-0.458179310740829 to -0.459707060010579
-0.820826173605323 to -0.822353922875073
Changing layer 2's weights from 
-0.0116379417794958 to -0.0131656910492458
-0.56569142717302 to -0.56721917644277
-0.281102136554122 to -0.282629885823872
-0.109902039946915 to -0.111429789216665
-0.287154749812484 to -0.288682499082234
-0.493164376201034 to -0.494692125470784
0.0256483398061981 to 0.0241205905364482
-0.0467736877816928 to -0.0483014370514428
-0.778771982373596 to -0.780299731643346
-0.733814463796021 to -0.735342213065771
Changing layer 3's weights from 
-0.0446882404702918 to -0.0462159897400418
-0.308241323413253 to -0.309769072683003
-0.0656314052957307 to -0.0671591545654806
-0.973070637051874 to -0.974598386321624
-0.188307300986648 to -0.189835050256398
-0.318555907191635 to -0.320083656461385
-0.648740754308105 to -0.650268503577855
-0.190227881850601 to -0.191755631120351
-0.52192385572374 to -0.52345160499349
-0.0490939773935097 to -0.0506217266632597
Changing layer 4's weights from 
-0.148367778243423 to -0.149895527513173
-0.357200578631759 to -0.358728327901509
-0.557678178729416 to -0.559205927999166
-0.270743445338607 to -0.272271194608357
-0.509996251048447 to -0.511524000318197
-0.278531030597091 to -0.280058779866841
-0.282889560641647 to -0.284417309911397
-0.744787082852722 to -0.746314832122472
-0.151090995253921 to -0.152618744523671
-0.583392933787705 to -0.584920683057455
Changing layer 5's weights from 
0.00441203933775317 to 0.00288429006800322
-0.437661484660507 to -0.439189233930257
-0.0124731220620878 to -0.0140008713318378
-0.947985303433539 to -0.949513052703289
-0.410716489734054 to -0.412244239003804
-0.809184432806374 to -0.810712182076124
-0.509219125689865 to -0.510746874959615
-0.436772898616195 to -0.438300647885945
-0.0471708931344758 to -0.0486986424042258
-0.0536790050882118 to -0.0552067543579618
Trying to learn from memory 68, 0, -0.2
sum 0.0375314827906928 distri -0.0176838653165217
Using diff 0.0458324774095414 and condRate 0.166666666666667
Changed category 0 weights from 
0.194796027709224 to 0.193268278439474
-0.205238936375401 to -0.206766685645151
-0.487759632896207 to -0.489287382165957
-0.336254266928456 to -0.337782016198206
Changing layer 0's weights from 
-0.242295184285786 to -0.243822933555536
-0.614221640975621 to -0.615749390245371
-0.84857090667024 to -0.85009865593999
-0.814011656792309 to -0.815539406062059
-0.0661440874649556 to -0.0676718367347056
-0.74878604069486 to -0.75031378996461
-0.594956317098287 to -0.596484066368037
-0.622410723121312 to -0.623938472391062
-0.859902584107068 to -0.861430333376818
-0.903652639241364 to -0.905180388511114
Changing layer 1's weights from 
-0.898555726857331 to -0.900083476127081
-0.297387042195943 to -0.298914791465693
-0.423089780957845 to -0.424617530227595
-0.432995119245198 to -0.434522868514948
-0.618047901542333 to -0.619575650812083
-0.756285735519079 to -0.757813484788829
-0.48556701198354 to -0.48709476125329
-0.871370040924696 to -0.872897790194446
-0.459707060010579 to -0.461234809280329
-0.822353922875073 to -0.823881672144823
Changing layer 2's weights from 
-0.0131656910492458 to -0.0146934403189957
-0.56721917644277 to -0.56874692571252
-0.282629885823872 to -0.284157635093622
-0.111429789216665 to -0.112957538486414
-0.288682499082234 to -0.290210248351984
-0.494692125470784 to -0.496219874740534
0.0241205905364482 to 0.0225928412666982
-0.0483014370514428 to -0.0498291863211927
-0.780299731643346 to -0.781827480913096
-0.735342213065771 to -0.736869962335521
Changing layer 3's weights from 
-0.0462159897400418 to -0.0477437390097917
-0.309769072683003 to -0.311296821952753
-0.0671591545654806 to -0.0686869038352306
-0.974598386321624 to -0.976126135591374
-0.189835050256398 to -0.191362799526148
-0.320083656461385 to -0.321611405731135
-0.650268503577855 to -0.651796252847605
-0.191755631120351 to -0.193283380390101
-0.52345160499349 to -0.52497935426324
-0.0506217266632597 to -0.0521494759330096
Changing layer 4's weights from 
-0.149895527513173 to -0.151423276782923
-0.358728327901509 to -0.360256077171259
-0.559205927999166 to -0.560733677268916
-0.272271194608357 to -0.273798943878107
-0.511524000318197 to -0.513051749587947
-0.280058779866841 to -0.281586529136591
-0.284417309911397 to -0.285945059181147
-0.746314832122472 to -0.747842581392222
-0.152618744523671 to -0.154146493793421
-0.584920683057455 to -0.586448432327205
Changing layer 5's weights from 
0.00288429006800322 to 0.00135654079825327
-0.439189233930257 to -0.440716983200007
-0.0140008713318378 to -0.0155286206015877
-0.949513052703289 to -0.951040801973039
-0.412244239003804 to -0.413771988273554
-0.810712182076124 to -0.812239931345874
-0.510746874959615 to -0.512274624229365
-0.438300647885945 to -0.439828397155695
-0.0486986424042258 to -0.0502263916739757
-0.0552067543579618 to -0.0567345036277117
10/5/2016 1:43:17 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 68, 0, -0.2
sum 0.0375314827906928 distri -0.0176838653165217
Using diff 0.0458324774095414 and condRate 0.166666666666667
Changed category 0 weights from 
0.193268278439474 to 0.191740529169724
-0.206766685645151 to -0.208294434914901
-0.489287382165957 to -0.490815131435707
-0.337782016198206 to -0.339309765467956
Changing layer 0's weights from 
-0.243822933555536 to -0.245350682825286
-0.615749390245371 to -0.617277139515121
-0.85009865593999 to -0.85162640520974
-0.815539406062059 to -0.817067155331809
-0.0676718367347056 to -0.0691995860044555
-0.75031378996461 to -0.75184153923436
-0.596484066368037 to -0.598011815637787
-0.623938472391062 to -0.625466221660812
-0.861430333376818 to -0.862958082646568
-0.905180388511114 to -0.906708137780864
Changing layer 1's weights from 
-0.900083476127081 to -0.901611225396831
-0.298914791465693 to -0.300442540735443
-0.424617530227595 to -0.426145279497345
-0.434522868514948 to -0.436050617784698
-0.619575650812083 to -0.621103400081833
-0.757813484788829 to -0.759341234058579
-0.48709476125329 to -0.48862251052304
-0.872897790194446 to -0.874425539464196
-0.461234809280329 to -0.462762558550079
-0.823881672144823 to -0.825409421414573
Changing layer 2's weights from 
-0.0146934403189957 to -0.0162211895887457
-0.56874692571252 to -0.57027467498227
-0.284157635093622 to -0.285685384363372
-0.112957538486414 to -0.114485287756164
-0.290210248351984 to -0.291737997621734
-0.496219874740534 to -0.497747624010284
0.0225928412666982 to 0.0210650919969483
-0.0498291863211927 to -0.0513569355909427
-0.781827480913096 to -0.783355230182846
-0.736869962335521 to -0.738397711605271
Changing layer 3's weights from 
-0.0477437390097917 to -0.0492714882795417
-0.311296821952753 to -0.312824571222503
-0.0686869038352306 to -0.0702146531049805
-0.976126135591374 to -0.977653884861124
-0.191362799526148 to -0.192890548795898
-0.321611405731135 to -0.323139155000885
-0.651796252847605 to -0.653324002117355
-0.193283380390101 to -0.194811129659851
-0.52497935426324 to -0.52650710353299
-0.0521494759330096 to -0.0536772252027596
Changing layer 4's weights from 
-0.151423276782923 to -0.152951026052673
-0.360256077171259 to -0.361783826441009
-0.560733677268916 to -0.562261426538666
-0.273798943878107 to -0.275326693147857
-0.513051749587947 to -0.514579498857697
-0.281586529136591 to -0.283114278406341
-0.285945059181147 to -0.287472808450897
-0.747842581392222 to -0.749370330661972
-0.154146493793421 to -0.155674243063171
-0.586448432327205 to -0.587976181596955
Changing layer 5's weights from 
0.00135654079825327 to -0.000171208471496681
-0.440716983200007 to -0.442244732469757
-0.0155286206015877 to -0.0170563698713377
-0.951040801973039 to -0.952568551242789
-0.413771988273554 to -0.415299737543304
-0.812239931345874 to -0.813767680615624
-0.512274624229365 to -0.513802373499115
-0.439828397155695 to -0.441356146425445
-0.0502263916739757 to -0.0517541409437257
-0.0567345036277117 to -0.0582622528974617
Trying to learn from memory 68, 1, -0.2
sum 0.0375314827906928 distri 0.0219874920605498
Using diff 0.00616112003246983 and condRate 0.166666666666667
Changed category 1 weights from 
0.294081722849424 to 0.293876352178615
0.527397309893185 to 0.527191939222376
0.102472071999128 to 0.102266701328319
0.106564884537275 to 0.106359513866466
Changing layer 0's weights from 
-0.245350682825286 to -0.245556053496095
-0.617277139515121 to -0.61748251018593
-0.85162640520974 to -0.851831775880549
-0.817067155331809 to -0.817272526002618
-0.0691995860044555 to -0.0694049566752648
-0.75184153923436 to -0.752046909905169
-0.598011815637787 to -0.598217186308596
-0.625466221660812 to -0.625671592331621
-0.862958082646568 to -0.863163453317377
-0.906708137780864 to -0.906913508451673
Changing layer 1's weights from 
-0.901611225396831 to -0.90181659606764
-0.300442540735443 to -0.300647911406252
-0.426145279497345 to -0.426350650168155
-0.436050617784698 to -0.436255988455508
-0.621103400081833 to -0.621308770752642
-0.759341234058579 to -0.759546604729388
-0.48862251052304 to -0.488827881193849
-0.874425539464196 to -0.874630910135005
-0.462762558550079 to -0.462967929220889
-0.825409421414573 to -0.825614792085382
Changing layer 2's weights from 
-0.0162211895887457 to -0.0164265602595549
-0.57027467498227 to -0.570480045653079
-0.285685384363372 to -0.285890755034181
-0.114485287756164 to -0.114690658426974
-0.291737997621734 to -0.291943368292543
-0.497747624010284 to -0.497952994681094
0.0210650919969483 to 0.020859721326139
-0.0513569355909427 to -0.0515623062617519
-0.783355230182846 to -0.783560600853655
-0.738397711605271 to -0.73860308227608
Changing layer 3's weights from 
-0.0492714882795417 to -0.0494768589503509
-0.312824571222503 to -0.313029941893313
-0.0702146531049805 to -0.0704200237757898
-0.977653884861124 to -0.977859255531933
-0.192890548795898 to -0.193095919466707
-0.323139155000885 to -0.323344525671695
-0.653324002117355 to -0.653529372788164
-0.194811129659851 to -0.19501650033066
-0.52650710353299 to -0.526712474203799
-0.0536772252027596 to -0.0538825958735688
Changing layer 4's weights from 
-0.152951026052673 to -0.153156396723482
-0.361783826441009 to -0.361989197111819
-0.562261426538666 to -0.562466797209475
-0.275326693147857 to -0.275532063818666
-0.514579498857697 to -0.514784869528506
-0.283114278406341 to -0.28331964907715
-0.287472808450897 to -0.287678179121706
-0.749370330661972 to -0.749575701332781
-0.155674243063171 to -0.15587961373398
-0.587976181596955 to -0.588181552267764
Changing layer 5's weights from 
-0.000171208471496681 to -0.000376579142305937
-0.442244732469757 to -0.442450103140567
-0.0170563698713377 to -0.0172617405421469
-0.952568551242789 to -0.952773921913598
-0.415299737543304 to -0.415505108214114
-0.813767680615624 to -0.813973051286433
-0.513802373499115 to -0.514007744169924
-0.441356146425445 to -0.441561517096255
-0.0517541409437257 to -0.0519595116145349
-0.0582622528974617 to -0.0584676235682709
Trying to learn from memory 69, 0, -0.2
sum 0.0375314827906928 distri -0.0176838653165217
Using diff 0.0458324774095414 and condRate 0.166666666666667
Changed category 0 weights from 
0.191740529169724 to 0.190212779899974
-0.208294434914901 to -0.209822184184651
-0.490815131435707 to -0.492342880705457
-0.339309765467956 to -0.340837514737706
Changing layer 0's weights from 
-0.245556053496095 to -0.247083802765845
-0.61748251018593 to -0.61901025945568
-0.851831775880549 to -0.853359525150299
-0.817272526002618 to -0.818800275272368
-0.0694049566752648 to -0.0709327059450147
-0.752046909905169 to -0.753574659174919
-0.598217186308596 to -0.599744935578346
-0.625671592331621 to -0.627199341601371
-0.863163453317377 to -0.864691202587127
-0.906913508451673 to -0.908441257721423
Changing layer 1's weights from 
-0.90181659606764 to -0.90334434533739
-0.300647911406252 to -0.302175660676002
-0.426350650168155 to -0.427878399437905
-0.436255988455508 to -0.437783737725258
-0.621308770752642 to -0.622836520022392
-0.759546604729388 to -0.761074353999138
-0.488827881193849 to -0.490355630463599
-0.874630910135005 to -0.876158659404755
-0.462967929220889 to -0.464495678490639
-0.825614792085382 to -0.827142541355132
Changing layer 2's weights from 
-0.0164265602595549 to -0.0179543095293049
-0.570480045653079 to -0.572007794922829
-0.285890755034181 to -0.287418504303931
-0.114690658426974 to -0.116218407696724
-0.291943368292543 to -0.293471117562293
-0.497952994681094 to -0.499480743950844
0.020859721326139 to 0.0193319720563891
-0.0515623062617519 to -0.0530900555315019
-0.783560600853655 to -0.785088350123405
-0.73860308227608 to -0.74013083154583
Changing layer 3's weights from 
-0.0494768589503509 to -0.0510046082201009
-0.313029941893313 to -0.314557691163062
-0.0704200237757898 to -0.0719477730455397
-0.977859255531933 to -0.979387004801683
-0.193095919466707 to -0.194623668736457
-0.323344525671695 to -0.324872274941445
-0.653529372788164 to -0.655057122057914
-0.19501650033066 to -0.19654424960041
-0.526712474203799 to -0.528240223473549
-0.0538825958735688 to -0.0554103451433188
Changing layer 4's weights from 
-0.153156396723482 to -0.154684145993232
-0.361989197111819 to -0.363516946381569
-0.562466797209475 to -0.563994546479225
-0.275532063818666 to -0.277059813088416
-0.514784869528506 to -0.516312618798256
-0.28331964907715 to -0.2848473983469
-0.287678179121706 to -0.289205928391456
-0.749575701332781 to -0.751103450602531
-0.15587961373398 to -0.15740736300373
-0.588181552267764 to -0.589709301537514
Changing layer 5's weights from 
-0.000376579142305937 to -0.00190432841205589
-0.442450103140567 to -0.443977852410317
-0.0172617405421469 to -0.0187894898118969
-0.952773921913598 to -0.954301671183348
-0.415505108214114 to -0.417032857483864
-0.813973051286433 to -0.815500800556183
-0.514007744169924 to -0.515535493439674
-0.441561517096255 to -0.443089266366005
-0.0519595116145349 to -0.0534872608842849
-0.0584676235682709 to -0.0599953728380209
Trying to learn from memory 69, 1, -0.2
sum 0.0375314827906928 distri 0.0219874920605498
Using diff 0.00616112003246983 and condRate 0.166666666666667
Changed category 1 weights from 
0.293876352178615 to 0.293670981507806
0.527191939222376 to 0.526986568551567
0.102266701328319 to 0.10206133065751
0.106359513866466 to 0.106154143195657
Changing layer 0's weights from 
-0.247083802765845 to -0.247289173436654
-0.61901025945568 to -0.619215630126489
-0.853359525150299 to -0.853564895821108
-0.818800275272368 to -0.819005645943177
-0.0709327059450147 to -0.071138076615824
-0.753574659174919 to -0.753780029845728
-0.599744935578346 to -0.599950306249155
-0.627199341601371 to -0.62740471227218
-0.864691202587127 to -0.864896573257936
-0.908441257721423 to -0.908646628392232
Changing layer 1's weights from 
-0.90334434533739 to -0.903549716008199
-0.302175660676002 to -0.302381031346812
-0.427878399437905 to -0.428083770108714
-0.437783737725258 to -0.437989108396067
-0.622836520022392 to -0.623041890693201
-0.761074353999138 to -0.761279724669947
-0.490355630463599 to -0.490561001134409
-0.876158659404755 to -0.876364030075564
-0.464495678490639 to -0.464701049161448
-0.827142541355132 to -0.827347912025941
Changing layer 2's weights from 
-0.0179543095293049 to -0.0181596802001141
-0.572007794922829 to -0.572213165593638
-0.287418504303931 to -0.287623874974741
-0.116218407696724 to -0.116423778367533
-0.293471117562293 to -0.293676488233103
-0.499480743950844 to -0.499686114621653
0.0193319720563891 to 0.0191266013855798
-0.0530900555315019 to -0.0532954262023111
-0.785088350123405 to -0.785293720794214
-0.74013083154583 to -0.740336202216639
Changing layer 3's weights from 
-0.0510046082201009 to -0.0512099788909101
-0.314557691163062 to -0.314763061833872
-0.0719477730455397 to -0.072153143716349
-0.979387004801683 to -0.979592375472492
-0.194623668736457 to -0.194829039407266
-0.324872274941445 to -0.325077645612254
-0.655057122057914 to -0.655262492728723
-0.19654424960041 to -0.196749620271219
-0.528240223473549 to -0.528445594144358
-0.0554103451433188 to -0.055615715814128
Changing layer 4's weights from 
-0.154684145993232 to -0.154889516664041
-0.363516946381569 to -0.363722317052378
-0.563994546479225 to -0.564199917150034
-0.277059813088416 to -0.277265183759226
-0.516312618798256 to -0.516517989469065
-0.2848473983469 to -0.28505276901771
-0.289205928391456 to -0.289411299062266
-0.751103450602531 to -0.75130882127334
-0.15740736300373 to -0.157612733674539
-0.589709301537514 to -0.589914672208323
Changing layer 5's weights from 
-0.00190432841205589 to -0.00210969908286514
-0.443977852410317 to -0.444183223081126
-0.0187894898118969 to -0.0189948604827061
-0.954301671183348 to -0.954507041854157
-0.417032857483864 to -0.417238228154673
-0.815500800556183 to -0.815706171226992
-0.515535493439674 to -0.515740864110483
-0.443089266366005 to -0.443294637036814
-0.0534872608842849 to -0.0536926315550941
-0.0599953728380209 to -0.0602007435088301
Trying to learn from memory 70, 1, -0.2
sum 0.0375314827906928 distri 0.0219874920605498
Using diff 0.00616112003246983 and condRate 0.166666666666667
Changed category 1 weights from 
0.293670981507806 to 0.293465610836996
0.526986568551567 to 0.526781197880758
0.10206133065751 to 0.1018559599867
0.106154143195657 to 0.105948772524847
Changing layer 0's weights from 
-0.247289173436654 to -0.247494544107464
-0.619215630126489 to -0.619421000797299
-0.853564895821108 to -0.853770266491918
-0.819005645943177 to -0.819211016613987
-0.071138076615824 to -0.0713434472866332
-0.753780029845728 to -0.753985400516538
-0.599950306249155 to -0.600155676919965
-0.62740471227218 to -0.627610082942989
-0.864896573257936 to -0.865101943928746
-0.908646628392232 to -0.908851999063041
Changing layer 1's weights from 
-0.903549716008199 to -0.903755086679009
-0.302381031346812 to -0.302586402017621
-0.428083770108714 to -0.428289140779523
-0.437989108396067 to -0.438194479066876
-0.623041890693201 to -0.623247261364011
-0.761279724669947 to -0.761485095340756
-0.490561001134409 to -0.490766371805218
-0.876364030075564 to -0.876569400746374
-0.464701049161448 to -0.464906419832257
-0.827347912025941 to -0.827553282696751
Changing layer 2's weights from 
-0.0181596802001141 to -0.0183650508709234
-0.572213165593638 to -0.572418536264448
-0.287623874974741 to -0.28782924564555
-0.116423778367533 to -0.116629149038342
-0.293676488233103 to -0.293881858903912
-0.499686114621653 to -0.499891485292462
0.0191266013855798 to 0.0189212307147705
-0.0532954262023111 to -0.0535007968731204
-0.785293720794214 to -0.785499091465024
-0.740336202216639 to -0.740541572887448
Changing layer 3's weights from 
-0.0512099788909101 to -0.0514153495617194
-0.314763061833872 to -0.314968432504681
-0.072153143716349 to -0.0723585143871582
-0.979592375472492 to -0.979797746143302
-0.194829039407266 to -0.195034410078076
-0.325077645612254 to -0.325283016283063
-0.655262492728723 to -0.655467863399533
-0.196749620271219 to -0.196954990942029
-0.528445594144358 to -0.528650964815168
-0.055615715814128 to -0.0558210864849373
Changing layer 4's weights from 
-0.154889516664041 to -0.155094887334851
-0.363722317052378 to -0.363927687723187
-0.564199917150034 to -0.564405287820843
-0.277265183759226 to -0.277470554430035
-0.516517989469065 to -0.516723360139874
-0.28505276901771 to -0.285258139688519
-0.289411299062266 to -0.289616669733075
-0.75130882127334 to -0.751514191944149
-0.157612733674539 to -0.157818104345349
-0.589914672208323 to -0.590120042879133
Changing layer 5's weights from 
-0.00210969908286514 to -0.0023150697536744
-0.444183223081126 to -0.444388593751935
-0.0189948604827061 to -0.0192002311535154
-0.954507041854157 to -0.954712412524967
-0.417238228154673 to -0.417443598825482
-0.815706171226992 to -0.815911541897802
-0.515740864110483 to -0.515946234781292
-0.443294637036814 to -0.443500007707623
-0.0536926315550941 to -0.0538980022259034
-0.0602007435088301 to -0.0604061141796394
Trying to learn from memory 67, 1, -0.2
sum 0.0375314827906928 distri 0.0219874920605498
Using diff 0.00616112003246983 and condRate 0.166666666666667
Changed category 1 weights from 
0.293465610836996 to 0.293260240166187
0.526781197880758 to 0.526575827209948
0.1018559599867 to 0.101650589315891
0.105948772524847 to 0.105743401854038
Changing layer 0's weights from 
-0.247494544107464 to -0.247699914778273
-0.619421000797299 to -0.619626371468108
-0.853770266491918 to -0.853975637162727
-0.819211016613987 to -0.819416387284796
-0.0713434472866332 to -0.0715488179574425
-0.753985400516538 to -0.754190771187347
-0.600155676919965 to -0.600361047590774
-0.627610082942989 to -0.627815453613799
-0.865101943928746 to -0.865307314599555
-0.908851999063041 to -0.909057369733851
Changing layer 1's weights from 
-0.903755086679009 to -0.903960457349818
-0.302586402017621 to -0.30279177268843
-0.428289140779523 to -0.428494511450332
-0.438194479066876 to -0.438399849737685
-0.623247261364011 to -0.62345263203482
-0.761485095340756 to -0.761690466011566
-0.490766371805218 to -0.490971742476027
-0.876569400746374 to -0.876774771417183
-0.464906419832257 to -0.465111790503066
-0.827553282696751 to -0.82775865336756
Changing layer 2's weights from 
-0.0183650508709234 to -0.0185704215417327
-0.572418536264448 to -0.572623906935257
-0.28782924564555 to -0.288034616316359
-0.116629149038342 to -0.116834519709151
-0.293881858903912 to -0.294087229574721
-0.499891485292462 to -0.500096855963271
0.0189212307147705 to 0.0187158600439613
-0.0535007968731204 to -0.0537061675439296
-0.785499091465024 to -0.785704462135833
-0.740541572887448 to -0.740746943558258
Changing layer 3's weights from 
-0.0514153495617194 to -0.0516207202325287
-0.314968432504681 to -0.31517380317549
-0.0723585143871582 to -0.0725638850579675
-0.979797746143302 to -0.980003116814111
-0.195034410078076 to -0.195239780748885
-0.325283016283063 to -0.325488386953872
-0.655467863399533 to -0.655673234070342
-0.196954990942029 to -0.197160361612838
-0.528650964815168 to -0.528856335485977
-0.0558210864849373 to -0.0560264571557465
Changing layer 4's weights from 
-0.155094887334851 to -0.15530025800566
-0.363927687723187 to -0.364133058393996
-0.564405287820843 to -0.564610658491653
-0.277470554430035 to -0.277675925100844
-0.516723360139874 to -0.516928730810684
-0.285258139688519 to -0.285463510359328
-0.289616669733075 to -0.289822040403884
-0.751514191944149 to -0.751719562614959
-0.157818104345349 to -0.158023475016158
-0.590120042879133 to -0.590325413549942
Changing layer 5's weights from 
-0.0023150697536744 to -0.00252044042448365
-0.444388593751935 to -0.444593964422744
-0.0192002311535154 to -0.0194056018243247
-0.954712412524967 to -0.954917783195776
-0.417443598825482 to -0.417648969496291
-0.815911541897802 to -0.816116912568611
-0.515946234781292 to -0.516151605452102
-0.443500007707623 to -0.443705378378432
-0.0538980022259034 to -0.0541033728967126
-0.0604061141796394 to -0.0606114848504487
Trying to learn from memory 68, 1, -0.2
sum 0.0375314827906928 distri 0.0219874920605498
Using diff 0.00616112003246983 and condRate 0.166666666666667
Changed category 1 weights from 
0.293260240166187 to 0.293054869495378
0.526575827209948 to 0.526370456539139
0.101650589315891 to 0.101445218645082
0.105743401854038 to 0.105538031183229
Changing layer 0's weights from 
-0.247699914778273 to -0.247905285449082
-0.619626371468108 to -0.619831742138917
-0.853975637162727 to -0.854181007833536
-0.819416387284796 to -0.819621757955605
-0.0715488179574425 to -0.0717541886282518
-0.754190771187347 to -0.754396141858156
-0.600361047590774 to -0.600566418261583
-0.627815453613799 to -0.628020824284608
-0.865307314599555 to -0.865512685270364
-0.909057369733851 to -0.90926274040466
Changing layer 1's weights from 
-0.903960457349818 to -0.904165828020627
-0.30279177268843 to -0.30299714335924
-0.428494511450332 to -0.428699882121142
-0.438399849737685 to -0.438605220408495
-0.62345263203482 to -0.623658002705629
-0.761690466011566 to -0.761895836682375
-0.490971742476027 to -0.491177113146837
-0.876774771417183 to -0.876980142087992
-0.465111790503066 to -0.465317161173876
-0.82775865336756 to -0.827964024038369
Changing layer 2's weights from 
-0.0185704215417327 to -0.0187757922125419
-0.572623906935257 to -0.572829277606066
-0.288034616316359 to -0.288239986987169
-0.116834519709151 to -0.117039890379961
-0.294087229574721 to -0.294292600245531
-0.500096855963271 to -0.500302226634081
0.0187158600439613 to 0.018510489373152
-0.0537061675439296 to -0.0539115382147389
-0.785704462135833 to -0.785909832806642
-0.740746943558258 to -0.740952314229067
Changing layer 3's weights from 
-0.0516207202325287 to -0.0518260909033379
-0.31517380317549 to -0.3153791738463
-0.0725638850579675 to -0.0727692557287768
-0.980003116814111 to -0.98020848748492
-0.195239780748885 to -0.195445151419694
-0.325488386953872 to -0.325693757624682
-0.655673234070342 to -0.655878604741151
-0.197160361612838 to -0.197365732283647
-0.528856335485977 to -0.529061706156786
-0.0560264571557465 to -0.0562318278265558
Changing layer 4's weights from 
-0.15530025800566 to -0.155505628676469
-0.364133058393996 to -0.364338429064806
-0.564610658491653 to -0.564816029162462
-0.277675925100844 to -0.277881295771653
-0.516928730810684 to -0.517134101481493
-0.285463510359328 to -0.285668881030138
-0.289822040403884 to -0.290027411074694
-0.751719562614959 to -0.751924933285768
-0.158023475016158 to -0.158228845686967
-0.590325413549942 to -0.590530784220751
Changing layer 5's weights from 
-0.00252044042448365 to -0.00272581109529291
-0.444593964422744 to -0.444799335093554
-0.0194056018243247 to -0.0196109724951339
-0.954917783195776 to -0.955123153866585
-0.417648969496291 to -0.417854340167101
-0.816116912568611 to -0.81632228323942
-0.516151605452102 to -0.516356976122911
-0.443705378378432 to -0.443910749049242
-0.0541033728967126 to -0.0543087435675219
-0.0606114848504487 to -0.0608168555212579
Trying to learn from memory 69, 0, -0.2
sum 0.0375314827906928 distri -0.0176838653165217
Using diff 0.0458324774095414 and condRate 0.166666666666667
Changed category 0 weights from 
0.190212779899974 to 0.188685030630225
-0.209822184184651 to -0.2113499334544
-0.492342880705457 to -0.493870629975207
-0.340837514737706 to -0.342365264007456
Changing layer 0's weights from 
-0.247905285449082 to -0.249433034718832
-0.619831742138917 to -0.621359491408667
-0.854181007833536 to -0.855708757103286
-0.819621757955605 to -0.821149507225355
-0.0717541886282518 to -0.0732819378980017
-0.754396141858156 to -0.755923891127906
-0.600566418261583 to -0.602094167531333
-0.628020824284608 to -0.629548573554358
-0.865512685270364 to -0.867040434540114
-0.90926274040466 to -0.91079048967441
Changing layer 1's weights from 
-0.904165828020627 to -0.905693577290377
-0.30299714335924 to -0.304524892628989
-0.428699882121142 to -0.430227631390892
-0.438605220408495 to -0.440132969678245
-0.623658002705629 to -0.625185751975379
-0.761895836682375 to -0.763423585952125
-0.491177113146837 to -0.492704862416587
-0.876980142087992 to -0.878507891357742
-0.465317161173876 to -0.466844910443626
-0.827964024038369 to -0.829491773308119
Changing layer 2's weights from 
-0.0187757922125419 to -0.0203035414822919
-0.572829277606066 to -0.574357026875816
-0.288239986987169 to -0.289767736256918
-0.117039890379961 to -0.118567639649711
-0.294292600245531 to -0.29582034951528
-0.500302226634081 to -0.501829975903831
0.018510489373152 to 0.0169827401034021
-0.0539115382147389 to -0.0554392874844888
-0.785909832806642 to -0.787437582076392
-0.740952314229067 to -0.742480063498817
Changing layer 3's weights from 
-0.0518260909033379 to -0.0533538401730879
-0.3153791738463 to -0.31690692311605
-0.0727692557287768 to -0.0742970049985267
-0.98020848748492 to -0.98173623675467
-0.195445151419694 to -0.196972900689444
-0.325693757624682 to -0.327221506894432
-0.655878604741151 to -0.657406354010901
-0.197365732283647 to -0.198893481553397
-0.529061706156786 to -0.530589455426536
-0.0562318278265558 to -0.0577595770963057
Changing layer 4's weights from 
-0.155505628676469 to -0.157033377946219
-0.364338429064806 to -0.365866178334556
-0.564816029162462 to -0.566343778432212
-0.277881295771653 to -0.279409045041403
-0.517134101481493 to -0.518661850751243
-0.285668881030138 to -0.287196630299888
-0.290027411074694 to -0.291555160344444
-0.751924933285768 to -0.753452682555518
-0.158228845686967 to -0.159756594956717
-0.590530784220751 to -0.592058533490501
Changing layer 5's weights from 
-0.00272581109529291 to -0.00425356036504286
-0.444799335093554 to -0.446327084363304
-0.0196109724951339 to -0.0211387217648839
-0.955123153866585 to -0.956650903136335
-0.417854340167101 to -0.419382089436851
-0.81632228323942 to -0.81785003250917
-0.516356976122911 to -0.517884725392661
-0.443910749049242 to -0.445438498318992
-0.0543087435675219 to -0.0558364928372718
-0.0608168555212579 to -0.0623446047910079
Trying to learn from memory 69, 0, -0.2
sum 0.0375314827906928 distri -0.0176838653165217
Using diff 0.0458324774095414 and condRate 0.166666666666667
Changed category 0 weights from 
0.188685030630225 to 0.187157281360475
-0.2113499334544 to -0.21287768272415
-0.493870629975207 to -0.495398379244957
-0.342365264007456 to -0.343893013277206
Changing layer 0's weights from 
-0.249433034718832 to -0.250960783988582
-0.621359491408667 to -0.622887240678417
-0.855708757103286 to -0.857236506373036
-0.821149507225355 to -0.822677256495105
-0.0732819378980017 to -0.0748096871677517
-0.755923891127906 to -0.757451640397656
-0.602094167531333 to -0.603621916801083
-0.629548573554358 to -0.631076322824108
-0.867040434540114 to -0.868568183809864
-0.91079048967441 to -0.91231823894416
Changing layer 1's weights from 
-0.905693577290377 to -0.907221326560127
-0.304524892628989 to -0.306052641898739
-0.430227631390892 to -0.431755380660642
-0.440132969678245 to -0.441660718947995
-0.625185751975379 to -0.626713501245129
-0.763423585952125 to -0.764951335221875
-0.492704862416587 to -0.494232611686336
-0.878507891357742 to -0.880035640627492
-0.466844910443626 to -0.468372659713376
-0.829491773308119 to -0.831019522577869
Changing layer 2's weights from 
-0.0203035414822919 to -0.0218312907520418
-0.574357026875816 to -0.575884776145566
-0.289767736256918 to -0.291295485526668
-0.118567639649711 to -0.120095388919461
-0.29582034951528 to -0.29734809878503
-0.501829975903831 to -0.503357725173581
0.0169827401034021 to 0.0154549908336521
-0.0554392874844888 to -0.0569670367542388
-0.787437582076392 to -0.788965331346142
-0.742480063498817 to -0.744007812768567
Changing layer 3's weights from 
-0.0533538401730879 to -0.0548815894428378
-0.31690692311605 to -0.3184346723858
-0.0742970049985267 to -0.0758247542682767
-0.98173623675467 to -0.98326398602442
-0.196972900689444 to -0.198500649959194
-0.327221506894432 to -0.328749256164182
-0.657406354010901 to -0.658934103280651
-0.198893481553397 to -0.200421230823147
-0.530589455426536 to -0.532117204696286
-0.0577595770963057 to -0.0592873263660557
Changing layer 4's weights from 
-0.157033377946219 to -0.158561127215969
-0.365866178334556 to -0.367393927604306
-0.566343778432212 to -0.567871527701962
-0.279409045041403 to -0.280936794311153
-0.518661850751243 to -0.520189600020993
-0.287196630299888 to -0.288724379569637
-0.291555160344444 to -0.293082909614193
-0.753452682555518 to -0.754980431825268
-0.159756594956717 to -0.161284344226467
-0.592058533490501 to -0.593586282760251
Changing layer 5's weights from 
-0.00425356036504286 to -0.00578130963479281
-0.446327084363304 to -0.447854833633054
-0.0211387217648839 to -0.0226664710346338
-0.956650903136335 to -0.958178652406085
-0.419382089436851 to -0.420909838706601
-0.81785003250917 to -0.81937778177892
-0.517884725392661 to -0.519412474662411
-0.445438498318992 to -0.446966247588742
-0.0558364928372718 to -0.0573642421070218
-0.0623446047910079 to -0.0638723540607578
Trying to learn from memory 69, 0, -0.2
sum 0.0375314827906928 distri -0.0176838653165217
Using diff 0.0458324774095414 and condRate 0.166666666666667
Changed category 0 weights from 
0.187157281360475 to 0.185629532090725
-0.21287768272415 to -0.2144054319939
-0.495398379244957 to -0.496926128514707
-0.343893013277206 to -0.345420762546956
Changing layer 0's weights from 
-0.250960783988582 to -0.252488533258332
-0.622887240678417 to -0.624414989948167
-0.857236506373036 to -0.858764255642786
-0.822677256495105 to -0.824205005764855
-0.0748096871677517 to -0.0763374364375016
-0.757451640397656 to -0.758979389667406
-0.603621916801083 to -0.605149666070833
-0.631076322824108 to -0.632604072093858
-0.868568183809864 to -0.870095933079614
-0.91231823894416 to -0.91384598821391
Changing layer 1's weights from 
-0.907221326560127 to -0.908749075829877
-0.306052641898739 to -0.307580391168489
-0.431755380660642 to -0.433283129930392
-0.441660718947995 to -0.443188468217745
-0.626713501245129 to -0.628241250514879
-0.764951335221875 to -0.766479084491625
-0.494232611686336 to -0.495760360956086
-0.880035640627492 to -0.881563389897242
-0.468372659713376 to -0.469900408983126
-0.831019522577869 to -0.832547271847619
Changing layer 2's weights from 
-0.0218312907520418 to -0.0233590400217918
-0.575884776145566 to -0.577412525415316
-0.291295485526668 to -0.292823234796418
-0.120095388919461 to -0.121623138189211
-0.29734809878503 to -0.29887584805478
-0.503357725173581 to -0.504885474443331
0.0154549908336521 to 0.0139272415639022
-0.0569670367542388 to -0.0584947860239887
-0.788965331346142 to -0.790493080615892
-0.744007812768567 to -0.745535562038317
Changing layer 3's weights from 
-0.0548815894428378 to -0.0564093387125878
-0.3184346723858 to -0.31996242165555
-0.0758247542682767 to -0.0773525035380266
-0.98326398602442 to -0.98479173529417
-0.198500649959194 to -0.200028399228944
-0.328749256164182 to -0.330277005433932
-0.658934103280651 to -0.660461852550401
-0.200421230823147 to -0.201948980092897
-0.532117204696286 to -0.533644953966036
-0.0592873263660557 to -0.0608150756358056
Changing layer 4's weights from 
-0.158561127215969 to -0.160088876485719
-0.367393927604306 to -0.368921676874056
-0.567871527701962 to -0.569399276971712
-0.280936794311153 to -0.282464543580903
-0.520189600020993 to -0.521717349290743
-0.288724379569637 to -0.290252128839387
-0.293082909614193 to -0.294610658883943
-0.754980431825268 to -0.756508181095018
-0.161284344226467 to -0.162812093496217
-0.593586282760251 to -0.595114032030001
Changing layer 5's weights from 
-0.00578130963479281 to -0.00730905890454276
-0.447854833633054 to -0.449382582902804
-0.0226664710346338 to -0.0241942203043838
-0.958178652406085 to -0.959706401675835
-0.420909838706601 to -0.422437587976351
-0.81937778177892 to -0.82090553104867
-0.519412474662411 to -0.520940223932161
-0.446966247588742 to -0.448493996858492
-0.0573642421070218 to -0.0588919913767717
-0.0638723540607578 to -0.0654001033305078
10/5/2016 1:43:17 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 69, 1, -0.2
sum 0.0375314827906928 distri 0.0219874920605498
Using diff 0.00616112003246983 and condRate 0.166666666666667
Changed category 1 weights from 
0.293054869495378 to 0.292849498824569
0.526370456539139 to 0.52616508586833
0.101445218645082 to 0.101239847974273
0.105538031183229 to 0.10533266051242
Changing layer 0's weights from 
-0.252488533258332 to -0.252693903929141
-0.624414989948167 to -0.624620360618976
-0.858764255642786 to -0.858969626313595
-0.824205005764855 to -0.824410376435664
-0.0763374364375016 to -0.0765428071083109
-0.758979389667406 to -0.759184760338215
-0.605149666070833 to -0.605355036741642
-0.632604072093858 to -0.632809442764667
-0.870095933079614 to -0.870301303750423
-0.91384598821391 to -0.914051358884719
Changing layer 1's weights from 
-0.908749075829877 to -0.908954446500686
-0.307580391168489 to -0.307785761839299
-0.433283129930392 to -0.433488500601201
-0.443188468217745 to -0.443393838888554
-0.628241250514879 to -0.628446621185688
-0.766479084491625 to -0.766684455162434
-0.495760360956086 to -0.495965731626896
-0.881563389897242 to -0.881768760568051
-0.469900408983126 to -0.470105779653935
-0.832547271847619 to -0.832752642518428
Changing layer 2's weights from 
-0.0233590400217918 to -0.023564410692601
-0.577412525415316 to -0.577617896086125
-0.292823234796418 to -0.293028605467228
-0.121623138189211 to -0.12182850886002
-0.29887584805478 to -0.29908121872559
-0.504885474443331 to -0.50509084511414
0.0139272415639022 to 0.0137218708930929
-0.0584947860239887 to -0.058700156694798
-0.790493080615892 to -0.790698451286701
-0.745535562038317 to -0.745740932709126
Changing layer 3's weights from 
-0.0564093387125878 to -0.056614709383397
-0.31996242165555 to -0.320167792326359
-0.0773525035380266 to -0.0775578742088359
-0.98479173529417 to -0.984997105964979
-0.200028399228944 to -0.200233769899753
-0.330277005433932 to -0.330482376104741
-0.660461852550401 to -0.66066722322121
-0.201948980092897 to -0.202154350763706
-0.533644953966036 to -0.533850324636845
-0.0608150756358056 to -0.0610204463066149
Changing layer 4's weights from 
-0.160088876485719 to -0.160294247156528
-0.368921676874056 to -0.369127047544865
-0.569399276971712 to -0.569604647642521
-0.282464543580903 to -0.282669914251713
-0.521717349290743 to -0.521922719961552
-0.290252128839387 to -0.290457499510197
-0.294610658883943 to -0.294816029554753
-0.756508181095018 to -0.756713551765827
-0.162812093496217 to -0.163017464167026
-0.595114032030001 to -0.59531940270081
Changing layer 5's weights from 
-0.00730905890454276 to -0.00751442957535201
-0.449382582902804 to -0.449587953573613
-0.0241942203043838 to -0.024399590975193
-0.959706401675835 to -0.959911772346644
-0.422437587976351 to -0.42264295864716
-0.82090553104867 to -0.821110901719479
-0.520940223932161 to -0.52114559460297
-0.448493996858492 to -0.448699367529301
-0.0588919913767717 to -0.059097362047581
-0.0654001033305078 to -0.065605474001317
Trying to learn from memory 70, 1, -0.2
sum 0.0375314827906928 distri 0.0219874920605498
Using diff 0.00616112003246983 and condRate 0.166666666666667
Changed category 1 weights from 
0.292849498824569 to 0.292644128153759
0.52616508586833 to 0.52595971519752
0.101239847974273 to 0.101034477303463
0.10533266051242 to 0.10512728984161
Changing layer 0's weights from 
-0.252693903929141 to -0.25289927459995
-0.624620360618976 to -0.624825731289786
-0.858969626313595 to -0.859174996984405
-0.824410376435664 to -0.824615747106474
-0.0765428071083109 to -0.0767481777791201
-0.759184760338215 to -0.759390131009025
-0.605355036741642 to -0.605560407412452
-0.632809442764667 to -0.633014813435477
-0.870301303750423 to -0.870506674421233
-0.914051358884719 to -0.914256729555529
Changing layer 1's weights from 
-0.908954446500686 to -0.909159817171496
-0.307785761839299 to -0.307991132510108
-0.433488500601201 to -0.43369387127201
-0.443393838888554 to -0.443599209559363
-0.628446621185688 to -0.628651991856498
-0.766684455162434 to -0.766889825833244
-0.495965731626896 to -0.496171102297705
-0.881768760568051 to -0.881974131238861
-0.470105779653935 to -0.470311150324744
-0.832752642518428 to -0.832958013189238
Changing layer 2's weights from 
-0.023564410692601 to -0.0237697813634103
-0.577617896086125 to -0.577823266756935
-0.293028605467228 to -0.293233976138037
-0.12182850886002 to -0.122033879530829
-0.29908121872559 to -0.299286589396399
-0.50509084511414 to -0.505296215784949
0.0137218708930929 to 0.0135165002222837
-0.058700156694798 to -0.0589055273656073
-0.790698451286701 to -0.790903821957511
-0.745740932709126 to -0.745946303379936
Changing layer 3's weights from 
-0.056614709383397 to -0.0568200800542063
-0.320167792326359 to -0.320373162997168
-0.0775578742088359 to -0.0777632448796451
-0.984997105964979 to -0.985202476635789
-0.200233769899753 to -0.200439140570562
-0.330482376104741 to -0.33068774677555
-0.66066722322121 to -0.66087259389202
-0.202154350763706 to -0.202359721434515
-0.533850324636845 to -0.534055695307655
-0.0610204463066149 to -0.0612258169774242
Changing layer 4's weights from 
-0.160294247156528 to -0.160499617827337
-0.369127047544865 to -0.369332418215674
-0.569604647642521 to -0.56981001831333
-0.282669914251713 to -0.282875284922522
-0.521922719961552 to -0.522128090632361
-0.290457499510197 to -0.290662870181006
-0.294816029554753 to -0.295021400225562
-0.756713551765827 to -0.756918922436637
-0.163017464167026 to -0.163222834837835
-0.59531940270081 to -0.59552477337162
Changing layer 5's weights from 
-0.00751442957535201 to -0.00771980024616127
-0.449587953573613 to -0.449793324244422
-0.024399590975193 to -0.0246049616460023
-0.959911772346644 to -0.960117143017454
-0.42264295864716 to -0.422848329317969
-0.821110901719479 to -0.821316272390289
-0.52114559460297 to -0.521350965273779
-0.448699367529301 to -0.44890473820011
-0.059097362047581 to -0.0593027327183903
-0.065605474001317 to -0.0658108446721263
Trying to learn from memory 71, 0, -0.2
sum 0.0375314827906928 distri -0.0176838653165217
Using diff 0.0458324774095414 and condRate 0.166666666666667
Changed category 0 weights from 
0.185629532090725 to 0.184101782820975
-0.2144054319939 to -0.21593318126365
-0.496926128514707 to -0.498453877784457
-0.345420762546956 to -0.346948511816706
Changing layer 0's weights from 
-0.25289927459995 to -0.2544270238697
-0.624825731289786 to -0.626353480559536
-0.859174996984405 to -0.860702746254155
-0.824615747106474 to -0.826143496376224
-0.0767481777791201 to -0.0782759270488701
-0.759390131009025 to -0.760917880278774
-0.605560407412452 to -0.607088156682202
-0.633014813435477 to -0.634542562705226
-0.870506674421233 to -0.872034423690983
-0.914256729555529 to -0.915784478825278
Changing layer 1's weights from 
-0.909159817171496 to -0.910687566441245
-0.307991132510108 to -0.309518881779858
-0.43369387127201 to -0.43522162054176
-0.443599209559363 to -0.445126958829113
-0.628651991856498 to -0.630179741126248
-0.766889825833244 to -0.768417575102993
-0.496171102297705 to -0.497698851567455
-0.881974131238861 to -0.883501880508611
-0.470311150324744 to -0.471838899594494
-0.832958013189238 to -0.834485762458988
Changing layer 2's weights from 
-0.0237697813634103 to -0.0252975306331602
-0.577823266756935 to -0.579351016026685
-0.293233976138037 to -0.294761725407787
-0.122033879530829 to -0.123561628800579
-0.299286589396399 to -0.300814338666149
-0.505296215784949 to -0.506823965054699
0.0135165002222837 to 0.0119887509525337
-0.0589055273656073 to -0.0604332766353572
-0.790903821957511 to -0.79243157122726
-0.745946303379936 to -0.747474052649685
Changing layer 3's weights from 
-0.0568200800542063 to -0.0583478293239562
-0.320373162997168 to -0.321900912266918
-0.0777632448796451 to -0.0792909941493951
-0.985202476635789 to -0.986730225905538
-0.200439140570562 to -0.201966889840312
-0.33068774677555 to -0.3322154960453
-0.66087259389202 to -0.66240034316177
-0.202359721434515 to -0.203887470704265
-0.534055695307655 to -0.535583444577405
-0.0612258169774242 to -0.0627535662471741
Changing layer 4's weights from 
-0.160499617827337 to -0.162027367097087
-0.369332418215674 to -0.370860167485424
-0.56981001831333 to -0.57133776758308
-0.282875284922522 to -0.284403034192272
-0.522128090632361 to -0.523655839902111
-0.290662870181006 to -0.292190619450756
-0.295021400225562 to -0.296549149495312
-0.756918922436637 to -0.758446671706386
-0.163222834837835 to -0.164750584107585
-0.59552477337162 to -0.59705252264137
Changing layer 5's weights from 
-0.00771980024616127 to -0.00924754951591122
-0.449793324244422 to -0.451321073514172
-0.0246049616460023 to -0.0261327109157522
-0.960117143017454 to -0.961644892287204
-0.422848329317969 to -0.424376078587719
-0.821316272390289 to -0.822844021660039
-0.521350965273779 to -0.522878714543529
-0.44890473820011 to -0.45043248746986
-0.0593027327183903 to -0.0608304819881402
-0.0658108446721263 to -0.0673385939418762
Trying to learn from memory 71, 0, -0.2
sum 0.0375314827906928 distri -0.0176838653165217
Using diff 0.0458324774095414 and condRate 0.166666666666667
Changed category 0 weights from 
0.184101782820975 to 0.182574033551225
-0.21593318126365 to -0.2174609305334
-0.498453877784457 to -0.499981627054207
-0.346948511816706 to -0.348476261086456
Changing layer 0's weights from 
-0.2544270238697 to -0.25595477313945
-0.626353480559536 to -0.627881229829286
-0.860702746254155 to -0.862230495523905
-0.826143496376224 to -0.827671245645974
-0.0782759270488701 to -0.07980367631862
-0.760917880278774 to -0.762445629548524
-0.607088156682202 to -0.608615905951951
-0.634542562705226 to -0.636070311974976
-0.872034423690983 to -0.873562172960732
-0.915784478825278 to -0.917312228095028
Changing layer 1's weights from 
-0.910687566441245 to -0.912215315710995
-0.309518881779858 to -0.311046631049608
-0.43522162054176 to -0.43674936981151
-0.445126958829113 to -0.446654708098863
-0.630179741126248 to -0.631707490395998
-0.768417575102993 to -0.769945324372743
-0.497698851567455 to -0.499226600837205
-0.883501880508611 to -0.885029629778361
-0.471838899594494 to -0.473366648864244
-0.834485762458988 to -0.836013511728738
Changing layer 2's weights from 
-0.0252975306331602 to -0.0268252799029102
-0.579351016026685 to -0.580878765296435
-0.294761725407787 to -0.296289474677537
-0.123561628800579 to -0.125089378070329
-0.300814338666149 to -0.302342087935899
-0.506823965054699 to -0.508351714324449
0.0119887509525337 to 0.0104610016827838
-0.0604332766353572 to -0.0619610259051072
-0.79243157122726 to -0.79395932049701
-0.747474052649685 to -0.749001801919435
Changing layer 3's weights from 
-0.0583478293239562 to -0.0598755785937062
-0.321900912266918 to -0.323428661536668
-0.0792909941493951 to -0.080818743419145
-0.986730225905538 to -0.988257975175288
-0.201966889840312 to -0.203494639110062
-0.3322154960453 to -0.33374324531505
-0.66240034316177 to -0.66392809243152
-0.203887470704265 to -0.205415219974015
-0.535583444577405 to -0.537111193847155
-0.0627535662471741 to -0.0642813155169241
Changing layer 4's weights from 
-0.162027367097087 to -0.163555116366837
-0.370860167485424 to -0.372387916755174
-0.57133776758308 to -0.57286551685283
-0.284403034192272 to -0.285930783462022
-0.523655839902111 to -0.525183589171861
-0.292190619450756 to -0.293718368720506
-0.296549149495312 to -0.298076898765062
-0.758446671706386 to -0.759974420976136
-0.164750584107585 to -0.166278333377335
-0.59705252264137 to -0.59858027191112
Changing layer 5's weights from 
-0.00924754951591122 to -0.0107752987856612
-0.451321073514172 to -0.452848822783922
-0.0261327109157522 to -0.0276604601855022
-0.961644892287204 to -0.963172641556953
-0.424376078587719 to -0.425903827857469
-0.822844021660039 to -0.824371770929789
-0.522878714543529 to -0.524406463813279
-0.45043248746986 to -0.45196023673961
-0.0608304819881402 to -0.0623582312578901
-0.0673385939418762 to -0.0688663432116262
Trying to learn from memory 71, 0, -0.2
sum 0.0375314827906928 distri -0.0176838653165217
Using diff 0.0458324774095414 and condRate 0.166666666666667
Changed category 0 weights from 
0.182574033551225 to 0.181046284281475
-0.2174609305334 to -0.21898867980315
-0.499981627054207 to -0.501509376323957
-0.348476261086456 to -0.350004010356206
Changing layer 0's weights from 
-0.25595477313945 to -0.2574825224092
-0.627881229829286 to -0.629408979099035
-0.862230495523905 to -0.863758244793654
-0.827671245645974 to -0.829198994915723
-0.07980367631862 to -0.08133142558837
-0.762445629548524 to -0.763973378818274
-0.608615905951951 to -0.610143655221701
-0.636070311974976 to -0.637598061244726
-0.873562172960732 to -0.875089922230482
-0.917312228095028 to -0.918839977364778
Changing layer 1's weights from 
-0.912215315710995 to -0.913743064980745
-0.311046631049608 to -0.312574380319358
-0.43674936981151 to -0.43827711908126
-0.446654708098863 to -0.448182457368613
-0.631707490395998 to -0.633235239665747
-0.769945324372743 to -0.771473073642493
-0.499226600837205 to -0.500754350106955
-0.885029629778361 to -0.88655737904811
-0.473366648864244 to -0.474894398133994
-0.836013511728738 to -0.837541260998487
Changing layer 2's weights from 
-0.0268252799029102 to -0.0283530291726601
-0.580878765296435 to -0.582406514566185
-0.296289474677537 to -0.297817223947287
-0.125089378070329 to -0.126617127340079
-0.302342087935899 to -0.303869837205649
-0.508351714324449 to -0.509879463594199
0.0104610016827838 to 0.00893325241303382
-0.0619610259051072 to -0.0634887751748571
-0.79395932049701 to -0.79548706976676
-0.749001801919435 to -0.750529551189185
Changing layer 3's weights from 
-0.0598755785937062 to -0.0614033278634561
-0.323428661536668 to -0.324956410806418
-0.080818743419145 to -0.082346492688895
-0.988257975175288 to -0.989785724445038
-0.203494639110062 to -0.205022388379812
-0.33374324531505 to -0.3352709945848
-0.66392809243152 to -0.665455841701269
-0.205415219974015 to -0.206942969243765
-0.537111193847155 to -0.538638943116905
-0.0642813155169241 to -0.065809064786674
Changing layer 4's weights from 
-0.163555116366837 to -0.165082865636587
-0.372387916755174 to -0.373915666024924
-0.57286551685283 to -0.57439326612258
-0.285930783462022 to -0.287458532731772
-0.525183589171861 to -0.526711338441611
-0.293718368720506 to -0.295246117990256
-0.298076898765062 to -0.299604648034812
-0.759974420976136 to -0.761502170245886
-0.166278333377335 to -0.167806082647085
-0.59858027191112 to -0.600108021180869
Changing layer 5's weights from 
-0.0107752987856612 to -0.0123030480554111
-0.452848822783922 to -0.454376572053672
-0.0276604601855022 to -0.0291882094552521
-0.963172641556953 to -0.964700390826703
-0.425903827857469 to -0.427431577127219
-0.824371770929789 to -0.825899520199538
-0.524406463813279 to -0.525934213083029
-0.45196023673961 to -0.45348798600936
-0.0623582312578901 to -0.0638859805276401
-0.0688663432116262 to -0.0703940924813761
Trying to learn from memory 71, 0, -0.2
sum 0.0375314827906928 distri -0.0176838653165217
Using diff 0.0458324774095414 and condRate 0.166666666666667
Changed category 0 weights from 
0.181046284281475 to 0.179518535011725
-0.21898867980315 to -0.2205164290729
-0.501509376323957 to -0.503037125593707
-0.350004010356206 to -0.351531759625956
Changing layer 0's weights from 
-0.2574825224092 to -0.25901027167895
-0.629408979099035 to -0.630936728368785
-0.863758244793654 to -0.865285994063404
-0.829198994915723 to -0.830726744185473
-0.08133142558837 to -0.0828591748581199
-0.763973378818274 to -0.765501128088024
-0.610143655221701 to -0.611671404491451
-0.637598061244726 to -0.639125810514476
-0.875089922230482 to -0.876617671500232
-0.918839977364778 to -0.920367726634528
Changing layer 1's weights from 
-0.913743064980745 to -0.915270814250495
-0.312574380319358 to -0.314102129589108
-0.43827711908126 to -0.43980486835101
-0.448182457368613 to -0.449710206638363
-0.633235239665747 to -0.634762988935497
-0.771473073642493 to -0.773000822912243
-0.500754350106955 to -0.502282099376705
-0.88655737904811 to -0.88808512831786
-0.474894398133994 to -0.476422147403744
-0.837541260998487 to -0.839069010268237
Changing layer 2's weights from 
-0.0283530291726601 to -0.0298807784424101
-0.582406514566185 to -0.583934263835935
-0.297817223947287 to -0.299344973217037
-0.126617127340079 to -0.128144876609829
-0.303869837205649 to -0.305397586475399
-0.509879463594199 to -0.511407212863949
0.00893325241303382 to 0.00740550314328387
-0.0634887751748571 to -0.0650165244446071
-0.79548706976676 to -0.79701481903651
-0.750529551189185 to -0.752057300458935
Changing layer 3's weights from 
-0.0614033278634561 to -0.0629310771332061
-0.324956410806418 to -0.326484160076168
-0.082346492688895 to -0.0838742419586449
-0.989785724445038 to -0.991313473714788
-0.205022388379812 to -0.206550137649562
-0.3352709945848 to -0.33679874385455
-0.665455841701269 to -0.666983590971019
-0.206942969243765 to -0.208470718513515
-0.538638943116905 to -0.540166692386654
-0.065809064786674 to -0.067336814056424
Changing layer 4's weights from 
-0.165082865636587 to -0.166610614906337
-0.373915666024924 to -0.375443415294674
-0.57439326612258 to -0.57592101539233
-0.287458532731772 to -0.288986282001522
-0.526711338441611 to -0.528239087711361
-0.295246117990256 to -0.296773867260006
-0.299604648034812 to -0.301132397304562
-0.761502170245886 to -0.763029919515636
-0.167806082647085 to -0.169333831916835
-0.600108021180869 to -0.601635770450619
Changing layer 5's weights from 
-0.0123030480554111 to -0.0138307973251611
-0.454376572053672 to -0.455904321323422
-0.0291882094552521 to -0.0307159587250021
-0.964700390826703 to -0.966228140096453
-0.427431577127219 to -0.428959326396969
-0.825899520199538 to -0.827427269469288
-0.525934213083029 to -0.527461962352779
-0.45348798600936 to -0.45501573527911
-0.0638859805276401 to -0.06541372979739
-0.0703940924813761 to -0.0719218417511261
10/5/2016 1:43:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:43:56 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
Modified index 9's learning in memoryPool to -0.2
Modified index 10's learning in memoryPool to -0.2
Modified index 11's learning in memoryPool to -0.2
Modified index 12's learning in memoryPool to -0.2
Modified index 13's learning in memoryPool to -0.2
Modified index 14's learning in memoryPool to -0.2
Modified index 15's learning in memoryPool to -0.2
Modified index 16's learning in memoryPool to -0.2
Modified index 17's learning in memoryPool to -0.2
Modified index 18's learning in memoryPool to -0.2
Modified index 19's learning in memoryPool to -0.2
Modified index 20's learning in memoryPool to -0.2
Modified index 21's learning in memoryPool to -0.2
Modified index 22's learning in memoryPool to -0.2
Modified index 23's learning in memoryPool to -0.2
Modified index 24's learning in memoryPool to -0.2
Modified index 25's learning in memoryPool to -0.2
10/5/2016 1:43:56 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 72, 1, -0.2
sum 0.037611825748971 distri 0.0228401487694015
Using diff 0.00536872054232676 and condRate 0.166666666666667
Changed category 1 weights from 
0.292644128153759 to 0.292465170799682
0.52595971519752 to 0.525780757843443
0.101034477303463 to 0.100855519949386
0.10512728984161 to 0.104948332487533
Changing layer 0's weights from 
-0.25901027167895 to -0.259189229033028
-0.630936728368785 to -0.631115685722863
-0.865285994063404 to -0.865464951417482
-0.830726744185473 to -0.830905701539551
-0.0828591748581199 to -0.0830381322121975
-0.765501128088024 to -0.765680085442102
-0.611671404491451 to -0.611850361845529
-0.639125810514476 to -0.639304767868554
-0.876617671500232 to -0.87679662885431
-0.920367726634528 to -0.920546683988606
Changing layer 1's weights from 
-0.915270814250495 to -0.915449771604573
-0.314102129589108 to -0.314281086943185
-0.43980486835101 to -0.439983825705088
-0.449710206638363 to -0.449889163992441
-0.634762988935497 to -0.634941946289575
-0.773000822912243 to -0.773179780266321
-0.502282099376705 to -0.502461056730782
-0.88808512831786 to -0.888264085671938
-0.476422147403744 to -0.476601104757822
-0.839069010268237 to -0.839247967622315
Changing layer 2's weights from 
-0.0298807784424101 to -0.0300597357964876
-0.583934263835935 to -0.584113221190012
-0.299344973217037 to -0.299523930571114
-0.128144876609829 to -0.128323833963906
-0.305397586475399 to -0.305576543829476
-0.511407212863949 to -0.511586170218027
0.00740550314328387 to 0.00722654578920631
-0.0650165244446071 to -0.0651954817986846
-0.79701481903651 to -0.797193776390588
-0.752057300458935 to -0.752236257813013
Changing layer 3's weights from 
-0.0629310771332061 to -0.0631100344872836
-0.326484160076168 to -0.326663117430246
-0.0838742419586449 to -0.0840531993127225
-0.991313473714788 to -0.991492431068866
-0.206550137649562 to -0.20672909500364
-0.33679874385455 to -0.336977701208628
-0.666983590971019 to -0.667162548325097
-0.208470718513515 to -0.208649675867593
-0.540166692386654 to -0.540345649740732
-0.067336814056424 to -0.0675157714105015
Changing layer 4's weights from 
-0.166610614906337 to -0.166789572260415
-0.375443415294674 to -0.375622372648752
-0.57592101539233 to -0.576099972746408
-0.288986282001522 to -0.289165239355599
-0.528239087711361 to -0.528418045065439
-0.296773867260006 to -0.296952824614083
-0.301132397304562 to -0.301311354658639
-0.763029919515636 to -0.763208876869714
-0.169333831916835 to -0.169512789270913
-0.601635770450619 to -0.601814727804697
Changing layer 5's weights from 
-0.0138307973251611 to -0.0140097546792386
-0.455904321323422 to -0.4560832786775
-0.0307159587250021 to -0.0308949160790796
-0.966228140096453 to -0.966407097450531
-0.428959326396969 to -0.429138283751047
-0.827427269469288 to -0.827606226823366
-0.527461962352779 to -0.527640919706857
-0.45501573527911 to -0.455194692633188
-0.06541372979739 to -0.0655926871514676
-0.0719218417511261 to -0.0721007991052036
Trying to learn from memory 73, 0, -0.2
sum 0.0377241180375053 distri -0.0209875575651362
Using diff 0.0492806460932651 and condRate 0.166666666666667
Changed category 0 weights from 
0.179518535011725 to 0.177875846784138
-0.2205164290729 to -0.222159117300487
-0.503037125593707 to -0.504679813821293
-0.351531759625956 to -0.353174447853543
Changing layer 0's weights from 
-0.259189229033028 to -0.260831917260615
-0.631115685722863 to -0.63275837395045
-0.865464951417482 to -0.867107639645069
-0.830905701539551 to -0.832548389767138
-0.0830381322121975 to -0.0846808204397843
-0.765680085442102 to -0.767322773669689
-0.611850361845529 to -0.613493050073116
-0.639304767868554 to -0.640947456096141
-0.87679662885431 to -0.878439317081897
-0.920546683988606 to -0.922189372216193
Changing layer 1's weights from 
-0.915449771604573 to -0.91709245983216
-0.314281086943185 to -0.315923775170772
-0.439983825705088 to -0.441626513932674
-0.449889163992441 to -0.451531852220027
-0.634941946289575 to -0.636584634517162
-0.773179780266321 to -0.774822468493908
-0.502461056730782 to -0.504103744958369
-0.888264085671938 to -0.889906773899525
-0.476601104757822 to -0.478243792985408
-0.839247967622315 to -0.840890655849902
Changing layer 2's weights from 
-0.0300597357964876 to -0.0317024240240744
-0.584113221190012 to -0.585755909417599
-0.299523930571114 to -0.301166618798701
-0.128323833963906 to -0.129966522191493
-0.305576543829476 to -0.307219232057063
-0.511586170218027 to -0.513228858445613
0.00722654578920631 to 0.00558385756161951
-0.0651954817986846 to -0.0668381700262714
-0.797193776390588 to -0.798836464618175
-0.752236257813013 to -0.7538789460406
Changing layer 3's weights from 
-0.0631100344872836 to -0.0647527227148704
-0.326663117430246 to -0.328305805657832
-0.0840531993127225 to -0.0856958875403093
-0.991492431068866 to -0.993135119296453
-0.20672909500364 to -0.208371783231227
-0.336977701208628 to -0.338620389436214
-0.667162548325097 to -0.668805236552684
-0.208649675867593 to -0.210292364095179
-0.540345649740732 to -0.541988337968319
-0.0675157714105015 to -0.0691584596380883
Changing layer 4's weights from 
-0.166789572260415 to -0.168432260488002
-0.375622372648752 to -0.377265060876338
-0.576099972746408 to -0.577742660973995
-0.289165239355599 to -0.290807927583186
-0.528418045065439 to -0.530060733293026
-0.296952824614083 to -0.29859551284167
-0.301311354658639 to -0.302954042886226
-0.763208876869714 to -0.764851565097301
-0.169512789270913 to -0.1711554774985
-0.601814727804697 to -0.603457416032284
Changing layer 5's weights from 
-0.0140097546792386 to -0.0156524429068254
-0.4560832786775 to -0.457725966905086
-0.0308949160790796 to -0.0325376043066664
-0.966407097450531 to -0.968049785678118
-0.429138283751047 to -0.430780971978633
-0.827606226823366 to -0.829248915050953
-0.527640919706857 to -0.529283607934444
-0.455194692633188 to -0.456837380860774
-0.0655926871514676 to -0.0672353753790544
-0.0721007991052036 to -0.0737434873327904
Trying to learn from memory 74, 0, -0.2
sum 0.0377241180375053 distri -0.0209875575651362
Using diff 0.0492806460932651 and condRate 0.166666666666667
Changed category 0 weights from 
0.177875846784138 to 0.176233158556551
-0.222159117300487 to -0.223801805528074
-0.504679813821293 to -0.50632250204888
-0.353174447853543 to -0.354817136081129
Changing layer 0's weights from 
-0.260831917260615 to -0.262474605488202
-0.63275837395045 to -0.634401062178037
-0.867107639645069 to -0.868750327872656
-0.832548389767138 to -0.834191077994725
-0.0846808204397843 to -0.0863235086673711
-0.767322773669689 to -0.768965461897276
-0.613493050073116 to -0.615135738300703
-0.640947456096141 to -0.642590144323728
-0.878439317081897 to -0.880082005309484
-0.922189372216193 to -0.92383206044378
Changing layer 1's weights from 
-0.91709245983216 to -0.918735148059747
-0.315923775170772 to -0.317566463398359
-0.441626513932674 to -0.443269202160261
-0.451531852220027 to -0.453174540447614
-0.636584634517162 to -0.638227322744749
-0.774822468493908 to -0.776465156721495
-0.504103744958369 to -0.505746433185956
-0.889906773899525 to -0.891549462127112
-0.478243792985408 to -0.479886481212995
-0.840890655849902 to -0.842533344077489
Changing layer 2's weights from 
-0.0317024240240744 to -0.0333451122516612
-0.585755909417599 to -0.587398597645186
-0.301166618798701 to -0.302809307026288
-0.129966522191493 to -0.13160921041908
-0.307219232057063 to -0.30886192028465
-0.513228858445613 to -0.5148715466732
0.00558385756161951 to 0.00394116933403271
-0.0668381700262714 to -0.0684808582538582
-0.798836464618175 to -0.800479152845762
-0.7538789460406 to -0.755521634268187
Changing layer 3's weights from 
-0.0647527227148704 to -0.0663954109424572
-0.328305805657832 to -0.329948493885419
-0.0856958875403093 to -0.0873385757678961
-0.993135119296453 to -0.99477780752404
-0.208371783231227 to -0.210014471458813
-0.338620389436214 to -0.340263077663801
-0.668805236552684 to -0.670447924780271
-0.210292364095179 to -0.211935052322766
-0.541988337968319 to -0.543631026195906
-0.0691584596380883 to -0.0708011478656751
Changing layer 4's weights from 
-0.168432260488002 to -0.170074948715588
-0.377265060876338 to -0.378907749103925
-0.577742660973995 to -0.579385349201582
-0.290807927583186 to -0.292450615810773
-0.530060733293026 to -0.531703421520613
-0.29859551284167 to -0.300238201069257
-0.302954042886226 to -0.304596731113813
-0.764851565097301 to -0.766494253324888
-0.1711554774985 to -0.172798165726086
-0.603457416032284 to -0.605100104259871
Changing layer 5's weights from 
-0.0156524429068254 to -0.0172951311344122
-0.457725966905086 to -0.459368655132673
-0.0325376043066664 to -0.0341802925342532
-0.968049785678118 to -0.969692473905705
-0.430780971978633 to -0.43242366020622
-0.829248915050953 to -0.83089160327854
-0.529283607934444 to -0.530926296162031
-0.456837380860774 to -0.458480069088361
-0.0672353753790544 to -0.0688780636066412
-0.0737434873327904 to -0.0753861755603772
Trying to learn from memory 75, 0, -0.2
sum 0.0377241180375053 distri -0.0209875575651362
Using diff 0.0492806460932651 and condRate 0.166666666666667
Changed category 0 weights from 
0.176233158556551 to 0.174590470328965
-0.223801805528074 to -0.225444493755661
-0.50632250204888 to -0.507965190276467
-0.354817136081129 to -0.356459824308716
Changing layer 0's weights from 
-0.262474605488202 to -0.264117293715788
-0.634401062178037 to -0.636043750405624
-0.868750327872656 to -0.870393016100243
-0.834191077994725 to -0.835833766222312
-0.0863235086673711 to -0.0879661968949579
-0.768965461897276 to -0.770608150124863
-0.615135738300703 to -0.61677842652829
-0.642590144323728 to -0.644232832551314
-0.880082005309484 to -0.881724693537071
-0.92383206044378 to -0.925474748671366
Changing layer 1's weights from 
-0.918735148059747 to -0.920377836287334
-0.317566463398359 to -0.319209151625946
-0.443269202160261 to -0.444911890387848
-0.453174540447614 to -0.454817228675201
-0.638227322744749 to -0.639870010972336
-0.776465156721495 to -0.778107844949081
-0.505746433185956 to -0.507389121413543
-0.891549462127112 to -0.893192150354699
-0.479886481212995 to -0.481529169440582
-0.842533344077489 to -0.844176032305076
Changing layer 2's weights from 
-0.0333451122516612 to -0.034987800479248
-0.587398597645186 to -0.589041285872773
-0.302809307026288 to -0.304451995253875
-0.13160921041908 to -0.133251898646667
-0.30886192028465 to -0.310504608512237
-0.5148715466732 to -0.516514234900787
0.00394116933403271 to 0.00229848110644591
-0.0684808582538582 to -0.070123546481445
-0.800479152845762 to -0.802121841073349
-0.755521634268187 to -0.757164322495773
Changing layer 3's weights from 
-0.0663954109424572 to -0.068038099170044
-0.329948493885419 to -0.331591182113006
-0.0873385757678961 to -0.0889812639954829
-0.99477780752404 to -0.996420495751627
-0.210014471458813 to -0.2116571596864
-0.340263077663801 to -0.341905765891388
-0.670447924780271 to -0.672090613007858
-0.211935052322766 to -0.213577740550353
-0.543631026195906 to -0.545273714423493
-0.0708011478656751 to -0.0724438360932619
Changing layer 4's weights from 
-0.170074948715588 to -0.171717636943175
-0.378907749103925 to -0.380550437331512
-0.579385349201582 to -0.581028037429168
-0.292450615810773 to -0.29409330403836
-0.531703421520613 to -0.533346109748199
-0.300238201069257 to -0.301880889296844
-0.304596731113813 to -0.3062394193414
-0.766494253324888 to -0.768136941552474
-0.172798165726086 to -0.174440853953673
-0.605100104259871 to -0.606742792487458
Changing layer 5's weights from 
-0.0172951311344122 to -0.018937819361999
-0.459368655132673 to -0.46101134336026
-0.0341802925342532 to -0.03582298076184
-0.969692473905705 to -0.971335162133292
-0.43242366020622 to -0.434066348433807
-0.83089160327854 to -0.832534291506127
-0.530926296162031 to -0.532568984389617
-0.458480069088361 to -0.460122757315948
-0.0688780636066412 to -0.070520751834228
-0.0753861755603772 to -0.077028863787964
Trying to learn from memory 76, 1, -0.2
sum 0.0377241180375053 distri 0.0232671721607484
Using diff 0.00502591636738061 and condRate 0.166666666666667
Changed category 1 weights from 
0.292465170799682 to 0.292297640251606
0.525780757843443 to 0.525613227295367
0.100855519949386 to 0.10068798940131
0.104948332487533 to 0.104780801939457
Changing layer 0's weights from 
-0.264117293715788 to -0.264284824263864
-0.636043750405624 to -0.636211280953699
-0.870393016100243 to -0.870560546648318
-0.835833766222312 to -0.836001296770387
-0.0879661968949579 to -0.0881337274430336
-0.770608150124863 to -0.770775680672938
-0.61677842652829 to -0.616945957076365
-0.644232832551314 to -0.64440036309939
-0.881724693537071 to -0.881892224085146
-0.925474748671366 to -0.925642279219442
Changing layer 1's weights from 
-0.920377836287334 to -0.920545366835409
-0.319209151625946 to -0.319376682174022
-0.444911890387848 to -0.445079420935924
-0.454817228675201 to -0.454984759223277
-0.639870010972336 to -0.640037541520411
-0.778107844949081 to -0.778275375497157
-0.507389121413543 to -0.507556651961619
-0.893192150354699 to -0.893359680902774
-0.481529169440582 to -0.481696699988658
-0.844176032305076 to -0.844343562853151
Changing layer 2's weights from 
-0.034987800479248 to -0.0351553310273238
-0.589041285872773 to -0.589208816420848
-0.304451995253875 to -0.304619525801951
-0.133251898646667 to -0.133419429194742
-0.310504608512237 to -0.310672139060313
-0.516514234900787 to -0.516681765448863
0.00229848110644591 to 0.00213095055837015
-0.070123546481445 to -0.0702910770295208
-0.802121841073349 to -0.802289371621424
-0.757164322495773 to -0.757331853043849
Changing layer 3's weights from 
-0.068038099170044 to -0.0682056297181198
-0.331591182113006 to -0.331758712661082
-0.0889812639954829 to -0.0891487945435586
-0.996420495751627 to -0.996588026299702
-0.2116571596864 to -0.211824690234476
-0.341905765891388 to -0.342073296439464
-0.672090613007858 to -0.672258143555933
-0.213577740550353 to -0.213745271098429
-0.545273714423493 to -0.545441244971568
-0.0724438360932619 to -0.0726113666413377
Changing layer 4's weights from 
-0.171717636943175 to -0.171885167491251
-0.380550437331512 to -0.380717967879588
-0.581028037429168 to -0.581195567977244
-0.29409330403836 to -0.294260834586435
-0.533346109748199 to -0.533513640296275
-0.301880889296844 to -0.30204841984492
-0.3062394193414 to -0.306406949889476
-0.768136941552474 to -0.76830447210055
-0.174440853953673 to -0.174608384501749
-0.606742792487458 to -0.606910323035533
Changing layer 5's weights from 
-0.018937819361999 to -0.0191053499100748
-0.46101134336026 to -0.461178873908336
-0.03582298076184 to -0.0359905113099158
-0.971335162133292 to -0.971502692681367
-0.434066348433807 to -0.434233878981883
-0.832534291506127 to -0.832701822054202
-0.532568984389617 to -0.532736514937693
-0.460122757315948 to -0.460290287864024
-0.070520751834228 to -0.0706882823823038
-0.077028863787964 to -0.0771963943360398
Trying to learn from memory 77, 1, -0.2
sum 0.0377241180375053 distri 0.0232671721607484
Using diff 0.00502591636738061 and condRate 0.166666666666667
Changed category 1 weights from 
0.292297640251606 to 0.29213010970353
0.525613227295367 to 0.525445696747291
0.10068798940131 to 0.100520458853234
0.104780801939457 to 0.104613271391381
Changing layer 0's weights from 
-0.264284824263864 to -0.26445235481194
-0.636211280953699 to -0.636378811501775
-0.870560546648318 to -0.870728077196394
-0.836001296770387 to -0.836168827318463
-0.0881337274430336 to -0.0883012579911094
-0.770775680672938 to -0.770943211221014
-0.616945957076365 to -0.617113487624441
-0.64440036309939 to -0.644567893647466
-0.881892224085146 to -0.882059754633222
-0.925642279219442 to -0.925809809767518
Changing layer 1's weights from 
-0.920545366835409 to -0.920712897383485
-0.319376682174022 to -0.319544212722097
-0.445079420935924 to -0.445246951483999
-0.454984759223277 to -0.455152289771352
-0.640037541520411 to -0.640205072068487
-0.778275375497157 to -0.778442906045233
-0.507556651961619 to -0.507724182509694
-0.893359680902774 to -0.89352721145085
-0.481696699988658 to -0.481864230536734
-0.844343562853151 to -0.844511093401227
Changing layer 2's weights from 
-0.0351553310273238 to -0.0353228615753995
-0.589208816420848 to -0.589376346968924
-0.304619525801951 to -0.304787056350026
-0.133419429194742 to -0.133586959742818
-0.310672139060313 to -0.310839669608388
-0.516681765448863 to -0.516849295996939
0.00213095055837015 to 0.0019634200102944
-0.0702910770295208 to -0.0704586075775965
-0.802289371621424 to -0.8024569021695
-0.757331853043849 to -0.757499383591925
Changing layer 3's weights from 
-0.0682056297181198 to -0.0683731602661955
-0.331758712661082 to -0.331926243209157
-0.0891487945435586 to -0.0893163250916344
-0.996588026299702 to -0.996755556847778
-0.211824690234476 to -0.211992220782552
-0.342073296439464 to -0.342240826987539
-0.672258143555933 to -0.672425674104009
-0.213745271098429 to -0.213912801646505
-0.545441244971568 to -0.545608775519644
-0.0726113666413377 to -0.0727788971894134
Changing layer 4's weights from 
-0.171885167491251 to -0.172052698039327
-0.380717967879588 to -0.380885498427663
-0.581195567977244 to -0.58136309852532
-0.294260834586435 to -0.294428365134511
-0.533513640296275 to -0.533681170844351
-0.30204841984492 to -0.302215950392995
-0.306406949889476 to -0.306574480437551
-0.76830447210055 to -0.768472002648626
-0.174608384501749 to -0.174775915049825
-0.606910323035533 to -0.607077853583609
Changing layer 5's weights from 
-0.0191053499100748 to -0.0192728804581505
-0.461178873908336 to -0.461346404456411
-0.0359905113099158 to -0.0361580418579915
-0.971502692681367 to -0.971670223229443
-0.434233878981883 to -0.434401409529958
-0.832701822054202 to -0.832869352602278
-0.532736514937693 to -0.532904045485769
-0.460290287864024 to -0.460457818412099
-0.0706882823823038 to -0.0708558129303795
-0.0771963943360398 to -0.0773639248841155
Trying to learn from memory 78, 1, -0.2
sum 0.0377241180375053 distri 0.0232671721607484
Using diff 0.00502591636738061 and condRate 0.166666666666667
Changed category 1 weights from 
0.29213010970353 to 0.291962579155454
0.525445696747291 to 0.525278166199216
0.100520458853234 to 0.100352928305159
0.104613271391381 to 0.104445740843306
Changing layer 0's weights from 
-0.26445235481194 to -0.264619885360016
-0.636378811501775 to -0.636546342049851
-0.870728077196394 to -0.87089560774447
-0.836168827318463 to -0.836336357866539
-0.0883012579911094 to -0.0884687885391851
-0.770943211221014 to -0.77111074176909
-0.617113487624441 to -0.617281018172517
-0.644567893647466 to -0.644735424195542
-0.882059754633222 to -0.882227285181298
-0.925809809767518 to -0.925977340315594
Changing layer 1's weights from 
-0.920712897383485 to -0.920880427931561
-0.319544212722097 to -0.319711743270173
-0.445246951483999 to -0.445414482032075
-0.455152289771352 to -0.455319820319428
-0.640205072068487 to -0.640372602616563
-0.778442906045233 to -0.778610436593309
-0.507724182509694 to -0.50789171305777
-0.89352721145085 to -0.893694741998926
-0.481864230536734 to -0.482031761084809
-0.844511093401227 to -0.844678623949303
Changing layer 2's weights from 
-0.0353228615753995 to -0.0354903921234753
-0.589376346968924 to -0.589543877517
-0.304787056350026 to -0.304954586898102
-0.133586959742818 to -0.133754490290894
-0.310839669608388 to -0.311007200156464
-0.516849295996939 to -0.517016826545014
0.0019634200102944 to 0.00179588946221865
-0.0704586075775965 to -0.0706261381256723
-0.8024569021695 to -0.802624432717576
-0.757499383591925 to -0.757666914140001
Changing layer 3's weights from 
-0.0683731602661955 to -0.0685406908142713
-0.331926243209157 to -0.332093773757233
-0.0893163250916344 to -0.0894838556397101
-0.996755556847778 to -0.996923087395854
-0.211992220782552 to -0.212159751330627
-0.342240826987539 to -0.342408357535615
-0.672425674104009 to -0.672593204652085
-0.213912801646505 to -0.21408033219458
-0.545608775519644 to -0.54577630606772
-0.0727788971894134 to -0.0729464277374892
Changing layer 4's weights from 
-0.172052698039327 to -0.172220228587402
-0.380885498427663 to -0.381053028975739
-0.58136309852532 to -0.581530629073396
-0.294428365134511 to -0.294595895682587
-0.533681170844351 to -0.533848701392427
-0.302215950392995 to -0.302383480941071
-0.306574480437551 to -0.306742010985627
-0.768472002648626 to -0.768639533196702
-0.174775915049825 to -0.1749434455979
-0.607077853583609 to -0.607245384131685
Changing layer 5's weights from 
-0.0192728804581505 to -0.0194404110062263
-0.461346404456411 to -0.461513935004487
-0.0361580418579915 to -0.0363255724060673
-0.971670223229443 to -0.971837753777519
-0.434401409529958 to -0.434568940078034
-0.832869352602278 to -0.833036883150354
-0.532904045485769 to -0.533071576033845
-0.460457818412099 to -0.460625348960175
-0.0708558129303795 to -0.0710233434784553
-0.0773639248841155 to -0.0775314554321913
Trying to learn from memory 71, 1, -0.2
sum 0.0377241180375053 distri 0.0232671721607484
Using diff 0.00502591636738061 and condRate 0.166666666666667
Changed category 1 weights from 
0.291962579155454 to 0.291795048607379
0.525278166199216 to 0.52511063565114
0.100352928305159 to 0.100185397757083
0.104445740843306 to 0.10427821029523
Changing layer 0's weights from 
-0.264619885360016 to -0.264787415908091
-0.636546342049851 to -0.636713872597926
-0.87089560774447 to -0.871063138292545
-0.836336357866539 to -0.836503888414614
-0.0884687885391851 to -0.0886363190872609
-0.77111074176909 to -0.771278272317165
-0.617281018172517 to -0.617448548720592
-0.644735424195542 to -0.644902954743617
-0.882227285181298 to -0.882394815729373
-0.925977340315594 to -0.926144870863669
Changing layer 1's weights from 
-0.920880427931561 to -0.921047958479636
-0.319711743270173 to -0.319879273818249
-0.445414482032075 to -0.445582012580151
-0.455319820319428 to -0.455487350867504
-0.640372602616563 to -0.640540133164638
-0.778610436593309 to -0.778777967141384
-0.50789171305777 to -0.508059243605846
-0.893694741998926 to -0.893862272547001
-0.482031761084809 to -0.482199291632885
-0.844678623949303 to -0.844846154497378
Changing layer 2's weights from 
-0.0354903921234753 to -0.035657922671551
-0.589543877517 to -0.589711408065075
-0.304954586898102 to -0.305122117446178
-0.133754490290894 to -0.13392202083897
-0.311007200156464 to -0.31117473070454
-0.517016826545014 to -0.51718435709309
0.00179588946221865 to 0.00162835891414289
-0.0706261381256723 to -0.070793668673748
-0.802624432717576 to -0.802791963265651
-0.757666914140001 to -0.757834444688076
Changing layer 3's weights from 
-0.0685406908142713 to -0.068708221362347
-0.332093773757233 to -0.332261304305309
-0.0894838556397101 to -0.0896513861877859
-0.996923087395854 to -0.997090617943929
-0.212159751330627 to -0.212327281878703
-0.342408357535615 to -0.342575888083691
-0.672593204652085 to -0.67276073520016
-0.21408033219458 to -0.214247862742656
-0.54577630606772 to -0.545943836615795
-0.0729464277374892 to -0.0731139582855649
Changing layer 4's weights from 
-0.172220228587402 to -0.172387759135478
-0.381053028975739 to -0.381220559523815
-0.581530629073396 to -0.581698159621471
-0.294595895682587 to -0.294763426230663
-0.533848701392427 to -0.534016231940502
-0.302383480941071 to -0.302551011489147
-0.306742010985627 to -0.306909541533703
-0.768639533196702 to -0.768807063744777
-0.1749434455979 to -0.175110976145976
-0.607245384131685 to -0.60741291467976
Changing layer 5's weights from 
-0.0194404110062263 to -0.0196079415543021
-0.461513935004487 to -0.461681465552563
-0.0363255724060673 to -0.036493102954143
-0.971837753777519 to -0.972005284325594
-0.434568940078034 to -0.43473647062611
-0.833036883150354 to -0.833204413698429
-0.533071576033845 to -0.53323910658192
-0.460625348960175 to -0.460792879508251
-0.0710233434784553 to -0.071190874026531
-0.0775314554321913 to -0.077698985980267
Trying to learn from memory 79, 1, -0.2
sum 0.0377241180375053 distri 0.0232671721607484
Using diff 0.00502591636738061 and condRate 0.166666666666667
Changed category 1 weights from 
0.291795048607379 to 0.291627518059303
0.52511063565114 to 0.524943105103064
0.100185397757083 to 0.100017867209007
0.10427821029523 to 0.104110679747154
Changing layer 0's weights from 
-0.264787415908091 to -0.264954946456167
-0.636713872597926 to -0.636881403146002
-0.871063138292545 to -0.871230668840621
-0.836503888414614 to -0.83667141896269
-0.0886363190872609 to -0.0888038496353366
-0.771278272317165 to -0.771445802865241
-0.617448548720592 to -0.617616079268668
-0.644902954743617 to -0.645070485291693
-0.882394815729373 to -0.882562346277449
-0.926144870863669 to -0.926312401411745
Changing layer 1's weights from 
-0.921047958479636 to -0.921215489027712
-0.319879273818249 to -0.320046804366325
-0.445582012580151 to -0.445749543128227
-0.455487350867504 to -0.45565488141558
-0.640540133164638 to -0.640707663712714
-0.778777967141384 to -0.77894549768946
-0.508059243605846 to -0.508226774153922
-0.893862272547001 to -0.894029803095077
-0.482199291632885 to -0.482366822180961
-0.844846154497378 to -0.845013685045454
Changing layer 2's weights from 
-0.035657922671551 to -0.0358254532196268
-0.589711408065075 to -0.589878938613151
-0.305122117446178 to -0.305289647994254
-0.13392202083897 to -0.134089551387045
-0.31117473070454 to -0.311342261252616
-0.51718435709309 to -0.517351887641166
0.00162835891414289 to 0.00146082836606714
-0.070793668673748 to -0.0709611992218238
-0.802791963265651 to -0.802959493813727
-0.757834444688076 to -0.758001975236152
Changing layer 3's weights from 
-0.068708221362347 to -0.0688757519104228
-0.332261304305309 to -0.332428834853385
-0.0896513861877859 to -0.0898189167358616
-0.997090617943929 to -0.997258148492005
-0.212327281878703 to -0.212494812426779
-0.342575888083691 to -0.342743418631767
-0.67276073520016 to -0.672928265748236
-0.214247862742656 to -0.214415393290732
-0.545943836615795 to -0.546111367163871
-0.0731139582855649 to -0.0732814888336407
Changing layer 4's weights from 
-0.172387759135478 to -0.172555289683554
-0.381220559523815 to -0.381388090071891
-0.581698159621471 to -0.581865690169547
-0.294763426230663 to -0.294930956778739
-0.534016231940502 to -0.534183762488578
-0.302551011489147 to -0.302718542037223
-0.306909541533703 to -0.307077072081779
-0.768807063744777 to -0.768974594292853
-0.175110976145976 to -0.175278506694052
-0.60741291467976 to -0.607580445227836
Changing layer 5's weights from 
-0.0196079415543021 to -0.0197754721023778
-0.461681465552563 to -0.461848996100639
-0.036493102954143 to -0.0366606335022188
-0.972005284325594 to -0.97217281487367
-0.43473647062611 to -0.434904001174186
-0.833204413698429 to -0.833371944246505
-0.53323910658192 to -0.533406637129996
-0.460792879508251 to -0.460960410056327
-0.071190874026531 to -0.0713584045746068
-0.077698985980267 to -0.0778665165283428
Trying to learn from memory 80, 0, -0.2
sum 0.0377241180375053 distri -0.0209875575651362
Using diff 0.0492806460932651 and condRate 0.166666666666667
Changed category 0 weights from 
0.174590470328965 to 0.172947782101378
-0.225444493755661 to -0.227087181983247
-0.507965190276467 to -0.509607878504054
-0.356459824308716 to -0.358102512536303
Changing layer 0's weights from 
-0.264954946456167 to -0.266597634683754
-0.636881403146002 to -0.638524091373589
-0.871230668840621 to -0.872873357068208
-0.83667141896269 to -0.838314107190277
-0.0888038496353366 to -0.0904465378629234
-0.771445802865241 to -0.773088491092828
-0.617616079268668 to -0.619258767496255
-0.645070485291693 to -0.64671317351928
-0.882562346277449 to -0.884205034505036
-0.926312401411745 to -0.927955089639332
Changing layer 1's weights from 
-0.921215489027712 to -0.922858177255299
-0.320046804366325 to -0.321689492593911
-0.445749543128227 to -0.447392231355814
-0.45565488141558 to -0.457297569643167
-0.640707663712714 to -0.642350351940301
-0.77894549768946 to -0.780588185917047
-0.508226774153922 to -0.509869462381508
-0.894029803095077 to -0.895672491322664
-0.482366822180961 to -0.484009510408548
-0.845013685045454 to -0.846656373273041
Changing layer 2's weights from 
-0.0358254532196268 to -0.0374681414472136
-0.589878938613151 to -0.591521626840738
-0.305289647994254 to -0.30693233622184
-0.134089551387045 to -0.135732239614632
-0.311342261252616 to -0.312984949480202
-0.517351887641166 to -0.518994575868753
0.00146082836606714 to -0.00018185986151966
-0.0709611992218238 to -0.0726038874494106
-0.802959493813727 to -0.804602182041314
-0.758001975236152 to -0.759644663463739
Changing layer 3's weights from 
-0.0688757519104228 to -0.0705184401380096
-0.332428834853385 to -0.334071523080972
-0.0898189167358616 to -0.0914616049634484
-0.997258148492005 to -0.998900836719592
-0.212494812426779 to -0.214137500654366
-0.342743418631767 to -0.344386106859354
-0.672928265748236 to -0.674570953975823
-0.214415393290732 to -0.216058081518319
-0.546111367163871 to -0.547754055391458
-0.0732814888336407 to -0.0749241770612275
Changing layer 4's weights from 
-0.172555289683554 to -0.174197977911141
-0.381388090071891 to -0.383030778299478
-0.581865690169547 to -0.583508378397134
-0.294930956778739 to -0.296573645006325
-0.534183762488578 to -0.535826450716165
-0.302718542037223 to -0.304361230264809
-0.307077072081779 to -0.308719760309365
-0.768974594292853 to -0.77061728252044
-0.175278506694052 to -0.176921194921639
-0.607580445227836 to -0.609223133455423
Changing layer 5's weights from 
-0.0197754721023778 to -0.0214181603299646
-0.461848996100639 to -0.463491684328226
-0.0366606335022188 to -0.0383033217298056
-0.97217281487367 to -0.973815503101257
-0.434904001174186 to -0.436546689401773
-0.833371944246505 to -0.835014632474092
-0.533406637129996 to -0.535049325357583
-0.460960410056327 to -0.462603098283914
-0.0713584045746068 to -0.0730010928021936
-0.0778665165283428 to -0.0795092047559296
10/5/2016 1:43:57 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 69, 1, -0.2
sum 0.0377241180375053 distri 0.0232671721607484
Using diff 0.00502591636738061 and condRate 0.166666666666667
Changed category 1 weights from 
0.291627518059303 to 0.291459987511227
0.524943105103064 to 0.524775574554989
0.100017867209007 to 0.0998503366609313
0.104110679747154 to 0.103943149199078
Changing layer 0's weights from 
-0.266597634683754 to -0.26676516523183
-0.638524091373589 to -0.638691621921665
-0.872873357068208 to -0.873040887616284
-0.838314107190277 to -0.838481637738353
-0.0904465378629234 to -0.0906140684109992
-0.773088491092828 to -0.773256021640904
-0.619258767496255 to -0.619426298044331
-0.64671317351928 to -0.646880704067356
-0.884205034505036 to -0.884372565053112
-0.927955089639332 to -0.928122620187408
Changing layer 1's weights from 
-0.922858177255299 to -0.923025707803375
-0.321689492593911 to -0.321857023141987
-0.447392231355814 to -0.447559761903889
-0.457297569643167 to -0.457465100191242
-0.642350351940301 to -0.642517882488377
-0.780588185917047 to -0.780755716465123
-0.509869462381508 to -0.510036992929584
-0.895672491322664 to -0.89584002187074
-0.484009510408548 to -0.484177040956623
-0.846656373273041 to -0.846823903821117
Changing layer 2's weights from 
-0.0374681414472136 to -0.0376356719952893
-0.591521626840738 to -0.591689157388814
-0.30693233622184 to -0.307099866769916
-0.135732239614632 to -0.135899770162708
-0.312984949480202 to -0.313152480028278
-0.518994575868753 to -0.519162106416828
-0.00018185986151966 to -0.000349390409595414
-0.0726038874494106 to -0.0727714179974863
-0.804602182041314 to -0.80476971258939
-0.759644663463739 to -0.759812194011815
Changing layer 3's weights from 
-0.0705184401380096 to -0.0706859706860853
-0.334071523080972 to -0.334239053629047
-0.0914616049634484 to -0.0916291355115242
-0.998900836719592 to -0.999068367267668
-0.214137500654366 to -0.214305031202441
-0.344386106859354 to -0.344553637407429
-0.674570953975823 to -0.674738484523899
-0.216058081518319 to -0.216225612066394
-0.547754055391458 to -0.547921585939534
-0.0749241770612275 to -0.0750917076093032
Changing layer 4's weights from 
-0.174197977911141 to -0.174365508459216
-0.383030778299478 to -0.383198308847553
-0.583508378397134 to -0.58367590894521
-0.296573645006325 to -0.296741175554401
-0.535826450716165 to -0.535993981264241
-0.304361230264809 to -0.304528760812885
-0.308719760309365 to -0.308887290857441
-0.77061728252044 to -0.770784813068516
-0.176921194921639 to -0.177088725469714
-0.609223133455423 to -0.609390664003499
Changing layer 5's weights from 
-0.0214181603299646 to -0.0215856908780404
-0.463491684328226 to -0.463659214876301
-0.0383033217298056 to -0.0384708522778813
-0.973815503101257 to -0.973983033649333
-0.436546689401773 to -0.436714219949848
-0.835014632474092 to -0.835182163022168
-0.535049325357583 to -0.535216855905659
-0.462603098283914 to -0.462770628831989
-0.0730010928021936 to -0.0731686233502693
-0.0795092047559296 to -0.0796767353040053
Trying to learn from memory 70, 0, -0.2
sum 0.0377241180375053 distri -0.0209875575651362
Using diff 0.0492806460932651 and condRate 0.166666666666667
Changed category 0 weights from 
0.172947782101378 to 0.171305093873791
-0.227087181983247 to -0.228729870210834
-0.509607878504054 to -0.511250566731641
-0.358102512536303 to -0.35974520076389
Changing layer 0's weights from 
-0.26676516523183 to -0.268407853459417
-0.638691621921665 to -0.640334310149252
-0.873040887616284 to -0.874683575843871
-0.838481637738353 to -0.84012432596594
-0.0906140684109992 to -0.092256756638586
-0.773256021640904 to -0.774898709868491
-0.619426298044331 to -0.621068986271918
-0.646880704067356 to -0.648523392294942
-0.884372565053112 to -0.886015253280699
-0.928122620187408 to -0.929765308414995
Changing layer 1's weights from 
-0.923025707803375 to -0.924668396030962
-0.321857023141987 to -0.323499711369574
-0.447559761903889 to -0.449202450131476
-0.457465100191242 to -0.459107788418829
-0.642517882488377 to -0.644160570715964
-0.780755716465123 to -0.782398404692709
-0.510036992929584 to -0.511679681157171
-0.89584002187074 to -0.897482710098327
-0.484177040956623 to -0.48581972918421
-0.846823903821117 to -0.848466592048704
Changing layer 2's weights from 
-0.0376356719952893 to -0.0392783602228761
-0.591689157388814 to -0.593331845616401
-0.307099866769916 to -0.308742554997503
-0.135899770162708 to -0.137542458390295
-0.313152480028278 to -0.314795168255865
-0.519162106416828 to -0.520804794644415
-0.000349390409595414 to -0.00199207863718221
-0.0727714179974863 to -0.0744141062250731
-0.80476971258939 to -0.806412400816977
-0.759812194011815 to -0.761454882239402
Changing layer 3's weights from 
-0.0706859706860853 to -0.0723286589136721
-0.334239053629047 to -0.335881741856634
-0.0916291355115242 to -0.093271823739111
-0.999068367267668 to -1.00071105549525
-0.214305031202441 to -0.215947719430028
-0.344553637407429 to -0.346196325635016
-0.674738484523899 to -0.676381172751486
-0.216225612066394 to -0.217868300293981
-0.547921585939534 to -0.549564274167121
-0.0750917076093032 to -0.07673439583689
Changing layer 4's weights from 
-0.174365508459216 to -0.176008196686803
-0.383198308847553 to -0.38484099707514
-0.58367590894521 to -0.585318597172796
-0.296741175554401 to -0.298383863781988
-0.535993981264241 to -0.537636669491827
-0.304528760812885 to -0.306171449040472
-0.308887290857441 to -0.310529979085028
-0.770784813068516 to -0.772427501296102
-0.177088725469714 to -0.178731413697301
-0.609390664003499 to -0.611033352231086
Changing layer 5's weights from 
-0.0215856908780404 to -0.0232283791056272
-0.463659214876301 to -0.465301903103888
-0.0384708522778813 to -0.0401135405054681
-0.973983033649333 to -0.97562572187692
-0.436714219949848 to -0.438356908177435
-0.835182163022168 to -0.836824851249755
-0.535216855905659 to -0.536859544133245
-0.462770628831989 to -0.464413317059576
-0.0731686233502693 to -0.0748113115778561
-0.0796767353040053 to -0.0813194235315921
Trying to learn from memory 70, 0, -0.2
sum 0.0377241180375053 distri -0.0209875575651362
Using diff 0.0492806460932651 and condRate 0.166666666666667
Changed category 0 weights from 
0.171305093873791 to 0.169662405646204
-0.228729870210834 to -0.230372558438421
-0.511250566731641 to -0.512893254959228
-0.35974520076389 to -0.361387888991476
Changing layer 0's weights from 
-0.268407853459417 to -0.270050541687003
-0.640334310149252 to -0.641976998376838
-0.874683575843871 to -0.876326264071457
-0.84012432596594 to -0.841767014193526
-0.092256756638586 to -0.0938994448661728
-0.774898709868491 to -0.776541398096077
-0.621068986271918 to -0.622711674499504
-0.648523392294942 to -0.650166080522529
-0.886015253280699 to -0.887657941508285
-0.929765308414995 to -0.931407996642581
Changing layer 1's weights from 
-0.924668396030962 to -0.926311084258548
-0.323499711369574 to -0.325142399597161
-0.449202450131476 to -0.450845138359063
-0.459107788418829 to -0.460750476646416
-0.644160570715964 to -0.64580325894355
-0.782398404692709 to -0.784041092920296
-0.511679681157171 to -0.513322369384758
-0.897482710098327 to -0.899125398325913
-0.48581972918421 to -0.487462417411797
-0.848466592048704 to -0.85010928027629
Changing layer 2's weights from 
-0.0392783602228761 to -0.0409210484504629
-0.593331845616401 to -0.594974533843987
-0.308742554997503 to -0.31038524322509
-0.137542458390295 to -0.139185146617882
-0.314795168255865 to -0.316437856483452
-0.520804794644415 to -0.522447482872002
-0.00199207863718221 to -0.00363476686476901
-0.0744141062250731 to -0.0760567944526599
-0.806412400816977 to -0.808055089044563
-0.761454882239402 to -0.763097570466988
Changing layer 3's weights from 
-0.0723286589136721 to -0.0739713471412589
-0.335881741856634 to -0.337524430084221
-0.093271823739111 to -0.0949145119666978
-1.00071105549525 to -1.00235374372284
-0.215947719430028 to -0.217590407657615
-0.346196325635016 to -0.347839013862603
-0.676381172751486 to -0.678023860979072
-0.217868300293981 to -0.219510988521568
-0.549564274167121 to -0.551206962394707
-0.07673439583689 to -0.0783770840644768
Changing layer 4's weights from 
-0.176008196686803 to -0.17765088491439
-0.38484099707514 to -0.386483685302727
-0.585318597172796 to -0.586961285400383
-0.298383863781988 to -0.300026552009575
-0.537636669491827 to -0.539279357719414
-0.306171449040472 to -0.307814137268059
-0.310529979085028 to -0.312172667312615
-0.772427501296102 to -0.774070189523689
-0.178731413697301 to -0.180374101924888
-0.611033352231086 to -0.612676040458672
Changing layer 5's weights from 
-0.0232283791056272 to -0.024871067333214
-0.465301903103888 to -0.466944591331475
-0.0401135405054681 to -0.0417562287330549
-0.97562572187692 to -0.977268410104506
-0.438356908177435 to -0.439999596405022
-0.836824851249755 to -0.838467539477341
-0.536859544133245 to -0.538502232360832
-0.464413317059576 to -0.466056005287163
-0.0748113115778561 to -0.0764539998054429
-0.0813194235315921 to -0.0829621117591789
Trying to learn from memory 70, 0, -0.2
sum 0.0377241180375053 distri -0.0209875575651362
Using diff 0.0492806460932651 and condRate 0.166666666666667
Changed category 0 weights from 
0.169662405646204 to 0.168019717418617
-0.230372558438421 to -0.232015246666008
-0.512893254959228 to -0.514535943186814
-0.361387888991476 to -0.363030577219063
Changing layer 0's weights from 
-0.270050541687003 to -0.27169322991459
-0.641976998376838 to -0.643619686604425
-0.876326264071457 to -0.877968952299044
-0.841767014193526 to -0.843409702421113
-0.0938994448661728 to -0.0955421330937596
-0.776541398096077 to -0.778184086323664
-0.622711674499504 to -0.624354362727091
-0.650166080522529 to -0.651808768750116
-0.887657941508285 to -0.889300629735872
-0.931407996642581 to -0.933050684870168
Changing layer 1's weights from 
-0.926311084258548 to -0.927953772486135
-0.325142399597161 to -0.326785087824748
-0.450845138359063 to -0.45248782658665
-0.460750476646416 to -0.462393164874003
-0.64580325894355 to -0.647445947171137
-0.784041092920296 to -0.785683781147883
-0.513322369384758 to -0.514965057612345
-0.899125398325913 to -0.9007680865535
-0.487462417411797 to -0.489105105639384
-0.85010928027629 to -0.851751968503877
Changing layer 2's weights from 
-0.0409210484504629 to -0.0425637366780497
-0.594974533843987 to -0.596617222071574
-0.31038524322509 to -0.312027931452677
-0.139185146617882 to -0.140827834845468
-0.316437856483452 to -0.318080544711039
-0.522447482872002 to -0.524090171099589
-0.00363476686476901 to -0.00527745509235581
-0.0760567944526599 to -0.0776994826802467
-0.808055089044563 to -0.80969777727215
-0.763097570466988 to -0.764740258694575
Changing layer 3's weights from 
-0.0739713471412589 to -0.0756140353688457
-0.337524430084221 to -0.339167118311808
-0.0949145119666978 to -0.0965572001942846
-1.00235374372284 to -1.00399643195043
-0.217590407657615 to -0.219233095885202
-0.347839013862603 to -0.34948170209019
-0.678023860979072 to -0.679666549206659
-0.219510988521568 to -0.221153676749155
-0.551206962394707 to -0.552849650622294
-0.0783770840644768 to -0.0800197722920636
Changing layer 4's weights from 
-0.17765088491439 to -0.179293573141977
-0.386483685302727 to -0.388126373530314
-0.586961285400383 to -0.58860397362797
-0.300026552009575 to -0.301669240237162
-0.539279357719414 to -0.540922045947001
-0.307814137268059 to -0.309456825495646
-0.312172667312615 to -0.313815355540202
-0.774070189523689 to -0.775712877751276
-0.180374101924888 to -0.182016790152475
-0.612676040458672 to -0.614318728686259
Changing layer 5's weights from 
-0.024871067333214 to -0.0265137555608008
-0.466944591331475 to -0.468587279559062
-0.0417562287330549 to -0.0433989169606417
-0.977268410104506 to -0.978911098332093
-0.439999596405022 to -0.441642284632609
-0.838467539477341 to -0.840110227704928
-0.538502232360832 to -0.540144920588419
-0.466056005287163 to -0.46769869351475
-0.0764539998054429 to -0.0780966880330297
-0.0829621117591789 to -0.0846047999867657
Trying to learn from memory 70, 1, -0.2
sum 0.0377241180375053 distri 0.0232671721607484
Using diff 0.00502591636738061 and condRate 0.166666666666667
Changed category 1 weights from 
0.291459987511227 to 0.291292456963151
0.524775574554989 to 0.524608044006913
0.0998503366609313 to 0.0996828061128555
0.103943149199078 to 0.103775618651003
Changing layer 0's weights from 
-0.27169322991459 to -0.271860760462666
-0.643619686604425 to -0.643787217152501
-0.877968952299044 to -0.87813648284712
-0.843409702421113 to -0.843577232969189
-0.0955421330937596 to -0.0957096636418353
-0.778184086323664 to -0.77835161687174
-0.624354362727091 to -0.624521893275167
-0.651808768750116 to -0.651976299298192
-0.889300629735872 to -0.889468160283948
-0.933050684870168 to -0.933218215418244
Changing layer 1's weights from 
-0.927953772486135 to -0.928121303034211
-0.326785087824748 to -0.326952618372823
-0.45248782658665 to -0.452655357134725
-0.462393164874003 to -0.462560695422079
-0.647445947171137 to -0.647613477719213
-0.785683781147883 to -0.785851311695959
-0.514965057612345 to -0.51513258816042
-0.9007680865535 to -0.900935617101576
-0.489105105639384 to -0.48927263618746
-0.851751968503877 to -0.851919499051953
Changing layer 2's weights from 
-0.0425637366780497 to -0.0427312672261255
-0.596617222071574 to -0.59678475261965
-0.312027931452677 to -0.312195462000752
-0.140827834845468 to -0.140995365393544
-0.318080544711039 to -0.318248075259114
-0.524090171099589 to -0.524257701647665
-0.00527745509235581 to -0.00544498564043157
-0.0776994826802467 to -0.0778670132283225
-0.80969777727215 to -0.809865307820226
-0.764740258694575 to -0.764907789242651
Changing layer 3's weights from 
-0.0756140353688457 to -0.0757815659169215
-0.339167118311808 to -0.339334648859883
-0.0965572001942846 to -0.0967247307423603
-1.00399643195043 to -1.0041639624985
-0.219233095885202 to -0.219400626433278
-0.34948170209019 to -0.349649232638266
-0.679666549206659 to -0.679834079754735
-0.221153676749155 to -0.221321207297231
-0.552849650622294 to -0.55301718117037
-0.0800197722920636 to -0.0801873028401394
Changing layer 4's weights from 
-0.179293573141977 to -0.179461103690053
-0.388126373530314 to -0.388293904078389
-0.58860397362797 to -0.588771504176046
-0.301669240237162 to -0.301836770785237
-0.540922045947001 to -0.541089576495077
-0.309456825495646 to -0.309624356043721
-0.313815355540202 to -0.313982886088277
-0.775712877751276 to -0.775880408299352
-0.182016790152475 to -0.182184320700551
-0.614318728686259 to -0.614486259234335
Changing layer 5's weights from 
-0.0265137555608008 to -0.0266812861088765
-0.468587279559062 to -0.468754810107137
-0.0433989169606417 to -0.0435664475087175
-0.978911098332093 to -0.979078628880169
-0.441642284632609 to -0.441809815180685
-0.840110227704928 to -0.840277758253004
-0.540144920588419 to -0.540312451136495
-0.46769869351475 to -0.467866224062826
-0.0780966880330297 to -0.0782642185811055
-0.0846047999867657 to -0.0847723305348415
Trying to learn from memory 67, 0, -0.2
sum 0.0377241180375053 distri -0.0209875575651362
Using diff 0.0492806460932651 and condRate 0.166666666666667
Changed category 0 weights from 
0.168019717418617 to 0.166377029191031
-0.232015246666008 to -0.233657934893594
-0.514535943186814 to -0.516178631414401
-0.363030577219063 to -0.36467326544665
Changing layer 0's weights from 
-0.271860760462666 to -0.273503448690253
-0.643787217152501 to -0.645429905380088
-0.87813648284712 to -0.879779171074707
-0.843577232969189 to -0.845219921196776
-0.0957096636418353 to -0.0973523518694221
-0.77835161687174 to -0.779994305099327
-0.624521893275167 to -0.626164581502754
-0.651976299298192 to -0.653618987525779
-0.889468160283948 to -0.891110848511535
-0.933218215418244 to -0.934860903645831
Changing layer 1's weights from 
-0.928121303034211 to -0.929763991261798
-0.326952618372823 to -0.32859530660041
-0.452655357134725 to -0.454298045362312
-0.462560695422079 to -0.464203383649665
-0.647613477719213 to -0.6492561659468
-0.785851311695959 to -0.787493999923546
-0.51513258816042 to -0.516775276388007
-0.900935617101576 to -0.902578305329163
-0.48927263618746 to -0.490915324415046
-0.851919499051953 to -0.85356218727954
Changing layer 2's weights from 
-0.0427312672261255 to -0.0443739554537123
-0.59678475261965 to -0.598427440847237
-0.312195462000752 to -0.313838150228339
-0.140995365393544 to -0.142638053621131
-0.318248075259114 to -0.319890763486701
-0.524257701647665 to -0.525900389875251
-0.00544498564043157 to -0.00708767386801837
-0.0778670132283225 to -0.0795097014559093
-0.809865307820226 to -0.811507996047813
-0.764907789242651 to -0.766550477470238
Changing layer 3's weights from 
-0.0757815659169215 to -0.0774242541445083
-0.339334648859883 to -0.34097733708747
-0.0967247307423603 to -0.0983674189699471
-1.0041639624985 to -1.00580665072609
-0.219400626433278 to -0.221043314660864
-0.349649232638266 to -0.351291920865852
-0.679834079754735 to -0.681476767982322
-0.221321207297231 to -0.222963895524817
-0.55301718117037 to -0.554659869397957
-0.0801873028401394 to -0.0818299910677262
Changing layer 4's weights from 
-0.179461103690053 to -0.181103791917639
-0.388293904078389 to -0.389936592305976
-0.588771504176046 to -0.590414192403633
-0.301836770785237 to -0.303479459012824
-0.541089576495077 to -0.542732264722664
-0.309624356043721 to -0.311267044271308
-0.313982886088277 to -0.315625574315864
-0.775880408299352 to -0.777523096526939
-0.182184320700551 to -0.183827008928137
-0.614486259234335 to -0.616128947461922
Changing layer 5's weights from 
-0.0266812861088765 to -0.0283239743364633
-0.468754810107137 to -0.470397498334724
-0.0435664475087175 to -0.0452091357363043
-0.979078628880169 to -0.980721317107756
-0.441809815180685 to -0.443452503408271
-0.840277758253004 to -0.841920446480591
-0.540312451136495 to -0.541955139364082
-0.467866224062826 to -0.469508912290412
-0.0782642185811055 to -0.0799069068086923
-0.0847723305348415 to -0.0864150187624283
Trying to learn from memory 71, 0, -0.2
sum 0.0377241180375053 distri -0.0209875575651362
Using diff 0.0492806460932651 and condRate 0.166666666666667
Changed category 0 weights from 
0.166377029191031 to 0.164734340963444
-0.233657934893594 to -0.235300623121181
-0.516178631414401 to -0.517821319641988
-0.36467326544665 to -0.366315953674237
Changing layer 0's weights from 
-0.273503448690253 to -0.275146136917839
-0.645429905380088 to -0.647072593607675
-0.879779171074707 to -0.881421859302294
-0.845219921196776 to -0.846862609424363
-0.0973523518694221 to -0.0989950400970089
-0.779994305099327 to -0.781636993326914
-0.626164581502754 to -0.627807269730341
-0.653618987525779 to -0.655261675753366
-0.891110848511535 to -0.892753536739122
-0.934860903645831 to -0.936503591873418
Changing layer 1's weights from 
-0.929763991261798 to -0.931406679489385
-0.32859530660041 to -0.330237994827997
-0.454298045362312 to -0.455940733589899
-0.464203383649665 to -0.465846071877252
-0.6492561659468 to -0.650898854174387
-0.787493999923546 to -0.789136688151133
-0.516775276388007 to -0.518417964615594
-0.902578305329163 to -0.90422099355675
-0.490915324415046 to -0.492558012642633
-0.85356218727954 to -0.855204875507127
Changing layer 2's weights from 
-0.0443739554537123 to -0.0460166436812991
-0.598427440847237 to -0.600070129074824
-0.313838150228339 to -0.315480838455926
-0.142638053621131 to -0.144280741848718
-0.319890763486701 to -0.321533451714288
-0.525900389875251 to -0.527543078102838
-0.00708767386801837 to -0.00873036209560517
-0.0795097014559093 to -0.0811523896834961
-0.811507996047813 to -0.8131506842754
-0.766550477470238 to -0.768193165697825
Changing layer 3's weights from 
-0.0774242541445083 to -0.0790669423720951
-0.34097733708747 to -0.342620025315057
-0.0983674189699471 to -0.100010107197534
-1.00580665072609 to -1.00744933895368
-0.221043314660864 to -0.222686002888451
-0.351291920865852 to -0.352934609093439
-0.681476767982322 to -0.683119456209909
-0.222963895524817 to -0.224606583752404
-0.554659869397957 to -0.556302557625544
-0.0818299910677262 to -0.083472679295313
Changing layer 4's weights from 
-0.181103791917639 to -0.182746480145226
-0.389936592305976 to -0.391579280533563
-0.590414192403633 to -0.59205688063122
-0.303479459012824 to -0.305122147240411
-0.542732264722664 to -0.544374952950251
-0.311267044271308 to -0.312909732498895
-0.315625574315864 to -0.317268262543451
-0.777523096526939 to -0.779165784754526
-0.183827008928137 to -0.185469697155724
-0.616128947461922 to -0.617771635689509
Changing layer 5's weights from 
-0.0283239743364633 to -0.0299666625640501
-0.470397498334724 to -0.472040186562311
-0.0452091357363043 to -0.0468518239638911
-0.980721317107756 to -0.982364005335343
-0.443452503408271 to -0.445095191635858
-0.841920446480591 to -0.843563134708178
-0.541955139364082 to -0.543597827591669
-0.469508912290412 to -0.471151600517999
-0.0799069068086923 to -0.0815495950362791
-0.0864150187624283 to -0.0880577069900151
Trying to learn from memory 71, 0, -0.2
sum 0.0377241180375053 distri -0.0209875575651362
Using diff 0.0492806460932651 and condRate 0.166666666666667
Changed category 0 weights from 
0.164734340963444 to 0.163091652735857
-0.235300623121181 to -0.236943311348768
-0.517821319641988 to -0.519464007869575
-0.366315953674237 to -0.367958641901824
Changing layer 0's weights from 
-0.275146136917839 to -0.276788825145426
-0.647072593607675 to -0.648715281835262
-0.881421859302294 to -0.883064547529881
-0.846862609424363 to -0.848505297651949
-0.0989950400970089 to -0.100637728324596
-0.781636993326914 to -0.7832796815545
-0.627807269730341 to -0.629449957957927
-0.655261675753366 to -0.656904363980952
-0.892753536739122 to -0.894396224966708
-0.936503591873418 to -0.938146280101004
Changing layer 1's weights from 
-0.931406679489385 to -0.933049367716971
-0.330237994827997 to -0.331880683055584
-0.455940733589899 to -0.457583421817486
-0.465846071877252 to -0.467488760104839
-0.650898854174387 to -0.652541542401973
-0.789136688151133 to -0.790779376378719
-0.518417964615594 to -0.520060652843181
-0.90422099355675 to -0.905863681784337
-0.492558012642633 to -0.49420070087022
-0.855204875507127 to -0.856847563734713
Changing layer 2's weights from 
-0.0460166436812991 to -0.0476593319088859
-0.600070129074824 to -0.601712817302411
-0.315480838455926 to -0.317123526683513
-0.144280741848718 to -0.145923430076305
-0.321533451714288 to -0.323176139941875
-0.527543078102838 to -0.529185766330425
-0.00873036209560517 to -0.010373050323192
-0.0811523896834961 to -0.0827950779110829
-0.8131506842754 to -0.814793372502986
-0.768193165697825 to -0.769835853925411
Changing layer 3's weights from 
-0.0790669423720951 to -0.0807096305996819
-0.342620025315057 to -0.344262713542644
-0.100010107197534 to -0.101652795425121
-1.00744933895368 to -1.00909202718126
-0.222686002888451 to -0.224328691116038
-0.352934609093439 to -0.354577297321026
-0.683119456209909 to -0.684762144437496
-0.224606583752404 to -0.226249271979991
-0.556302557625544 to -0.557945245853131
-0.083472679295313 to -0.0851153675228998
Changing layer 4's weights from 
-0.182746480145226 to -0.184389168372813
-0.391579280533563 to -0.39322196876115
-0.59205688063122 to -0.593699568858806
-0.305122147240411 to -0.306764835467998
-0.544374952950251 to -0.546017641177837
-0.312909732498895 to -0.314552420726482
-0.317268262543451 to -0.318910950771038
-0.779165784754526 to -0.780808472982112
-0.185469697155724 to -0.187112385383311
-0.617771635689509 to -0.619414323917096
Changing layer 5's weights from 
-0.0299666625640501 to -0.0316093507916369
-0.472040186562311 to -0.473682874789898
-0.0468518239638911 to -0.0484945121914779
-0.982364005335343 to -0.984006693562929
-0.445095191635858 to -0.446737879863445
-0.843563134708178 to -0.845205822935765
-0.543597827591669 to -0.545240515819255
-0.471151600517999 to -0.472794288745586
-0.0815495950362791 to -0.0831922832638659
-0.0880577069900151 to -0.0897003952176019
Trying to learn from memory 71, 1, -0.2
sum 0.0377241180375053 distri 0.0232671721607484
Using diff 0.00502591636738061 and condRate 0.166666666666667
Changed category 1 weights from 
0.291292456963151 to 0.291124926415075
0.524608044006913 to 0.524440513458837
0.0996828061128555 to 0.0995152755647798
0.103775618651003 to 0.103608088102927
Changing layer 0's weights from 
-0.276788825145426 to -0.276956355693502
-0.648715281835262 to -0.648882812383337
-0.883064547529881 to -0.883232078077956
-0.848505297651949 to -0.848672828200025
-0.100637728324596 to -0.100805258872672
-0.7832796815545 to -0.783447212102576
-0.629449957957927 to -0.629617488506003
-0.656904363980952 to -0.657071894529028
-0.894396224966708 to -0.894563755514784
-0.938146280101004 to -0.93831381064908
Changing layer 1's weights from 
-0.933049367716971 to -0.933216898265047
-0.331880683055584 to -0.332048213603659
-0.457583421817486 to -0.457750952365562
-0.467488760104839 to -0.467656290652915
-0.652541542401973 to -0.652709072950049
-0.790779376378719 to -0.790946906926795
-0.520060652843181 to -0.520228183391257
-0.905863681784337 to -0.906031212332412
-0.49420070087022 to -0.494368231418296
-0.856847563734713 to -0.857015094282789
Changing layer 2's weights from 
-0.0476593319088859 to -0.0478268624569616
-0.601712817302411 to -0.601880347850486
-0.317123526683513 to -0.317291057231589
-0.145923430076305 to -0.14609096062438
-0.323176139941875 to -0.323343670489951
-0.529185766330425 to -0.529353296878501
-0.010373050323192 to -0.0105405808712677
-0.0827950779110829 to -0.0829626084591586
-0.814793372502986 to -0.814960903051062
-0.769835853925411 to -0.770003384473487
Changing layer 3's weights from 
-0.0807096305996819 to -0.0808771611477576
-0.344262713542644 to -0.34443024409072
-0.101652795425121 to -0.101820325973197
-1.00909202718126 to -1.00925955772934
-0.224328691116038 to -0.224496221664114
-0.354577297321026 to -0.354744827869102
-0.684762144437496 to -0.684929674985571
-0.226249271979991 to -0.226416802528067
-0.557945245853131 to -0.558112776401206
-0.0851153675228998 to -0.0852828980709755
Changing layer 4's weights from 
-0.184389168372813 to -0.184556698920889
-0.39322196876115 to -0.393389499309226
-0.593699568858806 to -0.593867099406882
-0.306764835467998 to -0.306932366016073
-0.546017641177837 to -0.546185171725913
-0.314552420726482 to -0.314719951274558
-0.318910950771038 to -0.319078481319114
-0.780808472982112 to -0.780976003530188
-0.187112385383311 to -0.187279915931387
-0.619414323917096 to -0.619581854465171
Changing layer 5's weights from 
-0.0316093507916369 to -0.0317768813397127
-0.473682874789898 to -0.473850405337974
-0.0484945121914779 to -0.0486620427395536
-0.984006693562929 to -0.984174224111005
-0.446737879863445 to -0.446905410411521
-0.845205822935765 to -0.84537335348384
-0.545240515819255 to -0.545408046367331
-0.472794288745586 to -0.472961819293662
-0.0831922832638659 to -0.0833598138119416
-0.0897003952176019 to -0.0898679257656776
Trying to learn from memory 79, 0, -0.2
sum 0.0377241180375053 distri -0.0209875575651362
Using diff 0.0492806460932651 and condRate 0.166666666666667
Changed category 0 weights from 
0.163091652735857 to 0.16144896450827
-0.236943311348768 to -0.238585999576355
-0.519464007869575 to -0.521106696097162
-0.367958641901824 to -0.36960133012941
Changing layer 0's weights from 
-0.276956355693502 to -0.278599043921089
-0.648882812383337 to -0.650525500610924
-0.883232078077956 to -0.884874766305543
-0.848672828200025 to -0.850315516427612
-0.100805258872672 to -0.102447947100258
-0.783447212102576 to -0.785089900330163
-0.629617488506003 to -0.63126017673359
-0.657071894529028 to -0.658714582756615
-0.894563755514784 to -0.896206443742371
-0.93831381064908 to -0.939956498876667
Changing layer 1's weights from 
-0.933216898265047 to -0.934859586492634
-0.332048213603659 to -0.333690901831246
-0.457750952365562 to -0.459393640593148
-0.467656290652915 to -0.469298978880501
-0.652709072950049 to -0.654351761177636
-0.790946906926795 to -0.792589595154382
-0.520228183391257 to -0.521870871618843
-0.906031212332412 to -0.907673900559999
-0.494368231418296 to -0.496010919645882
-0.857015094282789 to -0.858657782510376
Changing layer 2's weights from 
-0.0478268624569616 to -0.0494695506845484
-0.601880347850486 to -0.603523036078073
-0.317291057231589 to -0.318933745459175
-0.14609096062438 to -0.147733648851967
-0.323343670489951 to -0.324986358717537
-0.529353296878501 to -0.530995985106088
-0.0105405808712677 to -0.0121832690988545
-0.0829626084591586 to -0.0846052966867454
-0.814960903051062 to -0.816603591278649
-0.770003384473487 to -0.771646072701074
Changing layer 3's weights from 
-0.0808771611477576 to -0.0825198493753444
-0.34443024409072 to -0.346072932318306
-0.101820325973197 to -0.103463014200783
-1.00925955772934 to -1.01090224595693
-0.224496221664114 to -0.2261389098917
-0.354744827869102 to -0.356387516096688
-0.684929674985571 to -0.686572363213158
-0.226416802528067 to -0.228059490755653
-0.558112776401206 to -0.559755464628793
-0.0852828980709755 to -0.0869255862985623
Changing layer 4's weights from 
-0.184556698920889 to -0.186199387148476
-0.393389499309226 to -0.395032187536812
-0.593867099406882 to -0.595509787634469
-0.306932366016073 to -0.30857505424366
-0.546185171725913 to -0.5478278599535
-0.314719951274558 to -0.316362639502144
-0.319078481319114 to -0.3207211695467
-0.780976003530188 to -0.782618691757775
-0.187279915931387 to -0.188922604158974
-0.619581854465171 to -0.621224542692758
Changing layer 5's weights from 
-0.0317768813397127 to -0.0334195695672995
-0.473850405337974 to -0.47549309356556
-0.0486620427395536 to -0.0503047309671404
-0.984174224111005 to -0.985816912338592
-0.446905410411521 to -0.448548098639107
-0.84537335348384 to -0.847016041711427
-0.545408046367331 to -0.547050734594918
-0.472961819293662 to -0.474604507521248
-0.0833598138119416 to -0.0850025020395284
-0.0898679257656776 to -0.0915106139932644
10/5/2016 1:43:57 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 79, 0, -0.2
sum 0.0377241180375053 distri -0.0209875575651362
Using diff 0.0492806460932651 and condRate 0.166666666666667
Changed category 0 weights from 
0.16144896450827 to 0.159806276280683
-0.238585999576355 to -0.240228687803942
-0.521106696097162 to -0.522749384324749
-0.36960133012941 to -0.371244018356997
Changing layer 0's weights from 
-0.278599043921089 to -0.280241732148676
-0.650525500610924 to -0.652168188838511
-0.884874766305543 to -0.88651745453313
-0.850315516427612 to -0.851958204655199
-0.102447947100258 to -0.104090635327845
-0.785089900330163 to -0.78673258855775
-0.63126017673359 to -0.632902864961177
-0.658714582756615 to -0.660357270984202
-0.896206443742371 to -0.897849131969958
-0.939956498876667 to -0.941599187104254
Changing layer 1's weights from 
-0.934859586492634 to -0.936502274720221
-0.333690901831246 to -0.335333590058833
-0.459393640593148 to -0.461036328820735
-0.469298978880501 to -0.470941667108088
-0.654351761177636 to -0.655994449405223
-0.792589595154382 to -0.794232283381969
-0.521870871618843 to -0.52351355984643
-0.907673900559999 to -0.909316588787586
-0.496010919645882 to -0.497653607873469
-0.858657782510376 to -0.860300470737963
Changing layer 2's weights from 
-0.0494695506845484 to -0.0511122389121352
-0.603523036078073 to -0.60516572430566
-0.318933745459175 to -0.320576433686762
-0.147733648851967 to -0.149376337079554
-0.324986358717537 to -0.326629046945124
-0.530995985106088 to -0.532638673333674
-0.0121832690988545 to -0.0138259573264413
-0.0846052966867454 to -0.0862479849143322
-0.816603591278649 to -0.818246279506236
-0.771646072701074 to -0.773288760928661
Changing layer 3's weights from 
-0.0825198493753444 to -0.0841625376029312
-0.346072932318306 to -0.347715620545893
-0.103463014200783 to -0.10510570242837
-1.01090224595693 to -1.01254493418451
-0.2261389098917 to -0.227781598119287
-0.356387516096688 to -0.358030204324275
-0.686572363213158 to -0.688215051440745
-0.228059490755653 to -0.22970217898324
-0.559755464628793 to -0.56139815285638
-0.0869255862985623 to -0.0885682745261491
Changing layer 4's weights from 
-0.186199387148476 to -0.187842075376062
-0.395032187536812 to -0.396674875764399
-0.595509787634469 to -0.597152475862056
-0.30857505424366 to -0.310217742471247
-0.5478278599535 to -0.549470548181087
-0.316362639502144 to -0.318005327729731
-0.3207211695467 to -0.322363857774287
-0.782618691757775 to -0.784261379985362
-0.188922604158974 to -0.19056529238656
-0.621224542692758 to -0.622867230920345
Changing layer 5's weights from 
-0.0334195695672995 to -0.0350622577948863
-0.47549309356556 to -0.477135781793147
-0.0503047309671404 to -0.0519474191947272
-0.985816912338592 to -0.987459600566179
-0.448548098639107 to -0.450190786866694
-0.847016041711427 to -0.848658729939014
-0.547050734594918 to -0.548693422822505
-0.474604507521248 to -0.476247195748835
-0.0850025020395284 to -0.0866451902671152
-0.0915106139932644 to -0.0931533022208512
Trying to learn from memory 79, 0, -0.2
sum 0.0377241180375053 distri -0.0209875575651362
Using diff 0.0492806460932651 and condRate 0.166666666666667
Changed category 0 weights from 
0.159806276280683 to 0.158163588053097
-0.240228687803942 to -0.241871376031528
-0.522749384324749 to -0.524392072552335
-0.371244018356997 to -0.372886706584584
Changing layer 0's weights from 
-0.280241732148676 to -0.281884420376262
-0.652168188838511 to -0.653810877066098
-0.88651745453313 to -0.888160142760717
-0.851958204655199 to -0.853600892882786
-0.104090635327845 to -0.105733323555432
-0.78673258855775 to -0.788375276785337
-0.632902864961177 to -0.634545553188764
-0.660357270984202 to -0.661999959211789
-0.897849131969958 to -0.899491820197545
-0.941599187104254 to -0.943241875331841
Changing layer 1's weights from 
-0.936502274720221 to -0.938144962947808
-0.335333590058833 to -0.33697627828642
-0.461036328820735 to -0.462679017048322
-0.470941667108088 to -0.472584355335675
-0.655994449405223 to -0.65763713763281
-0.794232283381969 to -0.795874971609556
-0.52351355984643 to -0.525156248074017
-0.909316588787586 to -0.910959277015173
-0.497653607873469 to -0.499296296101056
-0.860300470737963 to -0.86194315896555
Changing layer 2's weights from 
-0.0511122389121352 to -0.052754927139722
-0.60516572430566 to -0.606808412533247
-0.320576433686762 to -0.322219121914349
-0.149376337079554 to -0.151019025307141
-0.326629046945124 to -0.328271735172711
-0.532638673333674 to -0.534281361561261
-0.0138259573264413 to -0.0154686455540281
-0.0862479849143322 to -0.087890673141919
-0.818246279506236 to -0.819888967733823
-0.773288760928661 to -0.774931449156248
Changing layer 3's weights from 
-0.0841625376029312 to -0.085805225830518
-0.347715620545893 to -0.34935830877348
-0.10510570242837 to -0.106748390655957
-1.01254493418451 to -1.0141876224121
-0.227781598119287 to -0.229424286346874
-0.358030204324275 to -0.359672892551862
-0.688215051440745 to -0.689857739668332
-0.22970217898324 to -0.231344867210827
-0.56139815285638 to -0.563040841083967
-0.0885682745261491 to -0.0902109627537359
Changing layer 4's weights from 
-0.187842075376062 to -0.189484763603649
-0.396674875764399 to -0.398317563991986
-0.597152475862056 to -0.598795164089643
-0.310217742471247 to -0.311860430698834
-0.549470548181087 to -0.551113236408674
-0.318005327729731 to -0.319648015957318
-0.322363857774287 to -0.324006546001874
-0.784261379985362 to -0.785904068212949
-0.19056529238656 to -0.192207980614147
-0.622867230920345 to -0.624509919147932
Changing layer 5's weights from 
-0.0350622577948863 to -0.0367049460224731
-0.477135781793147 to -0.478778470020734
-0.0519474191947272 to -0.053590107422314
-0.987459600566179 to -0.989102288793766
-0.450190786866694 to -0.451833475094281
-0.848658729939014 to -0.850301418166601
-0.548693422822505 to -0.550336111050092
-0.476247195748835 to -0.477889883976422
-0.0866451902671152 to -0.088287878494702
-0.0931533022208512 to -0.094795990448438
Trying to learn from memory 79, 0, -0.2
sum 0.0377241180375053 distri -0.0209875575651362
Using diff 0.0492806460932651 and condRate 0.166666666666667
Changed category 0 weights from 
0.158163588053097 to 0.15652089982551
-0.241871376031528 to -0.243514064259115
-0.524392072552335 to -0.526034760779922
-0.372886706584584 to -0.374529394812171
Changing layer 0's weights from 
-0.281884420376262 to -0.283527108603849
-0.653810877066098 to -0.655453565293685
-0.888160142760717 to -0.889802830988304
-0.853600892882786 to -0.855243581110373
-0.105733323555432 to -0.107376011783019
-0.788375276785337 to -0.790017965012924
-0.634545553188764 to -0.636188241416351
-0.661999959211789 to -0.663642647439376
-0.899491820197545 to -0.901134508425132
-0.943241875331841 to -0.944884563559428
Changing layer 1's weights from 
-0.938144962947808 to -0.939787651175395
-0.33697627828642 to -0.338618966514007
-0.462679017048322 to -0.464321705275909
-0.472584355335675 to -0.474227043563262
-0.65763713763281 to -0.659279825860397
-0.795874971609556 to -0.797517659837143
-0.525156248074017 to -0.526798936301604
-0.910959277015173 to -0.91260196524276
-0.499296296101056 to -0.500938984328643
-0.86194315896555 to -0.863585847193137
Changing layer 2's weights from 
-0.052754927139722 to -0.0543976153673088
-0.606808412533247 to -0.608451100760834
-0.322219121914349 to -0.323861810141936
-0.151019025307141 to -0.152661713534727
-0.328271735172711 to -0.329914423400298
-0.534281361561261 to -0.535924049788848
-0.0154686455540281 to -0.0171113337816149
-0.087890673141919 to -0.0895333613695058
-0.819888967733823 to -0.82153165596141
-0.774931449156248 to -0.776574137383835
Changing layer 3's weights from 
-0.085805225830518 to -0.0874479140581048
-0.34935830877348 to -0.351000997001067
-0.106748390655957 to -0.108391078883544
-1.0141876224121 to -1.01583031063969
-0.229424286346874 to -0.231066974574461
-0.359672892551862 to -0.361315580779449
-0.689857739668332 to -0.691500427895919
-0.231344867210827 to -0.232987555438414
-0.563040841083967 to -0.564683529311554
-0.0902109627537359 to -0.0918536509813227
Changing layer 4's weights from 
-0.189484763603649 to -0.191127451831236
-0.398317563991986 to -0.399960252219573
-0.598795164089643 to -0.600437852317229
-0.311860430698834 to -0.313503118926421
-0.551113236408674 to -0.55275592463626
-0.319648015957318 to -0.321290704184905
-0.324006546001874 to -0.325649234229461
-0.785904068212949 to -0.787546756440536
-0.192207980614147 to -0.193850668841734
-0.624509919147932 to -0.626152607375519
Changing layer 5's weights from 
-0.0367049460224731 to -0.0383476342500599
-0.478778470020734 to -0.480421158248321
-0.053590107422314 to -0.0552327956499008
-0.989102288793766 to -0.990744977021353
-0.451833475094281 to -0.453476163321868
-0.850301418166601 to -0.851944106394188
-0.550336111050092 to -0.551978799277678
-0.477889883976422 to -0.479532572204009
-0.088287878494702 to -0.0899305667222888
-0.094795990448438 to -0.0964386786760248
Trying to learn from memory 79, 1, -0.2
sum 0.0377241180375053 distri 0.0232671721607484
Using diff 0.00502591636738061 and condRate 0.166666666666667
Changed category 1 weights from 
0.291124926415075 to 0.290957395867
0.524440513458837 to 0.524272982910761
0.0995152755647798 to 0.099347745016704
0.103608088102927 to 0.103440557554851
Changing layer 0's weights from 
-0.283527108603849 to -0.283694639151925
-0.655453565293685 to -0.65562109584176
-0.889802830988304 to -0.889970361536379
-0.855243581110373 to -0.855411111658448
-0.107376011783019 to -0.107543542331094
-0.790017965012924 to -0.790185495560999
-0.636188241416351 to -0.636355771964426
-0.663642647439376 to -0.663810177987451
-0.901134508425132 to -0.901302038973207
-0.944884563559428 to -0.945052094107503
Changing layer 1's weights from 
-0.939787651175395 to -0.93995518172347
-0.338618966514007 to -0.338786497062082
-0.464321705275909 to -0.464489235823985
-0.474227043563262 to -0.474394574111338
-0.659279825860397 to -0.659447356408472
-0.797517659837143 to -0.797685190385218
-0.526798936301604 to -0.52696646684968
-0.91260196524276 to -0.912769495790835
-0.500938984328643 to -0.501106514876719
-0.863585847193137 to -0.863753377741212
Changing layer 2's weights from 
-0.0543976153673088 to -0.0545651459153846
-0.608451100760834 to -0.608618631308909
-0.323861810141936 to -0.324029340690011
-0.152661713534727 to -0.152829244082803
-0.329914423400298 to -0.330081953948373
-0.535924049788848 to -0.536091580336924
-0.0171113337816149 to -0.0172788643296907
-0.0895333613695058 to -0.0897008919175816
-0.82153165596141 to -0.821699186509485
-0.776574137383835 to -0.77674166793191
Changing layer 3's weights from 
-0.0874479140581048 to -0.0876154446061806
-0.351000997001067 to -0.351168527549143
-0.108391078883544 to -0.108558609431619
-1.01583031063969 to -1.01599784118776
-0.231066974574461 to -0.231234505122537
-0.361315580779449 to -0.361483111327525
-0.691500427895919 to -0.691667958443994
-0.232987555438414 to -0.23315508598649
-0.564683529311554 to -0.564851059859629
-0.0918536509813227 to -0.0920211815293985
Changing layer 4's weights from 
-0.191127451831236 to -0.191294982379312
-0.399960252219573 to -0.400127782767649
-0.600437852317229 to -0.600605382865305
-0.313503118926421 to -0.313670649474496
-0.55275592463626 to -0.552923455184336
-0.321290704184905 to -0.32145823473298
-0.325649234229461 to -0.325816764777536
-0.787546756440536 to -0.787714286988611
-0.193850668841734 to -0.19401819938981
-0.626152607375519 to -0.626320137923594
Changing layer 5's weights from 
-0.0383476342500599 to -0.0385151647981356
-0.480421158248321 to -0.480588688796397
-0.0552327956499008 to -0.0554003261979766
-0.990744977021353 to -0.990912507569428
-0.453476163321868 to -0.453643693869944
-0.851944106394188 to -0.852111636942263
-0.551978799277678 to -0.552146329825754
-0.479532572204009 to -0.479700102752085
-0.0899305667222888 to -0.0900980972703646
-0.0964386786760248 to -0.0966062092241006
Trying to learn from memory 80, 0, -0.2
sum 0.0377241180375053 distri -0.0209875575651362
Using diff 0.0492806460932651 and condRate 0.166666666666667
Changed category 0 weights from 
0.15652089982551 to 0.154878211597923
-0.243514064259115 to -0.245156752486702
-0.526034760779922 to -0.527677449007509
-0.374529394812171 to -0.376172083039758
Changing layer 0's weights from 
-0.283694639151925 to -0.285337327379512
-0.65562109584176 to -0.657263784069347
-0.889970361536379 to -0.891613049763966
-0.855411111658448 to -0.857053799886035
-0.107543542331094 to -0.109186230558681
-0.790185495560999 to -0.791828183788586
-0.636355771964426 to -0.637998460192013
-0.663810177987451 to -0.665452866215038
-0.901302038973207 to -0.902944727200794
-0.945052094107503 to -0.94669478233509
Changing layer 1's weights from 
-0.93995518172347 to -0.941597869951057
-0.338786497062082 to -0.340429185289669
-0.464489235823985 to -0.466131924051571
-0.474394574111338 to -0.476037262338924
-0.659447356408472 to -0.661090044636059
-0.797685190385218 to -0.799327878612805
-0.52696646684968 to -0.528609155077267
-0.912769495790835 to -0.914412184018422
-0.501106514876719 to -0.502749203104305
-0.863753377741212 to -0.865396065968799
Changing layer 2's weights from 
-0.0545651459153846 to -0.0562078341429714
-0.608618631308909 to -0.610261319536496
-0.324029340690011 to -0.325672028917598
-0.152829244082803 to -0.15447193231039
-0.330081953948373 to -0.33172464217596
-0.536091580336924 to -0.537734268564511
-0.0172788643296907 to -0.0189215525572775
-0.0897008919175816 to -0.0913435801451684
-0.821699186509485 to -0.823341874737072
-0.77674166793191 to -0.778384356159497
Changing layer 3's weights from 
-0.0876154446061806 to -0.0892581328337674
-0.351168527549143 to -0.352811215776729
-0.108558609431619 to -0.110201297659206
-1.01599784118776 to -1.01764052941535
-0.231234505122537 to -0.232877193350123
-0.361483111327525 to -0.363125799555111
-0.691667958443994 to -0.693310646671581
-0.23315508598649 to -0.234797774214076
-0.564851059859629 to -0.566493748087216
-0.0920211815293985 to -0.0936638697569853
Changing layer 4's weights from 
-0.191294982379312 to -0.192937670606898
-0.400127782767649 to -0.401770470995235
-0.600605382865305 to -0.602248071092892
-0.313670649474496 to -0.315313337702083
-0.552923455184336 to -0.554566143411923
-0.32145823473298 to -0.323100922960567
-0.325816764777536 to -0.327459453005123
-0.787714286988611 to -0.789356975216198
-0.19401819938981 to -0.195660887617396
-0.626320137923594 to -0.627962826151181
Changing layer 5's weights from 
-0.0385151647981356 to -0.0401578530257224
-0.480588688796397 to -0.482231377023983
-0.0554003261979766 to -0.0570430144255634
-0.990912507569428 to -0.992555195797015
-0.453643693869944 to -0.45528638209753
-0.852111636942263 to -0.85375432516985
-0.552146329825754 to -0.553789018053341
-0.479700102752085 to -0.481342790979671
-0.0900980972703646 to -0.0917407854979514
-0.0966062092241006 to -0.0982488974516874
Trying to learn from memory 69, 1, -0.2
sum 0.0377241180375053 distri 0.0232671721607484
Using diff 0.00502591636738061 and condRate 0.166666666666667
Changed category 1 weights from 
0.290957395867 to 0.290789865318924
0.524272982910761 to 0.524105452362686
0.099347745016704 to 0.0991802144686283
0.103440557554851 to 0.103273027006775
Changing layer 0's weights from 
-0.285337327379512 to -0.285504857927588
-0.657263784069347 to -0.657431314617423
-0.891613049763966 to -0.891780580312042
-0.857053799886035 to -0.857221330434111
-0.109186230558681 to -0.109353761106757
-0.791828183788586 to -0.791995714336662
-0.637998460192013 to -0.638165990740089
-0.665452866215038 to -0.665620396763114
-0.902944727200794 to -0.90311225774887
-0.94669478233509 to -0.946862312883166
Changing layer 1's weights from 
-0.941597869951057 to -0.941765400499133
-0.340429185289669 to -0.340596715837745
-0.466131924051571 to -0.466299454599647
-0.476037262338924 to -0.476204792887
-0.661090044636059 to -0.661257575184135
-0.799327878612805 to -0.799495409160881
-0.528609155077267 to -0.528776685625342
-0.914412184018422 to -0.914579714566498
-0.502749203104305 to -0.502916733652381
-0.865396065968799 to -0.865563596516875
Changing layer 2's weights from 
-0.0562078341429714 to -0.0563753646910471
-0.610261319536496 to -0.610428850084572
-0.325672028917598 to -0.325839559465674
-0.15447193231039 to -0.154639462858466
-0.33172464217596 to -0.331892172724036
-0.537734268564511 to -0.537901799112586
-0.0189215525572775 to -0.0190890831053532
-0.0913435801451684 to -0.0915111106932441
-0.823341874737072 to -0.823509405285148
-0.778384356159497 to -0.778551886707573
Changing layer 3's weights from 
-0.0892581328337674 to -0.0894256633818431
-0.352811215776729 to -0.352978746324805
-0.110201297659206 to -0.110368828207282
-1.01764052941535 to -1.01780805996343
-0.232877193350123 to -0.233044723898199
-0.363125799555111 to -0.363293330103187
-0.693310646671581 to -0.693478177219657
-0.234797774214076 to -0.234965304762152
-0.566493748087216 to -0.566661278635292
-0.0936638697569853 to -0.093831400305061
Changing layer 4's weights from 
-0.192937670606898 to -0.193105201154974
-0.401770470995235 to -0.401938001543311
-0.602248071092892 to -0.602415601640968
-0.315313337702083 to -0.315480868250159
-0.554566143411923 to -0.554733673959999
-0.323100922960567 to -0.323268453508643
-0.327459453005123 to -0.327626983553199
-0.789356975216198 to -0.789524505764274
-0.195660887617396 to -0.195828418165472
-0.627962826151181 to -0.628130356699257
Changing layer 5's weights from 
-0.0401578530257224 to -0.0403253835737982
-0.482231377023983 to -0.482398907572059
-0.0570430144255634 to -0.0572105449736391
-0.992555195797015 to -0.992722726345091
-0.45528638209753 to -0.455453912645606
-0.85375432516985 to -0.853921855717926
-0.553789018053341 to -0.553956548601417
-0.481342790979671 to -0.481510321527747
-0.0917407854979514 to -0.0919083160460271
-0.0982488974516874 to -0.0984164279997631
10/5/2016 1:43:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:42 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
Modified index 9's learning in memoryPool to -0.2
Modified index 10's learning in memoryPool to -0.2
Modified index 11's learning in memoryPool to -0.2
Modified index 12's learning in memoryPool to -0.2
Modified index 13's learning in memoryPool to -0.2
Modified index 14's learning in memoryPool to -0.2
Modified index 15's learning in memoryPool to -0.2
Modified index 16's learning in memoryPool to -0.2
Modified index 17's learning in memoryPool to -0.2
Modified index 18's learning in memoryPool to -0.2
Modified index 19's learning in memoryPool to -0.2
Modified index 20's learning in memoryPool to -0.2
Modified index 21's learning in memoryPool to -0.2
Modified index 22's learning in memoryPool to -0.2
Modified index 23's learning in memoryPool to -0.2
Modified index 24's learning in memoryPool to -0.2
Modified index 25's learning in memoryPool to -0.2
10/5/2016 1:44:42 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 81, 0, -0.2
sum 0.0386382716628473 distri -0.0258073883634901
Using diff 0.0547860921106256 and condRate 0.166666666666667
Changed category 0 weights from 
0.154878211597923 to 0.153052008500356
-0.245156752486702 to -0.246982955584269
-0.527677449007509 to -0.529503652105076
-0.376172083039758 to -0.377998286137324
Changing layer 0's weights from 
-0.285504857927588 to -0.287331061025154
-0.657431314617423 to -0.65925751771499
-0.891780580312042 to -0.893606783409609
-0.857221330434111 to -0.859047533531678
-0.109353761106757 to -0.111179964204324
-0.791995714336662 to -0.793821917434229
-0.638165990740089 to -0.639992193837656
-0.665620396763114 to -0.667446599860681
-0.90311225774887 to -0.904938460846437
-0.946862312883166 to -0.948688515980733
Changing layer 1's weights from 
-0.941765400499133 to -0.9435916035967
-0.340596715837745 to -0.342422918935312
-0.466299454599647 to -0.468125657697214
-0.476204792887 to -0.478030995984567
-0.661257575184135 to -0.663083778281702
-0.799495409160881 to -0.801321612258448
-0.528776685625342 to -0.530602888722909
-0.914579714566498 to -0.916405917664065
-0.502916733652381 to -0.504742936749948
-0.865563596516875 to -0.867389799614442
Changing layer 2's weights from 
-0.0563753646910471 to -0.0582015677886139
-0.610428850084572 to -0.612255053182139
-0.325839559465674 to -0.327665762563241
-0.154639462858466 to -0.156465665956033
-0.331892172724036 to -0.333718375821603
-0.537901799112586 to -0.539728002210153
-0.0190890831053532 to -0.02091528620292
-0.0915111106932441 to -0.0933373137908109
-0.823509405285148 to -0.825335608382715
-0.778551886707573 to -0.78037808980514
Changing layer 3's weights from 
-0.0894256633818431 to -0.0912518664794099
-0.352978746324805 to -0.354804949422372
-0.110368828207282 to -0.112195031304849
-1.01780805996343 to -1.01963426306099
-0.233044723898199 to -0.234870926995766
-0.363293330103187 to -0.365119533200754
-0.693478177219657 to -0.695304380317224
-0.234965304762152 to -0.236791507859719
-0.566661278635292 to -0.568487481732859
-0.093831400305061 to -0.0956576034026278
Changing layer 4's weights from 
-0.193105201154974 to -0.194931404252541
-0.401938001543311 to -0.403764204640878
-0.602415601640968 to -0.604241804738535
-0.315480868250159 to -0.317307071347726
-0.554733673959999 to -0.556559877057565
-0.323268453508643 to -0.32509465660621
-0.327626983553199 to -0.329453186650766
-0.789524505764274 to -0.791350708861841
-0.195828418165472 to -0.197654621263039
-0.628130356699257 to -0.629956559796824
Changing layer 5's weights from 
-0.0403253835737982 to -0.0421515866713649
-0.482398907572059 to -0.484225110669626
-0.0572105449736391 to -0.0590367480712059
-0.992722726345091 to -0.994548929442658
-0.455453912645606 to -0.457280115743173
-0.853921855717926 to -0.855748058815493
-0.553956548601417 to -0.555782751698984
-0.481510321527747 to -0.483336524625314
-0.0919083160460271 to -0.0937345191435938
-0.0984164279997631 to -0.10024263109733
Trying to learn from memory 82, 1, -0.2
sum 0.0386382716628473 distri 0.0254581221644256
Using diff 0.00352058158270984 and condRate 0.166666666666667
Changed category 1 weights from 
0.290789865318924 to 0.290672512597752
0.524105452362686 to 0.523988099641513
0.0991802144686283 to 0.0990628617474559
0.103273027006775 to 0.103155674285603
Changing layer 0's weights from 
-0.287331061025154 to -0.287448413746327
-0.65925751771499 to -0.659374870436162
-0.893606783409609 to -0.893724136130781
-0.859047533531678 to -0.85916488625285
-0.111179964204324 to -0.111297316925496
-0.793821917434229 to -0.793939270155401
-0.639992193837656 to -0.640109546558828
-0.667446599860681 to -0.667563952581853
-0.904938460846437 to -0.905055813567609
-0.948688515980733 to -0.948805868701905
Changing layer 1's weights from 
-0.9435916035967 to -0.943708956317872
-0.342422918935312 to -0.342540271656484
-0.468125657697214 to -0.468243010418386
-0.478030995984567 to -0.478148348705739
-0.663083778281702 to -0.663201131002874
-0.801321612258448 to -0.80143896497962
-0.530602888722909 to -0.530720241444081
-0.916405917664065 to -0.916523270385237
-0.504742936749948 to -0.50486028947112
-0.867389799614442 to -0.867507152335614
Changing layer 2's weights from 
-0.0582015677886139 to -0.0583189205097862
-0.612255053182139 to -0.612372405903311
-0.327665762563241 to -0.327783115284413
-0.156465665956033 to -0.156583018677205
-0.333718375821603 to -0.333835728542775
-0.539728002210153 to -0.539845354931325
-0.02091528620292 to -0.0210326389240923
-0.0933373137908109 to -0.0934546665119832
-0.825335608382715 to -0.825452961103887
-0.78037808980514 to -0.780495442526312
Changing layer 3's weights from 
-0.0912518664794099 to -0.0913692192005822
-0.354804949422372 to -0.354922302143544
-0.112195031304849 to -0.112312384026021
-1.01963426306099 to -1.01975161578216
-0.234870926995766 to -0.234988279716938
-0.365119533200754 to -0.365236885921926
-0.695304380317224 to -0.695421733038396
-0.236791507859719 to -0.236908860580891
-0.568487481732859 to -0.568604834454031
-0.0956576034026278 to -0.0957749561238001
Changing layer 4's weights from 
-0.194931404252541 to -0.195048756973713
-0.403764204640878 to -0.40388155736205
-0.604241804738535 to -0.604359157459707
-0.317307071347726 to -0.317424424068898
-0.556559877057565 to -0.556677229778738
-0.32509465660621 to -0.325212009327382
-0.329453186650766 to -0.329570539371938
-0.791350708861841 to -0.791468061583013
-0.197654621263039 to -0.197771973984211
-0.629956559796824 to -0.630073912517996
Changing layer 5's weights from 
-0.0421515866713649 to -0.0422689393925373
-0.484225110669626 to -0.484342463390798
-0.0590367480712059 to -0.0591541007923782
-0.994548929442658 to -0.99466628216383
-0.457280115743173 to -0.457397468464345
-0.855748058815493 to -0.855865411536665
-0.555782751698984 to -0.555900104420156
-0.483336524625314 to -0.483453877346486
-0.0937345191435938 to -0.0938518718647662
-0.10024263109733 to -0.100359983818502
Trying to learn from memory 83, 1, -0.2
sum 0.0386382716628473 distri 0.0254581221644256
Using diff 0.00352058158270984 and condRate 0.166666666666667
Changed category 1 weights from 
0.290672512597752 to 0.290555159876579
0.523988099641513 to 0.523870746920341
0.0990628617474559 to 0.0989455090262836
0.103155674285603 to 0.103038321564431
Changing layer 0's weights from 
-0.287448413746327 to -0.287565766467499
-0.659374870436162 to -0.659492223157334
-0.893724136130781 to -0.893841488851953
-0.85916488625285 to -0.859282238974022
-0.111297316925496 to -0.111414669646668
-0.793939270155401 to -0.794056622876573
-0.640109546558828 to -0.64022689928
-0.667563952581853 to -0.667681305303025
-0.905055813567609 to -0.905173166288781
-0.948805868701905 to -0.948923221423077
Changing layer 1's weights from 
-0.943708956317872 to -0.943826309039044
-0.342540271656484 to -0.342657624377656
-0.468243010418386 to -0.468360363139559
-0.478148348705739 to -0.478265701426912
-0.663201131002874 to -0.663318483724046
-0.80143896497962 to -0.801556317700792
-0.530720241444081 to -0.530837594165254
-0.916523270385237 to -0.916640623106409
-0.50486028947112 to -0.504977642192292
-0.867507152335614 to -0.867624505056786
Changing layer 2's weights from 
-0.0583189205097862 to -0.0584362732309586
-0.612372405903311 to -0.612489758624483
-0.327783115284413 to -0.327900468005585
-0.156583018677205 to -0.156700371398377
-0.333835728542775 to -0.333953081263947
-0.539845354931325 to -0.539962707652498
-0.0210326389240923 to -0.0211499916452647
-0.0934546665119832 to -0.0935720192331556
-0.825452961103887 to -0.825570313825059
-0.780495442526312 to -0.780612795247484
Changing layer 3's weights from 
-0.0913692192005822 to -0.0914865719217546
-0.354922302143544 to -0.355039654864717
-0.112312384026021 to -0.112429736747193
-1.01975161578216 to -1.01986896850334
-0.234988279716938 to -0.235105632438111
-0.365236885921926 to -0.365354238643099
-0.695421733038396 to -0.695539085759568
-0.236908860580891 to -0.237026213302064
-0.568604834454031 to -0.568722187175203
-0.0957749561238001 to -0.0958923088449725
Changing layer 4's weights from 
-0.195048756973713 to -0.195166109694886
-0.40388155736205 to -0.403998910083223
-0.604359157459707 to -0.604476510180879
-0.317424424068898 to -0.31754177679007
-0.556677229778738 to -0.55679458249991
-0.325212009327382 to -0.325329362048555
-0.329570539371938 to -0.329687892093111
-0.791468061583013 to -0.791585414304185
-0.197771973984211 to -0.197889326705384
-0.630073912517996 to -0.630191265239168
Changing layer 5's weights from 
-0.0422689393925373 to -0.0423862921137096
-0.484342463390798 to -0.484459816111971
-0.0591541007923782 to -0.0592714535135506
-0.99466628216383 to -0.994783634885002
-0.457397468464345 to -0.457514821185518
-0.855865411536665 to -0.855982764257837
-0.555900104420156 to -0.556017457141328
-0.483453877346486 to -0.483571230067659
-0.0938518718647662 to -0.0939692245859386
-0.100359983818502 to -0.100477336539675
Trying to learn from memory 84, 0, -0.2
sum 0.0386382716628473 distri -0.0258073883634901
Using diff 0.0547860921106256 and condRate 0.166666666666667
Changed category 0 weights from 
0.153052008500356 to 0.15122580540279
-0.246982955584269 to -0.248809158681835
-0.529503652105076 to -0.531329855202643
-0.377998286137324 to -0.379824489234891
Changing layer 0's weights from 
-0.287565766467499 to -0.289391969565066
-0.659492223157334 to -0.661318426254901
-0.893841488851953 to -0.89566769194952
-0.859282238974022 to -0.861108442071589
-0.111414669646668 to -0.113240872744235
-0.794056622876573 to -0.79588282597414
-0.64022689928 to -0.642053102377567
-0.667681305303025 to -0.669507508400592
-0.905173166288781 to -0.906999369386348
-0.948923221423077 to -0.950749424520644
Changing layer 1's weights from 
-0.943826309039044 to -0.945652512136611
-0.342657624377656 to -0.344483827475223
-0.468360363139559 to -0.470186566237125
-0.478265701426912 to -0.480091904524478
-0.663318483724046 to -0.665144686821613
-0.801556317700792 to -0.803382520798359
-0.530837594165254 to -0.53266379726282
-0.916640623106409 to -0.918466826203976
-0.504977642192292 to -0.506803845289859
-0.867624505056786 to -0.869450708154353
Changing layer 2's weights from 
-0.0584362732309586 to -0.0602624763285253
-0.612489758624483 to -0.61431596172205
-0.327900468005585 to -0.329726671103152
-0.156700371398377 to -0.158526574495944
-0.333953081263947 to -0.335779284361514
-0.539962707652498 to -0.541788910750065
-0.0211499916452647 to -0.0229761947428314
-0.0935720192331556 to -0.0953982223307223
-0.825570313825059 to -0.827396516922626
-0.780612795247484 to -0.782438998345051
Changing layer 3's weights from 
-0.0914865719217546 to -0.0933127750193213
-0.355039654864717 to -0.356865857962283
-0.112429736747193 to -0.11425593984476
-1.01986896850334 to -1.0216951716009
-0.235105632438111 to -0.236931835535677
-0.365354238643099 to -0.367180441740665
-0.695539085759568 to -0.697365288857135
-0.237026213302064 to -0.23885241639963
-0.568722187175203 to -0.57054839027277
-0.0958923088449725 to -0.0977185119425392
Changing layer 4's weights from 
-0.195166109694886 to -0.196992312792452
-0.403998910083223 to -0.405825113180789
-0.604476510180879 to -0.606302713278446
-0.31754177679007 to -0.319367979887637
-0.55679458249991 to -0.558620785597477
-0.325329362048555 to -0.327155565146121
-0.329687892093111 to -0.331514095190677
-0.791585414304185 to -0.793411617401752
-0.197889326705384 to -0.19971552980295
-0.630191265239168 to -0.632017468336735
Changing layer 5's weights from 
-0.0423862921137096 to -0.0442124952112763
-0.484459816111971 to -0.486286019209537
-0.0592714535135506 to -0.0610976566111173
-0.994783634885002 to -0.996609837982569
-0.457514821185518 to -0.459341024283084
-0.855982764257837 to -0.857808967355404
-0.556017457141328 to -0.557843660238895
-0.483571230067659 to -0.485397433165225
-0.0939692245859386 to -0.0957954276835053
-0.100477336539675 to -0.102303539637241
Trying to learn from memory 84, 1, -0.2
sum 0.0386382716628473 distri 0.0254581221644256
Using diff 0.00352058158270984 and condRate 0.166666666666667
Changed category 1 weights from 
0.290555159876579 to 0.290437807155407
0.523870746920341 to 0.523753394199169
0.0989455090262836 to 0.0988281563051112
0.103038321564431 to 0.102920968843258
Changing layer 0's weights from 
-0.289391969565066 to -0.289509322286238
-0.661318426254901 to -0.661435778976073
-0.89566769194952 to -0.895785044670692
-0.861108442071589 to -0.861225794792761
-0.113240872744235 to -0.113358225465408
-0.79588282597414 to -0.796000178695312
-0.642053102377567 to -0.642170455098739
-0.669507508400592 to -0.669624861121764
-0.906999369386348 to -0.90711672210752
-0.950749424520644 to -0.950866777241816
Changing layer 1's weights from 
-0.945652512136611 to -0.945769864857783
-0.344483827475223 to -0.344601180196396
-0.470186566237125 to -0.470303918958298
-0.480091904524478 to -0.480209257245651
-0.665144686821613 to -0.665262039542785
-0.803382520798359 to -0.803499873519531
-0.53266379726282 to -0.532781149983993
-0.918466826203976 to -0.918584178925148
-0.506803845289859 to -0.506921198011032
-0.869450708154353 to -0.869568060875525
Changing layer 2's weights from 
-0.0602624763285253 to -0.0603798290496977
-0.61431596172205 to -0.614433314443222
-0.329726671103152 to -0.329844023824325
-0.158526574495944 to -0.158643927217116
-0.335779284361514 to -0.335896637082687
-0.541788910750065 to -0.541906263471237
-0.0229761947428314 to -0.0230935474640037
-0.0953982223307223 to -0.0955155750518947
-0.827396516922626 to -0.827513869643798
-0.782438998345051 to -0.782556351066223
Changing layer 3's weights from 
-0.0933127750193213 to -0.0934301277404937
-0.356865857962283 to -0.356983210683456
-0.11425593984476 to -0.114373292565933
-1.0216951716009 to -1.02181252432208
-0.236931835535677 to -0.23704918825685
-0.367180441740665 to -0.367297794461838
-0.697365288857135 to -0.697482641578307
-0.23885241639963 to -0.238969769120803
-0.57054839027277 to -0.570665742993942
-0.0977185119425392 to -0.0978358646637116
Changing layer 4's weights from 
-0.196992312792452 to -0.197109665513625
-0.405825113180789 to -0.405942465901962
-0.606302713278446 to -0.606420065999618
-0.319367979887637 to -0.31948533260881
-0.558620785597477 to -0.558738138318649
-0.327155565146121 to -0.327272917867294
-0.331514095190677 to -0.33163144791185
-0.793411617401752 to -0.793528970122924
-0.19971552980295 to -0.199832882524123
-0.632017468336735 to -0.632134821057907
Changing layer 5's weights from 
-0.0442124952112763 to -0.0443298479324487
-0.486286019209537 to -0.48640337193071
-0.0610976566111173 to -0.0612150093322896
-0.996609837982569 to -0.996727190703741
-0.459341024283084 to -0.459458377004257
-0.857808967355404 to -0.857926320076576
-0.557843660238895 to -0.557961012960067
-0.485397433165225 to -0.485514785886398
-0.0957954276835053 to -0.0959127804046777
-0.102303539637241 to -0.102420892358414
Trying to learn from memory 84, 0, -0.2
sum 0.0386382716628473 distri -0.0258073883634901
Using diff 0.0547860921106256 and condRate 0.166666666666667
Changed category 0 weights from 
0.15122580540279 to 0.149399602305223
-0.248809158681835 to -0.250635361779402
-0.531329855202643 to -0.533156058300209
-0.379824489234891 to -0.381650692332458
Changing layer 0's weights from 
-0.289509322286238 to -0.291335525383805
-0.661435778976073 to -0.66326198207364
-0.895785044670692 to -0.897611247768259
-0.861225794792761 to -0.863051997890328
-0.113358225465408 to -0.115184428562974
-0.796000178695312 to -0.797826381792879
-0.642170455098739 to -0.643996658196306
-0.669624861121764 to -0.671451064219331
-0.90711672210752 to -0.908942925205087
-0.950866777241816 to -0.952692980339383
Changing layer 1's weights from 
-0.945769864857783 to -0.94759606795535
-0.344601180196396 to -0.346427383293962
-0.470303918958298 to -0.472130122055864
-0.480209257245651 to -0.482035460343217
-0.665262039542785 to -0.667088242640352
-0.803499873519531 to -0.805326076617098
-0.532781149983993 to -0.534607353081559
-0.918584178925148 to -0.920410382022715
-0.506921198011032 to -0.508747401108598
-0.869568060875525 to -0.871394263973092
Changing layer 2's weights from 
-0.0603798290496977 to -0.0622060321472644
-0.614433314443222 to -0.616259517540789
-0.329844023824325 to -0.331670226921891
-0.158643927217116 to -0.160470130314683
-0.335896637082687 to -0.337722840180253
-0.541906263471237 to -0.543732466568804
-0.0230935474640037 to -0.0249197505615705
-0.0955155750518947 to -0.0973417781494614
-0.827513869643798 to -0.829340072741365
-0.782556351066223 to -0.78438255416379
Changing layer 3's weights from 
-0.0934301277404937 to -0.0952563308380604
-0.356983210683456 to -0.358809413781022
-0.114373292565933 to -0.116199495663499
-1.02181252432208 to -1.02363872741964
-0.23704918825685 to -0.238875391354416
-0.367297794461838 to -0.369123997559405
-0.697482641578307 to -0.699308844675874
-0.238969769120803 to -0.240795972218369
-0.570665742993942 to -0.572491946091509
-0.0978358646637116 to -0.0996620677612783
Changing layer 4's weights from 
-0.197109665513625 to -0.198935868611192
-0.405942465901962 to -0.407768668999528
-0.606420065999618 to -0.608246269097185
-0.31948533260881 to -0.321311535706376
-0.558738138318649 to -0.560564341416216
-0.327272917867294 to -0.32909912096486
-0.33163144791185 to -0.333457651009416
-0.793528970122924 to -0.795355173220491
-0.199832882524123 to -0.201659085621689
-0.632134821057907 to -0.633961024155474
Changing layer 5's weights from 
-0.0443298479324487 to -0.0461560510300154
-0.48640337193071 to -0.488229575028276
-0.0612150093322896 to -0.0630412124298564
-0.996727190703741 to -0.998553393801308
-0.459458377004257 to -0.461284580101823
-0.857926320076576 to -0.859752523174143
-0.557961012960067 to -0.559787216057634
-0.485514785886398 to -0.487340988983965
-0.0959127804046777 to -0.0977389835022444
-0.102420892358414 to -0.10424709545598
Trying to learn from memory 84, 0, -0.2
sum 0.0386382716628473 distri -0.0258073883634901
Using diff 0.0547860921106256 and condRate 0.166666666666667
Changed category 0 weights from 
0.149399602305223 to 0.147573399207656
-0.250635361779402 to -0.252461564876969
-0.533156058300209 to -0.534982261397776
-0.381650692332458 to -0.383476895430025
Changing layer 0's weights from 
-0.291335525383805 to -0.293161728481372
-0.66326198207364 to -0.665088185171207
-0.897611247768259 to -0.899437450865826
-0.863051997890328 to -0.864878200987895
-0.115184428562974 to -0.117010631660541
-0.797826381792879 to -0.799652584890446
-0.643996658196306 to -0.645822861293873
-0.671451064219331 to -0.673277267316898
-0.908942925205087 to -0.910769128302654
-0.952692980339383 to -0.95451918343695
Changing layer 1's weights from 
-0.94759606795535 to -0.949422271052917
-0.346427383293962 to -0.348253586391529
-0.472130122055864 to -0.473956325153431
-0.482035460343217 to -0.483861663440784
-0.667088242640352 to -0.668914445737919
-0.805326076617098 to -0.807152279714665
-0.534607353081559 to -0.536433556179126
-0.920410382022715 to -0.922236585120282
-0.508747401108598 to -0.510573604206165
-0.871394263973092 to -0.873220467070659
Changing layer 2's weights from 
-0.0622060321472644 to -0.0640322352448311
-0.616259517540789 to -0.618085720638356
-0.331670226921891 to -0.333496430019458
-0.160470130314683 to -0.16229633341225
-0.337722840180253 to -0.33954904327782
-0.543732466568804 to -0.54555866966637
-0.0249197505615705 to -0.0267459536591372
-0.0973417781494614 to -0.0991679812470281
-0.829340072741365 to -0.831166275838932
-0.78438255416379 to -0.786208757261357
Changing layer 3's weights from 
-0.0952563308380604 to -0.0970825339356271
-0.358809413781022 to -0.360635616878589
-0.116199495663499 to -0.118025698761066
-1.02363872741964 to -1.02546493051721
-0.238875391354416 to -0.240701594451983
-0.369123997559405 to -0.370950200656971
-0.699308844675874 to -0.701135047773441
-0.240795972218369 to -0.242622175315936
-0.572491946091509 to -0.574318149189076
-0.0996620677612783 to -0.101488270858845
Changing layer 4's weights from 
-0.198935868611192 to -0.200762071708758
-0.407768668999528 to -0.409594872097095
-0.608246269097185 to -0.610072472194752
-0.321311535706376 to -0.323137738803943
-0.560564341416216 to -0.562390544513783
-0.32909912096486 to -0.330925324062427
-0.333457651009416 to -0.335283854106983
-0.795355173220491 to -0.797181376318058
-0.201659085621689 to -0.203485288719256
-0.633961024155474 to -0.635787227253041
Changing layer 5's weights from 
-0.0461560510300154 to -0.0479822541275821
-0.488229575028276 to -0.490055778125843
-0.0630412124298564 to -0.0648674155274231
-0.998553393801308 to -1.00037959689887
-0.461284580101823 to -0.46311078319939
-0.859752523174143 to -0.86157872627171
-0.559787216057634 to -0.561613419155201
-0.487340988983965 to -0.489167192081531
-0.0977389835022444 to -0.0995651865998111
-0.10424709545598 to -0.106073298553547
Trying to learn from memory 84, 0, -0.2
sum 0.0386382716628473 distri -0.0258073883634901
Using diff 0.0547860921106256 and condRate 0.166666666666667
Changed category 0 weights from 
0.147573399207656 to 0.145747196110089
-0.252461564876969 to -0.254287767974536
-0.534982261397776 to -0.536808464495343
-0.383476895430025 to -0.385303098527591
Changing layer 0's weights from 
-0.293161728481372 to -0.294987931578938
-0.665088185171207 to -0.666914388268774
-0.899437450865826 to -0.901263653963393
-0.864878200987895 to -0.866704404085462
-0.117010631660541 to -0.118836834758108
-0.799652584890446 to -0.801478787988013
-0.645822861293873 to -0.64764906439144
-0.673277267316898 to -0.675103470414465
-0.910769128302654 to -0.912595331400221
-0.95451918343695 to -0.956345386534517
Changing layer 1's weights from 
-0.949422271052917 to -0.951248474150484
-0.348253586391529 to -0.350079789489096
-0.473956325153431 to -0.475782528250998
-0.483861663440784 to -0.485687866538351
-0.668914445737919 to -0.670740648835486
-0.807152279714665 to -0.808978482812232
-0.536433556179126 to -0.538259759276693
-0.922236585120282 to -0.924062788217849
-0.510573604206165 to -0.512399807303732
-0.873220467070659 to -0.875046670168226
Changing layer 2's weights from 
-0.0640322352448311 to -0.0658584383423979
-0.618085720638356 to -0.619911923735923
-0.333496430019458 to -0.335322633117025
-0.16229633341225 to -0.164122536509817
-0.33954904327782 to -0.341375246375387
-0.54555866966637 to -0.547384872763937
-0.0267459536591372 to -0.0285721567567039
-0.0991679812470281 to -0.100994184344595
-0.831166275838932 to -0.832992478936499
-0.786208757261357 to -0.788034960358924
Changing layer 3's weights from 
-0.0970825339356271 to -0.0989087370331939
-0.360635616878589 to -0.362461819976156
-0.118025698761066 to -0.119851901858633
-1.02546493051721 to -1.02729113361478
-0.240701594451983 to -0.24252779754955
-0.370950200656971 to -0.372776403754538
-0.701135047773441 to -0.702961250871008
-0.242622175315936 to -0.244448378413503
-0.574318149189076 to -0.576144352286643
-0.101488270858845 to -0.103314473956412
Changing layer 4's weights from 
-0.200762071708758 to -0.202588274806325
-0.409594872097095 to -0.411421075194662
-0.610072472194752 to -0.611898675292318
-0.323137738803943 to -0.32496394190151
-0.562390544513783 to -0.564216747611349
-0.330925324062427 to -0.332751527159994
-0.335283854106983 to -0.33711005720455
-0.797181376318058 to -0.799007579415625
-0.203485288719256 to -0.205311491816823
-0.635787227253041 to -0.637613430350608
Changing layer 5's weights from 
-0.0479822541275821 to -0.0498084572251489
-0.490055778125843 to -0.49188198122341
-0.0648674155274231 to -0.0666936186249898
-1.00037959689887 to -1.00220579999644
-0.46311078319939 to -0.464936986296957
-0.86157872627171 to -0.863404929369277
-0.561613419155201 to -0.563439622252767
-0.489167192081531 to -0.490993395179098
-0.0995651865998111 to -0.101391389697378
-0.106073298553547 to -0.107899501651114
Trying to learn from memory 84, 0, -0.2
sum 0.0386382716628473 distri -0.0258073883634901
Using diff 0.0547860921106256 and condRate 0.166666666666667
Changed category 0 weights from 
0.145747196110089 to 0.143920993012523
-0.254287767974536 to -0.256113971072102
-0.536808464495343 to -0.53863466759291
-0.385303098527591 to -0.387129301625158
Changing layer 0's weights from 
-0.294987931578938 to -0.296814134676505
-0.666914388268774 to -0.66874059136634
-0.901263653963393 to -0.903089857060959
-0.866704404085462 to -0.868530607183028
-0.118836834758108 to -0.120663037855674
-0.801478787988013 to -0.803304991085579
-0.64764906439144 to -0.649475267489006
-0.675103470414465 to -0.676929673512031
-0.912595331400221 to -0.914421534497787
-0.956345386534517 to -0.958171589632083
Changing layer 1's weights from 
-0.951248474150484 to -0.95307467724805
-0.350079789489096 to -0.351905992586663
-0.475782528250998 to -0.477608731348565
-0.485687866538351 to -0.487514069635918
-0.670740648835486 to -0.672566851933052
-0.808978482812232 to -0.810804685909798
-0.538259759276693 to -0.54008596237426
-0.924062788217849 to -0.925888991315415
-0.512399807303732 to -0.514226010401299
-0.875046670168226 to -0.876872873265792
Changing layer 2's weights from 
-0.0658584383423979 to -0.0676846414399646
-0.619911923735923 to -0.621738126833489
-0.335322633117025 to -0.337148836214592
-0.164122536509817 to -0.165948739607383
-0.341375246375387 to -0.343201449472954
-0.547384872763937 to -0.549211075861504
-0.0285721567567039 to -0.0303983598542707
-0.100994184344595 to -0.102820387442162
-0.832992478936499 to -0.834818682034065
-0.788034960358924 to -0.78986116345649
Changing layer 3's weights from 
-0.0989087370331939 to -0.100734940130761
-0.362461819976156 to -0.364288023073723
-0.119851901858633 to -0.121678104956199
-1.02729113361478 to -1.02911733671234
-0.24252779754955 to -0.244354000647117
-0.372776403754538 to -0.374602606852105
-0.702961250871008 to -0.704787453968574
-0.244448378413503 to -0.24627458151107
-0.576144352286643 to -0.577970555384209
-0.103314473956412 to -0.105140677053979
Changing layer 4's weights from 
-0.202588274806325 to -0.204414477903892
-0.411421075194662 to -0.413247278292229
-0.611898675292318 to -0.613724878389885
-0.32496394190151 to -0.326790144999077
-0.564216747611349 to -0.566042950708916
-0.332751527159994 to -0.334577730257561
-0.33711005720455 to -0.338936260302117
-0.799007579415625 to -0.800833782513191
-0.205311491816823 to -0.20713769491439
-0.637613430350608 to -0.639439633448174
Changing layer 5's weights from 
-0.0498084572251489 to -0.0516346603227156
-0.49188198122341 to -0.493708184320977
-0.0666936186249898 to -0.0685198217225566
-1.00220579999644 to -1.00403200309401
-0.464936986296957 to -0.466763189394524
-0.863404929369277 to -0.865231132466843
-0.563439622252767 to -0.565265825350334
-0.490993395179098 to -0.492819598276665
-0.101391389697378 to -0.103217592794945
-0.107899501651114 to -0.109725704748681
Trying to learn from memory 84, 0, -0.2
sum 0.0386382716628473 distri -0.0258073883634901
Using diff 0.0547860921106256 and condRate 0.166666666666667
Changed category 0 weights from 
0.143920993012523 to 0.142094789914956
-0.256113971072102 to -0.257940174169669
-0.53863466759291 to -0.540460870690476
-0.387129301625158 to -0.388955504722725
Changing layer 0's weights from 
-0.296814134676505 to -0.298640337774072
-0.66874059136634 to -0.670566794463907
-0.903089857060959 to -0.904916060158526
-0.868530607183028 to -0.870356810280595
-0.120663037855674 to -0.122489240953241
-0.803304991085579 to -0.805131194183146
-0.649475267489006 to -0.651301470586573
-0.676929673512031 to -0.678755876609598
-0.914421534497787 to -0.916247737595354
-0.958171589632083 to -0.95999779272965
Changing layer 1's weights from 
-0.95307467724805 to -0.954900880345617
-0.351905992586663 to -0.353732195684229
-0.477608731348565 to -0.479434934446132
-0.487514069635918 to -0.489340272733485
-0.672566851933052 to -0.674393055030619
-0.810804685909798 to -0.812630889007365
-0.54008596237426 to -0.541912165471827
-0.925888991315415 to -0.927715194412982
-0.514226010401299 to -0.516052213498865
-0.876872873265792 to -0.878699076363359
Changing layer 2's weights from 
-0.0676846414399646 to -0.0695108445375313
-0.621738126833489 to -0.623564329931056
-0.337148836214592 to -0.338975039312158
-0.165948739607383 to -0.16777494270495
-0.343201449472954 to -0.34502765257052
-0.549211075861504 to -0.551037278959071
-0.0303983598542707 to -0.0322245629518374
-0.102820387442162 to -0.104646590539728
-0.834818682034065 to -0.836644885131632
-0.78986116345649 to -0.791687366554057
Changing layer 3's weights from 
-0.100734940130761 to -0.102561143228327
-0.364288023073723 to -0.366114226171289
-0.121678104956199 to -0.123504308053766
-1.02911733671234 to -1.03094353980991
-0.244354000647117 to -0.246180203744683
-0.374602606852105 to -0.376428809949672
-0.704787453968574 to -0.706613657066141
-0.24627458151107 to -0.248100784608636
-0.577970555384209 to -0.579796758481776
-0.105140677053979 to -0.106966880151545
Changing layer 4's weights from 
-0.204414477903892 to -0.206240681001458
-0.413247278292229 to -0.415073481389796
-0.613724878389885 to -0.615551081487452
-0.326790144999077 to -0.328616348096643
-0.566042950708916 to -0.567869153806483
-0.334577730257561 to -0.336403933355127
-0.338936260302117 to -0.340762463399683
-0.800833782513191 to -0.802659985610758
-0.20713769491439 to -0.208963898011956
-0.639439633448174 to -0.641265836545741
Changing layer 5's weights from 
-0.0516346603227156 to -0.0534608634202823
-0.493708184320977 to -0.495534387418544
-0.0685198217225566 to -0.0703460248201233
-1.00403200309401 to -1.00585820619157
-0.466763189394524 to -0.468589392492091
-0.865231132466843 to -0.86705733556441
-0.565265825350334 to -0.567092028447901
-0.492819598276665 to -0.494645801374232
-0.103217592794945 to -0.105043795892511
-0.109725704748681 to -0.111551907846247
10/5/2016 1:44:43 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 84, 0, -0.2
sum 0.0386382716628473 distri -0.0258073883634901
Using diff 0.0547860921106256 and condRate 0.166666666666667
Changed category 0 weights from 
0.142094789914956 to 0.140268586817389
-0.257940174169669 to -0.259766377267236
-0.540460870690476 to -0.542287073788043
-0.388955504722725 to -0.390781707820292
Changing layer 0's weights from 
-0.298640337774072 to -0.300466540871639
-0.670566794463907 to -0.672392997561474
-0.904916060158526 to -0.906742263256093
-0.870356810280595 to -0.872183013378162
-0.122489240953241 to -0.124315444050808
-0.805131194183146 to -0.806957397280713
-0.651301470586573 to -0.65312767368414
-0.678755876609598 to -0.680582079707165
-0.916247737595354 to -0.918073940692921
-0.95999779272965 to -0.961823995827217
Changing layer 1's weights from 
-0.954900880345617 to -0.956727083443184
-0.353732195684229 to -0.355558398781796
-0.479434934446132 to -0.481261137543698
-0.489340272733485 to -0.491166475831051
-0.674393055030619 to -0.676219258128186
-0.812630889007365 to -0.814457092104932
-0.541912165471827 to -0.543738368569393
-0.927715194412982 to -0.929541397510549
-0.516052213498865 to -0.517878416596432
-0.878699076363359 to -0.880525279460926
Changing layer 2's weights from 
-0.0695108445375313 to -0.071337047635098
-0.623564329931056 to -0.625390533028623
-0.338975039312158 to -0.340801242409725
-0.16777494270495 to -0.169601145802517
-0.34502765257052 to -0.346853855668087
-0.551037278959071 to -0.552863482056637
-0.0322245629518374 to -0.0340507660494041
-0.104646590539728 to -0.106472793637295
-0.836644885131632 to -0.838471088229199
-0.791687366554057 to -0.793513569651624
Changing layer 3's weights from 
-0.102561143228327 to -0.104387346325894
-0.366114226171289 to -0.367940429268856
-0.123504308053766 to -0.125330511151333
-1.03094353980991 to -1.03276974290748
-0.246180203744683 to -0.24800640684225
-0.376428809949672 to -0.378255013047238
-0.706613657066141 to -0.708439860163708
-0.248100784608636 to -0.249926987706203
-0.579796758481776 to -0.581622961579343
-0.106966880151545 to -0.108793083249112
Changing layer 4's weights from 
-0.206240681001458 to -0.208066884099025
-0.415073481389796 to -0.416899684487362
-0.615551081487452 to -0.617377284585019
-0.328616348096643 to -0.33044255119421
-0.567869153806483 to -0.56969535690405
-0.336403933355127 to -0.338230136452694
-0.340762463399683 to -0.34258866649725
-0.802659985610758 to -0.804486188708325
-0.208963898011956 to -0.210790101109523
-0.641265836545741 to -0.643092039643308
Changing layer 5's weights from 
-0.0534608634202823 to -0.0552870665178491
-0.495534387418544 to -0.49736059051611
-0.0703460248201233 to -0.07217222791769
-1.00585820619157 to -1.00768440928914
-0.468589392492091 to -0.470415595589657
-0.86705733556441 to -0.868883538661977
-0.567092028447901 to -0.568918231545468
-0.494645801374232 to -0.496472004471798
-0.105043795892511 to -0.106869998990078
-0.111551907846247 to -0.113378110943814
Trying to learn from memory 84, 0, -0.2
sum 0.0386382716628473 distri -0.0258073883634901
Using diff 0.0547860921106256 and condRate 0.166666666666667
Changed category 0 weights from 
0.140268586817389 to 0.138442383719823
-0.259766377267236 to -0.261592580364803
-0.542287073788043 to -0.54411327688561
-0.390781707820292 to -0.392607910917858
Changing layer 0's weights from 
-0.300466540871639 to -0.302292743969205
-0.672392997561474 to -0.674219200659041
-0.906742263256093 to -0.90856846635366
-0.872183013378162 to -0.874009216475729
-0.124315444050808 to -0.126141647148375
-0.806957397280713 to -0.80878360037828
-0.65312767368414 to -0.654953876781707
-0.680582079707165 to -0.682408282804732
-0.918073940692921 to -0.919900143790488
-0.961823995827217 to -0.963650198924784
Changing layer 1's weights from 
-0.956727083443184 to -0.958553286540751
-0.355558398781796 to -0.357384601879363
-0.481261137543698 to -0.483087340641265
-0.491166475831051 to -0.492992678928618
-0.676219258128186 to -0.678045461225753
-0.814457092104932 to -0.816283295202499
-0.543738368569393 to -0.54556457166696
-0.929541397510549 to -0.931367600608116
-0.517878416596432 to -0.519704619693999
-0.880525279460926 to -0.882351482558493
Changing layer 2's weights from 
-0.071337047635098 to -0.0731632507326648
-0.625390533028623 to -0.62721673612619
-0.340801242409725 to -0.342627445507292
-0.169601145802517 to -0.171427348900083
-0.346853855668087 to -0.348680058765654
-0.552863482056637 to -0.554689685154204
-0.0340507660494041 to -0.0358769691469709
-0.106472793637295 to -0.108298996734862
-0.838471088229199 to -0.840297291326766
-0.793513569651624 to -0.795339772749191
Changing layer 3's weights from 
-0.104387346325894 to -0.106213549423461
-0.367940429268856 to -0.369766632366423
-0.125330511151333 to -0.1271567142489
-1.03276974290748 to -1.03459594600504
-0.24800640684225 to -0.249832609939817
-0.378255013047238 to -0.380081216144805
-0.708439860163708 to -0.710266063261275
-0.249926987706203 to -0.25175319080377
-0.581622961579343 to -0.58344916467691
-0.108793083249112 to -0.110619286346679
Changing layer 4's weights from 
-0.208066884099025 to -0.209893087196592
-0.416899684487362 to -0.418725887584929
-0.617377284585019 to -0.619203487682585
-0.33044255119421 to -0.332268754291777
-0.56969535690405 to -0.571521560001616
-0.338230136452694 to -0.340056339550261
-0.34258866649725 to -0.344414869594817
-0.804486188708325 to -0.806312391805892
-0.210790101109523 to -0.21261630420709
-0.643092039643308 to -0.644918242740875
Changing layer 5's weights from 
-0.0552870665178491 to -0.0571132696154158
-0.49736059051611 to -0.499186793613677
-0.07217222791769 to -0.0739984310152568
-1.00768440928914 to -1.00951061238671
-0.470415595589657 to -0.472241798687224
-0.868883538661977 to -0.870709741759544
-0.568918231545468 to -0.570744434643034
-0.496472004471798 to -0.498298207569365
-0.106869998990078 to -0.108696202087645
-0.113378110943814 to -0.115204314041381
Trying to learn from memory 84, 0, -0.2
sum 0.0386382716628473 distri -0.0258073883634901
Using diff 0.0547860921106256 and condRate 0.166666666666667
Changed category 0 weights from 
0.138442383719823 to 0.136616180622256
-0.261592580364803 to -0.263418783462369
-0.54411327688561 to -0.545939479983177
-0.392607910917858 to -0.394434114015425
Changing layer 0's weights from 
-0.302292743969205 to -0.304118947066772
-0.674219200659041 to -0.676045403756607
-0.90856846635366 to -0.910394669451226
-0.874009216475729 to -0.875835419573295
-0.126141647148375 to -0.127967850245941
-0.80878360037828 to -0.810609803475846
-0.654953876781707 to -0.656780079879273
-0.682408282804732 to -0.684234485902298
-0.919900143790488 to -0.921726346888054
-0.963650198924784 to -0.96547640202235
Changing layer 1's weights from 
-0.958553286540751 to -0.960379489638317
-0.357384601879363 to -0.35921080497693
-0.483087340641265 to -0.484913543738832
-0.492992678928618 to -0.494818882026185
-0.678045461225753 to -0.679871664323319
-0.816283295202499 to -0.818109498300065
-0.54556457166696 to -0.547390774764527
-0.931367600608116 to -0.933193803705682
-0.519704619693999 to -0.521530822791566
-0.882351482558493 to -0.884177685656059
Changing layer 2's weights from 
-0.0731632507326648 to -0.0749894538302315
-0.62721673612619 to -0.629042939223756
-0.342627445507292 to -0.344453648604859
-0.171427348900083 to -0.17325355199765
-0.348680058765654 to -0.350506261863221
-0.554689685154204 to -0.556515888251771
-0.0358769691469709 to -0.0377031722445376
-0.108298996734862 to -0.110125199832429
-0.840297291326766 to -0.842123494424332
-0.795339772749191 to -0.797165975846757
Changing layer 3's weights from 
-0.106213549423461 to -0.108039752521028
-0.369766632366423 to -0.37159283546399
-0.1271567142489 to -0.128982917346466
-1.03459594600504 to -1.03642214910261
-0.249832609939817 to -0.251658813037384
-0.380081216144805 to -0.381907419242372
-0.710266063261275 to -0.712092266358841
-0.25175319080377 to -0.253579393901337
-0.58344916467691 to -0.585275367774476
-0.110619286346679 to -0.112445489444245
Changing layer 4's weights from 
-0.209893087196592 to -0.211719290294159
-0.418725887584929 to -0.420552090682496
-0.619203487682585 to -0.621029690780152
-0.332268754291777 to -0.334094957389344
-0.571521560001616 to -0.573347763099183
-0.340056339550261 to -0.341882542647828
-0.344414869594817 to -0.346241072692384
-0.806312391805892 to -0.808138594903458
-0.21261630420709 to -0.214442507304657
-0.644918242740875 to -0.646744445838441
Changing layer 5's weights from 
-0.0571132696154158 to -0.0589394727129825
-0.499186793613677 to -0.501012996711244
-0.0739984310152568 to -0.0758246341128235
-1.00951061238671 to -1.01133681548427
-0.472241798687224 to -0.474068001784791
-0.870709741759544 to -0.87253594485711
-0.570744434643034 to -0.572570637740601
-0.498298207569365 to -0.500124410666932
-0.108696202087645 to -0.110522405185212
-0.115204314041381 to -0.117030517138948
Trying to learn from memory 84, 1, -0.2
sum 0.0386382716628473 distri 0.0254581221644256
Using diff 0.00352058158270984 and condRate 0.166666666666667
Changed category 1 weights from 
0.290437807155407 to 0.290320454434234
0.523753394199169 to 0.523636041477996
0.0988281563051112 to 0.0987108035839388
0.102920968843258 to 0.102803616122086
Changing layer 0's weights from 
-0.304118947066772 to -0.304236299787945
-0.676045403756607 to -0.67616275647778
-0.910394669451226 to -0.910512022172399
-0.875835419573295 to -0.875952772294468
-0.127967850245941 to -0.128085202967114
-0.810609803475846 to -0.810727156197019
-0.656780079879273 to -0.656897432600446
-0.684234485902298 to -0.684351838623471
-0.921726346888054 to -0.921843699609227
-0.96547640202235 to -0.965593754743523
Changing layer 1's weights from 
-0.960379489638317 to -0.96049684235949
-0.35921080497693 to -0.359328157698102
-0.484913543738832 to -0.485030896460004
-0.494818882026185 to -0.494936234747357
-0.679871664323319 to -0.679989017044492
-0.818109498300065 to -0.818226851021238
-0.547390774764527 to -0.547508127485699
-0.933193803705682 to -0.933311156426855
-0.521530822791566 to -0.521648175512738
-0.884177685656059 to -0.884295038377232
Changing layer 2's weights from 
-0.0749894538302315 to -0.0751068065514039
-0.629042939223756 to -0.629160291944929
-0.344453648604859 to -0.344571001326031
-0.17325355199765 to -0.173370904718823
-0.350506261863221 to -0.350623614584393
-0.556515888251771 to -0.556633240972943
-0.0377031722445376 to -0.0378205249657099
-0.110125199832429 to -0.110242552553601
-0.842123494424332 to -0.842240847145505
-0.797165975846757 to -0.79728332856793
Changing layer 3's weights from 
-0.108039752521028 to -0.1081571052422
-0.37159283546399 to -0.371710188185162
-0.128982917346466 to -0.129100270067639
-1.03642214910261 to -1.03653950182378
-0.251658813037384 to -0.251776165758556
-0.381907419242372 to -0.382024771963544
-0.712092266358841 to -0.712209619080014
-0.253579393901337 to -0.253696746622509
-0.585275367774476 to -0.585392720495649
-0.112445489444245 to -0.112562842165418
Changing layer 4's weights from 
-0.211719290294159 to -0.211836643015331
-0.420552090682496 to -0.420669443403668
-0.621029690780152 to -0.621147043501325
-0.334094957389344 to -0.334212310110516
-0.573347763099183 to -0.573465115820356
-0.341882542647828 to -0.341999895369
-0.346241072692384 to -0.346358425413556
-0.808138594903458 to -0.808255947624631
-0.214442507304657 to -0.214559860025829
-0.646744445838441 to -0.646861798559614
Changing layer 5's weights from 
-0.0589394727129825 to -0.0590568254341549
-0.501012996711244 to -0.501130349432416
-0.0758246341128235 to -0.0759419868339959
-1.01133681548427 to -1.01145416820545
-0.474068001784791 to -0.474185354505963
-0.87253594485711 to -0.872653297578283
-0.572570637740601 to -0.572687990461774
-0.500124410666932 to -0.500241763388104
-0.110522405185212 to -0.110639757906384
-0.117030517138948 to -0.11714786986012
Trying to learn from memory 84, 0, -0.2
sum 0.0386382716628473 distri -0.0258073883634901
Using diff 0.0547860921106256 and condRate 0.166666666666667
Changed category 0 weights from 
0.136616180622256 to 0.134789977524689
-0.263418783462369 to -0.265244986559936
-0.545939479983177 to -0.547765683080743
-0.394434114015425 to -0.396260317112992
Changing layer 0's weights from 
-0.304236299787945 to -0.306062502885511
-0.67616275647778 to -0.677988959575346
-0.910512022172399 to -0.912338225269965
-0.875952772294468 to -0.877778975392034
-0.128085202967114 to -0.12991140606468
-0.810727156197019 to -0.812553359294585
-0.656897432600446 to -0.658723635698012
-0.684351838623471 to -0.686178041721037
-0.921843699609227 to -0.923669902706793
-0.965593754743523 to -0.967419957841089
Changing layer 1's weights from 
-0.96049684235949 to -0.962323045457056
-0.359328157698102 to -0.361154360795669
-0.485030896460004 to -0.486857099557571
-0.494936234747357 to -0.496762437844924
-0.679989017044492 to -0.681815220142058
-0.818226851021238 to -0.820053054118804
-0.547508127485699 to -0.549334330583266
-0.933311156426855 to -0.935137359524421
-0.521648175512738 to -0.523474378610305
-0.884295038377232 to -0.886121241474798
Changing layer 2's weights from 
-0.0751068065514039 to -0.0769330096489706
-0.629160291944929 to -0.630986495042495
-0.344571001326031 to -0.346397204423598
-0.173370904718823 to -0.175197107816389
-0.350623614584393 to -0.35244981768196
-0.556633240972943 to -0.55845944407051
-0.0378205249657099 to -0.0396467280632767
-0.110242552553601 to -0.112068755651168
-0.842240847145505 to -0.844067050243071
-0.79728332856793 to -0.799109531665496
Changing layer 3's weights from 
-0.1081571052422 to -0.109983308339767
-0.371710188185162 to -0.373536391282729
-0.129100270067639 to -0.130926473165205
-1.03653950182378 to -1.03836570492135
-0.251776165758556 to -0.253602368856123
-0.382024771963544 to -0.383850975061111
-0.712209619080014 to -0.71403582217758
-0.253696746622509 to -0.255522949720076
-0.585392720495649 to -0.587218923593215
-0.112562842165418 to -0.114389045262985
Changing layer 4's weights from 
-0.211836643015331 to -0.213662846112898
-0.420669443403668 to -0.422495646501235
-0.621147043501325 to -0.622973246598891
-0.334212310110516 to -0.336038513208083
-0.573465115820356 to -0.575291318917922
-0.341999895369 to -0.343826098466567
-0.346358425413556 to -0.348184628511123
-0.808255947624631 to -0.810082150722197
-0.214559860025829 to -0.216386063123396
-0.646861798559614 to -0.64868800165718
Changing layer 5's weights from 
-0.0590568254341549 to -0.0608830285317216
-0.501130349432416 to -0.502956552529983
-0.0759419868339959 to -0.0777681899315626
-1.01145416820545 to -1.01328037130301
-0.474185354505963 to -0.47601155760353
-0.872653297578283 to -0.874479500675849
-0.572687990461774 to -0.57451419355934
-0.500241763388104 to -0.502067966485671
-0.110639757906384 to -0.112465961003951
-0.11714786986012 to -0.118974072957687
Trying to learn from memory 84, 1, -0.2
sum 0.0386382716628473 distri 0.0254581221644256
Using diff 0.00352058158270984 and condRate 0.166666666666667
Changed category 1 weights from 
0.290320454434234 to 0.290203101713062
0.523636041477996 to 0.523518688756824
0.0987108035839388 to 0.0985934508627665
0.102803616122086 to 0.102686263400913
Changing layer 0's weights from 
-0.306062502885511 to -0.306179855606684
-0.677988959575346 to -0.678106312296519
-0.912338225269965 to -0.912455577991138
-0.877778975392034 to -0.877896328113207
-0.12991140606468 to -0.130028758785853
-0.812553359294585 to -0.812670712015758
-0.658723635698012 to -0.658840988419185
-0.686178041721037 to -0.68629539444221
-0.923669902706793 to -0.923787255427966
-0.967419957841089 to -0.967537310562262
Changing layer 1's weights from 
-0.962323045457056 to -0.962440398178229
-0.361154360795669 to -0.361271713516841
-0.486857099557571 to -0.486974452278743
-0.496762437844924 to -0.496879790566096
-0.681815220142058 to -0.681932572863231
-0.820053054118804 to -0.820170406839977
-0.549334330583266 to -0.549451683304438
-0.935137359524421 to -0.935254712245594
-0.523474378610305 to -0.523591731331477
-0.886121241474798 to -0.886238594195971
Changing layer 2's weights from 
-0.0769330096489706 to -0.077050362370143
-0.630986495042495 to -0.631103847763668
-0.346397204423598 to -0.34651455714477
-0.175197107816389 to -0.175314460537562
-0.35244981768196 to -0.352567170403132
-0.55845944407051 to -0.558576796791682
-0.0396467280632767 to -0.039764080784449
-0.112068755651168 to -0.11218610837234
-0.844067050243071 to -0.844184402964244
-0.799109531665496 to -0.799226884386669
Changing layer 3's weights from 
-0.109983308339767 to -0.110100661060939
-0.373536391282729 to -0.373653744003901
-0.130926473165205 to -0.131043825886378
-1.03836570492135 to -1.03848305764252
-0.253602368856123 to -0.253719721577295
-0.383850975061111 to -0.383968327782283
-0.71403582217758 to -0.714153174898753
-0.255522949720076 to -0.255640302441248
-0.587218923593215 to -0.587336276314388
-0.114389045262985 to -0.114506397984157
Changing layer 4's weights from 
-0.213662846112898 to -0.21378019883407
-0.422495646501235 to -0.422612999222407
-0.622973246598891 to -0.623090599320064
-0.336038513208083 to -0.336155865929255
-0.575291318917922 to -0.575408671639095
-0.343826098466567 to -0.343943451187739
-0.348184628511123 to -0.348301981232295
-0.810082150722197 to -0.81019950344337
-0.216386063123396 to -0.216503415844568
-0.64868800165718 to -0.648805354378353
Changing layer 5's weights from 
-0.0608830285317216 to -0.061000381252894
-0.502956552529983 to -0.503073905251155
-0.0777681899315626 to -0.077885542652735
-1.01328037130301 to -1.01339772402419
-0.47601155760353 to -0.476128910324702
-0.874479500675849 to -0.874596853397022
-0.57451419355934 to -0.574631546280513
-0.502067966485671 to -0.502185319206843
-0.112465961003951 to -0.112583313725123
-0.118974072957687 to -0.119091425678859
Trying to learn from memory 84, 0, -0.2
sum 0.0386382716628473 distri -0.0258073883634901
Using diff 0.0547860921106256 and condRate 0.166666666666667
Changed category 0 weights from 
0.134789977524689 to 0.132963774427122
-0.265244986559936 to -0.267071189657503
-0.547765683080743 to -0.54959188617831
-0.396260317112992 to -0.398086520210559
Changing layer 0's weights from 
-0.306179855606684 to -0.30800605870425
-0.678106312296519 to -0.679932515394086
-0.912455577991138 to -0.914281781088705
-0.877896328113207 to -0.879722531210773
-0.130028758785853 to -0.13185496188342
-0.812670712015758 to -0.814496915113324
-0.658840988419185 to -0.660667191516751
-0.68629539444221 to -0.688121597539776
-0.923787255427966 to -0.925613458525532
-0.967537310562262 to -0.969363513659828
Changing layer 1's weights from 
-0.962440398178229 to -0.964266601275795
-0.361271713516841 to -0.363097916614408
-0.486974452278743 to -0.48880065537631
-0.496879790566096 to -0.498705993663663
-0.681932572863231 to -0.683758775960797
-0.820170406839977 to -0.821996609937543
-0.549451683304438 to -0.551277886402005
-0.935254712245594 to -0.937080915343161
-0.523591731331477 to -0.525417934429044
-0.886238594195971 to -0.888064797293537
Changing layer 2's weights from 
-0.077050362370143 to -0.0788765654677097
-0.631103847763668 to -0.632930050861235
-0.34651455714477 to -0.348340760242337
-0.175314460537562 to -0.177140663635128
-0.352567170403132 to -0.354393373500699
-0.558576796791682 to -0.560402999889249
-0.039764080784449 to -0.0415902838820158
-0.11218610837234 to -0.114012311469907
-0.844184402964244 to -0.84601060606181
-0.799226884386669 to -0.801053087484235
Changing layer 3's weights from 
-0.110100661060939 to -0.111926864158506
-0.373653744003901 to -0.375479947101468
-0.131043825886378 to -0.132870028983945
-1.03848305764252 to -1.04030926074009
-0.253719721577295 to -0.255545924674862
-0.383968327782283 to -0.38579453087985
-0.714153174898753 to -0.71597937799632
-0.255640302441248 to -0.257466505538815
-0.587336276314388 to -0.589162479411955
-0.114506397984157 to -0.116332601081724
Changing layer 4's weights from 
-0.21378019883407 to -0.215606401931637
-0.422612999222407 to -0.424439202319974
-0.623090599320064 to -0.62491680241763
-0.336155865929255 to -0.337982069026822
-0.575408671639095 to -0.577234874736661
-0.343943451187739 to -0.345769654285306
-0.348301981232295 to -0.350128184329862
-0.81019950344337 to -0.812025706540936
-0.216503415844568 to -0.218329618942135
-0.648805354378353 to -0.65063155747592
Changing layer 5's weights from 
-0.061000381252894 to -0.0628265843504607
-0.503073905251155 to -0.504900108348722
-0.077885542652735 to -0.0797117457503017
-1.01339772402419 to -1.01522392712175
-0.476128910324702 to -0.477955113422269
-0.874596853397022 to -0.876423056494589
-0.574631546280513 to -0.576457749378079
-0.502185319206843 to -0.50401152230441
-0.112583313725123 to -0.11440951682269
-0.119091425678859 to -0.120917628776426
Trying to learn from memory 84, 0, -0.2
sum 0.0386382716628473 distri -0.0258073883634901
Using diff 0.0547860921106256 and condRate 0.166666666666667
Changed category 0 weights from 
0.132963774427122 to 0.131137571329556
-0.267071189657503 to -0.26889739275507
-0.54959188617831 to -0.551418089275877
-0.398086520210559 to -0.399912723308125
Changing layer 0's weights from 
-0.30800605870425 to -0.309832261801817
-0.679932515394086 to -0.681758718491652
-0.914281781088705 to -0.916107984186271
-0.879722531210773 to -0.88154873430834
-0.13185496188342 to -0.133681164980986
-0.814496915113324 to -0.816323118210891
-0.660667191516751 to -0.662493394614318
-0.688121597539776 to -0.689947800637343
-0.925613458525532 to -0.927439661623099
-0.969363513659828 to -0.971189716757395
Changing layer 1's weights from 
-0.964266601275795 to -0.966092804373362
-0.363097916614408 to -0.364924119711975
-0.48880065537631 to -0.490626858473877
-0.498705993663663 to -0.50053219676123
-0.683758775960797 to -0.685584979058364
-0.821996609937543 to -0.82382281303511
-0.551277886402005 to -0.553104089499572
-0.937080915343161 to -0.938907118440727
-0.525417934429044 to -0.52724413752661
-0.888064797293537 to -0.889891000391104
Changing layer 2's weights from 
-0.0788765654677097 to -0.0807027685652764
-0.632930050861235 to -0.634756253958801
-0.348340760242337 to -0.350166963339904
-0.177140663635128 to -0.178966866732695
-0.354393373500699 to -0.356219576598266
-0.560402999889249 to -0.562229202986816
-0.0415902838820158 to -0.0434164869795825
-0.114012311469907 to -0.115838514567473
-0.84601060606181 to -0.847836809159377
-0.801053087484235 to -0.802879290581802
Changing layer 3's weights from 
-0.111926864158506 to -0.113753067256072
-0.375479947101468 to -0.377306150199035
-0.132870028983945 to -0.134696232081511
-1.04030926074009 to -1.04213546383765
-0.255545924674862 to -0.257372127772429
-0.38579453087985 to -0.387620733977417
-0.71597937799632 to -0.717805581093886
-0.257466505538815 to -0.259292708636382
-0.589162479411955 to -0.590988682509521
-0.116332601081724 to -0.11815880417929
Changing layer 4's weights from 
-0.215606401931637 to -0.217432605029204
-0.424439202319974 to -0.426265405417541
-0.62491680241763 to -0.626743005515197
-0.337982069026822 to -0.339808272124389
-0.577234874736661 to -0.579061077834228
-0.345769654285306 to -0.347595857382873
-0.350128184329862 to -0.351954387427429
-0.812025706540936 to -0.813851909638503
-0.218329618942135 to -0.220155822039702
-0.65063155747592 to -0.652457760573486
Changing layer 5's weights from 
-0.0628265843504607 to -0.0646527874480274
-0.504900108348722 to -0.506726311446289
-0.0797117457503017 to -0.0815379488478684
-1.01522392712175 to -1.01705013021932
-0.477955113422269 to -0.479781316519836
-0.876423056494589 to -0.878249259592155
-0.576457749378079 to -0.578283952475646
-0.50401152230441 to -0.505837725401977
-0.11440951682269 to -0.116235719920256
-0.120917628776426 to -0.122743831873992
Trying to learn from memory 84, 1, -0.2
sum 0.0386382716628473 distri 0.0254581221644256
Using diff 0.00352058158270984 and condRate 0.166666666666667
Changed category 1 weights from 
0.290203101713062 to 0.29008574899189
0.523518688756824 to 0.523401336035652
0.0985934508627665 to 0.0984760981415941
0.102686263400913 to 0.102568910679741
Changing layer 0's weights from 
-0.309832261801817 to -0.30994961452299
-0.681758718491652 to -0.681876071212825
-0.916107984186271 to -0.916225336907444
-0.88154873430834 to -0.881666087029513
-0.133681164980986 to -0.133798517702159
-0.816323118210891 to -0.816440470932064
-0.662493394614318 to -0.662610747335491
-0.689947800637343 to -0.690065153358515
-0.927439661623099 to -0.927557014344272
-0.971189716757395 to -0.971307069478567
Changing layer 1's weights from 
-0.966092804373362 to -0.966210157094535
-0.364924119711975 to -0.365041472433147
-0.490626858473877 to -0.490744211195049
-0.50053219676123 to -0.500649549482402
-0.685584979058364 to -0.685702331779537
-0.82382281303511 to -0.823940165756282
-0.553104089499572 to -0.553221442220744
-0.938907118440727 to -0.9390244711619
-0.52724413752661 to -0.527361490247783
-0.889891000391104 to -0.890008353112277
Changing layer 2's weights from 
-0.0807027685652764 to -0.0808201212864488
-0.634756253958801 to -0.634873606679974
-0.350166963339904 to -0.350284316061076
-0.178966866732695 to -0.179084219453867
-0.356219576598266 to -0.356336929319438
-0.562229202986816 to -0.562346555707988
-0.0434164869795825 to -0.0435338397007548
-0.115838514567473 to -0.115955867288646
-0.847836809159377 to -0.84795416188055
-0.802879290581802 to -0.802996643302974
Changing layer 3's weights from 
-0.113753067256072 to -0.113870419977245
-0.377306150199035 to -0.377423502920207
-0.134696232081511 to -0.134813584802684
-1.04213546383765 to -1.04225281655883
-0.257372127772429 to -0.257489480493601
-0.387620733977417 to -0.387738086698589
-0.717805581093886 to -0.717922933815059
-0.259292708636382 to -0.259410061357554
-0.590988682509521 to -0.591106035230694
-0.11815880417929 to -0.118276156900463
Changing layer 4's weights from 
-0.217432605029204 to -0.217549957750376
-0.426265405417541 to -0.426382758138713
-0.626743005515197 to -0.626860358236369
-0.339808272124389 to -0.339925624845561
-0.579061077834228 to -0.5791784305554
-0.347595857382873 to -0.347713210104045
-0.351954387427429 to -0.352071740148601
-0.813851909638503 to -0.813969262359675
-0.220155822039702 to -0.220273174760874
-0.652457760573486 to -0.652575113294659
Changing layer 5's weights from 
-0.0646527874480274 to -0.0647701401691998
-0.506726311446289 to -0.506843664167461
-0.0815379488478684 to -0.0816553015690408
-1.01705013021932 to -1.01716748294049
-0.479781316519836 to -0.479898669241008
-0.878249259592155 to -0.878366612313328
-0.578283952475646 to -0.578401305196818
-0.505837725401977 to -0.505955078123149
-0.116235719920256 to -0.116353072641429
-0.122743831873992 to -0.122861184595165
Trying to learn from memory 84, 0, -0.2
sum 0.0386382716628473 distri -0.0258073883634901
Using diff 0.0547860921106256 and condRate 0.166666666666667
Changed category 0 weights from 
0.131137571329556 to 0.129311368231989
-0.26889739275507 to -0.270723595852637
-0.551418089275877 to -0.553244292373444
-0.399912723308125 to -0.401738926405692
Changing layer 0's weights from 
-0.30994961452299 to -0.311775817620556
-0.681876071212825 to -0.683702274310391
-0.916225336907444 to -0.91805154000501
-0.881666087029513 to -0.883492290127079
-0.133798517702159 to -0.135624720799725
-0.816440470932064 to -0.81826667402963
-0.662610747335491 to -0.664436950433057
-0.690065153358515 to -0.691891356456082
-0.927557014344272 to -0.929383217441838
-0.971307069478567 to -0.973133272576134
Changing layer 1's weights from 
-0.966210157094535 to -0.968036360192101
-0.365041472433147 to -0.366867675530714
-0.490744211195049 to -0.492570414292616
-0.500649549482402 to -0.502475752579969
-0.685702331779537 to -0.687528534877103
-0.823940165756282 to -0.825766368853849
-0.553221442220744 to -0.555047645318311
-0.9390244711619 to -0.940850674259466
-0.527361490247783 to -0.52918769334535
-0.890008353112277 to -0.891834556209843
Changing layer 2's weights from 
-0.0808201212864488 to -0.0826463243840155
-0.634873606679974 to -0.63669980977754
-0.350284316061076 to -0.352110519158643
-0.179084219453867 to -0.180910422551434
-0.356336929319438 to -0.358163132417005
-0.562346555707988 to -0.564172758805555
-0.0435338397007548 to -0.0453600427983216
-0.115955867288646 to -0.117782070386213
-0.84795416188055 to -0.849780364978116
-0.802996643302974 to -0.804822846400541
Changing layer 3's weights from 
-0.113870419977245 to -0.115696623074812
-0.377423502920207 to -0.379249706017774
-0.134813584802684 to -0.13663978790025
-1.04225281655883 to -1.04407901965639
-0.257489480493601 to -0.259315683591168
-0.387738086698589 to -0.389564289796156
-0.717922933815059 to -0.719749136912625
-0.259410061357554 to -0.261236264455121
-0.591106035230694 to -0.59293223832826
-0.118276156900463 to -0.120102359998029
Changing layer 4's weights from 
-0.217549957750376 to -0.219376160847943
-0.426382758138713 to -0.42820896123628
-0.626860358236369 to -0.628686561333936
-0.339925624845561 to -0.341751827943128
-0.5791784305554 to -0.581004633652967
-0.347713210104045 to -0.349539413201612
-0.352071740148601 to -0.353897943246168
-0.813969262359675 to -0.815795465457242
-0.220273174760874 to -0.222099377858441
-0.652575113294659 to -0.654401316392225
Changing layer 5's weights from 
-0.0647701401691998 to -0.0665963432667665
-0.506843664167461 to -0.508669867265028
-0.0816553015690408 to -0.0834815046666075
-1.01716748294049 to -1.01899368603806
-0.479898669241008 to -0.481724872338575
-0.878366612313328 to -0.880192815410894
-0.578401305196818 to -0.580227508294385
-0.505955078123149 to -0.507781281220716
-0.116353072641429 to -0.118179275738996
-0.122861184595165 to -0.124687387692732
10/5/2016 1:44:43 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 84, 0, -0.2
sum 0.0386382716628473 distri -0.0258073883634901
Using diff 0.0547860921106256 and condRate 0.166666666666667
Changed category 0 weights from 
0.129311368231989 to 0.127485165134422
-0.270723595852637 to -0.272549798950203
-0.553244292373444 to -0.555070495471011
-0.401738926405692 to -0.403565129503259
Changing layer 0's weights from 
-0.311775817620556 to -0.313602020718123
-0.683702274310391 to -0.685528477407958
-0.91805154000501 to -0.919877743102577
-0.883492290127079 to -0.885318493224646
-0.135624720799725 to -0.137450923897292
-0.81826667402963 to -0.820092877127197
-0.664436950433057 to -0.666263153530624
-0.691891356456082 to -0.693717559553649
-0.929383217441838 to -0.931209420539405
-0.973133272576134 to -0.974959475673701
Changing layer 1's weights from 
-0.968036360192101 to -0.969862563289668
-0.366867675530714 to -0.368693878628281
-0.492570414292616 to -0.494396617390183
-0.502475752579969 to -0.504301955677536
-0.687528534877103 to -0.68935473797467
-0.825766368853849 to -0.827592571951416
-0.555047645318311 to -0.556873848415878
-0.940850674259466 to -0.942676877357033
-0.52918769334535 to -0.531013896442916
-0.891834556209843 to -0.89366075930741
Changing layer 2's weights from 
-0.0826463243840155 to -0.0844725274815822
-0.63669980977754 to -0.638526012875107
-0.352110519158643 to -0.35393672225621
-0.180910422551434 to -0.182736625649001
-0.358163132417005 to -0.359989335514572
-0.564172758805555 to -0.565998961903122
-0.0453600427983216 to -0.0471862458958883
-0.117782070386213 to -0.119608273483779
-0.849780364978116 to -0.851606568075683
-0.804822846400541 to -0.806649049498108
Changing layer 3's weights from 
-0.115696623074812 to -0.117522826172378
-0.379249706017774 to -0.381075909115341
-0.13663978790025 to -0.138465990997817
-1.04407901965639 to -1.04590522275396
-0.259315683591168 to -0.261141886688734
-0.389564289796156 to -0.391390492893723
-0.719749136912625 to -0.721575340010192
-0.261236264455121 to -0.263062467552687
-0.59293223832826 to -0.594758441425827
-0.120102359998029 to -0.121928563095596
Changing layer 4's weights from 
-0.219376160847943 to -0.221202363945509
-0.42820896123628 to -0.430035164333847
-0.628686561333936 to -0.630512764431503
-0.341751827943128 to -0.343578031040694
-0.581004633652967 to -0.582830836750534
-0.349539413201612 to -0.351365616299179
-0.353897943246168 to -0.355724146343735
-0.815795465457242 to -0.817621668554809
-0.222099377858441 to -0.223925580956007
-0.654401316392225 to -0.656227519489792
Changing layer 5's weights from 
-0.0665963432667665 to -0.0684225463643333
-0.508669867265028 to -0.510496070362594
-0.0834815046666075 to -0.0853077077641742
-1.01899368603806 to -1.02081988913563
-0.481724872338575 to -0.483551075436142
-0.880192815410894 to -0.882019018508461
-0.580227508294385 to -0.582053711391952
-0.507781281220716 to -0.509607484318283
-0.118179275738996 to -0.120005478836562
-0.124687387692732 to -0.126513590790298
Trying to learn from memory 84, 1, -0.2
sum 0.0386382716628473 distri 0.0254581221644256
Using diff 0.00352058158270984 and condRate 0.166666666666667
Changed category 1 weights from 
0.29008574899189 to 0.289968396270717
0.523401336035652 to 0.52328398331448
0.0984760981415941 to 0.0983587454204218
0.102568910679741 to 0.102451557958569
Changing layer 0's weights from 
-0.313602020718123 to -0.313719373439295
-0.685528477407958 to -0.68564583012913
-0.919877743102577 to -0.919995095823749
-0.885318493224646 to -0.885435845945818
-0.137450923897292 to -0.137568276618464
-0.820092877127197 to -0.820210229848369
-0.666263153530624 to -0.666380506251796
-0.693717559553649 to -0.693834912274821
-0.931209420539405 to -0.931326773260577
-0.974959475673701 to -0.975076828394873
Changing layer 1's weights from 
-0.969862563289668 to -0.96997991601084
-0.368693878628281 to -0.368811231349453
-0.494396617390183 to -0.494513970111355
-0.504301955677536 to -0.504419308398708
-0.68935473797467 to -0.689472090695842
-0.827592571951416 to -0.827709924672588
-0.556873848415878 to -0.55699120113705
-0.942676877357033 to -0.942794230078205
-0.531013896442916 to -0.531131249164089
-0.89366075930741 to -0.893778112028582
Changing layer 2's weights from 
-0.0844725274815822 to -0.0845898802027546
-0.638526012875107 to -0.638643365596279
-0.35393672225621 to -0.354054074977382
-0.182736625649001 to -0.182853978370173
-0.359989335514572 to -0.360106688235744
-0.565998961903122 to -0.566116314624294
-0.0471862458958883 to -0.0473035986170607
-0.119608273483779 to -0.119725626204952
-0.851606568075683 to -0.851723920796855
-0.806649049498108 to -0.80676640221928
Changing layer 3's weights from 
-0.117522826172378 to -0.117640178893551
-0.381075909115341 to -0.381193261836513
-0.138465990997817 to -0.138583343718989
-1.04590522275396 to -1.04602257547513
-0.261141886688734 to -0.261259239409907
-0.391390492893723 to -0.391507845614895
-0.721575340010192 to -0.721692692731364
-0.263062467552687 to -0.26317982027386
-0.594758441425827 to -0.594875794146999
-0.121928563095596 to -0.122045915816769
Changing layer 4's weights from 
-0.221202363945509 to -0.221319716666682
-0.430035164333847 to -0.430152517055019
-0.630512764431503 to -0.630630117152675
-0.343578031040694 to -0.343695383761867
-0.582830836750534 to -0.582948189471706
-0.351365616299179 to -0.351482969020351
-0.355724146343735 to -0.355841499064907
-0.817621668554809 to -0.817739021275981
-0.223925580956007 to -0.22404293367718
-0.656227519489792 to -0.656344872210964
Changing layer 5's weights from 
-0.0684225463643333 to -0.0685398990855056
-0.510496070362594 to -0.510613423083767
-0.0853077077641742 to -0.0854250604853466
-1.02081988913563 to -1.0209372418568
-0.483551075436142 to -0.483668428157314
-0.882019018508461 to -0.882136371229633
-0.582053711391952 to -0.582171064113124
-0.509607484318283 to -0.509724837039455
-0.120005478836562 to -0.120122831557735
-0.126513590790298 to -0.126630943511471
Trying to learn from memory 84, 0, -0.2
sum 0.0386382716628473 distri -0.0258073883634901
Using diff 0.0547860921106256 and condRate 0.166666666666667
Changed category 0 weights from 
0.127485165134422 to 0.125658962036855
-0.272549798950203 to -0.27437600204777
-0.555070495471011 to -0.556896698568577
-0.403565129503259 to -0.405391332600826
Changing layer 0's weights from 
-0.313719373439295 to -0.315545576536862
-0.68564583012913 to -0.687472033226697
-0.919995095823749 to -0.921821298921316
-0.885435845945818 to -0.887262049043385
-0.137568276618464 to -0.139394479716031
-0.820210229848369 to -0.822036432945936
-0.666380506251796 to -0.668206709349363
-0.693834912274821 to -0.695661115372388
-0.931326773260577 to -0.933152976358144
-0.975076828394873 to -0.97690303149244
Changing layer 1's weights from 
-0.96997991601084 to -0.971806119108407
-0.368811231349453 to -0.37063743444702
-0.494513970111355 to -0.496340173208922
-0.504419308398708 to -0.506245511496275
-0.689472090695842 to -0.691298293793409
-0.827709924672588 to -0.829536127770155
-0.55699120113705 to -0.558817404234617
-0.942794230078205 to -0.944620433175772
-0.531131249164089 to -0.532957452261655
-0.893778112028582 to -0.895604315126149
Changing layer 2's weights from 
-0.0845898802027546 to -0.0864160833003213
-0.638643365596279 to -0.640469568693846
-0.354054074977382 to -0.355880278074949
-0.182853978370173 to -0.18468018146774
-0.360106688235744 to -0.361932891333311
-0.566116314624294 to -0.567942517721861
-0.0473035986170607 to -0.0491298017146274
-0.119725626204952 to -0.121551829302518
-0.851723920796855 to -0.853550123894422
-0.80676640221928 to -0.808592605316847
Changing layer 3's weights from 
-0.117640178893551 to -0.119466381991117
-0.381193261836513 to -0.38301946493408
-0.138583343718989 to -0.140409546816556
-1.04602257547513 to -1.0478487785727
-0.261259239409907 to -0.263085442507474
-0.391507845614895 to -0.393334048712462
-0.721692692731364 to -0.723518895828931
-0.26317982027386 to -0.265006023371427
-0.594875794146999 to -0.596701997244566
-0.122045915816769 to -0.123872118914335
Changing layer 4's weights from 
-0.221319716666682 to -0.223145919764248
-0.430152517055019 to -0.431978720152586
-0.630630117152675 to -0.632456320250242
-0.343695383761867 to -0.345521586859434
-0.582948189471706 to -0.584774392569273
-0.351482969020351 to -0.353309172117918
-0.355841499064907 to -0.357667702162474
-0.817739021275981 to -0.819565224373548
-0.22404293367718 to -0.225869136774746
-0.656344872210964 to -0.658171075308531
Changing layer 5's weights from 
-0.0685398990855056 to -0.0703661021830723
-0.510613423083767 to -0.512439626181334
-0.0854250604853466 to -0.0872512635829133
-1.0209372418568 to -1.02276344495436
-0.483668428157314 to -0.485494631254881
-0.882136371229633 to -0.8839625743272
-0.582171064113124 to -0.583997267210691
-0.509724837039455 to -0.511551040137022
-0.120122831557735 to -0.121949034655301
-0.126630943511471 to -0.128457146609037
Trying to learn from memory 84, 1, -0.2
sum 0.0386382716628473 distri 0.0254581221644256
Using diff 0.00352058158270984 and condRate 0.166666666666667
Changed category 1 weights from 
0.289968396270717 to 0.289851043549545
0.52328398331448 to 0.523166630593307
0.0983587454204218 to 0.0982413926992494
0.102451557958569 to 0.102334205237396
Changing layer 0's weights from 
-0.315545576536862 to -0.315662929258035
-0.687472033226697 to -0.687589385947869
-0.921821298921316 to -0.921938651642488
-0.887262049043385 to -0.887379401764557
-0.139394479716031 to -0.139511832437204
-0.822036432945936 to -0.822153785667108
-0.668206709349363 to -0.668324062070535
-0.695661115372388 to -0.69577846809356
-0.933152976358144 to -0.933270329079316
-0.97690303149244 to -0.977020384213612
Changing layer 1's weights from 
-0.971806119108407 to -0.971923471829579
-0.37063743444702 to -0.370754787168192
-0.496340173208922 to -0.496457525930094
-0.506245511496275 to -0.506362864217447
-0.691298293793409 to -0.691415646514581
-0.829536127770155 to -0.829653480491327
-0.558817404234617 to -0.558934756955789
-0.944620433175772 to -0.944737785896944
-0.532957452261655 to -0.533074804982828
-0.895604315126149 to -0.895721667847321
Changing layer 2's weights from 
-0.0864160833003213 to -0.0865334360214937
-0.640469568693846 to -0.640586921415019
-0.355880278074949 to -0.355997630796121
-0.18468018146774 to -0.184797534188912
-0.361932891333311 to -0.362050244054483
-0.567942517721861 to -0.568059870443033
-0.0491298017146274 to -0.0492471544357997
-0.121551829302518 to -0.121669182023691
-0.853550123894422 to -0.853667476615594
-0.808592605316847 to -0.808709958038019
Changing layer 3's weights from 
-0.119466381991117 to -0.11958373471229
-0.38301946493408 to -0.383136817655252
-0.140409546816556 to -0.140526899537729
-1.0478487785727 to -1.04796613129387
-0.263085442507474 to -0.263202795228646
-0.393334048712462 to -0.393451401433634
-0.723518895828931 to -0.723636248550103
-0.265006023371427 to -0.265123376092599
-0.596701997244566 to -0.596819349965738
-0.123872118914335 to -0.123989471635508
Changing layer 4's weights from 
-0.223145919764248 to -0.223263272485421
-0.431978720152586 to -0.432096072873758
-0.632456320250242 to -0.632573672971414
-0.345521586859434 to -0.345638939580606
-0.584774392569273 to -0.584891745290445
-0.353309172117918 to -0.35342652483909
-0.357667702162474 to -0.357785054883646
-0.819565224373548 to -0.81968257709472
-0.225869136774746 to -0.225986489495919
-0.658171075308531 to -0.658288428029703
Changing layer 5's weights from 
-0.0703661021830723 to -0.0704834549042447
-0.512439626181334 to -0.512556978902506
-0.0872512635829133 to -0.0873686163040857
-1.02276344495436 to -1.02288079767554
-0.485494631254881 to -0.485611983976053
-0.8839625743272 to -0.884079927048372
-0.583997267210691 to -0.584114619931863
-0.511551040137022 to -0.511668392858194
-0.121949034655301 to -0.122066387376474
-0.128457146609037 to -0.12857449933021
Trying to learn from memory 84, 1, -0.2
sum 0.0386382716628473 distri 0.0254581221644256
Using diff 0.00352058158270984 and condRate 0.166666666666667
Changed category 1 weights from 
0.289851043549545 to 0.289733690828373
0.523166630593307 to 0.523049277872135
0.0982413926992494 to 0.098124039978077
0.102334205237396 to 0.102216852516224
Changing layer 0's weights from 
-0.315662929258035 to -0.315780281979207
-0.687589385947869 to -0.687706738669042
-0.921938651642488 to -0.922056004363661
-0.887379401764557 to -0.88749675448573
-0.139511832437204 to -0.139629185158376
-0.822153785667108 to -0.822271138388281
-0.668324062070535 to -0.668441414791708
-0.69577846809356 to -0.695895820814733
-0.933270329079316 to -0.933387681800489
-0.977020384213612 to -0.977137736934785
Changing layer 1's weights from 
-0.971923471829579 to -0.972040824550752
-0.370754787168192 to -0.370872139889364
-0.496457525930094 to -0.496574878651266
-0.506362864217447 to -0.506480216938619
-0.691415646514581 to -0.691532999235754
-0.829653480491327 to -0.8297708332125
-0.558934756955789 to -0.559052109676961
-0.944737785896944 to -0.944855138618117
-0.533074804982828 to -0.533192157704
-0.895721667847321 to -0.895839020568494
Changing layer 2's weights from 
-0.0865334360214937 to -0.086650788742666
-0.640586921415019 to -0.640704274136191
-0.355997630796121 to -0.356114983517293
-0.184797534188912 to -0.184914886910085
-0.362050244054483 to -0.362167596775655
-0.568059870443033 to -0.568177223164205
-0.0492471544357997 to -0.0493645071569721
-0.121669182023691 to -0.121786534744863
-0.853667476615594 to -0.853784829336767
-0.808709958038019 to -0.808827310759192
Changing layer 3's weights from 
-0.11958373471229 to -0.119701087433462
-0.383136817655252 to -0.383254170376424
-0.140526899537729 to -0.140644252258901
-1.04796613129387 to -1.04808348401504
-0.263202795228646 to -0.263320147949818
-0.393451401433634 to -0.393568754154807
-0.723636248550103 to -0.723753601271276
-0.265123376092599 to -0.265240728813771
-0.596819349965738 to -0.596936702686911
-0.123989471635508 to -0.12410682435668
Changing layer 4's weights from 
-0.223263272485421 to -0.223380625206593
-0.432096072873758 to -0.43221342559493
-0.632573672971414 to -0.632691025692587
-0.345638939580606 to -0.345756292301778
-0.584891745290445 to -0.585009098011618
-0.35342652483909 to -0.353543877560262
-0.357785054883646 to -0.357902407604818
-0.81968257709472 to -0.819799929815893
-0.225986489495919 to -0.226103842217091
-0.658288428029703 to -0.658405780750876
Changing layer 5's weights from 
-0.0704834549042447 to -0.0706008076254171
-0.512556978902506 to -0.512674331623678
-0.0873686163040857 to -0.087485969025258
-1.02288079767554 to -1.02299815039671
-0.485611983976053 to -0.485729336697226
-0.884079927048372 to -0.884197279769545
-0.584114619931863 to -0.584231972653036
-0.511668392858194 to -0.511785745579366
-0.122066387376474 to -0.122183740097646
-0.12857449933021 to -0.128691852051382
Trying to learn from memory 84, 0, -0.2
sum 0.0386382716628473 distri -0.0258073883634901
Using diff 0.0547860921106256 and condRate 0.166666666666667
Changed category 0 weights from 
0.125658962036855 to 0.123832758939289
-0.27437600204777 to -0.276202205145337
-0.556896698568577 to -0.558722901666144
-0.405391332600826 to -0.407217535698392
Changing layer 0's weights from 
-0.315780281979207 to -0.317606485076774
-0.687706738669042 to -0.689532941766609
-0.922056004363661 to -0.923882207461228
-0.88749675448573 to -0.889322957583296
-0.139629185158376 to -0.141455388255943
-0.822271138388281 to -0.824097341485847
-0.668441414791708 to -0.670267617889274
-0.695895820814733 to -0.697722023912299
-0.933387681800489 to -0.935213884898055
-0.977137736934785 to -0.978963940032351
Changing layer 1's weights from 
-0.972040824550752 to -0.973867027648318
-0.370872139889364 to -0.372698342986931
-0.496574878651266 to -0.498401081748833
-0.506480216938619 to -0.508306420036186
-0.691532999235754 to -0.69335920233332
-0.8297708332125 to -0.831597036310066
-0.559052109676961 to -0.560878312774528
-0.944855138618117 to -0.946681341715684
-0.533192157704 to -0.535018360801567
-0.895839020568494 to -0.897665223666061
Changing layer 2's weights from 
-0.086650788742666 to -0.0884769918402328
-0.640704274136191 to -0.642530477233758
-0.356114983517293 to -0.35794118661486
-0.184914886910085 to -0.186741090007651
-0.362167596775655 to -0.363993799873222
-0.568177223164205 to -0.570003426261772
-0.0493645071569721 to -0.0511907102545388
-0.121786534744863 to -0.12361273784243
-0.853784829336767 to -0.855611032434333
-0.808827310759192 to -0.810653513856758
Changing layer 3's weights from 
-0.119701087433462 to -0.121527290531029
-0.383254170376424 to -0.385080373473991
-0.140644252258901 to -0.142470455356468
-1.04808348401504 to -1.04990968711261
-0.263320147949818 to -0.265146351047385
-0.393568754154807 to -0.395394957252373
-0.723753601271276 to -0.725579804368843
-0.265240728813771 to -0.267066931911338
-0.596936702686911 to -0.598762905784478
-0.12410682435668 to -0.125933027454247
Changing layer 4's weights from 
-0.223380625206593 to -0.22520682830416
-0.43221342559493 to -0.434039628692497
-0.632691025692587 to -0.634517228790153
-0.345756292301778 to -0.347582495399345
-0.585009098011618 to -0.586835301109184
-0.353543877560262 to -0.355370080657829
-0.357902407604818 to -0.359728610702385
-0.819799929815893 to -0.821626132913459
-0.226103842217091 to -0.227930045314658
-0.658405780750876 to -0.660231983848443
Changing layer 5's weights from 
-0.0706008076254171 to -0.0724270107229838
-0.512674331623678 to -0.514500534721245
-0.087485969025258 to -0.0893121721228248
-1.02299815039671 to -1.02482435349428
-0.485729336697226 to -0.487555539794792
-0.884197279769545 to -0.886023482867112
-0.584231972653036 to -0.586058175750602
-0.511785745579366 to -0.513611948676933
-0.122183740097646 to -0.124009943195213
-0.128691852051382 to -0.130518055148949
10/5/2016 1:44:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:44:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:22 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
Modified index 9's learning in memoryPool to -0.2
Modified index 10's learning in memoryPool to -0.2
Modified index 11's learning in memoryPool to -0.2
Modified index 12's learning in memoryPool to -0.2
Modified index 13's learning in memoryPool to -0.2
Modified index 14's learning in memoryPool to -0.2
Modified index 15's learning in memoryPool to -0.2
Modified index 16's learning in memoryPool to -0.2
Modified index 17's learning in memoryPool to -0.2
Modified index 18's learning in memoryPool to -0.2
Modified index 19's learning in memoryPool to -0.2
Modified index 20's learning in memoryPool to -0.2
Modified index 21's learning in memoryPool to -0.2
Modified index 22's learning in memoryPool to -0.2
Modified index 23's learning in memoryPool to -0.2
Modified index 24's learning in memoryPool to -0.2
Modified index 25's learning in memoryPool to -0.2
10/5/2016 1:45:22 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 85, 0, -0.2
sum 0.0393588213485535 distri -0.0287981637617231
Using diff 0.0583172797731382 and condRate 0.166666666666667
Changed category 0 weights from 
0.123832758939289 to 0.121888849584551
-0.276202205145337 to -0.278146114500075
-0.558722901666144 to -0.560666811020882
-0.407217535698392 to -0.40916144505313
Changing layer 0's weights from 
-0.317606485076774 to -0.319550394431511
-0.689532941766609 to -0.691476851121346
-0.923882207461228 to -0.925826116815965
-0.889322957583296 to -0.891266866938034
-0.141455388255943 to -0.14339929761068
-0.824097341485847 to -0.826041250840585
-0.670267617889274 to -0.672211527244012
-0.697722023912299 to -0.699665933267037
-0.935213884898055 to -0.937157794252793
-0.978963940032351 to -0.980907849387089
Changing layer 1's weights from 
-0.973867027648318 to -0.975810937003056
-0.372698342986931 to -0.374642252341669
-0.498401081748833 to -0.500344991103571
-0.508306420036186 to -0.510250329390924
-0.69335920233332 to -0.695303111688058
-0.831597036310066 to -0.833540945664804
-0.560878312774528 to -0.562822222129266
-0.946681341715684 to -0.948625251070421
-0.535018360801567 to -0.536962270156304
-0.897665223666061 to -0.899609133020798
Changing layer 2's weights from 
-0.0884769918402328 to -0.0904209011949706
-0.642530477233758 to -0.644474386588495
-0.35794118661486 to -0.359885095969598
-0.186741090007651 to -0.188684999362389
-0.363993799873222 to -0.36593770922796
-0.570003426261772 to -0.57194733561651
-0.0511907102545388 to -0.0531346196092766
-0.12361273784243 to -0.125556647197168
-0.855611032434333 to -0.857554941789071
-0.810653513856758 to -0.812597423211496
Changing layer 3's weights from 
-0.121527290531029 to -0.123471199885767
-0.385080373473991 to -0.387024282828729
-0.142470455356468 to -0.144414364711205
-1.04990968711261 to -1.05185359646735
-0.265146351047385 to -0.267090260402123
-0.395394957252373 to -0.397338866607111
-0.725579804368843 to -0.72752371372358
-0.267066931911338 to -0.269010841266076
-0.598762905784478 to -0.600706815139215
-0.125933027454247 to -0.127876936808984
Changing layer 4's weights from 
-0.22520682830416 to -0.227150737658898
-0.434039628692497 to -0.435983538047235
-0.634517228790153 to -0.636461138144891
-0.347582495399345 to -0.349526404754083
-0.586835301109184 to -0.588779210463922
-0.355370080657829 to -0.357313990012567
-0.359728610702385 to -0.361672520057123
-0.821626132913459 to -0.823570042268197
-0.227930045314658 to -0.229873954669396
-0.660231983848443 to -0.66217589320318
Changing layer 5's weights from 
-0.0724270107229838 to -0.0743709200777216
-0.514500534721245 to -0.516444444075983
-0.0893121721228248 to -0.0912560814775625
-1.02482435349428 to -1.02676826284901
-0.487555539794792 to -0.48949944914953
-0.886023482867112 to -0.887967392221849
-0.586058175750602 to -0.58800208510534
-0.513611948676933 to -0.515555858031671
-0.124009943195213 to -0.125953852549951
-0.130518055148949 to -0.132461964503687
Trying to learn from memory 86, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.121888849584551 to 0.119741595591303
-0.278146114500075 to -0.280293368493323
-0.560666811020882 to -0.56281406501413
-0.40916144505313 to -0.411308699046378
Changing layer 0's weights from 
-0.319550394431511 to -0.321697648424759
-0.691476851121346 to -0.693624105114594
-0.925826116815965 to -0.927973370809213
-0.891266866938034 to -0.893414120931282
-0.14339929761068 to -0.145546551603928
-0.826041250840585 to -0.828188504833833
-0.672211527244012 to -0.67435878123726
-0.699665933267037 to -0.701813187260285
-0.937157794252793 to -0.939305048246041
-0.980907849387089 to -0.983055103380337
Changing layer 1's weights from 
-0.975810937003056 to -0.977958190996304
-0.374642252341669 to -0.376789506334917
-0.500344991103571 to -0.502492245096819
-0.510250329390924 to -0.512397583384172
-0.695303111688058 to -0.697450365681306
-0.833540945664804 to -0.835688199658052
-0.562822222129266 to -0.564969476122514
-0.948625251070421 to -0.950772505063669
-0.536962270156304 to -0.539109524149552
-0.899609133020798 to -0.901756387014046
Changing layer 2's weights from 
-0.0904209011949706 to -0.0925681551882185
-0.644474386588495 to -0.646621640581743
-0.359885095969598 to -0.362032349962846
-0.188684999362389 to -0.190832253355637
-0.36593770922796 to -0.368084963221208
-0.57194733561651 to -0.574094589609758
-0.0531346196092766 to -0.0552818736025246
-0.125556647197168 to -0.127703901190416
-0.857554941789071 to -0.859702195782319
-0.812597423211496 to -0.814744677204744
Changing layer 3's weights from 
-0.123471199885767 to -0.125618453879015
-0.387024282828729 to -0.389171536821977
-0.144414364711205 to -0.146561618704453
-1.05185359646735 to -1.0540008504606
-0.267090260402123 to -0.269237514395371
-0.397338866607111 to -0.399486120600359
-0.72752371372358 to -0.729670967716828
-0.269010841266076 to -0.271158095259324
-0.600706815139215 to -0.602854069132463
-0.127876936808984 to -0.130024190802232
Changing layer 4's weights from 
-0.227150737658898 to -0.229297991652146
-0.435983538047235 to -0.438130792040483
-0.636461138144891 to -0.638608392138139
-0.349526404754083 to -0.351673658747331
-0.588779210463922 to -0.59092646445717
-0.357313990012567 to -0.359461244005815
-0.361672520057123 to -0.363819774050371
-0.823570042268197 to -0.825717296261445
-0.229873954669396 to -0.232021208662644
-0.66217589320318 to -0.664323147196428
Changing layer 5's weights from 
-0.0743709200777216 to -0.0765181740709696
-0.516444444075983 to -0.518591698069231
-0.0912560814775625 to -0.0934033354708105
-1.02676826284901 to -1.02891551684226
-0.48949944914953 to -0.491646703142778
-0.887967392221849 to -0.890114646215097
-0.58800208510534 to -0.590149339098588
-0.515555858031671 to -0.517703112024919
-0.125953852549951 to -0.128101106543199
-0.132461964503687 to -0.134609218496935
Trying to learn from memory 87, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.119741595591303 to 0.117594341598055
-0.280293368493323 to -0.282440622486571
-0.56281406501413 to -0.564961319007378
-0.411308699046378 to -0.413455953039626
Changing layer 0's weights from 
-0.321697648424759 to -0.323844902418007
-0.693624105114594 to -0.695771359107842
-0.927973370809213 to -0.930120624802461
-0.893414120931282 to -0.89556137492453
-0.145546551603928 to -0.147693805597176
-0.828188504833833 to -0.830335758827081
-0.67435878123726 to -0.676506035230508
-0.701813187260285 to -0.703960441253533
-0.939305048246041 to -0.941452302239289
-0.983055103380337 to -0.985202357373585
Changing layer 1's weights from 
-0.977958190996304 to -0.980105444989552
-0.376789506334917 to -0.378936760328165
-0.502492245096819 to -0.504639499090067
-0.512397583384172 to -0.51454483737742
-0.697450365681306 to -0.699597619674554
-0.835688199658052 to -0.8378354536513
-0.564969476122514 to -0.567116730115762
-0.950772505063669 to -0.952919759056917
-0.539109524149552 to -0.5412567781428
-0.901756387014046 to -0.903903641007294
Changing layer 2's weights from 
-0.0925681551882185 to -0.0947154091814665
-0.646621640581743 to -0.648768894574991
-0.362032349962846 to -0.364179603956094
-0.190832253355637 to -0.192979507348885
-0.368084963221208 to -0.370232217214456
-0.574094589609758 to -0.576241843603006
-0.0552818736025246 to -0.0574291275957726
-0.127703901190416 to -0.129851155183664
-0.859702195782319 to -0.861849449775567
-0.814744677204744 to -0.816891931197992
Changing layer 3's weights from 
-0.125618453879015 to -0.127765707872263
-0.389171536821977 to -0.391318790815225
-0.146561618704453 to -0.148708872697701
-1.0540008504606 to -1.05614810445384
-0.269237514395371 to -0.271384768388619
-0.399486120600359 to -0.401633374593607
-0.729670967716828 to -0.731818221710076
-0.271158095259324 to -0.273305349252572
-0.602854069132463 to -0.605001323125711
-0.130024190802232 to -0.13217144479548
Changing layer 4's weights from 
-0.229297991652146 to -0.231445245645394
-0.438130792040483 to -0.440278046033731
-0.638608392138139 to -0.640755646131387
-0.351673658747331 to -0.353820912740579
-0.59092646445717 to -0.593073718450418
-0.359461244005815 to -0.361608497999063
-0.363819774050371 to -0.365967028043619
-0.825717296261445 to -0.827864550254693
-0.232021208662644 to -0.234168462655892
-0.664323147196428 to -0.666470401189676
Changing layer 5's weights from 
-0.0765181740709696 to -0.0786654280642175
-0.518591698069231 to -0.520738952062479
-0.0934033354708105 to -0.0955505894640585
-1.02891551684226 to -1.03106277083551
-0.491646703142778 to -0.493793957136026
-0.890114646215097 to -0.892261900208345
-0.590149339098588 to -0.592296593091836
-0.517703112024919 to -0.519850366018167
-0.128101106543199 to -0.130248360536447
-0.134609218496935 to -0.136756472490183
Trying to learn from memory 88, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.117594341598055 to 0.115447087604807
-0.282440622486571 to -0.284587876479819
-0.564961319007378 to -0.567108573000626
-0.413455953039626 to -0.415603207032874
Changing layer 0's weights from 
-0.323844902418007 to -0.325992156411255
-0.695771359107842 to -0.69791861310109
-0.930120624802461 to -0.932267878795709
-0.89556137492453 to -0.897708628917778
-0.147693805597176 to -0.149841059590424
-0.830335758827081 to -0.832483012820329
-0.676506035230508 to -0.678653289223756
-0.703960441253533 to -0.706107695246781
-0.941452302239289 to -0.943599556232537
-0.985202357373585 to -0.987349611366833
Changing layer 1's weights from 
-0.980105444989552 to -0.9822526989828
-0.378936760328165 to -0.381084014321413
-0.504639499090067 to -0.506786753083315
-0.51454483737742 to -0.516692091370668
-0.699597619674554 to -0.701744873667802
-0.8378354536513 to -0.839982707644548
-0.567116730115762 to -0.56926398410901
-0.952919759056917 to -0.955067013050165
-0.5412567781428 to -0.543404032136048
-0.903903641007294 to -0.906050895000542
Changing layer 2's weights from 
-0.0947154091814665 to -0.0968626631747145
-0.648768894574991 to -0.650916148568239
-0.364179603956094 to -0.366326857949342
-0.192979507348885 to -0.195126761342133
-0.370232217214456 to -0.372379471207704
-0.576241843603006 to -0.578389097596254
-0.0574291275957726 to -0.0595763815890206
-0.129851155183664 to -0.131998409176912
-0.861849449775567 to -0.863996703768815
-0.816891931197992 to -0.81903918519124
Changing layer 3's weights from 
-0.127765707872263 to -0.129912961865511
-0.391318790815225 to -0.393466044808473
-0.148708872697701 to -0.150856126690949
-1.05614810445384 to -1.05829535844709
-0.271384768388619 to -0.273532022381867
-0.401633374593607 to -0.403780628586855
-0.731818221710076 to -0.733965475703324
-0.273305349252572 to -0.27545260324582
-0.605001323125711 to -0.607148577118959
-0.13217144479548 to -0.134318698788728
Changing layer 4's weights from 
-0.231445245645394 to -0.233592499638642
-0.440278046033731 to -0.442425300026979
-0.640755646131387 to -0.642902900124635
-0.353820912740579 to -0.355968166733827
-0.593073718450418 to -0.595220972443666
-0.361608497999063 to -0.363755751992311
-0.365967028043619 to -0.368114282036867
-0.827864550254693 to -0.830011804247941
-0.234168462655892 to -0.23631571664914
-0.666470401189676 to -0.668617655182924
Changing layer 5's weights from 
-0.0786654280642175 to -0.0808126820574655
-0.520738952062479 to -0.522886206055727
-0.0955505894640585 to -0.0976978434573065
-1.03106277083551 to -1.03321002482876
-0.493793957136026 to -0.495941211129274
-0.892261900208345 to -0.894409154201593
-0.592296593091836 to -0.594443847085084
-0.519850366018167 to -0.521997620011415
-0.130248360536447 to -0.132395614529695
-0.136756472490183 to -0.138903726483431
Trying to learn from memory 89, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.115447087604807 to 0.113299833611559
-0.284587876479819 to -0.286735130473067
-0.567108573000626 to -0.569255826993874
-0.415603207032874 to -0.417750461026122
Changing layer 0's weights from 
-0.325992156411255 to -0.328139410404503
-0.69791861310109 to -0.700065867094338
-0.932267878795709 to -0.934415132788957
-0.897708628917778 to -0.899855882911026
-0.149841059590424 to -0.151988313583672
-0.832483012820329 to -0.834630266813577
-0.678653289223756 to -0.680800543217004
-0.706107695246781 to -0.708254949240029
-0.943599556232537 to -0.945746810225785
-0.987349611366833 to -0.989496865360081
Changing layer 1's weights from 
-0.9822526989828 to -0.984399952976048
-0.381084014321413 to -0.383231268314661
-0.506786753083315 to -0.508934007076563
-0.516692091370668 to -0.518839345363916
-0.701744873667802 to -0.70389212766105
-0.839982707644548 to -0.842129961637796
-0.56926398410901 to -0.571411238102258
-0.955067013050165 to -0.957214267043413
-0.543404032136048 to -0.545551286129296
-0.906050895000542 to -0.90819814899379
Changing layer 2's weights from 
-0.0968626631747145 to -0.0990099171679625
-0.650916148568239 to -0.653063402561487
-0.366326857949342 to -0.36847411194259
-0.195126761342133 to -0.197274015335381
-0.372379471207704 to -0.374526725200952
-0.578389097596254 to -0.580536351589502
-0.0595763815890206 to -0.0617236355822686
-0.131998409176912 to -0.13414566317016
-0.863996703768815 to -0.866143957762063
-0.81903918519124 to -0.821186439184488
Changing layer 3's weights from 
-0.129912961865511 to -0.132060215858759
-0.393466044808473 to -0.395613298801721
-0.150856126690949 to -0.153003380684197
-1.05829535844709 to -1.06044261244034
-0.273532022381867 to -0.275679276375115
-0.403780628586855 to -0.405927882580103
-0.733965475703324 to -0.736112729696572
-0.27545260324582 to -0.277599857239068
-0.607148577118959 to -0.609295831112207
-0.134318698788728 to -0.136465952781976
Changing layer 4's weights from 
-0.233592499638642 to -0.23573975363189
-0.442425300026979 to -0.444572554020227
-0.642902900124635 to -0.645050154117883
-0.355968166733827 to -0.358115420727075
-0.595220972443666 to -0.597368226436914
-0.363755751992311 to -0.365903005985559
-0.368114282036867 to -0.370261536030115
-0.830011804247941 to -0.832159058241189
-0.23631571664914 to -0.238462970642388
-0.668617655182924 to -0.670764909176172
Changing layer 5's weights from 
-0.0808126820574655 to -0.0829599360507135
-0.522886206055727 to -0.525033460048975
-0.0976978434573065 to -0.0998450974505545
-1.03321002482876 to -1.035357278822
-0.495941211129274 to -0.498088465122522
-0.894409154201593 to -0.896556408194841
-0.594443847085084 to -0.596591101078332
-0.521997620011415 to -0.524144874004663
-0.132395614529695 to -0.134542868522943
-0.138903726483431 to -0.141050980476679
Trying to learn from memory 90, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.113299833611559 to 0.111152579618311
-0.286735130473067 to -0.288882384466315
-0.569255826993874 to -0.571403080987122
-0.417750461026122 to -0.41989771501937
Changing layer 0's weights from 
-0.328139410404503 to -0.330286664397751
-0.700065867094338 to -0.702213121087586
-0.934415132788957 to -0.936562386782205
-0.899855882911026 to -0.902003136904274
-0.151988313583672 to -0.15413556757692
-0.834630266813577 to -0.836777520806825
-0.680800543217004 to -0.682947797210252
-0.708254949240029 to -0.710402203233277
-0.945746810225785 to -0.947894064219033
-0.989496865360081 to -0.991644119353329
Changing layer 1's weights from 
-0.984399952976048 to -0.986547206969296
-0.383231268314661 to -0.385378522307909
-0.508934007076563 to -0.511081261069811
-0.518839345363916 to -0.520986599357164
-0.70389212766105 to -0.706039381654298
-0.842129961637796 to -0.844277215631044
-0.571411238102258 to -0.573558492095506
-0.957214267043413 to -0.959361521036661
-0.545551286129296 to -0.547698540122544
-0.90819814899379 to -0.910345402987038
Changing layer 2's weights from 
-0.0990099171679625 to -0.10115717116121
-0.653063402561487 to -0.655210656554735
-0.36847411194259 to -0.370621365935838
-0.197274015335381 to -0.199421269328629
-0.374526725200952 to -0.3766739791942
-0.580536351589502 to -0.58268360558275
-0.0617236355822686 to -0.0638708895755166
-0.13414566317016 to -0.136292917163408
-0.866143957762063 to -0.868291211755311
-0.821186439184488 to -0.823333693177736
Changing layer 3's weights from 
-0.132060215858759 to -0.134207469852007
-0.395613298801721 to -0.397760552794969
-0.153003380684197 to -0.155150634677445
-1.06044261244034 to -1.06258986643359
-0.275679276375115 to -0.277826530368363
-0.405927882580103 to -0.408075136573351
-0.736112729696572 to -0.73825998368982
-0.277599857239068 to -0.279747111232316
-0.609295831112207 to -0.611443085105455
-0.136465952781976 to -0.138613206775224
Changing layer 4's weights from 
-0.23573975363189 to -0.237887007625138
-0.444572554020227 to -0.446719808013475
-0.645050154117883 to -0.647197408111131
-0.358115420727075 to -0.360262674720323
-0.597368226436914 to -0.599515480430162
-0.365903005985559 to -0.368050259978807
-0.370261536030115 to -0.372408790023363
-0.832159058241189 to -0.834306312234437
-0.238462970642388 to -0.240610224635636
-0.670764909176172 to -0.67291216316942
Changing layer 5's weights from 
-0.0829599360507135 to -0.0851071900439615
-0.525033460048975 to -0.527180714042223
-0.0998450974505545 to -0.101992351443802
-1.035357278822 to -1.03750453281525
-0.498088465122522 to -0.50023571911577
-0.896556408194841 to -0.898703662188089
-0.596591101078332 to -0.59873835507158
-0.524144874004663 to -0.526292127997911
-0.134542868522943 to -0.136690122516191
-0.141050980476679 to -0.143198234469927
Trying to learn from memory 91, 1, -0.2
sum 0.0406977860075751 distri 0.0294392923730961
Using diff 0.00108404713258518 and condRate 0.166666666666667
Changed category 1 weights from 
0.289733690828373 to 0.289697555923415
0.523049277872135 to 0.523013142967177
0.098124039978077 to 0.0980879050731191
0.102216852516224 to 0.102180717611266
Changing layer 0's weights from 
-0.330286664397751 to -0.330322799302709
-0.702213121087586 to -0.702249255992544
-0.936562386782205 to -0.936598521687163
-0.902003136904274 to -0.902039271809232
-0.15413556757692 to -0.154171702481878
-0.836777520806825 to -0.836813655711783
-0.682947797210252 to -0.68298393211521
-0.710402203233277 to -0.710438338138235
-0.947894064219033 to -0.947930199123991
-0.991644119353329 to -0.991680254258287
Changing layer 1's weights from 
-0.986547206969296 to -0.986583341874254
-0.385378522307909 to -0.385414657212867
-0.511081261069811 to -0.511117395974769
-0.520986599357164 to -0.521022734262122
-0.706039381654298 to -0.706075516559256
-0.844277215631044 to -0.844313350536002
-0.573558492095506 to -0.573594627000464
-0.959361521036661 to -0.959397655941619
-0.547698540122544 to -0.547734675027502
-0.910345402987038 to -0.910381537891996
Changing layer 2's weights from 
-0.10115717116121 to -0.101193306066168
-0.655210656554735 to -0.655246791459693
-0.370621365935838 to -0.370657500840796
-0.199421269328629 to -0.199457404233587
-0.3766739791942 to -0.376710114099158
-0.58268360558275 to -0.582719740487708
-0.0638708895755166 to -0.0639070244804745
-0.136292917163408 to -0.136329052068366
-0.868291211755311 to -0.868327346660269
-0.823333693177736 to -0.823369828082694
Changing layer 3's weights from 
-0.134207469852007 to -0.134243604756965
-0.397760552794969 to -0.397796687699927
-0.155150634677445 to -0.155186769582403
-1.06258986643359 to -1.06262600133855
-0.277826530368363 to -0.277862665273321
-0.408075136573351 to -0.408111271478309
-0.73825998368982 to -0.738296118594778
-0.279747111232316 to -0.279783246137274
-0.611443085105455 to -0.611479220010413
-0.138613206775224 to -0.138649341680182
Changing layer 4's weights from 
-0.237887007625138 to -0.237923142530096
-0.446719808013475 to -0.446755942918433
-0.647197408111131 to -0.647233543016089
-0.360262674720323 to -0.360298809625281
-0.599515480430162 to -0.59955161533512
-0.368050259978807 to -0.368086394883765
-0.372408790023363 to -0.372444924928321
-0.834306312234437 to -0.834342447139395
-0.240610224635636 to -0.240646359540594
-0.67291216316942 to -0.672948298074378
Changing layer 5's weights from 
-0.0851071900439615 to -0.0851433249489195
-0.527180714042223 to -0.527216848947181
-0.101992351443802 to -0.10202848634876
-1.03750453281525 to -1.03754066772021
-0.50023571911577 to -0.500271854020728
-0.898703662188089 to -0.898739797093047
-0.59873835507158 to -0.598774489976538
-0.526292127997911 to -0.526328262902869
-0.136690122516191 to -0.136726257421149
-0.143198234469927 to -0.143234369374885
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.111152579618311 to 0.109005325625063
-0.288882384466315 to -0.291029638459563
-0.571403080987122 to -0.57355033498037
-0.41989771501937 to -0.422044969012618
Changing layer 0's weights from 
-0.330322799302709 to -0.332470053295957
-0.702249255992544 to -0.704396509985792
-0.936598521687163 to -0.938745775680411
-0.902039271809232 to -0.90418652580248
-0.154171702481878 to -0.156318956475126
-0.836813655711783 to -0.838960909705031
-0.68298393211521 to -0.685131186108458
-0.710438338138235 to -0.712585592131483
-0.947930199123991 to -0.950077453117239
-0.991680254258287 to -0.993827508251535
Changing layer 1's weights from 
-0.986583341874254 to -0.988730595867502
-0.385414657212867 to -0.387561911206115
-0.511117395974769 to -0.513264649968017
-0.521022734262122 to -0.52316998825537
-0.706075516559256 to -0.708222770552504
-0.844313350536002 to -0.84646060452925
-0.573594627000464 to -0.575741880993712
-0.959397655941619 to -0.961544909934867
-0.547734675027502 to -0.54988192902075
-0.910381537891996 to -0.912528791885244
Changing layer 2's weights from 
-0.101193306066168 to -0.103340560059416
-0.655246791459693 to -0.657394045452941
-0.370657500840796 to -0.372804754834044
-0.199457404233587 to -0.201604658226835
-0.376710114099158 to -0.378857368092406
-0.582719740487708 to -0.584866994480956
-0.0639070244804745 to -0.0660542784737225
-0.136329052068366 to -0.138476306061614
-0.868327346660269 to -0.870474600653517
-0.823369828082694 to -0.825517082075942
Changing layer 3's weights from 
-0.134243604756965 to -0.136390858750213
-0.397796687699927 to -0.399943941693175
-0.155186769582403 to -0.157334023575651
-1.06262600133855 to -1.06477325533179
-0.277862665273321 to -0.280009919266569
-0.408111271478309 to -0.410258525471557
-0.738296118594778 to -0.740443372588026
-0.279783246137274 to -0.281930500130522
-0.611479220010413 to -0.613626474003661
-0.138649341680182 to -0.14079659567343
Changing layer 4's weights from 
-0.237923142530096 to -0.240070396523344
-0.446755942918433 to -0.448903196911681
-0.647233543016089 to -0.649380797009337
-0.360298809625281 to -0.362446063618529
-0.59955161533512 to -0.601698869328368
-0.368086394883765 to -0.370233648877013
-0.372444924928321 to -0.374592178921569
-0.834342447139395 to -0.836489701132643
-0.240646359540594 to -0.242793613533842
-0.672948298074378 to -0.675095552067626
Changing layer 5's weights from 
-0.0851433249489195 to -0.0872905789421674
-0.527216848947181 to -0.529364102940429
-0.10202848634876 to -0.104175740342008
-1.03754066772021 to -1.03968792171346
-0.500271854020728 to -0.502419108013976
-0.898739797093047 to -0.900887051086295
-0.598774489976538 to -0.600921743969786
-0.526328262902869 to -0.528475516896117
-0.136726257421149 to -0.138873511414397
-0.143234369374885 to -0.145381623368133
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.109005325625063 to 0.106858071631815
-0.291029638459563 to -0.293176892452811
-0.57355033498037 to -0.575697588973618
-0.422044969012618 to -0.424192223005866
Changing layer 0's weights from 
-0.332470053295957 to -0.334617307289205
-0.704396509985792 to -0.70654376397904
-0.938745775680411 to -0.940893029673659
-0.90418652580248 to -0.906333779795728
-0.156318956475126 to -0.158466210468374
-0.838960909705031 to -0.841108163698279
-0.685131186108458 to -0.687278440101706
-0.712585592131483 to -0.714732846124731
-0.950077453117239 to -0.952224707110487
-0.993827508251535 to -0.995974762244783
Changing layer 1's weights from 
-0.988730595867502 to -0.99087784986075
-0.387561911206115 to -0.389709165199363
-0.513264649968017 to -0.515411903961265
-0.52316998825537 to -0.525317242248618
-0.708222770552504 to -0.710370024545752
-0.84646060452925 to -0.848607858522498
-0.575741880993712 to -0.57788913498696
-0.961544909934867 to -0.963692163928115
-0.54988192902075 to -0.552029183013998
-0.912528791885244 to -0.914676045878492
Changing layer 2's weights from 
-0.103340560059416 to -0.105487814052664
-0.657394045452941 to -0.659541299446189
-0.372804754834044 to -0.374952008827292
-0.201604658226835 to -0.203751912220083
-0.378857368092406 to -0.381004622085654
-0.584866994480956 to -0.587014248474204
-0.0660542784737225 to -0.0682015324669705
-0.138476306061614 to -0.140623560054862
-0.870474600653517 to -0.872621854646765
-0.825517082075942 to -0.82766433606919
Changing layer 3's weights from 
-0.136390858750213 to -0.138538112743461
-0.399943941693175 to -0.402091195686423
-0.157334023575651 to -0.159481277568899
-1.06477325533179 to -1.06692050932504
-0.280009919266569 to -0.282157173259817
-0.410258525471557 to -0.412405779464805
-0.740443372588026 to -0.742590626581274
-0.281930500130522 to -0.28407775412377
-0.613626474003661 to -0.615773727996909
-0.14079659567343 to -0.142943849666678
Changing layer 4's weights from 
-0.240070396523344 to -0.242217650516592
-0.448903196911681 to -0.451050450904929
-0.649380797009337 to -0.651528051002585
-0.362446063618529 to -0.364593317611777
-0.601698869328368 to -0.603846123321616
-0.370233648877013 to -0.372380902870261
-0.374592178921569 to -0.376739432914817
-0.836489701132643 to -0.838636955125891
-0.242793613533842 to -0.24494086752709
-0.675095552067626 to -0.677242806060874
Changing layer 5's weights from 
-0.0872905789421674 to -0.0894378329354154
-0.529364102940429 to -0.531511356933677
-0.104175740342008 to -0.106322994335256
-1.03968792171346 to -1.04183517570671
-0.502419108013976 to -0.504566362007224
-0.900887051086295 to -0.903034305079543
-0.600921743969786 to -0.603068997963034
-0.528475516896117 to -0.530622770889365
-0.138873511414397 to -0.141020765407645
-0.145381623368133 to -0.147528877361381
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.106858071631815 to 0.104710817638567
-0.293176892452811 to -0.295324146446059
-0.575697588973618 to -0.577844842966866
-0.424192223005866 to -0.426339476999114
Changing layer 0's weights from 
-0.334617307289205 to -0.336764561282453
-0.70654376397904 to -0.708691017972288
-0.940893029673659 to -0.943040283666907
-0.906333779795728 to -0.908481033788976
-0.158466210468374 to -0.160613464461622
-0.841108163698279 to -0.843255417691527
-0.687278440101706 to -0.689425694094954
-0.714732846124731 to -0.716880100117979
-0.952224707110487 to -0.954371961103735
-0.995974762244783 to -0.998122016238031
Changing layer 1's weights from 
-0.99087784986075 to -0.993025103853998
-0.389709165199363 to -0.391856419192611
-0.515411903961265 to -0.517559157954513
-0.525317242248618 to -0.527464496241866
-0.710370024545752 to -0.712517278539
-0.848607858522498 to -0.850755112515746
-0.57788913498696 to -0.580036388980208
-0.963692163928115 to -0.965839417921363
-0.552029183013998 to -0.554176437007246
-0.914676045878492 to -0.91682329987174
Changing layer 2's weights from 
-0.105487814052664 to -0.107635068045912
-0.659541299446189 to -0.661688553439437
-0.374952008827292 to -0.37709926282054
-0.203751912220083 to -0.205899166213331
-0.381004622085654 to -0.383151876078902
-0.587014248474204 to -0.589161502467452
-0.0682015324669705 to -0.0703487864602185
-0.140623560054862 to -0.14277081404811
-0.872621854646765 to -0.874769108640013
-0.82766433606919 to -0.829811590062438
Changing layer 3's weights from 
-0.138538112743461 to -0.140685366736709
-0.402091195686423 to -0.404238449679671
-0.159481277568899 to -0.161628531562147
-1.06692050932504 to -1.06906776331829
-0.282157173259817 to -0.284304427253065
-0.412405779464805 to -0.414553033458053
-0.742590626581274 to -0.744737880574522
-0.28407775412377 to -0.286225008117018
-0.615773727996909 to -0.617920981990157
-0.142943849666678 to -0.145091103659926
Changing layer 4's weights from 
-0.242217650516592 to -0.24436490450984
-0.451050450904929 to -0.453197704898177
-0.651528051002585 to -0.653675304995833
-0.364593317611777 to -0.366740571605025
-0.603846123321616 to -0.605993377314864
-0.372380902870261 to -0.374528156863509
-0.376739432914817 to -0.378886686908065
-0.838636955125891 to -0.840784209119139
-0.24494086752709 to -0.247088121520338
-0.677242806060874 to -0.679390060054122
Changing layer 5's weights from 
-0.0894378329354154 to -0.0915850869286634
-0.531511356933677 to -0.533658610926925
-0.106322994335256 to -0.108470248328504
-1.04183517570671 to -1.04398242969995
-0.504566362007224 to -0.506713616000472
-0.903034305079543 to -0.905181559072791
-0.603068997963034 to -0.605216251956282
-0.530622770889365 to -0.532770024882613
-0.141020765407645 to -0.143168019400893
-0.147528877361381 to -0.149676131354629
10/5/2016 1:45:23 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.104710817638567 to 0.102563563645319
-0.295324146446059 to -0.297471400439307
-0.577844842966866 to -0.579992096960114
-0.426339476999114 to -0.428486730992362
Changing layer 0's weights from 
-0.336764561282453 to -0.338911815275701
-0.708691017972288 to -0.710838271965536
-0.943040283666907 to -0.945187537660155
-0.908481033788976 to -0.910628287782224
-0.160613464461622 to -0.16276071845487
-0.843255417691527 to -0.845402671684775
-0.689425694094954 to -0.691572948088202
-0.716880100117979 to -0.719027354111227
-0.954371961103735 to -0.956519215096983
-0.998122016238031 to -1.00026927023128
Changing layer 1's weights from 
-0.993025103853998 to -0.995172357847246
-0.391856419192611 to -0.394003673185859
-0.517559157954513 to -0.519706411947761
-0.527464496241866 to -0.529611750235114
-0.712517278539 to -0.714664532532248
-0.850755112515746 to -0.852902366508994
-0.580036388980208 to -0.582183642973456
-0.965839417921363 to -0.967986671914611
-0.554176437007246 to -0.556323691000494
-0.91682329987174 to -0.918970553864988
Changing layer 2's weights from 
-0.107635068045912 to -0.10978232203916
-0.661688553439437 to -0.663835807432685
-0.37709926282054 to -0.379246516813788
-0.205899166213331 to -0.208046420206579
-0.383151876078902 to -0.38529913007215
-0.589161502467452 to -0.5913087564607
-0.0703487864602185 to -0.0724960404534665
-0.14277081404811 to -0.144918068041358
-0.874769108640013 to -0.876916362633261
-0.829811590062438 to -0.831958844055686
Changing layer 3's weights from 
-0.140685366736709 to -0.142832620729957
-0.404238449679671 to -0.406385703672919
-0.161628531562147 to -0.163775785555395
-1.06906776331829 to -1.07121501731154
-0.284304427253065 to -0.286451681246313
-0.414553033458053 to -0.416700287451301
-0.744737880574522 to -0.74688513456777
-0.286225008117018 to -0.288372262110266
-0.617920981990157 to -0.620068235983405
-0.145091103659926 to -0.147238357653174
Changing layer 4's weights from 
-0.24436490450984 to -0.246512158503088
-0.453197704898177 to -0.455344958891425
-0.653675304995833 to -0.655822558989081
-0.366740571605025 to -0.368887825598273
-0.605993377314864 to -0.608140631308112
-0.374528156863509 to -0.376675410856757
-0.378886686908065 to -0.381033940901313
-0.840784209119139 to -0.842931463112387
-0.247088121520338 to -0.249235375513586
-0.679390060054122 to -0.68153731404737
Changing layer 5's weights from 
-0.0915850869286634 to -0.0937323409219114
-0.533658610926925 to -0.535805864920173
-0.108470248328504 to -0.110617502321752
-1.04398242969995 to -1.0461296836932
-0.506713616000472 to -0.50886086999372
-0.905181559072791 to -0.907328813066039
-0.605216251956282 to -0.60736350594953
-0.532770024882613 to -0.534917278875861
-0.143168019400893 to -0.145315273394141
-0.149676131354629 to -0.151823385347877
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.102563563645319 to 0.100416309652071
-0.297471400439307 to -0.299618654432555
-0.579992096960114 to -0.582139350953362
-0.428486730992362 to -0.43063398498561
Changing layer 0's weights from 
-0.338911815275701 to -0.341059069268949
-0.710838271965536 to -0.712985525958784
-0.945187537660155 to -0.947334791653403
-0.910628287782224 to -0.912775541775472
-0.16276071845487 to -0.164907972448118
-0.845402671684775 to -0.847549925678023
-0.691572948088202 to -0.69372020208145
-0.719027354111227 to -0.721174608104475
-0.956519215096983 to -0.958666469090231
-1.00026927023128 to -1.00241652422453
Changing layer 1's weights from 
-0.995172357847246 to -0.997319611840494
-0.394003673185859 to -0.396150927179107
-0.519706411947761 to -0.521853665941009
-0.529611750235114 to -0.531759004228362
-0.714664532532248 to -0.716811786525496
-0.852902366508994 to -0.855049620502242
-0.582183642973456 to -0.584330896966704
-0.967986671914611 to -0.970133925907859
-0.556323691000494 to -0.558470944993742
-0.918970553864988 to -0.921117807858236
Changing layer 2's weights from 
-0.10978232203916 to -0.111929576032408
-0.663835807432685 to -0.665983061425933
-0.379246516813788 to -0.381393770807036
-0.208046420206579 to -0.210193674199827
-0.38529913007215 to -0.387446384065398
-0.5913087564607 to -0.593456010453948
-0.0724960404534665 to -0.0746432944467145
-0.144918068041358 to -0.147065322034606
-0.876916362633261 to -0.879063616626509
-0.831958844055686 to -0.834106098048934
Changing layer 3's weights from 
-0.142832620729957 to -0.144979874723205
-0.406385703672919 to -0.408532957666167
-0.163775785555395 to -0.165923039548643
-1.07121501731154 to -1.07336227130479
-0.286451681246313 to -0.288598935239561
-0.416700287451301 to -0.418847541444549
-0.74688513456777 to -0.749032388561018
-0.288372262110266 to -0.290519516103514
-0.620068235983405 to -0.622215489976653
-0.147238357653174 to -0.149385611646422
Changing layer 4's weights from 
-0.246512158503088 to -0.248659412496336
-0.455344958891425 to -0.457492212884673
-0.655822558989081 to -0.657969812982329
-0.368887825598273 to -0.371035079591521
-0.608140631308112 to -0.61028788530136
-0.376675410856757 to -0.378822664850005
-0.381033940901313 to -0.383181194894561
-0.842931463112387 to -0.845078717105635
-0.249235375513586 to -0.251382629506834
-0.68153731404737 to -0.683684568040618
Changing layer 5's weights from 
-0.0937323409219114 to -0.0958795949151594
-0.535805864920173 to -0.537953118913421
-0.110617502321752 to -0.112764756315
-1.0461296836932 to -1.04827693768645
-0.50886086999372 to -0.511008123986968
-0.907328813066039 to -0.909476067059287
-0.60736350594953 to -0.609510759942778
-0.534917278875861 to -0.537064532869109
-0.145315273394141 to -0.147462527387389
-0.151823385347877 to -0.153970639341125
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.100416309652071 to 0.098269055658823
-0.299618654432555 to -0.301765908425803
-0.582139350953362 to -0.58428660494661
-0.43063398498561 to -0.432781238978858
Changing layer 0's weights from 
-0.341059069268949 to -0.343206323262197
-0.712985525958784 to -0.715132779952032
-0.947334791653403 to -0.949482045646651
-0.912775541775472 to -0.91492279576872
-0.164907972448118 to -0.167055226441366
-0.847549925678023 to -0.849697179671271
-0.69372020208145 to -0.695867456074698
-0.721174608104475 to -0.723321862097723
-0.958666469090231 to -0.960813723083479
-1.00241652422453 to -1.00456377821777
Changing layer 1's weights from 
-0.997319611840494 to -0.999466865833742
-0.396150927179107 to -0.398298181172355
-0.521853665941009 to -0.524000919934257
-0.531759004228362 to -0.53390625822161
-0.716811786525496 to -0.718959040518744
-0.855049620502242 to -0.85719687449549
-0.584330896966704 to -0.586478150959952
-0.970133925907859 to -0.972281179901107
-0.558470944993742 to -0.56061819898699
-0.921117807858236 to -0.923265061851484
Changing layer 2's weights from 
-0.111929576032408 to -0.114076830025656
-0.665983061425933 to -0.668130315419181
-0.381393770807036 to -0.383541024800284
-0.210193674199827 to -0.212340928193075
-0.387446384065398 to -0.389593638058646
-0.593456010453948 to -0.595603264447196
-0.0746432944467145 to -0.0767905484399624
-0.147065322034606 to -0.149212576027854
-0.879063616626509 to -0.881210870619757
-0.834106098048934 to -0.836253352042182
Changing layer 3's weights from 
-0.144979874723205 to -0.147127128716453
-0.408532957666167 to -0.410680211659415
-0.165923039548643 to -0.168070293541891
-1.07336227130479 to -1.07550952529803
-0.288598935239561 to -0.290746189232809
-0.418847541444549 to -0.420994795437797
-0.749032388561018 to -0.751179642554266
-0.290519516103514 to -0.292666770096762
-0.622215489976653 to -0.624362743969901
-0.149385611646422 to -0.15153286563967
Changing layer 4's weights from 
-0.248659412496336 to -0.250806666489584
-0.457492212884673 to -0.459639466877921
-0.657969812982329 to -0.660117066975577
-0.371035079591521 to -0.373182333584769
-0.61028788530136 to -0.612435139294608
-0.378822664850005 to -0.380969918843253
-0.383181194894561 to -0.385328448887809
-0.845078717105635 to -0.847225971098883
-0.251382629506834 to -0.253529883500082
-0.683684568040618 to -0.685831822033866
Changing layer 5's weights from 
-0.0958795949151594 to -0.0980268489084074
-0.537953118913421 to -0.540100372906669
-0.112764756315 to -0.114912010308248
-1.04827693768645 to -1.0504241916797
-0.511008123986968 to -0.513155377980216
-0.909476067059287 to -0.911623321052535
-0.609510759942778 to -0.611658013936026
-0.537064532869109 to -0.539211786862357
-0.147462527387389 to -0.149609781380637
-0.153970639341125 to -0.156117893334373
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.098269055658823 to 0.0961218016655751
-0.301765908425803 to -0.303913162419051
-0.58428660494661 to -0.586433858939858
-0.432781238978858 to -0.434928492972106
Changing layer 0's weights from 
-0.343206323262197 to -0.345353577255445
-0.715132779952032 to -0.71728003394528
-0.949482045646651 to -0.951629299639899
-0.91492279576872 to -0.917070049761968
-0.167055226441366 to -0.169202480434614
-0.849697179671271 to -0.851844433664519
-0.695867456074698 to -0.698014710067946
-0.723321862097723 to -0.725469116090971
-0.960813723083479 to -0.962960977076727
-1.00456377821777 to -1.00671103221102
Changing layer 1's weights from 
-0.999466865833742 to -1.00161411982699
-0.398298181172355 to -0.400445435165603
-0.524000919934257 to -0.526148173927505
-0.53390625822161 to -0.536053512214858
-0.718959040518744 to -0.721106294511992
-0.85719687449549 to -0.859344128488738
-0.586478150959952 to -0.5886254049532
-0.972281179901107 to -0.974428433894355
-0.56061819898699 to -0.562765452980238
-0.923265061851484 to -0.925412315844732
Changing layer 2's weights from 
-0.114076830025656 to -0.116224084018904
-0.668130315419181 to -0.670277569412429
-0.383541024800284 to -0.385688278793532
-0.212340928193075 to -0.214488182186323
-0.389593638058646 to -0.391740892051894
-0.595603264447196 to -0.597750518440444
-0.0767905484399624 to -0.0789378024332104
-0.149212576027854 to -0.151359830021102
-0.881210870619757 to -0.883358124613005
-0.836253352042182 to -0.83840060603543
Changing layer 3's weights from 
-0.147127128716453 to -0.149274382709701
-0.410680211659415 to -0.412827465652663
-0.168070293541891 to -0.170217547535139
-1.07550952529803 to -1.07765677929128
-0.290746189232809 to -0.292893443226057
-0.420994795437797 to -0.423142049431045
-0.751179642554266 to -0.753326896547514
-0.292666770096762 to -0.29481402409001
-0.624362743969901 to -0.626509997963149
-0.15153286563967 to -0.153680119632918
Changing layer 4's weights from 
-0.250806666489584 to -0.252953920482832
-0.459639466877921 to -0.461786720871169
-0.660117066975577 to -0.662264320968825
-0.373182333584769 to -0.375329587578017
-0.612435139294608 to -0.614582393287856
-0.380969918843253 to -0.383117172836501
-0.385328448887809 to -0.387475702881057
-0.847225971098883 to -0.849373225092131
-0.253529883500082 to -0.25567713749333
-0.685831822033866 to -0.687979076027114
Changing layer 5's weights from 
-0.0980268489084074 to -0.100174102901655
-0.540100372906669 to -0.542247626899917
-0.114912010308248 to -0.117059264301496
-1.0504241916797 to -1.05257144567295
-0.513155377980216 to -0.515302631973464
-0.911623321052535 to -0.913770575045783
-0.611658013936026 to -0.613805267929274
-0.539211786862357 to -0.541359040855605
-0.149609781380637 to -0.151757035373885
-0.156117893334373 to -0.158265147327621
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.0961218016655751 to 0.0939745476723271
-0.303913162419051 to -0.306060416412299
-0.586433858939858 to -0.588581112933106
-0.434928492972106 to -0.437075746965354
Changing layer 0's weights from 
-0.345353577255445 to -0.347500831248693
-0.71728003394528 to -0.719427287938528
-0.951629299639899 to -0.953776553633147
-0.917070049761968 to -0.919217303755216
-0.169202480434614 to -0.171349734427862
-0.851844433664519 to -0.853991687657767
-0.698014710067946 to -0.700161964061194
-0.725469116090971 to -0.727616370084219
-0.962960977076727 to -0.965108231069975
-1.00671103221102 to -1.00885828620427
Changing layer 1's weights from 
-1.00161411982699 to -1.00376137382024
-0.400445435165603 to -0.402592689158851
-0.526148173927505 to -0.528295427920753
-0.536053512214858 to -0.538200766208106
-0.721106294511992 to -0.72325354850524
-0.859344128488738 to -0.861491382481986
-0.5886254049532 to -0.590772658946448
-0.974428433894355 to -0.976575687887603
-0.562765452980238 to -0.564912706973486
-0.925412315844732 to -0.92755956983798
Changing layer 2's weights from 
-0.116224084018904 to -0.118371338012152
-0.670277569412429 to -0.672424823405677
-0.385688278793532 to -0.38783553278678
-0.214488182186323 to -0.216635436179571
-0.391740892051894 to -0.393888146045142
-0.597750518440444 to -0.599897772433692
-0.0789378024332104 to -0.0810850564264584
-0.151359830021102 to -0.15350708401435
-0.883358124613005 to -0.885505378606253
-0.83840060603543 to -0.840547860028678
Changing layer 3's weights from 
-0.149274382709701 to -0.151421636702949
-0.412827465652663 to -0.414974719645911
-0.170217547535139 to -0.172364801528387
-1.07765677929128 to -1.07980403328453
-0.292893443226057 to -0.295040697219305
-0.423142049431045 to -0.425289303424293
-0.753326896547514 to -0.755474150540762
-0.29481402409001 to -0.296961278083258
-0.626509997963149 to -0.628657251956397
-0.153680119632918 to -0.155827373626166
Changing layer 4's weights from 
-0.252953920482832 to -0.25510117447608
-0.461786720871169 to -0.463933974864417
-0.662264320968825 to -0.664411574962073
-0.375329587578017 to -0.377476841571265
-0.614582393287856 to -0.616729647281104
-0.383117172836501 to -0.385264426829749
-0.387475702881057 to -0.389622956874305
-0.849373225092131 to -0.851520479085379
-0.25567713749333 to -0.257824391486578
-0.687979076027114 to -0.690126330020362
Changing layer 5's weights from 
-0.100174102901655 to -0.102321356894903
-0.542247626899917 to -0.544394880893165
-0.117059264301496 to -0.119206518294744
-1.05257144567295 to -1.05471869966619
-0.515302631973464 to -0.517449885966712
-0.913770575045783 to -0.915917829039031
-0.613805267929274 to -0.615952521922522
-0.541359040855605 to -0.543506294848853
-0.151757035373885 to -0.153904289367133
-0.158265147327621 to -0.160412401320869
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.0939745476723271 to 0.0918272936790791
-0.306060416412299 to -0.308207670405547
-0.588581112933106 to -0.590728366926354
-0.437075746965354 to -0.439223000958602
Changing layer 0's weights from 
-0.347500831248693 to -0.349648085241941
-0.719427287938528 to -0.721574541931776
-0.953776553633147 to -0.955923807626395
-0.919217303755216 to -0.921364557748464
-0.171349734427862 to -0.17349698842111
-0.853991687657767 to -0.856138941651015
-0.700161964061194 to -0.702309218054442
-0.727616370084219 to -0.729763624077467
-0.965108231069975 to -0.967255485063223
-1.00885828620427 to -1.01100554019752
Changing layer 1's weights from 
-1.00376137382024 to -1.00590862781349
-0.402592689158851 to -0.404739943152099
-0.528295427920753 to -0.530442681914001
-0.538200766208106 to -0.540348020201354
-0.72325354850524 to -0.725400802498488
-0.861491382481986 to -0.863638636475234
-0.590772658946448 to -0.592919912939696
-0.976575687887603 to -0.978722941880851
-0.564912706973486 to -0.567059960966734
-0.92755956983798 to -0.929706823831228
Changing layer 2's weights from 
-0.118371338012152 to -0.1205185920054
-0.672424823405677 to -0.674572077398925
-0.38783553278678 to -0.389982786780028
-0.216635436179571 to -0.218782690172819
-0.393888146045142 to -0.39603540003839
-0.599897772433692 to -0.60204502642694
-0.0810850564264584 to -0.0832323104197064
-0.15350708401435 to -0.155654338007598
-0.885505378606253 to -0.887652632599501
-0.840547860028678 to -0.842695114021926
Changing layer 3's weights from 
-0.151421636702949 to -0.153568890696197
-0.414974719645911 to -0.417121973639159
-0.172364801528387 to -0.174512055521635
-1.07980403328453 to -1.08195128727778
-0.295040697219305 to -0.297187951212553
-0.425289303424293 to -0.427436557417541
-0.755474150540762 to -0.75762140453401
-0.296961278083258 to -0.299108532076506
-0.628657251956397 to -0.630804505949645
-0.155827373626166 to -0.157974627619414
Changing layer 4's weights from 
-0.25510117447608 to -0.257248428469328
-0.463933974864417 to -0.466081228857665
-0.664411574962073 to -0.666558828955321
-0.377476841571265 to -0.379624095564513
-0.616729647281104 to -0.618876901274352
-0.385264426829749 to -0.387411680822997
-0.389622956874305 to -0.391770210867553
-0.851520479085379 to -0.853667733078627
-0.257824391486578 to -0.259971645479826
-0.690126330020362 to -0.69227358401361
Changing layer 5's weights from 
-0.102321356894903 to -0.104468610888151
-0.544394880893165 to -0.546542134886413
-0.119206518294744 to -0.121353772287992
-1.05471869966619 to -1.05686595365944
-0.517449885966712 to -0.51959713995996
-0.915917829039031 to -0.918065083032279
-0.615952521922522 to -0.61809977591577
-0.543506294848853 to -0.545653548842101
-0.153904289367133 to -0.156051543360381
-0.160412401320869 to -0.162559655314117
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.0918272936790791 to 0.0896800396858311
-0.308207670405547 to -0.310354924398795
-0.590728366926354 to -0.592875620919602
-0.439223000958602 to -0.44137025495185
Changing layer 0's weights from 
-0.349648085241941 to -0.351795339235189
-0.721574541931776 to -0.723721795925024
-0.955923807626395 to -0.958071061619643
-0.921364557748464 to -0.923511811741712
-0.17349698842111 to -0.175644242414358
-0.856138941651015 to -0.858286195644263
-0.702309218054442 to -0.70445647204769
-0.729763624077467 to -0.731910878070715
-0.967255485063223 to -0.969402739056471
-1.01100554019752 to -1.01315279419077
Changing layer 1's weights from 
-1.00590862781349 to -1.00805588180673
-0.404739943152099 to -0.406887197145347
-0.530442681914001 to -0.532589935907249
-0.540348020201354 to -0.542495274194602
-0.725400802498488 to -0.727548056491736
-0.863638636475234 to -0.865785890468482
-0.592919912939696 to -0.595067166932944
-0.978722941880851 to -0.980870195874099
-0.567059960966734 to -0.569207214959982
-0.929706823831228 to -0.931854077824476
Changing layer 2's weights from 
-0.1205185920054 to -0.122665845998648
-0.674572077398925 to -0.676719331392173
-0.389982786780028 to -0.392130040773276
-0.218782690172819 to -0.220929944166067
-0.39603540003839 to -0.398182654031638
-0.60204502642694 to -0.604192280420188
-0.0832323104197064 to -0.0853795644129544
-0.155654338007598 to -0.157801592000846
-0.887652632599501 to -0.889799886592749
-0.842695114021926 to -0.844842368015174
Changing layer 3's weights from 
-0.153568890696197 to -0.155716144689445
-0.417121973639159 to -0.419269227632407
-0.174512055521635 to -0.176659309514883
-1.08195128727778 to -1.08409854127102
-0.297187951212553 to -0.299335205205801
-0.427436557417541 to -0.429583811410789
-0.75762140453401 to -0.759768658527258
-0.299108532076506 to -0.301255786069754
-0.630804505949645 to -0.632951759942893
-0.157974627619414 to -0.160121881612662
Changing layer 4's weights from 
-0.257248428469328 to -0.259395682462576
-0.466081228857665 to -0.468228482850913
-0.666558828955321 to -0.668706082948569
-0.379624095564513 to -0.381771349557761
-0.618876901274352 to -0.6210241552676
-0.387411680822997 to -0.389558934816245
-0.391770210867553 to -0.393917464860801
-0.853667733078627 to -0.855814987071875
-0.259971645479826 to -0.262118899473074
-0.69227358401361 to -0.694420838006858
Changing layer 5's weights from 
-0.104468610888151 to -0.106615864881399
-0.546542134886413 to -0.548689388879661
-0.121353772287992 to -0.12350102628124
-1.05686595365944 to -1.05901320765269
-0.51959713995996 to -0.521744393953208
-0.918065083032279 to -0.920212337025527
-0.61809977591577 to -0.620247029909018
-0.545653548842101 to -0.547800802835349
-0.156051543360381 to -0.158198797353629
-0.162559655314117 to -0.164706909307365
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.0896800396858311 to 0.0875327856925831
-0.310354924398795 to -0.312502178392043
-0.592875620919602 to -0.59502287491285
-0.44137025495185 to -0.443517508945098
Changing layer 0's weights from 
-0.351795339235189 to -0.353942593228437
-0.723721795925024 to -0.725869049918272
-0.958071061619643 to -0.960218315612891
-0.923511811741712 to -0.92565906573496
-0.175644242414358 to -0.177791496407606
-0.858286195644263 to -0.860433449637511
-0.70445647204769 to -0.706603726040938
-0.731910878070715 to -0.734058132063963
-0.969402739056471 to -0.971549993049719
-1.01315279419077 to -1.01530004818401
Changing layer 1's weights from 
-1.00805588180673 to -1.01020313579998
-0.406887197145347 to -0.409034451138595
-0.532589935907249 to -0.534737189900497
-0.542495274194602 to -0.54464252818785
-0.727548056491736 to -0.729695310484984
-0.865785890468482 to -0.86793314446173
-0.595067166932944 to -0.597214420926192
-0.980870195874099 to -0.983017449867347
-0.569207214959982 to -0.57135446895323
-0.931854077824476 to -0.934001331817724
Changing layer 2's weights from 
-0.122665845998648 to -0.124813099991896
-0.676719331392173 to -0.678866585385421
-0.392130040773276 to -0.394277294766524
-0.220929944166067 to -0.223077198159315
-0.398182654031638 to -0.400329908024886
-0.604192280420188 to -0.606339534413436
-0.0853795644129544 to -0.0875268184062024
-0.157801592000846 to -0.159948845994094
-0.889799886592749 to -0.891947140585997
-0.844842368015174 to -0.846989622008422
Changing layer 3's weights from 
-0.155716144689445 to -0.157863398682693
-0.419269227632407 to -0.421416481625655
-0.176659309514883 to -0.178806563508131
-1.08409854127102 to -1.08624579526427
-0.299335205205801 to -0.301482459199049
-0.429583811410789 to -0.431731065404037
-0.759768658527258 to -0.761915912520506
-0.301255786069754 to -0.303403040063002
-0.632951759942893 to -0.635099013936141
-0.160121881612662 to -0.16226913560591
Changing layer 4's weights from 
-0.259395682462576 to -0.261542936455824
-0.468228482850913 to -0.470375736844161
-0.668706082948569 to -0.670853336941817
-0.381771349557761 to -0.383918603551009
-0.6210241552676 to -0.623171409260848
-0.389558934816245 to -0.391706188809493
-0.393917464860801 to -0.396064718854049
-0.855814987071875 to -0.857962241065123
-0.262118899473074 to -0.264266153466322
-0.694420838006858 to -0.696568092000106
Changing layer 5's weights from 
-0.106615864881399 to -0.108763118874647
-0.548689388879661 to -0.550836642872909
-0.12350102628124 to -0.125648280274488
-1.05901320765269 to -1.06116046164594
-0.521744393953208 to -0.523891647946456
-0.920212337025527 to -0.922359591018775
-0.620247029909018 to -0.622394283902266
-0.547800802835349 to -0.549948056828597
-0.158198797353629 to -0.160346051346877
-0.164706909307365 to -0.166854163300613
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.0875327856925831 to 0.0853855316993351
-0.312502178392043 to -0.314649432385291
-0.59502287491285 to -0.597170128906098
-0.443517508945098 to -0.445664762938346
Changing layer 0's weights from 
-0.353942593228437 to -0.356089847221685
-0.725869049918272 to -0.72801630391152
-0.960218315612891 to -0.962365569606139
-0.92565906573496 to -0.927806319728208
-0.177791496407606 to -0.179938750400854
-0.860433449637511 to -0.862580703630759
-0.706603726040938 to -0.708750980034186
-0.734058132063963 to -0.736205386057211
-0.971549993049719 to -0.973697247042967
-1.01530004818401 to -1.01744730217726
Changing layer 1's weights from 
-1.01020313579998 to -1.01235038979323
-0.409034451138595 to -0.411181705131843
-0.534737189900497 to -0.536884443893745
-0.54464252818785 to -0.546789782181098
-0.729695310484984 to -0.731842564478232
-0.86793314446173 to -0.870080398454978
-0.597214420926192 to -0.59936167491944
-0.983017449867347 to -0.985164703860595
-0.57135446895323 to -0.573501722946478
-0.934001331817724 to -0.936148585810972
Changing layer 2's weights from 
-0.124813099991896 to -0.126960353985144
-0.678866585385421 to -0.681013839378669
-0.394277294766524 to -0.396424548759772
-0.223077198159315 to -0.225224452152563
-0.400329908024886 to -0.402477162018134
-0.606339534413436 to -0.608486788406684
-0.0875268184062024 to -0.0896740723994504
-0.159948845994094 to -0.162096099987342
-0.891947140585997 to -0.894094394579245
-0.846989622008422 to -0.84913687600167
Changing layer 3's weights from 
-0.157863398682693 to -0.160010652675941
-0.421416481625655 to -0.423563735618903
-0.178806563508131 to -0.180953817501379
-1.08624579526427 to -1.08839304925752
-0.301482459199049 to -0.303629713192297
-0.431731065404037 to -0.433878319397285
-0.761915912520506 to -0.764063166513754
-0.303403040063002 to -0.30555029405625
-0.635099013936141 to -0.637246267929389
-0.16226913560591 to -0.164416389599158
Changing layer 4's weights from 
-0.261542936455824 to -0.263690190449072
-0.470375736844161 to -0.472522990837409
-0.670853336941817 to -0.673000590935065
-0.383918603551009 to -0.386065857544257
-0.623171409260848 to -0.625318663254096
-0.391706188809493 to -0.393853442802741
-0.396064718854049 to -0.398211972847297
-0.857962241065123 to -0.860109495058371
-0.264266153466322 to -0.26641340745957
-0.696568092000106 to -0.698715345993354
Changing layer 5's weights from 
-0.108763118874647 to -0.110910372867895
-0.550836642872909 to -0.552983896866157
-0.125648280274488 to -0.127795534267736
-1.06116046164594 to -1.06330771563919
-0.523891647946456 to -0.526038901939704
-0.922359591018775 to -0.924506845012023
-0.622394283902266 to -0.624541537895514
-0.549948056828597 to -0.552095310821845
-0.160346051346877 to -0.162493305340125
-0.166854163300613 to -0.169001417293861
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.0853855316993351 to 0.0832382777060871
-0.314649432385291 to -0.316796686378539
-0.597170128906098 to -0.599317382899346
-0.445664762938346 to -0.447812016931594
Changing layer 0's weights from 
-0.356089847221685 to -0.358237101214933
-0.72801630391152 to -0.730163557904768
-0.962365569606139 to -0.964512823599387
-0.927806319728208 to -0.929953573721456
-0.179938750400854 to -0.182086004394102
-0.862580703630759 to -0.864727957624007
-0.708750980034186 to -0.710898234027434
-0.736205386057211 to -0.738352640050459
-0.973697247042967 to -0.975844501036215
-1.01744730217726 to -1.01959455617051
Changing layer 1's weights from 
-1.01235038979323 to -1.01449764378648
-0.411181705131843 to -0.413328959125091
-0.536884443893745 to -0.539031697886993
-0.546789782181098 to -0.548937036174346
-0.731842564478232 to -0.73398981847148
-0.870080398454978 to -0.872227652448226
-0.59936167491944 to -0.601508928912688
-0.985164703860595 to -0.987311957853843
-0.573501722946478 to -0.575648976939726
-0.936148585810972 to -0.93829583980422
Changing layer 2's weights from 
-0.126960353985144 to -0.129107607978392
-0.681013839378669 to -0.683161093371917
-0.396424548759772 to -0.39857180275302
-0.225224452152563 to -0.227371706145811
-0.402477162018134 to -0.404624416011382
-0.608486788406684 to -0.610634042399932
-0.0896740723994504 to -0.0918213263926983
-0.162096099987342 to -0.16424335398059
-0.894094394579245 to -0.896241648572493
-0.84913687600167 to -0.851284129994918
Changing layer 3's weights from 
-0.160010652675941 to -0.162157906669189
-0.423563735618903 to -0.425710989612151
-0.180953817501379 to -0.183101071494627
-1.08839304925752 to -1.09054030325077
-0.303629713192297 to -0.305776967185545
-0.433878319397285 to -0.436025573390533
-0.764063166513754 to -0.766210420507002
-0.30555029405625 to -0.307697548049498
-0.637246267929389 to -0.639393521922637
-0.164416389599158 to -0.166563643592406
Changing layer 4's weights from 
-0.263690190449072 to -0.26583744444232
-0.472522990837409 to -0.474670244830657
-0.673000590935065 to -0.675147844928313
-0.386065857544257 to -0.388213111537505
-0.625318663254096 to -0.627465917247344
-0.393853442802741 to -0.396000696795989
-0.398211972847297 to -0.400359226840545
-0.860109495058371 to -0.862256749051619
-0.26641340745957 to -0.268560661452818
-0.698715345993354 to -0.700862599986602
Changing layer 5's weights from 
-0.110910372867895 to -0.113057626861143
-0.552983896866157 to -0.555131150859405
-0.127795534267736 to -0.129942788260984
-1.06330771563919 to -1.06545496963243
-0.526038901939704 to -0.528186155932952
-0.924506845012023 to -0.926654099005271
-0.624541537895514 to -0.626688791888762
-0.552095310821845 to -0.554242564815093
-0.162493305340125 to -0.164640559333373
-0.169001417293861 to -0.171148671287109
10/5/2016 1:45:23 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.0832382777060871 to 0.0810910237128392
-0.316796686378539 to -0.318943940371787
-0.599317382899346 to -0.601464636892594
-0.447812016931594 to -0.449959270924842
Changing layer 0's weights from 
-0.358237101214933 to -0.360384355208181
-0.730163557904768 to -0.732310811898016
-0.964512823599387 to -0.966660077592635
-0.929953573721456 to -0.932100827714704
-0.182086004394102 to -0.18423325838735
-0.864727957624007 to -0.866875211617255
-0.710898234027434 to -0.713045488020682
-0.738352640050459 to -0.740499894043707
-0.975844501036215 to -0.977991755029463
-1.01959455617051 to -1.02174181016376
Changing layer 1's weights from 
-1.01449764378648 to -1.01664489777973
-0.413328959125091 to -0.415476213118339
-0.539031697886993 to -0.541178951880241
-0.548937036174346 to -0.551084290167594
-0.73398981847148 to -0.736137072464728
-0.872227652448226 to -0.874374906441474
-0.601508928912688 to -0.603656182905936
-0.987311957853843 to -0.989459211847091
-0.575648976939726 to -0.577796230932974
-0.93829583980422 to -0.940443093797468
Changing layer 2's weights from 
-0.129107607978392 to -0.13125486197164
-0.683161093371917 to -0.685308347365165
-0.39857180275302 to -0.400719056746268
-0.227371706145811 to -0.229518960139059
-0.404624416011382 to -0.40677167000463
-0.610634042399932 to -0.61278129639318
-0.0918213263926983 to -0.0939685803859463
-0.16424335398059 to -0.166390607973838
-0.896241648572493 to -0.898388902565741
-0.851284129994918 to -0.853431383988166
Changing layer 3's weights from 
-0.162157906669189 to -0.164305160662437
-0.425710989612151 to -0.427858243605399
-0.183101071494627 to -0.185248325487875
-1.09054030325077 to -1.09268755724402
-0.305776967185545 to -0.307924221178793
-0.436025573390533 to -0.438172827383781
-0.766210420507002 to -0.76835767450025
-0.307697548049498 to -0.309844802042746
-0.639393521922637 to -0.641540775915885
-0.166563643592406 to -0.168710897585654
Changing layer 4's weights from 
-0.26583744444232 to -0.267984698435568
-0.474670244830657 to -0.476817498823905
-0.675147844928313 to -0.677295098921561
-0.388213111537505 to -0.390360365530753
-0.627465917247344 to -0.629613171240592
-0.396000696795989 to -0.398147950789237
-0.400359226840545 to -0.402506480833793
-0.862256749051619 to -0.864404003044867
-0.268560661452818 to -0.270707915446066
-0.700862599986602 to -0.70300985397985
Changing layer 5's weights from 
-0.113057626861143 to -0.115204880854391
-0.555131150859405 to -0.557278404852653
-0.129942788260984 to -0.132090042254232
-1.06545496963243 to -1.06760222362568
-0.528186155932952 to -0.5303334099262
-0.926654099005271 to -0.928801352998519
-0.626688791888762 to -0.62883604588201
-0.554242564815093 to -0.556389818808341
-0.164640559333373 to -0.166787813326621
-0.171148671287109 to -0.173295925280357
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.0810910237128392 to 0.0789437697195912
-0.318943940371787 to -0.321091194365035
-0.601464636892594 to -0.603611890885842
-0.449959270924842 to -0.45210652491809
Changing layer 0's weights from 
-0.360384355208181 to -0.362531609201429
-0.732310811898016 to -0.734458065891264
-0.966660077592635 to -0.968807331585883
-0.932100827714704 to -0.934248081707952
-0.18423325838735 to -0.186380512380598
-0.866875211617255 to -0.869022465610503
-0.713045488020682 to -0.71519274201393
-0.740499894043707 to -0.742647148036955
-0.977991755029463 to -0.980139009022711
-1.02174181016376 to -1.02388906415701
Changing layer 1's weights from 
-1.01664489777973 to -1.01879215177297
-0.415476213118339 to -0.417623467111587
-0.541178951880241 to -0.543326205873489
-0.551084290167594 to -0.553231544160842
-0.736137072464728 to -0.738284326457976
-0.874374906441474 to -0.876522160434722
-0.603656182905936 to -0.605803436899184
-0.989459211847091 to -0.991606465840339
-0.577796230932974 to -0.579943484926222
-0.940443093797468 to -0.942590347790716
Changing layer 2's weights from 
-0.13125486197164 to -0.133402115964888
-0.685308347365165 to -0.687455601358413
-0.400719056746268 to -0.402866310739516
-0.229518960139059 to -0.231666214132307
-0.40677167000463 to -0.408918923997878
-0.61278129639318 to -0.614928550386428
-0.0939685803859463 to -0.0961158343791943
-0.166390607973838 to -0.168537861967086
-0.898388902565741 to -0.900536156558989
-0.853431383988166 to -0.855578637981414
Changing layer 3's weights from 
-0.164305160662437 to -0.166452414655685
-0.427858243605399 to -0.430005497598647
-0.185248325487875 to -0.187395579481123
-1.09268755724402 to -1.09483481123726
-0.307924221178793 to -0.310071475172041
-0.438172827383781 to -0.440320081377029
-0.76835767450025 to -0.770504928493498
-0.309844802042746 to -0.311992056035994
-0.641540775915885 to -0.643688029909133
-0.168710897585654 to -0.170858151578902
Changing layer 4's weights from 
-0.267984698435568 to -0.270131952428816
-0.476817498823905 to -0.478964752817153
-0.677295098921561 to -0.679442352914809
-0.390360365530753 to -0.392507619524001
-0.629613171240592 to -0.63176042523384
-0.398147950789237 to -0.400295204782485
-0.402506480833793 to -0.404653734827041
-0.864404003044867 to -0.866551257038115
-0.270707915446066 to -0.272855169439314
-0.70300985397985 to -0.705157107973098
Changing layer 5's weights from 
-0.115204880854391 to -0.117352134847639
-0.557278404852653 to -0.559425658845901
-0.132090042254232 to -0.13423729624748
-1.06760222362568 to -1.06974947761893
-0.5303334099262 to -0.532480663919448
-0.928801352998519 to -0.930948606991767
-0.62883604588201 to -0.630983299875258
-0.556389818808341 to -0.558537072801589
-0.166787813326621 to -0.168935067319869
-0.173295925280357 to -0.175443179273605
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.0789437697195912 to 0.0767965157263432
-0.321091194365035 to -0.323238448358283
-0.603611890885842 to -0.60575914487909
-0.45210652491809 to -0.454253778911338
Changing layer 0's weights from 
-0.362531609201429 to -0.364678863194677
-0.734458065891264 to -0.736605319884512
-0.968807331585883 to -0.970954585579131
-0.934248081707952 to -0.9363953357012
-0.186380512380598 to -0.188527766373846
-0.869022465610503 to -0.871169719603751
-0.71519274201393 to -0.717339996007178
-0.742647148036955 to -0.744794402030203
-0.980139009022711 to -0.982286263015959
-1.02388906415701 to -1.02603631815025
Changing layer 1's weights from 
-1.01879215177297 to -1.02093940576622
-0.417623467111587 to -0.419770721104835
-0.543326205873489 to -0.545473459866737
-0.553231544160842 to -0.55537879815409
-0.738284326457976 to -0.740431580451224
-0.876522160434722 to -0.87866941442797
-0.605803436899184 to -0.607950690892432
-0.991606465840339 to -0.993753719833587
-0.579943484926222 to -0.58209073891947
-0.942590347790716 to -0.944737601783964
Changing layer 2's weights from 
-0.133402115964888 to -0.135549369958136
-0.687455601358413 to -0.689602855351661
-0.402866310739516 to -0.405013564732764
-0.231666214132307 to -0.233813468125555
-0.408918923997878 to -0.411066177991126
-0.614928550386428 to -0.617075804379676
-0.0961158343791943 to -0.0982630883724423
-0.168537861967086 to -0.170685115960334
-0.900536156558989 to -0.902683410552237
-0.855578637981414 to -0.857725891974662
Changing layer 3's weights from 
-0.166452414655685 to -0.168599668648933
-0.430005497598647 to -0.432152751591895
-0.187395579481123 to -0.189542833474371
-1.09483481123726 to -1.09698206523051
-0.310071475172041 to -0.312218729165289
-0.440320081377029 to -0.442467335370277
-0.770504928493498 to -0.772652182486746
-0.311992056035994 to -0.314139310029242
-0.643688029909133 to -0.645835283902381
-0.170858151578902 to -0.17300540557215
Changing layer 4's weights from 
-0.270131952428816 to -0.272279206422064
-0.478964752817153 to -0.481112006810401
-0.679442352914809 to -0.681589606908057
-0.392507619524001 to -0.394654873517249
-0.63176042523384 to -0.633907679227088
-0.400295204782485 to -0.402442458775733
-0.404653734827041 to -0.406800988820289
-0.866551257038115 to -0.868698511031363
-0.272855169439314 to -0.275002423432562
-0.705157107973098 to -0.707304361966346
Changing layer 5's weights from 
-0.117352134847639 to -0.119499388840887
-0.559425658845901 to -0.561572912839149
-0.13423729624748 to -0.136384550240728
-1.06974947761893 to -1.07189673161218
-0.532480663919448 to -0.534627917912696
-0.930948606991767 to -0.933095860985015
-0.630983299875258 to -0.633130553868506
-0.558537072801589 to -0.560684326794837
-0.168935067319869 to -0.171082321313117
-0.175443179273605 to -0.177590433266853
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.0767965157263432 to 0.0746492617330952
-0.323238448358283 to -0.325385702351531
-0.60575914487909 to -0.607906398872338
-0.454253778911338 to -0.456401032904586
Changing layer 0's weights from 
-0.364678863194677 to -0.366826117187925
-0.736605319884512 to -0.73875257387776
-0.970954585579131 to -0.973101839572379
-0.9363953357012 to -0.938542589694448
-0.188527766373846 to -0.190675020367094
-0.871169719603751 to -0.873316973596999
-0.717339996007178 to -0.719487250000426
-0.744794402030203 to -0.746941656023451
-0.982286263015959 to -0.984433517009207
-1.02603631815025 to -1.0281835721435
Changing layer 1's weights from 
-1.02093940576622 to -1.02308665975947
-0.419770721104835 to -0.421917975098083
-0.545473459866737 to -0.547620713859985
-0.55537879815409 to -0.557526052147338
-0.740431580451224 to -0.742578834444472
-0.87866941442797 to -0.880816668421218
-0.607950690892432 to -0.61009794488568
-0.993753719833587 to -0.995900973826835
-0.58209073891947 to -0.584237992912718
-0.944737601783964 to -0.946884855777212
Changing layer 2's weights from 
-0.135549369958136 to -0.137696623951384
-0.689602855351661 to -0.691750109344909
-0.405013564732764 to -0.407160818726012
-0.233813468125555 to -0.235960722118803
-0.411066177991126 to -0.413213431984374
-0.617075804379676 to -0.619223058372924
-0.0982630883724423 to -0.10041034236569
-0.170685115960334 to -0.172832369953582
-0.902683410552237 to -0.904830664545485
-0.857725891974662 to -0.85987314596791
Changing layer 3's weights from 
-0.168599668648933 to -0.170746922642181
-0.432152751591895 to -0.434300005585143
-0.189542833474371 to -0.191690087467619
-1.09698206523051 to -1.09912931922376
-0.312218729165289 to -0.314365983158537
-0.442467335370277 to -0.444614589363525
-0.772652182486746 to -0.774799436479994
-0.314139310029242 to -0.31628656402249
-0.645835283902381 to -0.647982537895629
-0.17300540557215 to -0.175152659565398
Changing layer 4's weights from 
-0.272279206422064 to -0.274426460415312
-0.481112006810401 to -0.483259260803649
-0.681589606908057 to -0.683736860901305
-0.394654873517249 to -0.396802127510497
-0.633907679227088 to -0.636054933220336
-0.402442458775733 to -0.404589712768981
-0.406800988820289 to -0.408948242813537
-0.868698511031363 to -0.870845765024611
-0.275002423432562 to -0.27714967742581
-0.707304361966346 to -0.709451615959594
Changing layer 5's weights from 
-0.119499388840887 to -0.121646642834135
-0.561572912839149 to -0.563720166832397
-0.136384550240728 to -0.138531804233976
-1.07189673161218 to -1.07404398560542
-0.534627917912696 to -0.536775171905944
-0.933095860985015 to -0.935243114978263
-0.633130553868506 to -0.635277807861754
-0.560684326794837 to -0.562831580788085
-0.171082321313117 to -0.173229575306365
-0.177590433266853 to -0.179737687260101
Trying to learn from memory 91, 1, -0.2
sum 0.0406977860075751 distri 0.0294392923730961
Using diff 0.00108404713258518 and condRate 0.166666666666667
Changed category 1 weights from 
0.289697555923415 to 0.289661421018457
0.523013142967177 to 0.522977008062219
0.0980879050731191 to 0.0980517701681611
0.102180717611266 to 0.102144582706308
Changing layer 0's weights from 
-0.366826117187925 to -0.366862252092883
-0.73875257387776 to -0.738788708782718
-0.973101839572379 to -0.973137974477337
-0.938542589694448 to -0.938578724599406
-0.190675020367094 to -0.190711155272052
-0.873316973596999 to -0.873353108501957
-0.719487250000426 to -0.719523384905384
-0.746941656023451 to -0.746977790928409
-0.984433517009207 to -0.984469651914165
-1.0281835721435 to -1.02821970704846
Changing layer 1's weights from 
-1.02308665975947 to -1.02312279466443
-0.421917975098083 to -0.421954110003041
-0.547620713859985 to -0.547656848764943
-0.557526052147338 to -0.557562187052296
-0.742578834444472 to -0.74261496934943
-0.880816668421218 to -0.880852803326176
-0.61009794488568 to -0.610134079790638
-0.995900973826835 to -0.995937108731793
-0.584237992912718 to -0.584274127817676
-0.946884855777212 to -0.94692099068217
Changing layer 2's weights from 
-0.137696623951384 to -0.137732758856342
-0.691750109344909 to -0.691786244249867
-0.407160818726012 to -0.40719695363097
-0.235960722118803 to -0.235996857023761
-0.413213431984374 to -0.413249566889332
-0.619223058372924 to -0.619259193277882
-0.10041034236569 to -0.100446477270648
-0.172832369953582 to -0.172868504858539
-0.904830664545485 to -0.904866799450443
-0.85987314596791 to -0.859909280872868
Changing layer 3's weights from 
-0.170746922642181 to -0.170783057547138
-0.434300005585143 to -0.434336140490101
-0.191690087467619 to -0.191726222372577
-1.09912931922376 to -1.09916545412872
-0.314365983158537 to -0.314402118063495
-0.444614589363525 to -0.444650724268483
-0.774799436479994 to -0.774835571384952
-0.31628656402249 to -0.316322698927448
-0.647982537895629 to -0.648018672800587
-0.175152659565398 to -0.175188794470356
Changing layer 4's weights from 
-0.274426460415312 to -0.27446259532027
-0.483259260803649 to -0.483295395708607
-0.683736860901305 to -0.683772995806263
-0.396802127510497 to -0.396838262415455
-0.636054933220336 to -0.636091068125294
-0.404589712768981 to -0.404625847673939
-0.408948242813537 to -0.408984377718495
-0.870845765024611 to -0.870881899929569
-0.27714967742581 to -0.277185812330768
-0.709451615959594 to -0.709487750864552
Changing layer 5's weights from 
-0.121646642834135 to -0.121682777739093
-0.563720166832397 to -0.563756301737354
-0.138531804233976 to -0.138567939138934
-1.07404398560542 to -1.07408012051038
-0.536775171905944 to -0.536811306810902
-0.935243114978263 to -0.935279249883221
-0.635277807861754 to -0.635313942766712
-0.562831580788085 to -0.562867715693043
-0.173229575306365 to -0.173265710211322
-0.179737687260101 to -0.179773822165059
Trying to learn from memory 91, 0, -0.2
sum 0.0406977860075751 distri -0.0338942793318611
Using diff 0.0644176188375424 and condRate 0.166666666666667
Changed category 0 weights from 
0.0746492617330952 to 0.0725020077398472
-0.325385702351531 to -0.327532956344779
-0.607906398872338 to -0.610053652865586
-0.456401032904586 to -0.458548286897834
Changing layer 0's weights from 
-0.366862252092883 to -0.369009506086131
-0.738788708782718 to -0.740935962775966
-0.973137974477337 to -0.975285228470585
-0.938578724599406 to -0.940725978592654
-0.190711155272052 to -0.1928584092653
-0.873353108501957 to -0.875500362495205
-0.719523384905384 to -0.721670638898632
-0.746977790928409 to -0.749125044921657
-0.984469651914165 to -0.986616905907413
-1.02821970704846 to -1.03036696104171
Changing layer 1's weights from 
-1.02312279466443 to -1.02527004865767
-0.421954110003041 to -0.424101363996289
-0.547656848764943 to -0.549804102758191
-0.557562187052296 to -0.559709441045544
-0.74261496934943 to -0.744762223342678
-0.880852803326176 to -0.883000057319424
-0.610134079790638 to -0.612281333783886
-0.995937108731793 to -0.998084362725041
-0.584274127817676 to -0.586421381810924
-0.94692099068217 to -0.949068244675418
Changing layer 2's weights from 
-0.137732758856342 to -0.13988001284959
-0.691786244249867 to -0.693933498243115
-0.40719695363097 to -0.409344207624218
-0.235996857023761 to -0.238144111017009
-0.413249566889332 to -0.41539682088258
-0.619259193277882 to -0.62140644727113
-0.100446477270648 to -0.102593731263896
-0.172868504858539 to -0.175015758851787
-0.904866799450443 to -0.907014053443691
-0.859909280872868 to -0.862056534866116
Changing layer 3's weights from 
-0.170783057547138 to -0.172930311540386
-0.434336140490101 to -0.436483394483349
-0.191726222372577 to -0.193873476365825
-1.09916545412872 to -1.10131270812197
-0.314402118063495 to -0.316549372056743
-0.444650724268483 to -0.446797978261731
-0.774835571384952 to -0.7769828253782
-0.316322698927448 to -0.318469952920696
-0.648018672800587 to -0.650165926793835
-0.175188794470356 to -0.177336048463604
Changing layer 4's weights from 
-0.27446259532027 to -0.276609849313518
-0.483295395708607 to -0.485442649701855
-0.683772995806263 to -0.685920249799511
-0.396838262415455 to -0.398985516408703
-0.636091068125294 to -0.638238322118542
-0.404625847673939 to -0.406773101667187
-0.408984377718495 to -0.411131631711743
-0.870881899929569 to -0.873029153922817
-0.277185812330768 to -0.279333066324016
-0.709487750864552 to -0.7116350048578
Changing layer 5's weights from 
-0.121682777739093 to -0.123830031732341
-0.563756301737354 to -0.565903555730602
-0.138567939138934 to -0.140715193132182
-1.07408012051038 to -1.07622737450363
-0.536811306810902 to -0.53895856080415
-0.935279249883221 to -0.937426503876469
-0.635313942766712 to -0.63746119675996
-0.562867715693043 to -0.565014969686291
-0.173265710211322 to -0.17541296420457
-0.179773822165059 to -0.181921076158307
10/5/2016 1:45:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:45:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting AI
Reading weights from 10/5/2016 1:46:14 PMStarting AI
Weights.txt
Layer 0's weights: -0.369009506086131 -0.740935962775966 -0.975285228470585 -0.940725978592654 -0.1928584092653 -0.875500362495205 -0.721670638898632 -0.749125044921657 -0.986616905907413 -1.03036696104171 14 11 
Layer 1's weights: -1.02527004865767 -0.424101363996289 -0.549804102758191 -0.559709441045544 -0.744762223342678 -0.883000057319424 -0.612281333783886 -0.998084362725041 -0.586421381810924 -0.949068244675418 12 9 
Layer 2's weights: -0.13988001284959 -0.693933498243115 -0.409344207624218 -0.238144111017009 -0.41539682088258 -0.62140644727113 -0.102593731263896 -0.175015758851787 -0.907014053443691 -0.862056534866116 10 7 
Layer 3's weights: -0.172930311540386 -0.436483394483349 -0.193873476365825 -1.10131270812197 -0.316549372056743 -0.446797978261731 -0.7769828253782 -0.318469952920696 -0.650165926793835 -0.177336048463604 8 5 
Layer 4's weights: -0.276609849313518 -0.485442649701855 -0.685920249799511 -0.398985516408703 -0.638238322118542 -0.406773101667187 -0.411131631711743 -0.873029153922817 -0.279333066324016 -0.7116350048578 6 3 
Layer 5's weights: -0.123830031732341 -0.565903555730602 -0.140715193132182 -1.07622737450363 -0.53895856080415 -0.937426503876469 -0.63746119675996 -0.565014969686291 -0.17541296420457 -0.181921076158307 4 1 
Layer 6's weights: 0.0725020077398472 -0.327532956344779 -0.610053652865586 -0.458548286897834 
Layer 7's weights: 0.289661421018457 0.522977008062219 0.0980517701681611 0.102144582706308 
Layer 8's weights: 0.391144796536028 0.669586762116017 0.582546397373737 -0.072025307192743 
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:25 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:26 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:27 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:28 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:29 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:30 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:31 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:32 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:33 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:34 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:54 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
Modified index 9's learning in memoryPool to -0.2
Modified index 10's learning in memoryPool to -0.2
Modified index 11's learning in memoryPool to -0.2
Modified index 12's learning in memoryPool to -0.2
Modified index 13's learning in memoryPool to -0.2
Modified index 14's learning in memoryPool to -0.2
Modified index 15's learning in memoryPool to -0.2
Modified index 16's learning in memoryPool to -0.2
Modified index 17's learning in memoryPool to -0.2
Modified index 18's learning in memoryPool to -0.2
Modified index 19's learning in memoryPool to -0.2
Modified index 20's learning in memoryPool to -0.2
Modified index 21's learning in memoryPool to -0.2
Modified index 22's learning in memoryPool to -0.2
Modified index 23's learning in memoryPool to -0.2
Modified index 24's learning in memoryPool to -0.2
Modified index 25's learning in memoryPool to -0.2
Modified index 26's learning in memoryPool to -0.2
10/5/2016 1:46:54 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 0, 0, -0.2
sum 0.0448115931088215 distri -0.0491546004972124
Using diff 0.0827632953288286 and condRate 0.166666666666667
Changed category 0 weights from 
0.0725020077398472 to 0.0697432311877773
-0.327532956344779 to -0.330291732896849
-0.610053652865586 to -0.612812429417656
-0.458548286897834 to -0.461307063449904
Changing layer 0's weights from 
-0.369009506086131 to -0.371768282638201
-0.740935962775966 to -0.743694739328036
-0.975285228470585 to -0.978044005022655
-0.940725978592654 to -0.943484755144724
-0.1928584092653 to -0.19561718581737
-0.875500362495205 to -0.878259139047275
-0.721670638898632 to -0.724429415450702
-0.749125044921657 to -0.751883821473727
-0.986616905907413 to -0.989375682459483
-1.03036696104171 to -1.03312573759378
Changing layer 1's weights from 
-1.02527004865767 to -1.02802882520974
-0.424101363996289 to -0.426860140548359
-0.549804102758191 to -0.552562879310261
-0.559709441045544 to -0.562468217597614
-0.744762223342678 to -0.747520999894748
-0.883000057319424 to -0.885758833871494
-0.612281333783886 to -0.615040110335956
-0.998084362725041 to -1.00084313927711
-0.586421381810924 to -0.589180158362994
-0.949068244675418 to -0.951827021227488
Changing layer 2's weights from 
-0.13988001284959 to -0.14263878940166
-0.693933498243115 to -0.696692274795185
-0.409344207624218 to -0.412102984176288
-0.238144111017009 to -0.240902887569079
-0.41539682088258 to -0.41815559743465
-0.62140644727113 to -0.6241652238232
-0.102593731263896 to -0.105352507815966
-0.175015758851787 to -0.177774535403857
-0.907014053443691 to -0.909772829995761
-0.862056534866116 to -0.864815311418186
Changing layer 3's weights from 
-0.172930311540386 to -0.175689088092456
-0.436483394483349 to -0.439242171035419
-0.193873476365825 to -0.196632252917895
-1.10131270812197 to -1.10407148467404
-0.316549372056743 to -0.319308148608813
-0.446797978261731 to -0.449556754813801
-0.7769828253782 to -0.77974160193027
-0.318469952920696 to -0.321228729472766
-0.650165926793835 to -0.652924703345905
-0.177336048463604 to -0.180094825015674
Changing layer 4's weights from 
-0.276609849313518 to -0.279368625865588
-0.485442649701855 to -0.488201426253925
-0.685920249799511 to -0.688679026351581
-0.398985516408703 to -0.401744292960773
-0.638238322118542 to -0.640997098670612
-0.406773101667187 to -0.409531878219257
-0.411131631711743 to -0.413890408263813
-0.873029153922817 to -0.875787930474887
-0.279333066324016 to -0.282091842876086
-0.7116350048578 to -0.71439378140987
Changing layer 5's weights from 
-0.123830031732341 to -0.126588808284411
-0.565903555730602 to -0.568662332282672
-0.140715193132182 to -0.143473969684252
-1.07622737450363 to -1.0789861510557
-0.53895856080415 to -0.54171733735622
-0.937426503876469 to -0.940185280428539
-0.63746119675996 to -0.64021997331203
-0.565014969686291 to -0.567773746238361
-0.17541296420457 to -0.17817174075664
-0.181921076158307 to -0.184679852710377
Trying to learn from memory 1, 0, -0.2
sum 0.0448320312899885 distri -0.0491526334283296
Using diff 0.082776656895821 and condRate 0.166666666666667
Changed category 0 weights from 
0.0697432311877773 to 0.0669840092501343
-0.330291732896849 to -0.333050954834492
-0.612812429417656 to -0.615571651355299
-0.461307063449904 to -0.464066285387547
Changing layer 0's weights from 
-0.371768282638201 to -0.374527504575844
-0.743694739328036 to -0.746453961265679
-0.978044005022655 to -0.980803226960298
-0.943484755144724 to -0.946243977082367
-0.19561718581737 to -0.198376407755013
-0.878259139047275 to -0.881018360984918
-0.724429415450702 to -0.727188637388345
-0.751883821473727 to -0.75464304341137
-0.989375682459483 to -0.992134904397126
-1.03312573759378 to -1.03588495953142
Changing layer 1's weights from 
-1.02802882520974 to -1.03078804714738
-0.426860140548359 to -0.429619362486002
-0.552562879310261 to -0.555322101247904
-0.562468217597614 to -0.565227439535257
-0.747520999894748 to -0.750280221832391
-0.885758833871494 to -0.888518055809137
-0.615040110335956 to -0.617799332273599
-1.00084313927711 to -1.00360236121475
-0.589180158362994 to -0.591939380300637
-0.951827021227488 to -0.954586243165131
Changing layer 2's weights from 
-0.14263878940166 to -0.145398011339303
-0.696692274795185 to -0.699451496732828
-0.412102984176288 to -0.414862206113931
-0.240902887569079 to -0.243662109506722
-0.41815559743465 to -0.420914819372293
-0.6241652238232 to -0.626924445760843
-0.105352507815966 to -0.108111729753609
-0.177774535403857 to -0.1805337573415
-0.909772829995761 to -0.912532051933404
-0.864815311418186 to -0.867574533355829
Changing layer 3's weights from 
-0.175689088092456 to -0.178448310030099
-0.439242171035419 to -0.442001392973062
-0.196632252917895 to -0.199391474855538
-1.10407148467404 to -1.10683070661168
-0.319308148608813 to -0.322067370546456
-0.449556754813801 to -0.452315976751444
-0.77974160193027 to -0.782500823867913
-0.321228729472766 to -0.323987951410409
-0.652924703345905 to -0.655683925283548
-0.180094825015674 to -0.182854046953317
Changing layer 4's weights from 
-0.279368625865588 to -0.282127847803231
-0.488201426253925 to -0.490960648191568
-0.688679026351581 to -0.691438248289224
-0.401744292960773 to -0.404503514898416
-0.640997098670612 to -0.643756320608255
-0.409531878219257 to -0.4122911001569
-0.413890408263813 to -0.416649630201456
-0.875787930474887 to -0.87854715241253
-0.282091842876086 to -0.284851064813729
-0.71439378140987 to -0.717153003347513
Changing layer 5's weights from 
-0.126588808284411 to -0.129348030222054
-0.568662332282672 to -0.571421554220315
-0.143473969684252 to -0.146233191621895
-1.0789861510557 to -1.08174537299334
-0.54171733735622 to -0.544476559293863
-0.940185280428539 to -0.942944502366182
-0.64021997331203 to -0.642979195249673
-0.567773746238361 to -0.570532968176004
-0.17817174075664 to -0.180930962694283
-0.184679852710377 to -0.18743907464802
Trying to learn from memory 2, 0, -0.2
sum 0.0448505612160801 distri -0.0491508500190958
Using diff 0.0827887709311558 and condRate 0.166666666666667
Changed category 0 weights from 
0.0669840092501343 to 0.0642243835113075
-0.333050954834492 to -0.335810580573319
-0.615571651355299 to -0.618331277094126
-0.464066285387547 to -0.466825911126374
Changing layer 0's weights from 
-0.374527504575844 to -0.377287130314671
-0.746453961265679 to -0.749213587004506
-0.980803226960298 to -0.983562852699125
-0.946243977082367 to -0.949003602821194
-0.198376407755013 to -0.20113603349384
-0.881018360984918 to -0.883777986723745
-0.727188637388345 to -0.729948263127172
-0.75464304341137 to -0.757402669150197
-0.992134904397126 to -0.994894530135953
-1.03588495953142 to -1.03864458527025
Changing layer 1's weights from 
-1.03078804714738 to -1.03354767288621
-0.429619362486002 to -0.432378988224829
-0.555322101247904 to -0.558081726986731
-0.565227439535257 to -0.567987065274084
-0.750280221832391 to -0.753039847571218
-0.888518055809137 to -0.891277681547964
-0.617799332273599 to -0.620558958012426
-1.00360236121475 to -1.00636198695358
-0.591939380300637 to -0.594699006039464
-0.954586243165131 to -0.957345868903958
Changing layer 2's weights from 
-0.145398011339303 to -0.14815763707813
-0.699451496732828 to -0.702211122471655
-0.414862206113931 to -0.417621831852758
-0.243662109506722 to -0.246421735245549
-0.420914819372293 to -0.42367444511112
-0.626924445760843 to -0.62968407149967
-0.108111729753609 to -0.110871355492436
-0.1805337573415 to -0.183293383080327
-0.912532051933404 to -0.915291677672231
-0.867574533355829 to -0.870334159094656
Changing layer 3's weights from 
-0.178448310030099 to -0.181207935768926
-0.442001392973062 to -0.444761018711889
-0.199391474855538 to -0.202151100594365
-1.10683070661168 to -1.10959033235051
-0.322067370546456 to -0.324826996285283
-0.452315976751444 to -0.455075602490271
-0.782500823867913 to -0.78526044960674
-0.323987951410409 to -0.326747577149236
-0.655683925283548 to -0.658443551022375
-0.182854046953317 to -0.185613672692144
Changing layer 4's weights from 
-0.282127847803231 to -0.284887473542058
-0.490960648191568 to -0.493720273930395
-0.691438248289224 to -0.694197874028051
-0.404503514898416 to -0.407263140637243
-0.643756320608255 to -0.646515946347082
-0.4122911001569 to -0.415050725895727
-0.416649630201456 to -0.419409255940283
-0.87854715241253 to -0.881306778151357
-0.284851064813729 to -0.287610690552556
-0.717153003347513 to -0.71991262908634
Changing layer 5's weights from 
-0.129348030222054 to -0.132107655960881
-0.571421554220315 to -0.574181179959142
-0.146233191621895 to -0.148992817360722
-1.08174537299334 to -1.08450499873217
-0.544476559293863 to -0.54723618503269
-0.942944502366182 to -0.945704128105009
-0.642979195249673 to -0.6457388209885
-0.570532968176004 to -0.573292593914831
-0.180930962694283 to -0.18369058843311
-0.18743907464802 to -0.190198700386847
Trying to learn from memory 3, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0642243835113075 to 0.0614653331358935
-0.335810580573319 to -0.338569630948733
-0.618331277094126 to -0.62109032746954
-0.466825911126374 to -0.469584961501788
Changing layer 0's weights from 
-0.377287130314671 to -0.380046180690085
-0.749213587004506 to -0.75197263737992
-0.983562852699125 to -0.986321903074539
-0.949003602821194 to -0.951762653196608
-0.20113603349384 to -0.203895083869254
-0.883777986723745 to -0.886537037099159
-0.729948263127172 to -0.732707313502586
-0.757402669150197 to -0.760161719525611
-0.994894530135953 to -0.997653580511367
-1.03864458527025 to -1.04140363564566
Changing layer 1's weights from 
-1.03354767288621 to -1.03630672326162
-0.432378988224829 to -0.435138038600243
-0.558081726986731 to -0.560840777362145
-0.567987065274084 to -0.570746115649498
-0.753039847571218 to -0.755798897946632
-0.891277681547964 to -0.894036731923378
-0.620558958012426 to -0.62331800838784
-1.00636198695358 to -1.00912103732899
-0.594699006039464 to -0.597458056414878
-0.957345868903958 to -0.960104919279372
Changing layer 2's weights from 
-0.14815763707813 to -0.150916687453544
-0.702211122471655 to -0.704970172847069
-0.417621831852758 to -0.420380882228172
-0.246421735245549 to -0.249180785620963
-0.42367444511112 to -0.426433495486534
-0.62968407149967 to -0.632443121875084
-0.110871355492436 to -0.11363040586785
-0.183293383080327 to -0.186052433455741
-0.915291677672231 to -0.918050728047645
-0.870334159094656 to -0.87309320947007
Changing layer 3's weights from 
-0.181207935768926 to -0.18396698614434
-0.444761018711889 to -0.447520069087303
-0.202151100594365 to -0.204910150969779
-1.10959033235051 to -1.11234938272592
-0.324826996285283 to -0.327586046660697
-0.455075602490271 to -0.457834652865685
-0.78526044960674 to -0.788019499982154
-0.326747577149236 to -0.32950662752465
-0.658443551022375 to -0.661202601397789
-0.185613672692144 to -0.188372723067558
Changing layer 4's weights from 
-0.284887473542058 to -0.287646523917472
-0.493720273930395 to -0.496479324305809
-0.694197874028051 to -0.696956924403465
-0.407263140637243 to -0.410022191012657
-0.646515946347082 to -0.649274996722496
-0.415050725895727 to -0.417809776271141
-0.419409255940283 to -0.422168306315697
-0.881306778151357 to -0.884065828526771
-0.287610690552556 to -0.29036974092797
-0.71991262908634 to -0.722671679461754
Changing layer 5's weights from 
-0.132107655960881 to -0.134866706336295
-0.574181179959142 to -0.576940230334556
-0.148992817360722 to -0.151751867736136
-1.08450499873217 to -1.08726404910758
-0.54723618503269 to -0.549995235408104
-0.945704128105009 to -0.948463178480423
-0.6457388209885 to -0.648497871363914
-0.573292593914831 to -0.576051644290245
-0.18369058843311 to -0.186449638808524
-0.190198700386847 to -0.192957750762261
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0614653331358935 to 0.0587062827604795
-0.338569630948733 to -0.341328681324147
-0.62109032746954 to -0.623849377844954
-0.469584961501788 to -0.472344011877202
Changing layer 0's weights from 
-0.380046180690085 to -0.382805231065499
-0.75197263737992 to -0.754731687755334
-0.986321903074539 to -0.989080953449953
-0.951762653196608 to -0.954521703572022
-0.203895083869254 to -0.206654134244668
-0.886537037099159 to -0.889296087474573
-0.732707313502586 to -0.735466363878
-0.760161719525611 to -0.762920769901025
-0.997653580511367 to -1.00041263088678
-1.04140363564566 to -1.04416268602108
Changing layer 1's weights from 
-1.03630672326162 to -1.03906577363704
-0.435138038600243 to -0.437897088975657
-0.560840777362145 to -0.563599827737559
-0.570746115649498 to -0.573505166024912
-0.755798897946632 to -0.758557948322046
-0.894036731923378 to -0.896795782298792
-0.62331800838784 to -0.626077058763254
-1.00912103732899 to -1.01188008770441
-0.597458056414878 to -0.600217106790292
-0.960104919279372 to -0.962863969654786
Changing layer 2's weights from 
-0.150916687453544 to -0.153675737828958
-0.704970172847069 to -0.707729223222483
-0.420380882228172 to -0.423139932603586
-0.249180785620963 to -0.251939835996377
-0.426433495486534 to -0.429192545861948
-0.632443121875084 to -0.635202172250498
-0.11363040586785 to -0.116389456243264
-0.186052433455741 to -0.188811483831155
-0.918050728047645 to -0.920809778423059
-0.87309320947007 to -0.875852259845484
Changing layer 3's weights from 
-0.18396698614434 to -0.186726036519754
-0.447520069087303 to -0.450279119462717
-0.204910150969779 to -0.207669201345193
-1.11234938272592 to -1.11510843310134
-0.327586046660697 to -0.330345097036111
-0.457834652865685 to -0.460593703241099
-0.788019499982154 to -0.790778550357568
-0.32950662752465 to -0.332265677900064
-0.661202601397789 to -0.663961651773203
-0.188372723067558 to -0.191131773442972
Changing layer 4's weights from 
-0.287646523917472 to -0.290405574292886
-0.496479324305809 to -0.499238374681223
-0.696956924403465 to -0.699715974778879
-0.410022191012657 to -0.412781241388071
-0.649274996722496 to -0.65203404709791
-0.417809776271141 to -0.420568826646555
-0.422168306315697 to -0.424927356691111
-0.884065828526771 to -0.886824878902185
-0.29036974092797 to -0.293128791303384
-0.722671679461754 to -0.725430729837168
Changing layer 5's weights from 
-0.134866706336295 to -0.137625756711709
-0.576940230334556 to -0.57969928070997
-0.151751867736136 to -0.15451091811155
-1.08726404910758 to -1.090023099483
-0.549995235408104 to -0.552754285783518
-0.948463178480423 to -0.951222228855837
-0.648497871363914 to -0.651256921739328
-0.576051644290245 to -0.578810694665659
-0.186449638808524 to -0.189208689183938
-0.192957750762261 to -0.195716801137675
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0587062827604795 to 0.0559472323850655
-0.341328681324147 to -0.344087731699561
-0.623849377844954 to -0.626608428220368
-0.472344011877202 to -0.475103062252616
Changing layer 0's weights from 
-0.382805231065499 to -0.385564281440913
-0.754731687755334 to -0.757490738130748
-0.989080953449953 to -0.991840003825367
-0.954521703572022 to -0.957280753947436
-0.206654134244668 to -0.209413184620082
-0.889296087474573 to -0.892055137849987
-0.735466363878 to -0.738225414253414
-0.762920769901025 to -0.765679820276439
-1.00041263088678 to -1.00317168126219
-1.04416268602108 to -1.04692173639649
Changing layer 1's weights from 
-1.03906577363704 to -1.04182482401245
-0.437897088975657 to -0.440656139351071
-0.563599827737559 to -0.566358878112973
-0.573505166024912 to -0.576264216400326
-0.758557948322046 to -0.76131699869746
-0.896795782298792 to -0.899554832674206
-0.626077058763254 to -0.628836109138668
-1.01188008770441 to -1.01463913807982
-0.600217106790292 to -0.602976157165706
-0.962863969654786 to -0.9656230200302
Changing layer 2's weights from 
-0.153675737828958 to -0.156434788204372
-0.707729223222483 to -0.710488273597897
-0.423139932603586 to -0.425898982979
-0.251939835996377 to -0.254698886371791
-0.429192545861948 to -0.431951596237362
-0.635202172250498 to -0.637961222625912
-0.116389456243264 to -0.119148506618678
-0.188811483831155 to -0.191570534206569
-0.920809778423059 to -0.923568828798473
-0.875852259845484 to -0.878611310220898
Changing layer 3's weights from 
-0.186726036519754 to -0.189485086895168
-0.450279119462717 to -0.453038169838131
-0.207669201345193 to -0.210428251720607
-1.11510843310134 to -1.11786748347675
-0.330345097036111 to -0.333104147411525
-0.460593703241099 to -0.463352753616513
-0.790778550357568 to -0.793537600732982
-0.332265677900064 to -0.335024728275478
-0.663961651773203 to -0.666720702148617
-0.191131773442972 to -0.193890823818386
Changing layer 4's weights from 
-0.290405574292886 to -0.2931646246683
-0.499238374681223 to -0.501997425056637
-0.699715974778879 to -0.702475025154293
-0.412781241388071 to -0.415540291763485
-0.65203404709791 to -0.654793097473324
-0.420568826646555 to -0.423327877021969
-0.424927356691111 to -0.427686407066525
-0.886824878902185 to -0.889583929277599
-0.293128791303384 to -0.295887841678798
-0.725430729837168 to -0.728189780212582
Changing layer 5's weights from 
-0.137625756711709 to -0.140384807087123
-0.57969928070997 to -0.582458331085384
-0.15451091811155 to -0.157269968486964
-1.090023099483 to -1.09278214985841
-0.552754285783518 to -0.555513336158932
-0.951222228855837 to -0.953981279231251
-0.651256921739328 to -0.654015972114742
-0.578810694665659 to -0.581569745041073
-0.189208689183938 to -0.191967739559352
-0.195716801137675 to -0.198475851513089
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0559472323850655 to 0.0531881820096516
-0.344087731699561 to -0.346846782074975
-0.626608428220368 to -0.629367478595782
-0.475103062252616 to -0.47786211262803
Changing layer 0's weights from 
-0.385564281440913 to -0.388323331816327
-0.757490738130748 to -0.760249788506162
-0.991840003825367 to -0.994599054200781
-0.957280753947436 to -0.96003980432285
-0.209413184620082 to -0.212172234995496
-0.892055137849987 to -0.894814188225401
-0.738225414253414 to -0.740984464628828
-0.765679820276439 to -0.768438870651853
-1.00317168126219 to -1.00593073163761
-1.04692173639649 to -1.04968078677191
Changing layer 1's weights from 
-1.04182482401245 to -1.04458387438787
-0.440656139351071 to -0.443415189726485
-0.566358878112973 to -0.569117928488387
-0.576264216400326 to -0.57902326677574
-0.76131699869746 to -0.764076049072874
-0.899554832674206 to -0.90231388304962
-0.628836109138668 to -0.631595159514082
-1.01463913807982 to -1.01739818845524
-0.602976157165706 to -0.60573520754112
-0.9656230200302 to -0.968382070405614
Changing layer 2's weights from 
-0.156434788204372 to -0.159193838579786
-0.710488273597897 to -0.713247323973311
-0.425898982979 to -0.428658033354414
-0.254698886371791 to -0.257457936747205
-0.431951596237362 to -0.434710646612776
-0.637961222625912 to -0.640720273001326
-0.119148506618678 to -0.121907556994092
-0.191570534206569 to -0.194329584581983
-0.923568828798473 to -0.926327879173887
-0.878611310220898 to -0.881370360596312
Changing layer 3's weights from 
-0.189485086895168 to -0.192244137270582
-0.453038169838131 to -0.455797220213545
-0.210428251720607 to -0.213187302096021
-1.11786748347675 to -1.12062653385217
-0.333104147411525 to -0.335863197786939
-0.463352753616513 to -0.466111803991927
-0.793537600732982 to -0.796296651108396
-0.335024728275478 to -0.337783778650892
-0.666720702148617 to -0.669479752524031
-0.193890823818386 to -0.1966498741938
Changing layer 4's weights from 
-0.2931646246683 to -0.295923675043714
-0.501997425056637 to -0.504756475432051
-0.702475025154293 to -0.705234075529707
-0.415540291763485 to -0.418299342138899
-0.654793097473324 to -0.657552147848738
-0.423327877021969 to -0.426086927397383
-0.427686407066525 to -0.430445457441939
-0.889583929277599 to -0.892342979653013
-0.295887841678798 to -0.298646892054212
-0.728189780212582 to -0.730948830587996
Changing layer 5's weights from 
-0.140384807087123 to -0.143143857462537
-0.582458331085384 to -0.585217381460798
-0.157269968486964 to -0.160029018862378
-1.09278214985841 to -1.09554120023383
-0.555513336158932 to -0.558272386534346
-0.953981279231251 to -0.956740329606665
-0.654015972114742 to -0.656775022490156
-0.581569745041073 to -0.584328795416487
-0.191967739559352 to -0.194726789934766
-0.198475851513089 to -0.201234901888503
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0531881820096516 to 0.0504291316342376
-0.346846782074975 to -0.349605832450389
-0.629367478595782 to -0.632126528971196
-0.47786211262803 to -0.480621163003444
Changing layer 0's weights from 
-0.388323331816327 to -0.391082382191741
-0.760249788506162 to -0.763008838881576
-0.994599054200781 to -0.997358104576195
-0.96003980432285 to -0.962798854698264
-0.212172234995496 to -0.21493128537091
-0.894814188225401 to -0.897573238600815
-0.740984464628828 to -0.743743515004242
-0.768438870651853 to -0.771197921027267
-1.00593073163761 to -1.00868978201302
-1.04968078677191 to -1.05243983714732
Changing layer 1's weights from 
-1.04458387438787 to -1.04734292476328
-0.443415189726485 to -0.446174240101899
-0.569117928488387 to -0.571876978863801
-0.57902326677574 to -0.581782317151154
-0.764076049072874 to -0.766835099448288
-0.90231388304962 to -0.905072933425034
-0.631595159514082 to -0.634354209889496
-1.01739818845524 to -1.02015723883065
-0.60573520754112 to -0.608494257916534
-0.968382070405614 to -0.971141120781028
Changing layer 2's weights from 
-0.159193838579786 to -0.1619528889552
-0.713247323973311 to -0.716006374348725
-0.428658033354414 to -0.431417083729828
-0.257457936747205 to -0.260216987122619
-0.434710646612776 to -0.43746969698819
-0.640720273001326 to -0.64347932337674
-0.121907556994092 to -0.124666607369506
-0.194329584581983 to -0.197088634957397
-0.926327879173887 to -0.929086929549301
-0.881370360596312 to -0.884129410971726
Changing layer 3's weights from 
-0.192244137270582 to -0.195003187645996
-0.455797220213545 to -0.458556270588959
-0.213187302096021 to -0.215946352471435
-1.12062653385217 to -1.12338558422758
-0.335863197786939 to -0.338622248162353
-0.466111803991927 to -0.468870854367341
-0.796296651108396 to -0.79905570148381
-0.337783778650892 to -0.340542829026306
-0.669479752524031 to -0.672238802899445
-0.1966498741938 to -0.199408924569214
Changing layer 4's weights from 
-0.295923675043714 to -0.298682725419128
-0.504756475432051 to -0.507515525807465
-0.705234075529707 to -0.707993125905121
-0.418299342138899 to -0.421058392514313
-0.657552147848738 to -0.660311198224152
-0.426086927397383 to -0.428845977772797
-0.430445457441939 to -0.433204507817353
-0.892342979653013 to -0.895102030028427
-0.298646892054212 to -0.301405942429626
-0.730948830587996 to -0.73370788096341
Changing layer 5's weights from 
-0.143143857462537 to -0.145902907837951
-0.585217381460798 to -0.587976431836212
-0.160029018862378 to -0.162788069237792
-1.09554120023383 to -1.09830025060924
-0.558272386534346 to -0.56103143690976
-0.956740329606665 to -0.959499379982079
-0.656775022490156 to -0.65953407286557
-0.584328795416487 to -0.587087845791901
-0.194726789934766 to -0.19748584031018
-0.201234901888503 to -0.203993952263917
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0504291316342376 to 0.0476700812588236
-0.349605832450389 to -0.352364882825803
-0.632126528971196 to -0.63488557934661
-0.480621163003444 to -0.483380213378858
Changing layer 0's weights from 
-0.391082382191741 to -0.393841432567154
-0.763008838881576 to -0.76576788925699
-0.997358104576195 to -1.00011715495161
-0.962798854698264 to -0.965557905073678
-0.21493128537091 to -0.217690335746324
-0.897573238600815 to -0.900332288976229
-0.743743515004242 to -0.746502565379656
-0.771197921027267 to -0.773956971402681
-1.00868978201302 to -1.01144883238844
-1.05243983714732 to -1.05519888752273
Changing layer 1's weights from 
-1.04734292476328 to -1.05010197513869
-0.446174240101899 to -0.448933290477313
-0.571876978863801 to -0.574636029239215
-0.581782317151154 to -0.584541367526568
-0.766835099448288 to -0.769594149823702
-0.905072933425034 to -0.907831983800448
-0.634354209889496 to -0.63711326026491
-1.02015723883065 to -1.02291628920606
-0.608494257916534 to -0.611253308291948
-0.971141120781028 to -0.973900171156442
Changing layer 2's weights from 
-0.1619528889552 to -0.164711939330614
-0.716006374348725 to -0.718765424724139
-0.431417083729828 to -0.434176134105242
-0.260216987122619 to -0.262976037498033
-0.43746969698819 to -0.440228747363604
-0.64347932337674 to -0.646238373752154
-0.124666607369506 to -0.12742565774492
-0.197088634957397 to -0.199847685332811
-0.929086929549301 to -0.931845979924715
-0.884129410971726 to -0.88688846134714
Changing layer 3's weights from 
-0.195003187645996 to -0.197762238021409
-0.458556270588959 to -0.461315320964373
-0.215946352471435 to -0.218705402846849
-1.12338558422758 to -1.12614463460299
-0.338622248162353 to -0.341381298537767
-0.468870854367341 to -0.471629904742755
-0.79905570148381 to -0.801814751859224
-0.340542829026306 to -0.34330187940172
-0.672238802899445 to -0.674997853274859
-0.199408924569214 to -0.202167974944628
Changing layer 4's weights from 
-0.298682725419128 to -0.301441775794542
-0.507515525807465 to -0.510274576182879
-0.707993125905121 to -0.710752176280535
-0.421058392514313 to -0.423817442889727
-0.660311198224152 to -0.663070248599566
-0.428845977772797 to -0.431605028148211
-0.433204507817353 to -0.435963558192767
-0.895102030028427 to -0.897861080403841
-0.301405942429626 to -0.30416499280504
-0.73370788096341 to -0.736466931338824
Changing layer 5's weights from 
-0.145902907837951 to -0.148661958213365
-0.587976431836212 to -0.590735482211626
-0.162788069237792 to -0.165547119613206
-1.09830025060924 to -1.10105930098465
-0.56103143690976 to -0.563790487285174
-0.959499379982079 to -0.962258430357493
-0.65953407286557 to -0.662293123240984
-0.587087845791901 to -0.589846896167315
-0.19748584031018 to -0.200244890685594
-0.203993952263917 to -0.206753002639331
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0476700812588236 to 0.0449110308834096
-0.352364882825803 to -0.355123933201216
-0.63488557934661 to -0.637644629722024
-0.483380213378858 to -0.486139263754271
Changing layer 0's weights from 
-0.393841432567154 to -0.396600482942568
-0.76576788925699 to -0.768526939632404
-1.00011715495161 to -1.00287620532702
-0.965557905073678 to -0.968316955449092
-0.217690335746324 to -0.220449386121737
-0.900332288976229 to -0.903091339351643
-0.746502565379656 to -0.74926161575507
-0.773956971402681 to -0.776716021778095
-1.01144883238844 to -1.01420788276385
-1.05519888752273 to -1.05795793789815
Changing layer 1's weights from 
-1.05010197513869 to -1.05286102551411
-0.448933290477313 to -0.451692340852726
-0.574636029239215 to -0.577395079614629
-0.584541367526568 to -0.587300417901982
-0.769594149823702 to -0.772353200199116
-0.907831983800448 to -0.910591034175862
-0.63711326026491 to -0.639872310640324
-1.02291628920606 to -1.02567533958148
-0.611253308291948 to -0.614012358667362
-0.973900171156442 to -0.976659221531856
Changing layer 2's weights from 
-0.164711939330614 to -0.167470989706027
-0.718765424724139 to -0.721524475099553
-0.434176134105242 to -0.436935184480655
-0.262976037498033 to -0.265735087873446
-0.440228747363604 to -0.442987797739017
-0.646238373752154 to -0.648997424127568
-0.12742565774492 to -0.130184708120334
-0.199847685332811 to -0.202606735708224
-0.931845979924715 to -0.934605030300129
-0.88688846134714 to -0.889647511722554
Changing layer 3's weights from 
-0.197762238021409 to -0.200521288396823
-0.461315320964373 to -0.464074371339786
-0.218705402846849 to -0.221464453222262
-1.12614463460299 to -1.12890368497841
-0.341381298537767 to -0.34414034891318
-0.471629904742755 to -0.474388955118168
-0.801814751859224 to -0.804573802234638
-0.34330187940172 to -0.346060929777133
-0.674997853274859 to -0.677756903650273
-0.202167974944628 to -0.204927025320041
Changing layer 4's weights from 
-0.301441775794542 to -0.304200826169955
-0.510274576182879 to -0.513033626558293
-0.710752176280535 to -0.713511226655949
-0.423817442889727 to -0.42657649326514
-0.663070248599566 to -0.66582929897498
-0.431605028148211 to -0.434364078523624
-0.435963558192767 to -0.43872260856818
-0.897861080403841 to -0.900620130779255
-0.30416499280504 to -0.306924043180453
-0.736466931338824 to -0.739225981714238
Changing layer 5's weights from 
-0.148661958213365 to -0.151421008588778
-0.590735482211626 to -0.59349453258704
-0.165547119613206 to -0.168306169988619
-1.10105930098465 to -1.10381835136007
-0.563790487285174 to -0.566549537660588
-0.962258430357493 to -0.965017480732907
-0.662293123240984 to -0.665052173616398
-0.589846896167315 to -0.592605946542729
-0.200244890685594 to -0.203003941061007
-0.206753002639331 to -0.209512053014744
10/5/2016 1:46:54 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0449110308834096 to 0.0421519805079957
-0.355123933201216 to -0.35788298357663
-0.637644629722024 to -0.640403680097438
-0.486139263754271 to -0.488898314129685
Changing layer 0's weights from 
-0.396600482942568 to -0.399359533317982
-0.768526939632404 to -0.771285990007818
-1.00287620532702 to -1.00563525570244
-0.968316955449092 to -0.971076005824506
-0.220449386121737 to -0.223208436497151
-0.903091339351643 to -0.905850389727057
-0.74926161575507 to -0.752020666130484
-0.776716021778095 to -0.779475072153509
-1.01420788276385 to -1.01696693313926
-1.05795793789815 to -1.06071698827356
Changing layer 1's weights from 
-1.05286102551411 to -1.05562007588952
-0.451692340852726 to -0.45445139122814
-0.577395079614629 to -0.580154129990043
-0.587300417901982 to -0.590059468277396
-0.772353200199116 to -0.77511225057453
-0.910591034175862 to -0.913350084551276
-0.639872310640324 to -0.642631361015738
-1.02567533958148 to -1.02843438995689
-0.614012358667362 to -0.616771409042776
-0.976659221531856 to -0.97941827190727
Changing layer 2's weights from 
-0.167470989706027 to -0.170230040081441
-0.721524475099553 to -0.724283525474967
-0.436935184480655 to -0.439694234856069
-0.265735087873446 to -0.26849413824886
-0.442987797739017 to -0.445746848114431
-0.648997424127568 to -0.651756474502982
-0.130184708120334 to -0.132943758495747
-0.202606735708224 to -0.205365786083638
-0.934605030300129 to -0.937364080675543
-0.889647511722554 to -0.892406562097968
Changing layer 3's weights from 
-0.200521288396823 to -0.203280338772237
-0.464074371339786 to -0.4668334217152
-0.221464453222262 to -0.224223503597676
-1.12890368497841 to -1.13166273535382
-0.34414034891318 to -0.346899399288594
-0.474388955118168 to -0.477148005493582
-0.804573802234638 to -0.807332852610052
-0.346060929777133 to -0.348819980152547
-0.677756903650273 to -0.680515954025687
-0.204927025320041 to -0.207686075695455
Changing layer 4's weights from 
-0.304200826169955 to -0.306959876545369
-0.513033626558293 to -0.515792676933707
-0.713511226655949 to -0.716270277031363
-0.42657649326514 to -0.429335543640554
-0.66582929897498 to -0.668588349350394
-0.434364078523624 to -0.437123128899038
-0.43872260856818 to -0.441481658943594
-0.900620130779255 to -0.903379181154669
-0.306924043180453 to -0.309683093555867
-0.739225981714238 to -0.741985032089652
Changing layer 5's weights from 
-0.151421008588778 to -0.154180058964192
-0.59349453258704 to -0.596253582962454
-0.168306169988619 to -0.171065220364033
-1.10381835136007 to -1.10657740173548
-0.566549537660588 to -0.569308588036002
-0.965017480732907 to -0.967776531108321
-0.665052173616398 to -0.667811223991812
-0.592605946542729 to -0.595364996918143
-0.203003941061007 to -0.205762991436421
-0.209512053014744 to -0.212271103390158
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0421519805079957 to 0.0393929301325817
-0.35788298357663 to -0.360642033952044
-0.640403680097438 to -0.643162730472852
-0.488898314129685 to -0.491657364505099
Changing layer 0's weights from 
-0.399359533317982 to -0.402118583693396
-0.771285990007818 to -0.774045040383232
-1.00563525570244 to -1.00839430607785
-0.971076005824506 to -0.97383505619992
-0.223208436497151 to -0.225967486872565
-0.905850389727057 to -0.908609440102471
-0.752020666130484 to -0.754779716505898
-0.779475072153509 to -0.782234122528923
-1.01696693313926 to -1.01972598351468
-1.06071698827356 to -1.06347603864898
Changing layer 1's weights from 
-1.05562007588952 to -1.05837912626494
-0.45445139122814 to -0.457210441603554
-0.580154129990043 to -0.582913180365457
-0.590059468277396 to -0.59281851865281
-0.77511225057453 to -0.777871300949944
-0.913350084551276 to -0.91610913492669
-0.642631361015738 to -0.645390411391152
-1.02843438995689 to -1.03119344033231
-0.616771409042776 to -0.61953045941819
-0.97941827190727 to -0.982177322282684
Changing layer 2's weights from 
-0.170230040081441 to -0.172989090456855
-0.724283525474967 to -0.727042575850381
-0.439694234856069 to -0.442453285231483
-0.26849413824886 to -0.271253188624274
-0.445746848114431 to -0.448505898489845
-0.651756474502982 to -0.654515524878396
-0.132943758495747 to -0.135702808871161
-0.205365786083638 to -0.208124836459052
-0.937364080675543 to -0.940123131050957
-0.892406562097968 to -0.895165612473382
Changing layer 3's weights from 
-0.203280338772237 to -0.206039389147651
-0.4668334217152 to -0.469592472090614
-0.224223503597676 to -0.22698255397309
-1.13166273535382 to -1.13442178572924
-0.346899399288594 to -0.349658449664008
-0.477148005493582 to -0.479907055868996
-0.807332852610052 to -0.810091902985466
-0.348819980152547 to -0.351579030527961
-0.680515954025687 to -0.683275004401101
-0.207686075695455 to -0.210445126070869
Changing layer 4's weights from 
-0.306959876545369 to -0.309718926920783
-0.515792676933707 to -0.518551727309121
-0.716270277031363 to -0.719029327406777
-0.429335543640554 to -0.432094594015968
-0.668588349350394 to -0.671347399725808
-0.437123128899038 to -0.439882179274452
-0.441481658943594 to -0.444240709319008
-0.903379181154669 to -0.906138231530083
-0.309683093555867 to -0.312442143931281
-0.741985032089652 to -0.744744082465066
Changing layer 5's weights from 
-0.154180058964192 to -0.156939109339606
-0.596253582962454 to -0.599012633337868
-0.171065220364033 to -0.173824270739447
-1.10657740173548 to -1.1093364521109
-0.569308588036002 to -0.572067638411416
-0.967776531108321 to -0.970535581483735
-0.667811223991812 to -0.670570274367226
-0.595364996918143 to -0.598124047293557
-0.205762991436421 to -0.208522041811835
-0.212271103390158 to -0.215030153765572
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0393929301325817 to 0.0366338797571677
-0.360642033952044 to -0.363401084327458
-0.643162730472852 to -0.645921780848266
-0.491657364505099 to -0.494416414880513
Changing layer 0's weights from 
-0.402118583693396 to -0.40487763406881
-0.774045040383232 to -0.776804090758646
-1.00839430607785 to -1.01115335645326
-0.97383505619992 to -0.976594106575334
-0.225967486872565 to -0.228726537247979
-0.908609440102471 to -0.911368490477885
-0.754779716505898 to -0.757538766881312
-0.782234122528923 to -0.784993172904337
-1.01972598351468 to -1.02248503389009
-1.06347603864898 to -1.06623508902439
Changing layer 1's weights from 
-1.05837912626494 to -1.06113817664035
-0.457210441603554 to -0.459969491978968
-0.582913180365457 to -0.585672230740871
-0.59281851865281 to -0.595577569028224
-0.777871300949944 to -0.780630351325358
-0.91610913492669 to -0.918868185302104
-0.645390411391152 to -0.648149461766566
-1.03119344033231 to -1.03395249070772
-0.61953045941819 to -0.622289509793604
-0.982177322282684 to -0.984936372658098
Changing layer 2's weights from 
-0.172989090456855 to -0.175748140832269
-0.727042575850381 to -0.729801626225795
-0.442453285231483 to -0.445212335606897
-0.271253188624274 to -0.274012238999688
-0.448505898489845 to -0.451264948865259
-0.654515524878396 to -0.65727457525381
-0.135702808871161 to -0.138461859246575
-0.208124836459052 to -0.210883886834466
-0.940123131050957 to -0.942882181426371
-0.895165612473382 to -0.897924662848796
Changing layer 3's weights from 
-0.206039389147651 to -0.208798439523065
-0.469592472090614 to -0.472351522466028
-0.22698255397309 to -0.229741604348504
-1.13442178572924 to -1.13718083610465
-0.349658449664008 to -0.352417500039422
-0.479907055868996 to -0.48266610624441
-0.810091902985466 to -0.81285095336088
-0.351579030527961 to -0.354338080903375
-0.683275004401101 to -0.686034054776515
-0.210445126070869 to -0.213204176446283
Changing layer 4's weights from 
-0.309718926920783 to -0.312477977296197
-0.518551727309121 to -0.521310777684535
-0.719029327406777 to -0.721788377782191
-0.432094594015968 to -0.434853644391382
-0.671347399725808 to -0.674106450101222
-0.439882179274452 to -0.442641229649866
-0.444240709319008 to -0.446999759694422
-0.906138231530083 to -0.908897281905497
-0.312442143931281 to -0.315201194306695
-0.744744082465066 to -0.74750313284048
Changing layer 5's weights from 
-0.156939109339606 to -0.15969815971502
-0.599012633337868 to -0.601771683713282
-0.173824270739447 to -0.176583321114861
-1.1093364521109 to -1.11209550248631
-0.572067638411416 to -0.57482668878683
-0.970535581483735 to -0.973294631859149
-0.670570274367226 to -0.67332932474264
-0.598124047293557 to -0.600883097668971
-0.208522041811835 to -0.211281092187249
-0.215030153765572 to -0.217789204140986
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0366338797571677 to 0.0338748293817537
-0.363401084327458 to -0.366160134702872
-0.645921780848266 to -0.64868083122368
-0.494416414880513 to -0.497175465255927
Changing layer 0's weights from 
-0.40487763406881 to -0.407636684444224
-0.776804090758646 to -0.77956314113406
-1.01115335645326 to -1.01391240682868
-0.976594106575334 to -0.979353156950748
-0.228726537247979 to -0.231485587623393
-0.911368490477885 to -0.914127540853299
-0.757538766881312 to -0.760297817256726
-0.784993172904337 to -0.787752223279751
-1.02248503389009 to -1.02524408426551
-1.06623508902439 to -1.0689941393998
Changing layer 1's weights from 
-1.06113817664035 to -1.06389722701576
-0.459969491978968 to -0.462728542354382
-0.585672230740871 to -0.588431281116285
-0.595577569028224 to -0.598336619403638
-0.780630351325358 to -0.783389401700772
-0.918868185302104 to -0.921627235677518
-0.648149461766566 to -0.65090851214198
-1.03395249070772 to -1.03671154108313
-0.622289509793604 to -0.625048560169018
-0.984936372658098 to -0.987695423033512
Changing layer 2's weights from 
-0.175748140832269 to -0.178507191207683
-0.729801626225795 to -0.732560676601209
-0.445212335606897 to -0.447971385982311
-0.274012238999688 to -0.276771289375102
-0.451264948865259 to -0.454023999240673
-0.65727457525381 to -0.660033625629224
-0.138461859246575 to -0.141220909621989
-0.210883886834466 to -0.21364293720988
-0.942882181426371 to -0.945641231801785
-0.897924662848796 to -0.90068371322421
Changing layer 3's weights from 
-0.208798439523065 to -0.211557489898479
-0.472351522466028 to -0.475110572841442
-0.229741604348504 to -0.232500654723918
-1.13718083610465 to -1.13993988648006
-0.352417500039422 to -0.355176550414836
-0.48266610624441 to -0.485425156619824
-0.81285095336088 to -0.815610003736294
-0.354338080903375 to -0.357097131278789
-0.686034054776515 to -0.688793105151929
-0.213204176446283 to -0.215963226821697
Changing layer 4's weights from 
-0.312477977296197 to -0.315237027671611
-0.521310777684535 to -0.524069828059949
-0.721788377782191 to -0.724547428157605
-0.434853644391382 to -0.437612694766796
-0.674106450101222 to -0.676865500476636
-0.442641229649866 to -0.44540028002528
-0.446999759694422 to -0.449758810069836
-0.908897281905497 to -0.911656332280911
-0.315201194306695 to -0.317960244682109
-0.74750313284048 to -0.750262183215894
Changing layer 5's weights from 
-0.15969815971502 to -0.162457210090434
-0.601771683713282 to -0.604530734088696
-0.176583321114861 to -0.179342371490275
-1.11209550248631 to -1.11485455286172
-0.57482668878683 to -0.577585739162244
-0.973294631859149 to -0.976053682234563
-0.67332932474264 to -0.676088375118054
-0.600883097668971 to -0.603642148044385
-0.211281092187249 to -0.214040142562663
-0.217789204140986 to -0.2205482545164
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0338748293817537 to 0.0311157790063398
-0.366160134702872 to -0.368919185078286
-0.64868083122368 to -0.651439881599094
-0.497175465255927 to -0.499934515631341
Changing layer 0's weights from 
-0.407636684444224 to -0.410395734819638
-0.77956314113406 to -0.782322191509474
-1.01391240682868 to -1.01667145720409
-0.979353156950748 to -0.982112207326162
-0.231485587623393 to -0.234244637998807
-0.914127540853299 to -0.916886591228713
-0.760297817256726 to -0.76305686763214
-0.787752223279751 to -0.790511273655165
-1.02524408426551 to -1.02800313464092
-1.0689941393998 to -1.07175318977522
Changing layer 1's weights from 
-1.06389722701576 to -1.06665627739118
-0.462728542354382 to -0.465487592729796
-0.588431281116285 to -0.591190331491699
-0.598336619403638 to -0.601095669779052
-0.783389401700772 to -0.786148452076186
-0.921627235677518 to -0.924386286052932
-0.65090851214198 to -0.653667562517394
-1.03671154108313 to -1.03947059145855
-0.625048560169018 to -0.627807610544432
-0.987695423033512 to -0.990454473408926
Changing layer 2's weights from 
-0.178507191207683 to -0.181266241583097
-0.732560676601209 to -0.735319726976623
-0.447971385982311 to -0.450730436357725
-0.276771289375102 to -0.279530339750516
-0.454023999240673 to -0.456783049616087
-0.660033625629224 to -0.662792676004638
-0.141220909621989 to -0.143979959997403
-0.21364293720988 to -0.216401987585294
-0.945641231801785 to -0.948400282177199
-0.90068371322421 to -0.903442763599624
Changing layer 3's weights from 
-0.211557489898479 to -0.214316540273893
-0.475110572841442 to -0.477869623216856
-0.232500654723918 to -0.235259705099332
-1.13993988648006 to -1.14269893685548
-0.355176550414836 to -0.35793560079025
-0.485425156619824 to -0.488184206995238
-0.815610003736294 to -0.818369054111708
-0.357097131278789 to -0.359856181654203
-0.688793105151929 to -0.691552155527343
-0.215963226821697 to -0.218722277197111
Changing layer 4's weights from 
-0.315237027671611 to -0.317996078047025
-0.524069828059949 to -0.526828878435363
-0.724547428157605 to -0.727306478533019
-0.437612694766796 to -0.44037174514221
-0.676865500476636 to -0.67962455085205
-0.44540028002528 to -0.448159330400694
-0.449758810069836 to -0.45251786044525
-0.911656332280911 to -0.914415382656325
-0.317960244682109 to -0.320719295057523
-0.750262183215894 to -0.753021233591308
Changing layer 5's weights from 
-0.162457210090434 to -0.165216260465848
-0.604530734088696 to -0.60728978446411
-0.179342371490275 to -0.182101421865689
-1.11485455286172 to -1.11761360323714
-0.577585739162244 to -0.580344789537658
-0.976053682234563 to -0.978812732609977
-0.676088375118054 to -0.678847425493468
-0.603642148044385 to -0.606401198419799
-0.214040142562663 to -0.216799192938077
-0.2205482545164 to -0.223307304891814
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0311157790063398 to 0.0283567286309258
-0.368919185078286 to -0.3716782354537
-0.651439881599094 to -0.654198931974508
-0.499934515631341 to -0.502693566006755
Changing layer 0's weights from 
-0.410395734819638 to -0.413154785195052
-0.782322191509474 to -0.785081241884888
-1.01667145720409 to -1.01943050757951
-0.982112207326162 to -0.984871257701576
-0.234244637998807 to -0.237003688374221
-0.916886591228713 to -0.919645641604127
-0.76305686763214 to -0.765815918007554
-0.790511273655165 to -0.793270324030579
-1.02800313464092 to -1.03076218501633
-1.07175318977522 to -1.07451224015063
Changing layer 1's weights from 
-1.06665627739118 to -1.06941532776659
-0.465487592729796 to -0.46824664310521
-0.591190331491699 to -0.593949381867113
-0.601095669779052 to -0.603854720154466
-0.786148452076186 to -0.7889075024516
-0.924386286052932 to -0.927145336428346
-0.653667562517394 to -0.656426612892808
-1.03947059145855 to -1.04222964183396
-0.627807610544432 to -0.630566660919846
-0.990454473408926 to -0.99321352378434
Changing layer 2's weights from 
-0.181266241583097 to -0.184025291958511
-0.735319726976623 to -0.738078777352037
-0.450730436357725 to -0.453489486733139
-0.279530339750516 to -0.28228939012593
-0.456783049616087 to -0.459542099991501
-0.662792676004638 to -0.665551726380052
-0.143979959997403 to -0.146739010372817
-0.216401987585294 to -0.219161037960708
-0.948400282177199 to -0.951159332552613
-0.903442763599624 to -0.906201813975038
Changing layer 3's weights from 
-0.214316540273893 to -0.217075590649307
-0.477869623216856 to -0.48062867359227
-0.235259705099332 to -0.238018755474746
-1.14269893685548 to -1.14545798723089
-0.35793560079025 to -0.360694651165664
-0.488184206995238 to -0.490943257370652
-0.818369054111708 to -0.821128104487122
-0.359856181654203 to -0.362615232029617
-0.691552155527343 to -0.694311205902757
-0.218722277197111 to -0.221481327572525
Changing layer 4's weights from 
-0.317996078047025 to -0.320755128422439
-0.526828878435363 to -0.529587928810777
-0.727306478533019 to -0.730065528908433
-0.44037174514221 to -0.443130795517624
-0.67962455085205 to -0.682383601227464
-0.448159330400694 to -0.450918380776108
-0.45251786044525 to -0.455276910820664
-0.914415382656325 to -0.917174433031739
-0.320719295057523 to -0.323478345432937
-0.753021233591308 to -0.755780283966722
Changing layer 5's weights from 
-0.165216260465848 to -0.167975310841262
-0.60728978446411 to -0.610048834839524
-0.182101421865689 to -0.184860472241103
-1.11761360323714 to -1.12037265361255
-0.580344789537658 to -0.583103839913072
-0.978812732609977 to -0.981571782985391
-0.678847425493468 to -0.681606475868882
-0.606401198419799 to -0.609160248795213
-0.216799192938077 to -0.219558243313491
-0.223307304891814 to -0.226066355267228
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0283567286309258 to 0.0255976782555118
-0.3716782354537 to -0.374437285829114
-0.654198931974508 to -0.656957982349922
-0.502693566006755 to -0.505452616382169
Changing layer 0's weights from 
-0.413154785195052 to -0.415913835570466
-0.785081241884888 to -0.787840292260302
-1.01943050757951 to -1.02218955795492
-0.984871257701576 to -0.98763030807699
-0.237003688374221 to -0.239762738749635
-0.919645641604127 to -0.922404691979541
-0.765815918007554 to -0.768574968382968
-0.793270324030579 to -0.796029374405993
-1.03076218501633 to -1.03352123539175
-1.07451224015063 to -1.07727129052605
Changing layer 1's weights from 
-1.06941532776659 to -1.07217437814201
-0.46824664310521 to -0.471005693480624
-0.593949381867113 to -0.596708432242527
-0.603854720154466 to -0.60661377052988
-0.7889075024516 to -0.791666552827014
-0.927145336428346 to -0.92990438680376
-0.656426612892808 to -0.659185663268222
-1.04222964183396 to -1.04498869220938
-0.630566660919846 to -0.63332571129526
-0.99321352378434 to -0.995972574159754
Changing layer 2's weights from 
-0.184025291958511 to -0.186784342333925
-0.738078777352037 to -0.740837827727451
-0.453489486733139 to -0.456248537108553
-0.28228939012593 to -0.285048440501344
-0.459542099991501 to -0.462301150366915
-0.665551726380052 to -0.668310776755466
-0.146739010372817 to -0.149498060748231
-0.219161037960708 to -0.221920088336122
-0.951159332552613 to -0.953918382928027
-0.906201813975038 to -0.908960864350452
Changing layer 3's weights from 
-0.217075590649307 to -0.219834641024721
-0.48062867359227 to -0.483387723967684
-0.238018755474746 to -0.24077780585016
-1.14545798723089 to -1.14821703760631
-0.360694651165664 to -0.363453701541078
-0.490943257370652 to -0.493702307746066
-0.821128104487122 to -0.823887154862536
-0.362615232029617 to -0.365374282405031
-0.694311205902757 to -0.697070256278171
-0.221481327572525 to -0.224240377947939
Changing layer 4's weights from 
-0.320755128422439 to -0.323514178797853
-0.529587928810777 to -0.532346979186191
-0.730065528908433 to -0.732824579283847
-0.443130795517624 to -0.445889845893038
-0.682383601227464 to -0.685142651602878
-0.450918380776108 to -0.453677431151522
-0.455276910820664 to -0.458035961196078
-0.917174433031739 to -0.919933483407153
-0.323478345432937 to -0.326237395808351
-0.755780283966722 to -0.758539334342136
Changing layer 5's weights from 
-0.167975310841262 to -0.170734361216676
-0.610048834839524 to -0.612807885214938
-0.184860472241103 to -0.187619522616517
-1.12037265361255 to -1.12313170398797
-0.583103839913072 to -0.585862890288486
-0.981571782985391 to -0.984330833360805
-0.681606475868882 to -0.684365526244296
-0.609160248795213 to -0.611919299170627
-0.219558243313491 to -0.222317293688905
-0.226066355267228 to -0.228825405642642
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0255976782555118 to 0.0228386278800978
-0.374437285829114 to -0.377196336204528
-0.656957982349922 to -0.659717032725336
-0.505452616382169 to -0.508211666757583
Changing layer 0's weights from 
-0.415913835570466 to -0.41867288594588
-0.787840292260302 to -0.790599342635716
-1.02218955795492 to -1.02494860833034
-0.98763030807699 to -0.990389358452404
-0.239762738749635 to -0.242521789125049
-0.922404691979541 to -0.925163742354955
-0.768574968382968 to -0.771334018758382
-0.796029374405993 to -0.798788424781407
-1.03352123539175 to -1.03628028576716
-1.07727129052605 to -1.08003034090146
Changing layer 1's weights from 
-1.07217437814201 to -1.07493342851742
-0.471005693480624 to -0.473764743856038
-0.596708432242527 to -0.599467482617941
-0.60661377052988 to -0.609372820905294
-0.791666552827014 to -0.794425603202428
-0.92990438680376 to -0.932663437179174
-0.659185663268222 to -0.661944713643636
-1.04498869220938 to -1.04774774258479
-0.63332571129526 to -0.636084761670674
-0.995972574159754 to -0.998731624535168
Changing layer 2's weights from 
-0.186784342333925 to -0.189543392709339
-0.740837827727451 to -0.743596878102865
-0.456248537108553 to -0.459007587483967
-0.285048440501344 to -0.287807490876758
-0.462301150366915 to -0.465060200742329
-0.668310776755466 to -0.67106982713088
-0.149498060748231 to -0.152257111123645
-0.221920088336122 to -0.224679138711536
-0.953918382928027 to -0.956677433303441
-0.908960864350452 to -0.911719914725866
Changing layer 3's weights from 
-0.219834641024721 to -0.222593691400135
-0.483387723967684 to -0.486146774343098
-0.24077780585016 to -0.243536856225574
-1.14821703760631 to -1.15097608798172
-0.363453701541078 to -0.366212751916492
-0.493702307746066 to -0.49646135812148
-0.823887154862536 to -0.82664620523795
-0.365374282405031 to -0.368133332780445
-0.697070256278171 to -0.699829306653585
-0.224240377947939 to -0.226999428323353
Changing layer 4's weights from 
-0.323514178797853 to -0.326273229173267
-0.532346979186191 to -0.535106029561605
-0.732824579283847 to -0.735583629659261
-0.445889845893038 to -0.448648896268452
-0.685142651602878 to -0.687901701978292
-0.453677431151522 to -0.456436481526936
-0.458035961196078 to -0.460795011571492
-0.919933483407153 to -0.922692533782567
-0.326237395808351 to -0.328996446183765
-0.758539334342136 to -0.76129838471755
Changing layer 5's weights from 
-0.170734361216676 to -0.17349341159209
-0.612807885214938 to -0.615566935590352
-0.187619522616517 to -0.190378572991931
-1.12313170398797 to -1.12589075436338
-0.585862890288486 to -0.5886219406639
-0.984330833360805 to -0.987089883736219
-0.684365526244296 to -0.68712457661971
-0.611919299170627 to -0.614678349546041
-0.222317293688905 to -0.225076344064319
-0.228825405642642 to -0.231584456018056
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0228386278800978 to 0.0200795775046839
-0.377196336204528 to -0.379955386579942
-0.659717032725336 to -0.66247608310075
-0.508211666757583 to -0.510970717132997
Changing layer 0's weights from 
-0.41867288594588 to -0.421431936321294
-0.790599342635716 to -0.79335839301113
-1.02494860833034 to -1.02770765870575
-0.990389358452404 to -0.993148408827818
-0.242521789125049 to -0.245280839500463
-0.925163742354955 to -0.927922792730369
-0.771334018758382 to -0.774093069133796
-0.798788424781407 to -0.801547475156821
-1.03628028576716 to -1.03903933614258
-1.08003034090146 to -1.08278939127687
Changing layer 1's weights from 
-1.07493342851742 to -1.07769247889283
-0.473764743856038 to -0.476523794231452
-0.599467482617941 to -0.602226532993355
-0.609372820905294 to -0.612131871280708
-0.794425603202428 to -0.797184653577842
-0.932663437179174 to -0.935422487554588
-0.661944713643636 to -0.66470376401905
-1.04774774258479 to -1.05050679296021
-0.636084761670674 to -0.638843812046088
-0.998731624535168 to -1.00149067491058
Changing layer 2's weights from 
-0.189543392709339 to -0.192302443084753
-0.743596878102865 to -0.746355928478279
-0.459007587483967 to -0.461766637859381
-0.287807490876758 to -0.290566541252172
-0.465060200742329 to -0.467819251117743
-0.67106982713088 to -0.673828877506294
-0.152257111123645 to -0.155016161499059
-0.224679138711536 to -0.22743818908695
-0.956677433303441 to -0.959436483678855
-0.911719914725866 to -0.91447896510128
Changing layer 3's weights from 
-0.222593691400135 to -0.225352741775549
-0.486146774343098 to -0.488905824718512
-0.243536856225574 to -0.246295906600988
-1.15097608798172 to -1.15373513835713
-0.366212751916492 to -0.368971802291906
-0.49646135812148 to -0.499220408496894
-0.82664620523795 to -0.829405255613364
-0.368133332780445 to -0.370892383155859
-0.699829306653585 to -0.702588357028999
-0.226999428323353 to -0.229758478698767
Changing layer 4's weights from 
-0.326273229173267 to -0.329032279548681
-0.535106029561605 to -0.537865079937019
-0.735583629659261 to -0.738342680034675
-0.448648896268452 to -0.451407946643866
-0.687901701978292 to -0.690660752353706
-0.456436481526936 to -0.45919553190235
-0.460795011571492 to -0.463554061946906
-0.922692533782567 to -0.925451584157981
-0.328996446183765 to -0.331755496559179
-0.76129838471755 to -0.764057435092964
Changing layer 5's weights from 
-0.17349341159209 to -0.176252461967504
-0.615566935590352 to -0.618325985965766
-0.190378572991931 to -0.193137623367345
-1.12589075436338 to -1.12864980473879
-0.5886219406639 to -0.591380991039314
-0.987089883736219 to -0.989848934111633
-0.68712457661971 to -0.689883626995124
-0.614678349546041 to -0.617437399921455
-0.225076344064319 to -0.227835394439733
-0.231584456018056 to -0.23434350639347
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0200795775046839 to 0.0173205271292699
-0.379955386579942 to -0.382714436955356
-0.66247608310075 to -0.665235133476164
-0.510970717132997 to -0.513729767508411
Changing layer 0's weights from 
-0.421431936321294 to -0.424190986696708
-0.79335839301113 to -0.796117443386544
-1.02770765870575 to -1.03046670908116
-0.993148408827818 to -0.995907459203232
-0.245280839500463 to -0.248039889875877
-0.927922792730369 to -0.930681843105783
-0.774093069133796 to -0.77685211950921
-0.801547475156821 to -0.804306525532235
-1.03903933614258 to -1.04179838651799
-1.08278939127687 to -1.08554844165229
Changing layer 1's weights from 
-1.07769247889283 to -1.08045152926825
-0.476523794231452 to -0.479282844606866
-0.602226532993355 to -0.604985583368769
-0.612131871280708 to -0.614890921656122
-0.797184653577842 to -0.799943703953256
-0.935422487554588 to -0.938181537930002
-0.66470376401905 to -0.667462814394464
-1.05050679296021 to -1.05326584333562
-0.638843812046088 to -0.641602862421502
-1.00149067491058 to -1.004249725286
Changing layer 2's weights from 
-0.192302443084753 to -0.195061493460167
-0.746355928478279 to -0.749114978853693
-0.461766637859381 to -0.464525688234795
-0.290566541252172 to -0.293325591627586
-0.467819251117743 to -0.470578301493157
-0.673828877506294 to -0.676587927881708
-0.155016161499059 to -0.157775211874473
-0.22743818908695 to -0.230197239462364
-0.959436483678855 to -0.962195534054269
-0.91447896510128 to -0.917238015476694
Changing layer 3's weights from 
-0.225352741775549 to -0.228111792150963
-0.488905824718512 to -0.491664875093926
-0.246295906600988 to -0.249054956976402
-1.15373513835713 to -1.15649418873255
-0.368971802291906 to -0.37173085266732
-0.499220408496894 to -0.501979458872308
-0.829405255613364 to -0.832164305988778
-0.370892383155859 to -0.373651433531273
-0.702588357028999 to -0.705347407404413
-0.229758478698767 to -0.232517529074181
Changing layer 4's weights from 
-0.329032279548681 to -0.331791329924095
-0.537865079937019 to -0.540624130312433
-0.738342680034675 to -0.741101730410089
-0.451407946643866 to -0.45416699701928
-0.690660752353706 to -0.69341980272912
-0.45919553190235 to -0.461954582277764
-0.463554061946906 to -0.46631311232232
-0.925451584157981 to -0.928210634533395
-0.331755496559179 to -0.334514546934593
-0.764057435092964 to -0.766816485468378
Changing layer 5's weights from 
-0.176252461967504 to -0.179011512342918
-0.618325985965766 to -0.62108503634118
-0.193137623367345 to -0.195896673742759
-1.12864980473879 to -1.13140885511421
-0.591380991039314 to -0.594140041414728
-0.989848934111633 to -0.992607984487047
-0.689883626995124 to -0.692642677370538
-0.617437399921455 to -0.620196450296869
-0.227835394439733 to -0.230594444815147
-0.23434350639347 to -0.237102556768884
10/5/2016 1:46:55 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0173205271292699 to 0.0145614767538559
-0.382714436955356 to -0.38547348733077
-0.665235133476164 to -0.667994183851578
-0.513729767508411 to -0.516488817883825
Changing layer 0's weights from 
-0.424190986696708 to -0.426950037072122
-0.796117443386544 to -0.798876493761958
-1.03046670908116 to -1.03322575945658
-0.995907459203232 to -0.998666509578646
-0.248039889875877 to -0.250798940251291
-0.930681843105783 to -0.933440893481197
-0.77685211950921 to -0.779611169884624
-0.804306525532235 to -0.807065575907649
-1.04179838651799 to -1.04455743689341
-1.08554844165229 to -1.0883074920277
Changing layer 1's weights from 
-1.08045152926825 to -1.08321057964366
-0.479282844606866 to -0.48204189498228
-0.604985583368769 to -0.607744633744183
-0.614890921656122 to -0.617649972031536
-0.799943703953256 to -0.80270275432867
-0.938181537930002 to -0.940940588305416
-0.667462814394464 to -0.670221864769878
-1.05326584333562 to -1.05602489371103
-0.641602862421502 to -0.644361912796916
-1.004249725286 to -1.00700877566141
Changing layer 2's weights from 
-0.195061493460167 to -0.197820543835581
-0.749114978853693 to -0.751874029229107
-0.464525688234795 to -0.467284738610209
-0.293325591627586 to -0.296084642003
-0.470578301493157 to -0.473337351868571
-0.676587927881708 to -0.679346978257122
-0.157775211874473 to -0.160534262249887
-0.230197239462364 to -0.232956289837778
-0.962195534054269 to -0.964954584429683
-0.917238015476694 to -0.919997065852108
Changing layer 3's weights from 
-0.228111792150963 to -0.230870842526377
-0.491664875093926 to -0.49442392546934
-0.249054956976402 to -0.251814007351816
-1.15649418873255 to -1.15925323910796
-0.37173085266732 to -0.374489903042734
-0.501979458872308 to -0.504738509247722
-0.832164305988778 to -0.834923356364192
-0.373651433531273 to -0.376410483906687
-0.705347407404413 to -0.708106457779827
-0.232517529074181 to -0.235276579449595
Changing layer 4's weights from 
-0.331791329924095 to -0.334550380299509
-0.540624130312433 to -0.543383180687847
-0.741101730410089 to -0.743860780785503
-0.45416699701928 to -0.456926047394694
-0.69341980272912 to -0.696178853104534
-0.461954582277764 to -0.464713632653178
-0.46631311232232 to -0.469072162697734
-0.928210634533395 to -0.930969684908809
-0.334514546934593 to -0.337273597310007
-0.766816485468378 to -0.769575535843792
Changing layer 5's weights from 
-0.179011512342918 to -0.181770562718332
-0.62108503634118 to -0.623844086716594
-0.195896673742759 to -0.198655724118173
-1.13140885511421 to -1.13416790548962
-0.594140041414728 to -0.596899091790142
-0.992607984487047 to -0.995367034862461
-0.692642677370538 to -0.695401727745952
-0.620196450296869 to -0.622955500672283
-0.230594444815147 to -0.233353495190561
-0.237102556768884 to -0.239861607144298
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0145614767538559 to 0.0118024263784419
-0.38547348733077 to -0.388232537706184
-0.667994183851578 to -0.670753234226992
-0.516488817883825 to -0.519247868259239
Changing layer 0's weights from 
-0.426950037072122 to -0.429709087447536
-0.798876493761958 to -0.801635544137372
-1.03322575945658 to -1.03598480983199
-0.998666509578646 to -1.00142555995406
-0.250798940251291 to -0.253557990626705
-0.933440893481197 to -0.936199943856611
-0.779611169884624 to -0.782370220260038
-0.807065575907649 to -0.809824626283063
-1.04455743689341 to -1.04731648726882
-1.0883074920277 to -1.09106654240312
Changing layer 1's weights from 
-1.08321057964366 to -1.08596963001908
-0.48204189498228 to -0.484800945357694
-0.607744633744183 to -0.610503684119597
-0.617649972031536 to -0.62040902240695
-0.80270275432867 to -0.805461804704084
-0.940940588305416 to -0.94369963868083
-0.670221864769878 to -0.672980915145292
-1.05602489371103 to -1.05878394408645
-0.644361912796916 to -0.64712096317233
-1.00700877566141 to -1.00976782603682
Changing layer 2's weights from 
-0.197820543835581 to -0.200579594210995
-0.751874029229107 to -0.754633079604521
-0.467284738610209 to -0.470043788985623
-0.296084642003 to -0.298843692378414
-0.473337351868571 to -0.476096402243985
-0.679346978257122 to -0.682106028632536
-0.160534262249887 to -0.163293312625301
-0.232956289837778 to -0.235715340213192
-0.964954584429683 to -0.967713634805097
-0.919997065852108 to -0.922756116227522
Changing layer 3's weights from 
-0.230870842526377 to -0.233629892901791
-0.49442392546934 to -0.497182975844754
-0.251814007351816 to -0.25457305772723
-1.15925323910796 to -1.16201228948338
-0.374489903042734 to -0.377248953418148
-0.504738509247722 to -0.507497559623136
-0.834923356364192 to -0.837682406739606
-0.376410483906687 to -0.379169534282101
-0.708106457779827 to -0.710865508155241
-0.235276579449595 to -0.238035629825009
Changing layer 4's weights from 
-0.334550380299509 to -0.337309430674923
-0.543383180687847 to -0.546142231063261
-0.743860780785503 to -0.746619831160917
-0.456926047394694 to -0.459685097770108
-0.696178853104534 to -0.698937903479948
-0.464713632653178 to -0.467472683028592
-0.469072162697734 to -0.471831213073148
-0.930969684908809 to -0.933728735284223
-0.337273597310007 to -0.340032647685421
-0.769575535843792 to -0.772334586219206
Changing layer 5's weights from 
-0.181770562718332 to -0.184529613093746
-0.623844086716594 to -0.626603137092008
-0.198655724118173 to -0.201414774493587
-1.13416790548962 to -1.13692695586504
-0.596899091790142 to -0.599658142165556
-0.995367034862461 to -0.998126085237875
-0.695401727745952 to -0.698160778121366
-0.622955500672283 to -0.625714551047697
-0.233353495190561 to -0.236112545565975
-0.239861607144298 to -0.242620657519712
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.0118024263784419 to 0.00904337600302798
-0.388232537706184 to -0.390991588081598
-0.670753234226992 to -0.673512284602406
-0.519247868259239 to -0.522006918634653
Changing layer 0's weights from 
-0.429709087447536 to -0.43246813782295
-0.801635544137372 to -0.804394594512786
-1.03598480983199 to -1.03874386020741
-1.00142555995406 to -1.00418461032947
-0.253557990626705 to -0.256317041002119
-0.936199943856611 to -0.938958994232025
-0.782370220260038 to -0.785129270635452
-0.809824626283063 to -0.812583676658477
-1.04731648726882 to -1.05007553764423
-1.09106654240312 to -1.09382559277853
Changing layer 1's weights from 
-1.08596963001908 to -1.08872868039449
-0.484800945357694 to -0.487559995733108
-0.610503684119597 to -0.613262734495011
-0.62040902240695 to -0.623168072782364
-0.805461804704084 to -0.808220855079498
-0.94369963868083 to -0.946458689056244
-0.672980915145292 to -0.675739965520706
-1.05878394408645 to -1.06154299446186
-0.64712096317233 to -0.649880013547744
-1.00976782603682 to -1.01252687641224
Changing layer 2's weights from 
-0.200579594210995 to -0.203338644586409
-0.754633079604521 to -0.757392129979935
-0.470043788985623 to -0.472802839361037
-0.298843692378414 to -0.301602742753828
-0.476096402243985 to -0.478855452619399
-0.682106028632536 to -0.68486507900795
-0.163293312625301 to -0.166052363000715
-0.235715340213192 to -0.238474390588606
-0.967713634805097 to -0.970472685180511
-0.922756116227522 to -0.925515166602936
Changing layer 3's weights from 
-0.233629892901791 to -0.236388943277205
-0.497182975844754 to -0.499942026220168
-0.25457305772723 to -0.257332108102644
-1.16201228948338 to -1.16477133985879
-0.377248953418148 to -0.380008003793562
-0.507497559623136 to -0.51025660999855
-0.837682406739606 to -0.84044145711502
-0.379169534282101 to -0.381928584657515
-0.710865508155241 to -0.713624558530655
-0.238035629825009 to -0.240794680200423
Changing layer 4's weights from 
-0.337309430674923 to -0.340068481050337
-0.546142231063261 to -0.548901281438675
-0.746619831160917 to -0.749378881536331
-0.459685097770108 to -0.462444148145522
-0.698937903479948 to -0.701696953855362
-0.467472683028592 to -0.470231733404006
-0.471831213073148 to -0.474590263448562
-0.933728735284223 to -0.936487785659637
-0.340032647685421 to -0.342791698060835
-0.772334586219206 to -0.77509363659462
Changing layer 5's weights from 
-0.184529613093746 to -0.18728866346916
-0.626603137092008 to -0.629362187467422
-0.201414774493587 to -0.204173824869001
-1.13692695586504 to -1.13968600624045
-0.599658142165556 to -0.60241719254097
-0.998126085237875 to -1.00088513561329
-0.698160778121366 to -0.70091982849678
-0.625714551047697 to -0.628473601423111
-0.236112545565975 to -0.238871595941389
-0.242620657519712 to -0.245379707895126
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.00904337600302798 to 0.006284325627614
-0.390991588081598 to -0.393750638457012
-0.673512284602406 to -0.67627133497782
-0.522006918634653 to -0.524765969010067
Changing layer 0's weights from 
-0.43246813782295 to -0.435227188198364
-0.804394594512786 to -0.8071536448882
-1.03874386020741 to -1.04150291058282
-1.00418461032947 to -1.00694366070489
-0.256317041002119 to -0.259076091377533
-0.938958994232025 to -0.941718044607439
-0.785129270635452 to -0.787888321010866
-0.812583676658477 to -0.815342727033891
-1.05007553764423 to -1.05283458801965
-1.09382559277853 to -1.09658464315394
Changing layer 1's weights from 
-1.08872868039449 to -1.0914877307699
-0.487559995733108 to -0.490319046108522
-0.613262734495011 to -0.616021784870425
-0.623168072782364 to -0.625927123157778
-0.808220855079498 to -0.810979905454912
-0.946458689056244 to -0.949217739431658
-0.675739965520706 to -0.67849901589612
-1.06154299446186 to -1.06430204483728
-0.649880013547744 to -0.652639063923158
-1.01252687641224 to -1.01528592678765
Changing layer 2's weights from 
-0.203338644586409 to -0.206097694961823
-0.757392129979935 to -0.760151180355349
-0.472802839361037 to -0.475561889736451
-0.301602742753828 to -0.304361793129242
-0.478855452619399 to -0.481614502994813
-0.68486507900795 to -0.687624129383364
-0.166052363000715 to -0.168811413376129
-0.238474390588606 to -0.24123344096402
-0.970472685180511 to -0.973231735555925
-0.925515166602936 to -0.92827421697835
Changing layer 3's weights from 
-0.236388943277205 to -0.239147993652619
-0.499942026220168 to -0.502701076595582
-0.257332108102644 to -0.260091158478058
-1.16477133985879 to -1.1675303902342
-0.380008003793562 to -0.382767054168976
-0.51025660999855 to -0.513015660373964
-0.84044145711502 to -0.843200507490434
-0.381928584657515 to -0.384687635032929
-0.713624558530655 to -0.716383608906069
-0.240794680200423 to -0.243553730575837
Changing layer 4's weights from 
-0.340068481050337 to -0.342827531425751
-0.548901281438675 to -0.551660331814089
-0.749378881536331 to -0.752137931911745
-0.462444148145522 to -0.465203198520936
-0.701696953855362 to -0.704456004230776
-0.470231733404006 to -0.47299078377942
-0.474590263448562 to -0.477349313823976
-0.936487785659637 to -0.939246836035051
-0.342791698060835 to -0.345550748436249
-0.77509363659462 to -0.777852686970034
Changing layer 5's weights from 
-0.18728866346916 to -0.190047713844574
-0.629362187467422 to -0.632121237842836
-0.204173824869001 to -0.206932875244415
-1.13968600624045 to -1.14244505661586
-0.60241719254097 to -0.605176242916384
-1.00088513561329 to -1.0036441859887
-0.70091982849678 to -0.703678878872194
-0.628473601423111 to -0.631232651798525
-0.238871595941389 to -0.241630646316803
-0.245379707895126 to -0.24813875827054
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.006284325627614 to 0.00352527525220003
-0.393750638457012 to -0.396509688832426
-0.67627133497782 to -0.679030385353234
-0.524765969010067 to -0.527525019385481
Changing layer 0's weights from 
-0.435227188198364 to -0.437986238573778
-0.8071536448882 to -0.809912695263614
-1.04150291058282 to -1.04426196095823
-1.00694366070489 to -1.0097027110803
-0.259076091377533 to -0.261835141752947
-0.941718044607439 to -0.944477094982853
-0.787888321010866 to -0.79064737138628
-0.815342727033891 to -0.818101777409305
-1.05283458801965 to -1.05559363839506
-1.09658464315394 to -1.09934369352936
Changing layer 1's weights from 
-1.0914877307699 to -1.09424678114532
-0.490319046108522 to -0.493078096483936
-0.616021784870425 to -0.618780835245839
-0.625927123157778 to -0.628686173533192
-0.810979905454912 to -0.813738955830326
-0.949217739431658 to -0.951976789807072
-0.67849901589612 to -0.681258066271534
-1.06430204483728 to -1.06706109521269
-0.652639063923158 to -0.655398114298572
-1.01528592678765 to -1.01804497716307
Changing layer 2's weights from 
-0.206097694961823 to -0.208856745337237
-0.760151180355349 to -0.762910230730763
-0.475561889736451 to -0.478320940111865
-0.304361793129242 to -0.307120843504656
-0.481614502994813 to -0.484373553370227
-0.687624129383364 to -0.690383179758778
-0.168811413376129 to -0.171570463751543
-0.24123344096402 to -0.243992491339434
-0.973231735555925 to -0.975990785931339
-0.92827421697835 to -0.931033267353764
Changing layer 3's weights from 
-0.239147993652619 to -0.241907044028033
-0.502701076595582 to -0.505460126970996
-0.260091158478058 to -0.262850208853472
-1.1675303902342 to -1.17028944060962
-0.382767054168976 to -0.38552610454439
-0.513015660373964 to -0.515774710749378
-0.843200507490434 to -0.845959557865848
-0.384687635032929 to -0.387446685408343
-0.716383608906069 to -0.719142659281483
-0.243553730575837 to -0.246312780951251
Changing layer 4's weights from 
-0.342827531425751 to -0.345586581801165
-0.551660331814089 to -0.554419382189503
-0.752137931911745 to -0.754896982287159
-0.465203198520936 to -0.46796224889635
-0.704456004230776 to -0.70721505460619
-0.47299078377942 to -0.475749834154834
-0.477349313823976 to -0.48010836419939
-0.939246836035051 to -0.942005886410465
-0.345550748436249 to -0.348309798811663
-0.777852686970034 to -0.780611737345448
Changing layer 5's weights from 
-0.190047713844574 to -0.192806764219988
-0.632121237842836 to -0.63488028821825
-0.206932875244415 to -0.209691925619829
-1.14244505661586 to -1.14520410699128
-0.605176242916384 to -0.607935293291798
-1.0036441859887 to -1.00640323636412
-0.703678878872194 to -0.706437929247608
-0.631232651798525 to -0.633991702173939
-0.241630646316803 to -0.244389696692217
-0.24813875827054 to -0.250897808645954
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.00352527525220003 to 0.000766224876786055
-0.396509688832426 to -0.39926873920784
-0.679030385353234 to -0.681789435728648
-0.527525019385481 to -0.530284069760895
Changing layer 0's weights from 
-0.437986238573778 to -0.440745288949192
-0.809912695263614 to -0.812671745639028
-1.04426196095823 to -1.04702101133365
-1.0097027110803 to -1.01246176145572
-0.261835141752947 to -0.264594192128361
-0.944477094982853 to -0.947236145358267
-0.79064737138628 to -0.793406421761694
-0.818101777409305 to -0.820860827784719
-1.05559363839506 to -1.05835268877048
-1.09934369352936 to -1.10210274390477
Changing layer 1's weights from 
-1.09424678114532 to -1.09700583152073
-0.493078096483936 to -0.49583714685935
-0.618780835245839 to -0.621539885621253
-0.628686173533192 to -0.631445223908606
-0.813738955830326 to -0.81649800620574
-0.951976789807072 to -0.954735840182486
-0.681258066271534 to -0.684017116646948
-1.06706109521269 to -1.0698201455881
-0.655398114298572 to -0.658157164673986
-1.01804497716307 to -1.02080402753848
Changing layer 2's weights from 
-0.208856745337237 to -0.211615795712651
-0.762910230730763 to -0.765669281106177
-0.478320940111865 to -0.481079990487279
-0.307120843504656 to -0.30987989388007
-0.484373553370227 to -0.487132603745641
-0.690383179758778 to -0.693142230134192
-0.171570463751543 to -0.174329514126957
-0.243992491339434 to -0.246751541714848
-0.975990785931339 to -0.978749836306753
-0.931033267353764 to -0.933792317729178
Changing layer 3's weights from 
-0.241907044028033 to -0.244666094403447
-0.505460126970996 to -0.50821917734641
-0.262850208853472 to -0.265609259228886
-1.17028944060962 to -1.17304849098503
-0.38552610454439 to -0.388285154919804
-0.515774710749378 to -0.518533761124792
-0.845959557865848 to -0.848718608241262
-0.387446685408343 to -0.390205735783757
-0.719142659281483 to -0.721901709656897
-0.246312780951251 to -0.249071831326665
Changing layer 4's weights from 
-0.345586581801165 to -0.348345632176579
-0.554419382189503 to -0.557178432564917
-0.754896982287159 to -0.757656032662573
-0.46796224889635 to -0.470721299271764
-0.70721505460619 to -0.709974104981604
-0.475749834154834 to -0.478508884530248
-0.48010836419939 to -0.482867414574804
-0.942005886410465 to -0.944764936785879
-0.348309798811663 to -0.351068849187077
-0.780611737345448 to -0.783370787720862
Changing layer 5's weights from 
-0.192806764219988 to -0.195565814595402
-0.63488028821825 to -0.637639338593664
-0.209691925619829 to -0.212450975995243
-1.14520410699128 to -1.14796315736669
-0.607935293291798 to -0.610694343667212
-1.00640323636412 to -1.00916228673953
-0.706437929247608 to -0.709196979623022
-0.633991702173939 to -0.636750752549353
-0.244389696692217 to -0.247148747067631
-0.250897808645954 to -0.253656859021368
Trying to learn from memory 4, 0, -0.2
sum 0.0448241585160822 distri -0.049153391141966
Using diff 0.0827715100290276 and condRate 0.166666666666667
Changed category 0 weights from 
0.000766224876786055 to -0.00199282549862792
-0.39926873920784 to -0.402027789583254
-0.681789435728648 to -0.684548486104062
-0.530284069760895 to -0.53304312013631
Changing layer 0's weights from 
-0.440745288949192 to -0.443504339324606
-0.812671745639028 to -0.815430796014442
-1.04702101133365 to -1.04978006170906
-1.01246176145572 to -1.01522081183113
-0.264594192128361 to -0.267353242503775
-0.947236145358267 to -0.949995195733681
-0.793406421761694 to -0.796165472137108
-0.820860827784719 to -0.823619878160133
-1.05835268877048 to -1.06111173914589
-1.10210274390477 to -1.10486179428019
Changing layer 1's weights from 
-1.09700583152073 to -1.09976488189615
-0.49583714685935 to -0.498596197234764
-0.621539885621253 to -0.624298935996667
-0.631445223908606 to -0.63420427428402
-0.81649800620574 to -0.819257056581154
-0.954735840182486 to -0.9574948905579
-0.684017116646948 to -0.686776167022362
-1.0698201455881 to -1.07257919596352
-0.658157164673986 to -0.6609162150494
-1.02080402753848 to -1.02356307791389
Changing layer 2's weights from 
-0.211615795712651 to -0.214374846088065
-0.765669281106177 to -0.768428331481591
-0.481079990487279 to -0.483839040862693
-0.30987989388007 to -0.312638944255484
-0.487132603745641 to -0.489891654121055
-0.693142230134192 to -0.695901280509606
-0.174329514126957 to -0.177088564502371
-0.246751541714848 to -0.249510592090262
-0.978749836306753 to -0.981508886682167
-0.933792317729178 to -0.936551368104592
Changing layer 3's weights from 
-0.244666094403447 to -0.247425144778861
-0.50821917734641 to -0.510978227721824
-0.265609259228886 to -0.2683683096043
-1.17304849098503 to -1.17580754136045
-0.388285154919804 to -0.391044205295218
-0.518533761124792 to -0.521292811500206
-0.848718608241262 to -0.851477658616676
-0.390205735783757 to -0.392964786159171
-0.721901709656897 to -0.724660760032311
-0.249071831326665 to -0.251830881702079
Changing layer 4's weights from 
-0.348345632176579 to -0.351104682551993
-0.557178432564917 to -0.559937482940331
-0.757656032662573 to -0.760415083037987
-0.470721299271764 to -0.473480349647178
-0.709974104981604 to -0.712733155357018
-0.478508884530248 to -0.481267934905662
-0.482867414574804 to -0.485626464950218
-0.944764936785879 to -0.947523987161293
-0.351068849187077 to -0.353827899562491
-0.783370787720862 to -0.786129838096276
Changing layer 5's weights from 
-0.195565814595402 to -0.198324864970816
-0.637639338593664 to -0.640398388969078
-0.212450975995243 to -0.215210026370657
-1.14796315736669 to -1.15072220774211
-0.610694343667212 to -0.613453394042626
-1.00916228673953 to -1.01192133711495
-0.709196979623022 to -0.711956029998436
-0.636750752549353 to -0.639509802924767
-0.247148747067631 to -0.249907797443045
-0.253656859021368 to -0.256415909396782
10/5/2016 1:46:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:46:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:05 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:06 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:07 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:08 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:09 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:10 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:11 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:12 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:13 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:14 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:15 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:16 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:17 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:18 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:19 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:20 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:21 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:22 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:23 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:24 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:34 PMStarting learning phase with deltaScore: -1
Modified index 0's learning in memoryPool to -0.2
Modified index 1's learning in memoryPool to -0.2
Modified index 2's learning in memoryPool to -0.2
Modified index 3's learning in memoryPool to -0.2
Modified index 4's learning in memoryPool to -0.2
Modified index 5's learning in memoryPool to -0.2
Modified index 6's learning in memoryPool to -0.2
Modified index 7's learning in memoryPool to -0.2
Modified index 8's learning in memoryPool to -0.2
Modified index 9's learning in memoryPool to -0.2
Modified index 10's learning in memoryPool to -0.2
Modified index 11's learning in memoryPool to -0.2
Modified index 12's learning in memoryPool to -0.2
Modified index 13's learning in memoryPool to -0.2
Modified index 14's learning in memoryPool to -0.2
Modified index 15's learning in memoryPool to -0.2
Modified index 16's learning in memoryPool to -0.2
Modified index 17's learning in memoryPool to -0.2
Modified index 18's learning in memoryPool to -0.2
Modified index 19's learning in memoryPool to -0.2
Modified index 20's learning in memoryPool to -0.2
Modified index 21's learning in memoryPool to -0.2
Modified index 22's learning in memoryPool to -0.2
Modified index 23's learning in memoryPool to -0.2
Modified index 24's learning in memoryPool to -0.2
Modified index 25's learning in memoryPool to -0.2
Modified index 26's learning in memoryPool to -0.2
10/5/2016 1:47:34 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 5, 0, -0.2
sum 0.0520869665853144 distri -0.0820975515244525
Using diff 0.121162776463438 and condRate 0.166666666666667
Changed category 0 weights from 
-0.00199282549862792 to -0.00603158477425806
-0.402027789583254 to -0.406066548858884
-0.684548486104062 to -0.688587245379692
-0.53304312013631 to -0.53708187941194
Changing layer 0's weights from 
-0.443504339324606 to -0.447543098600236
-0.815430796014442 to -0.819469555290072
-1.04978006170906 to -1.05381882098469
-1.01522081183113 to -1.01925957110676
-0.267353242503775 to -0.271392001779405
-0.949995195733681 to -0.954033955009311
-0.796165472137108 to -0.800204231412738
-0.823619878160133 to -0.827658637435763
-1.06111173914589 to -1.06515049842152
-1.10486179428019 to -1.10890055355582
Changing layer 1's weights from 
-1.09976488189615 to -1.10380364117178
-0.498596197234764 to -0.502634956510394
-0.624298935996667 to -0.628337695272297
-0.63420427428402 to -0.63824303355965
-0.819257056581154 to -0.823295815856784
-0.9574948905579 to -0.96153364983353
-0.686776167022362 to -0.690814926297992
-1.07257919596352 to -1.07661795523915
-0.6609162150494 to -0.66495497432503
-1.02356307791389 to -1.02760183718952
Changing layer 2's weights from 
-0.214374846088065 to -0.218413605363695
-0.768428331481591 to -0.772467090757221
-0.483839040862693 to -0.487877800138323
-0.312638944255484 to -0.316677703531114
-0.489891654121055 to -0.493930413396685
-0.695901280509606 to -0.699940039785236
-0.177088564502371 to -0.181127323778001
-0.249510592090262 to -0.253549351365892
-0.981508886682167 to -0.985547645957797
-0.936551368104592 to -0.940590127380222
Changing layer 3's weights from 
-0.247425144778861 to -0.251463904054491
-0.510978227721824 to -0.515016986997454
-0.2683683096043 to -0.27240706887993
-1.17580754136045 to -1.17984630063608
-0.391044205295218 to -0.395082964570848
-0.521292811500206 to -0.525331570775836
-0.851477658616676 to -0.855516417892306
-0.392964786159171 to -0.397003545434801
-0.724660760032311 to -0.728699519307941
-0.251830881702079 to -0.255869640977709
Changing layer 4's weights from 
-0.351104682551993 to -0.355143441827623
-0.559937482940331 to -0.563976242215961
-0.760415083037987 to -0.764453842313617
-0.473480349647178 to -0.477519108922808
-0.712733155357018 to -0.716771914632648
-0.481267934905662 to -0.485306694181292
-0.485626464950218 to -0.489665224225848
-0.947523987161293 to -0.951562746436923
-0.353827899562491 to -0.357866658838121
-0.786129838096276 to -0.790168597371906
Changing layer 5's weights from 
-0.198324864970816 to -0.202363624246446
-0.640398388969078 to -0.644437148244708
-0.215210026370657 to -0.219248785646287
-1.15072220774211 to -1.15476096701774
-0.613453394042626 to -0.617492153318256
-1.01192133711495 to -1.01596009639058
-0.711956029998436 to -0.715994789274066
-0.639509802924767 to -0.643548562200397
-0.249907797443045 to -0.253946556718675
-0.256415909396782 to -0.260454668672412
Trying to learn from memory 6, 0, -0.2
sum 0.0547729415332752 distri -0.0985215440664436
Using diff 0.1396012502164 and condRate 0.166666666666667
Changed category 0 weights from 
-0.00603158477425806 to -0.0106849598508121
-0.406066548858884 to -0.410719923935438
-0.688587245379692 to -0.693240620456246
-0.53708187941194 to -0.541735254488494
Changing layer 0's weights from 
-0.447543098600236 to -0.45219647367679
-0.819469555290072 to -0.824122930366626
-1.05381882098469 to -1.05847219606125
-1.01925957110676 to -1.02391294618331
-0.271392001779405 to -0.276045376855959
-0.954033955009311 to -0.958687330085865
-0.800204231412738 to -0.804857606489292
-0.827658637435763 to -0.832312012512317
-1.06515049842152 to -1.06980387349807
-1.10890055355582 to -1.11355392863237
Changing layer 1's weights from 
-1.10380364117178 to -1.10845701624833
-0.502634956510394 to -0.507288331586948
-0.628337695272297 to -0.632991070348851
-0.63824303355965 to -0.642896408636204
-0.823295815856784 to -0.827949190933338
-0.96153364983353 to -0.966187024910084
-0.690814926297992 to -0.695468301374546
-1.07661795523915 to -1.0812713303157
-0.66495497432503 to -0.669608349401584
-1.02760183718952 to -1.03225521226608
Changing layer 2's weights from 
-0.218413605363695 to -0.223066980440249
-0.772467090757221 to -0.777120465833775
-0.487877800138323 to -0.492531175214877
-0.316677703531114 to -0.321331078607668
-0.493930413396685 to -0.498583788473239
-0.699940039785236 to -0.70459341486179
-0.181127323778001 to -0.185780698854555
-0.253549351365892 to -0.258202726442446
-0.985547645957797 to -0.990201021034351
-0.940590127380222 to -0.945243502456776
Changing layer 3's weights from 
-0.251463904054491 to -0.256117279131045
-0.515016986997454 to -0.519670362074008
-0.27240706887993 to -0.277060443956484
-1.17984630063608 to -1.18449967571263
-0.395082964570848 to -0.399736339647402
-0.525331570775836 to -0.52998494585239
-0.855516417892306 to -0.86016979296886
-0.397003545434801 to -0.401656920511355
-0.728699519307941 to -0.733352894384495
-0.255869640977709 to -0.260523016054263
Changing layer 4's weights from 
-0.355143441827623 to -0.359796816904177
-0.563976242215961 to -0.568629617292515
-0.764453842313617 to -0.769107217390171
-0.477519108922808 to -0.482172483999362
-0.716771914632648 to -0.721425289709202
-0.485306694181292 to -0.489960069257846
-0.489665224225848 to -0.494318599302402
-0.951562746436923 to -0.956216121513477
-0.357866658838121 to -0.362520033914675
-0.790168597371906 to -0.79482197244846
Changing layer 5's weights from 
-0.202363624246446 to -0.207016999323
-0.644437148244708 to -0.649090523321262
-0.219248785646287 to -0.223902160722841
-1.15476096701774 to -1.15941434209429
-0.617492153318256 to -0.62214552839481
-1.01596009639058 to -1.02061347146713
-0.715994789274066 to -0.72064816435062
-0.643548562200397 to -0.648201937276951
-0.253946556718675 to -0.258599931795229
-0.260454668672412 to -0.265108043748966
Trying to learn from memory 7, 0, -0.2
sum 0.054833225606501 distri -0.0985217210453302
Using diff 0.139646640250206 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0106849598508121 to -0.0153398479285155
-0.410719923935438 to -0.415374812013141
-0.693240620456246 to -0.69789550853395
-0.541735254488494 to -0.546390142566197
Changing layer 0's weights from 
-0.45219647367679 to -0.456851361754493
-0.824122930366626 to -0.82877781844433
-1.05847219606125 to -1.06312708413895
-1.02391294618331 to -1.02856783426102
-0.276045376855959 to -0.280700264933662
-0.958687330085865 to -0.963342218163569
-0.804857606489292 to -0.809512494566996
-0.832312012512317 to -0.836966900590021
-1.06980387349807 to -1.07445876157578
-1.11355392863237 to -1.11820881671007
Changing layer 1's weights from 
-1.10845701624833 to -1.11311190432603
-0.507288331586948 to -0.511943219664652
-0.632991070348851 to -0.637645958426555
-0.642896408636204 to -0.647551296713908
-0.827949190933338 to -0.832604079011042
-0.966187024910084 to -0.970841912987788
-0.695468301374546 to -0.70012318945225
-1.0812713303157 to -1.0859262183934
-0.669608349401584 to -0.674263237479288
-1.03225521226608 to -1.03691010034378
Changing layer 2's weights from 
-0.223066980440249 to -0.227721868517952
-0.777120465833775 to -0.781775353911479
-0.492531175214877 to -0.49718606329258
-0.321331078607668 to -0.325985966685371
-0.498583788473239 to -0.503238676550942
-0.70459341486179 to -0.709248302939494
-0.185780698854555 to -0.190435586932258
-0.258202726442446 to -0.262857614520149
-0.990201021034351 to -0.994855909112055
-0.945243502456776 to -0.94989839053448
Changing layer 3's weights from 
-0.256117279131045 to -0.260772167208748
-0.519670362074008 to -0.524325250151712
-0.277060443956484 to -0.281715332034187
-1.18449967571263 to -1.18915456379033
-0.399736339647402 to -0.404391227725105
-0.52998494585239 to -0.534639833930094
-0.86016979296886 to -0.864824681046564
-0.401656920511355 to -0.406311808589058
-0.733352894384495 to -0.738007782462199
-0.260523016054263 to -0.265177904131966
Changing layer 4's weights from 
-0.359796816904177 to -0.36445170498188
-0.568629617292515 to -0.573284505370219
-0.769107217390171 to -0.773762105467875
-0.482172483999362 to -0.486827372077065
-0.721425289709202 to -0.726080177786906
-0.489960069257846 to -0.494614957335549
-0.494318599302402 to -0.498973487380105
-0.956216121513477 to -0.960871009591181
-0.362520033914675 to -0.367174921992378
-0.79482197244846 to -0.799476860526164
Changing layer 5's weights from 
-0.207016999323 to -0.211671887400703
-0.649090523321262 to -0.653745411398966
-0.223902160722841 to -0.228557048800544
-1.15941434209429 to -1.16406923017199
-0.62214552839481 to -0.626800416472514
-1.02061347146713 to -1.02526835954483
-0.72064816435062 to -0.725303052428324
-0.648201937276951 to -0.652856825354655
-0.258599931795229 to -0.263254819872932
-0.265108043748966 to -0.269762931826669
Trying to learn from memory 8, 0, -0.2
sum 0.054775196371883 distri -0.098521550686083
Using diff 0.139602947964995 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0153398479285155 to -0.0199932795966902
-0.415374812013141 to -0.420028243681316
-0.69789550853395 to -0.702548940202124
-0.546390142566197 to -0.551043574234372
Changing layer 0's weights from 
-0.456851361754493 to -0.461504793422668
-0.82877781844433 to -0.833431250112504
-1.06312708413895 to -1.06778051580712
-1.02856783426102 to -1.03322126592919
-0.280700264933662 to -0.285353696601837
-0.963342218163569 to -0.967995649831743
-0.809512494566996 to -0.81416592623517
-0.836966900590021 to -0.841620332258195
-1.07445876157578 to -1.07911219324395
-1.11820881671007 to -1.12286224837825
Changing layer 1's weights from 
-1.11311190432603 to -1.11776533599421
-0.511943219664652 to -0.516596651332826
-0.637645958426555 to -0.642299390094729
-0.647551296713908 to -0.652204728382082
-0.832604079011042 to -0.837257510679217
-0.970841912987788 to -0.975495344655962
-0.70012318945225 to -0.704776621120425
-1.0859262183934 to -1.09057965006158
-0.674263237479288 to -0.678916669147462
-1.03691010034378 to -1.04156353201196
Changing layer 2's weights from 
-0.227721868517952 to -0.232375300186127
-0.781775353911479 to -0.786428785579653
-0.49718606329258 to -0.501839494960755
-0.325985966685371 to -0.330639398353546
-0.503238676550942 to -0.507892108219117
-0.709248302939494 to -0.713901734607668
-0.190435586932258 to -0.195089018600433
-0.262857614520149 to -0.267511046188324
-0.994855909112055 to -0.999509340780229
-0.94989839053448 to -0.954551822202654
Changing layer 3's weights from 
-0.260772167208748 to -0.265425598876923
-0.524325250151712 to -0.528978681819886
-0.281715332034187 to -0.286368763702362
-1.18915456379033 to -1.19380799545851
-0.404391227725105 to -0.40904465939328
-0.534639833930094 to -0.539293265598269
-0.864824681046564 to -0.869478112714738
-0.406311808589058 to -0.410965240257233
-0.738007782462199 to -0.742661214130373
-0.265177904131966 to -0.269831335800141
Changing layer 4's weights from 
-0.36445170498188 to -0.369105136650055
-0.573284505370219 to -0.577937937038393
-0.773762105467875 to -0.77841553713605
-0.486827372077065 to -0.49148080374524
-0.726080177786906 to -0.73073360945508
-0.494614957335549 to -0.499268389003724
-0.498973487380105 to -0.50362691904828
-0.960871009591181 to -0.965524441259355
-0.367174921992378 to -0.371828353660553
-0.799476860526164 to -0.804130292194338
Changing layer 5's weights from 
-0.211671887400703 to -0.216325319068878
-0.653745411398966 to -0.65839884306714
-0.228557048800544 to -0.233210480468719
-1.16406923017199 to -1.16872266184017
-0.626800416472514 to -0.631453848140689
-1.02526835954483 to -1.02992179121301
-0.725303052428324 to -0.729956484096498
-0.652856825354655 to -0.657510257022829
-0.263254819872932 to -0.267908251541107
-0.269762931826669 to -0.274416363494844
Trying to learn from memory 9, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0199932795966902 to -0.024646256969474
-0.420028243681316 to -0.4246812210541
-0.702548940202124 to -0.707201917574908
-0.551043574234372 to -0.555696551607156
Changing layer 0's weights from 
-0.461504793422668 to -0.466157770795452
-0.833431250112504 to -0.838084227485288
-1.06778051580712 to -1.07243349317991
-1.03322126592919 to -1.03787424330198
-0.285353696601837 to -0.290006673974621
-0.967995649831743 to -0.972648627204527
-0.81416592623517 to -0.818818903607954
-0.841620332258195 to -0.846273309630979
-1.07911219324395 to -1.08376517061674
-1.12286224837825 to -1.12751522575103
Changing layer 1's weights from 
-1.11776533599421 to -1.12241831336699
-0.516596651332826 to -0.52124962870561
-0.642299390094729 to -0.646952367467513
-0.652204728382082 to -0.656857705754866
-0.837257510679217 to -0.841910488052
-0.975495344655962 to -0.980148322028746
-0.704776621120425 to -0.709429598493208
-1.09057965006158 to -1.09523262743436
-0.678916669147462 to -0.683569646520246
-1.04156353201196 to -1.04621650938474
Changing layer 2's weights from 
-0.232375300186127 to -0.237028277558911
-0.786428785579653 to -0.791081762952437
-0.501839494960755 to -0.506492472333539
-0.330639398353546 to -0.33529237572633
-0.507892108219117 to -0.512545085591901
-0.713901734607668 to -0.718554711980452
-0.195089018600433 to -0.199741995973217
-0.267511046188324 to -0.272164023561108
-0.999509340780229 to -1.00416231815301
-0.954551822202654 to -0.959204799575438
Changing layer 3's weights from 
-0.265425598876923 to -0.270078576249707
-0.528978681819886 to -0.53363165919267
-0.286368763702362 to -0.291021741075146
-1.19380799545851 to -1.19846097283129
-0.40904465939328 to -0.413697636766064
-0.539293265598269 to -0.543946242971052
-0.869478112714738 to -0.874131090087522
-0.410965240257233 to -0.415618217630017
-0.742661214130373 to -0.747314191503157
-0.269831335800141 to -0.274484313172925
Changing layer 4's weights from 
-0.369105136650055 to -0.373758114022839
-0.577937937038393 to -0.582590914411177
-0.77841553713605 to -0.783068514508833
-0.49148080374524 to -0.496133781118024
-0.73073360945508 to -0.735386586827864
-0.499268389003724 to -0.503921366376508
-0.50362691904828 to -0.508279896421064
-0.965524441259355 to -0.970177418632139
-0.371828353660553 to -0.376481331033337
-0.804130292194338 to -0.808783269567122
Changing layer 5's weights from 
-0.216325319068878 to -0.220978296441662
-0.65839884306714 to -0.663051820439924
-0.233210480468719 to -0.237863457841503
-1.16872266184017 to -1.17337563921295
-0.631453848140689 to -0.636106825513472
-1.02992179121301 to -1.03457476858579
-0.729956484096498 to -0.734609461469282
-0.657510257022829 to -0.662163234395613
-0.267908251541107 to -0.272561228913891
-0.274416363494844 to -0.279069340867628
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.024646256969474 to -0.0292992343422578
-0.4246812210541 to -0.429334198426884
-0.707201917574908 to -0.711854894947692
-0.555696551607156 to -0.560349528979939
Changing layer 0's weights from 
-0.466157770795452 to -0.470810748168236
-0.838084227485288 to -0.842737204858072
-1.07243349317991 to -1.07708647055269
-1.03787424330198 to -1.04252722067476
-0.290006673974621 to -0.294659651347405
-0.972648627204527 to -0.977301604577311
-0.818818903607954 to -0.823471880980738
-0.846273309630979 to -0.850926287003763
-1.08376517061674 to -1.08841814798952
-1.12751522575103 to -1.13216820312382
Changing layer 1's weights from 
-1.12241831336699 to -1.12707129073978
-0.52124962870561 to -0.525902606078394
-0.646952367467513 to -0.651605344840297
-0.656857705754866 to -0.66151068312765
-0.841910488052 to -0.846563465424784
-0.980148322028746 to -0.98480129940153
-0.709429598493208 to -0.714082575865992
-1.09523262743436 to -1.09988560480715
-0.683569646520246 to -0.68822262389303
-1.04621650938474 to -1.05086948675752
Changing layer 2's weights from 
-0.237028277558911 to -0.241681254931695
-0.791081762952437 to -0.795734740325221
-0.506492472333539 to -0.511145449706323
-0.33529237572633 to -0.339945353099114
-0.512545085591901 to -0.517198062964685
-0.718554711980452 to -0.723207689353236
-0.199741995973217 to -0.204394973346001
-0.272164023561108 to -0.276817000933892
-1.00416231815301 to -1.0088152955258
-0.959204799575438 to -0.963857776948222
Changing layer 3's weights from 
-0.270078576249707 to -0.274731553622491
-0.53363165919267 to -0.538284636565454
-0.291021741075146 to -0.29567471844793
-1.19846097283129 to -1.20311395020408
-0.413697636766064 to -0.418350614138848
-0.543946242971052 to -0.548599220343836
-0.874131090087522 to -0.878784067460306
-0.415618217630017 to -0.420271195002801
-0.747314191503157 to -0.751967168875941
-0.274484313172925 to -0.279137290545709
Changing layer 4's weights from 
-0.373758114022839 to -0.378411091395623
-0.582590914411177 to -0.587243891783961
-0.783068514508833 to -0.787721491881617
-0.496133781118024 to -0.500786758490808
-0.735386586827864 to -0.740039564200648
-0.503921366376508 to -0.508574343749292
-0.508279896421064 to -0.512932873793848
-0.970177418632139 to -0.974830396004923
-0.376481331033337 to -0.381134308406121
-0.808783269567122 to -0.813436246939906
Changing layer 5's weights from 
-0.220978296441662 to -0.225631273814446
-0.663051820439924 to -0.667704797812708
-0.237863457841503 to -0.242516435214287
-1.17337563921295 to -1.17802861658574
-0.636106825513472 to -0.640759802886256
-1.03457476858579 to -1.03922774595858
-0.734609461469282 to -0.739262438842066
-0.662163234395613 to -0.666816211768397
-0.272561228913891 to -0.277214206286675
-0.279069340867628 to -0.283722318240412
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0292992343422578 to -0.0339522117150416
-0.429334198426884 to -0.433987175799668
-0.711854894947692 to -0.716507872320476
-0.560349528979939 to -0.565002506352723
Changing layer 0's weights from 
-0.470810748168236 to -0.47546372554102
-0.842737204858072 to -0.847390182230856
-1.07708647055269 to -1.08173944792547
-1.04252722067476 to -1.04718019804754
-0.294659651347405 to -0.299312628720189
-0.977301604577311 to -0.981954581950095
-0.823471880980738 to -0.828124858353522
-0.850926287003763 to -0.855579264376547
-1.08841814798952 to -1.0930711253623
-1.13216820312382 to -1.1368211804966
Changing layer 1's weights from 
-1.12707129073978 to -1.13172426811256
-0.525902606078394 to -0.530555583451178
-0.651605344840297 to -0.656258322213081
-0.66151068312765 to -0.666163660500434
-0.846563465424784 to -0.851216442797568
-0.98480129940153 to -0.989454276774314
-0.714082575865992 to -0.718735553238776
-1.09988560480715 to -1.10453858217993
-0.68822262389303 to -0.692875601265814
-1.05086948675752 to -1.05552246413031
Changing layer 2's weights from 
-0.241681254931695 to -0.246334232304479
-0.795734740325221 to -0.800387717698005
-0.511145449706323 to -0.515798427079107
-0.339945353099114 to -0.344598330471898
-0.517198062964685 to -0.521851040337469
-0.723207689353236 to -0.72786066672602
-0.204394973346001 to -0.209047950718785
-0.276817000933892 to -0.281469978306676
-1.0088152955258 to -1.01346827289858
-0.963857776948222 to -0.968510754321006
Changing layer 3's weights from 
-0.274731553622491 to -0.279384530995275
-0.538284636565454 to -0.542937613938238
-0.29567471844793 to -0.300327695820714
-1.20311395020408 to -1.20776692757686
-0.418350614138848 to -0.423003591511632
-0.548599220343836 to -0.55325219771662
-0.878784067460306 to -0.88343704483309
-0.420271195002801 to -0.424924172375585
-0.751967168875941 to -0.756620146248725
-0.279137290545709 to -0.283790267918493
Changing layer 4's weights from 
-0.378411091395623 to -0.383064068768407
-0.587243891783961 to -0.591896869156745
-0.787721491881617 to -0.792374469254401
-0.500786758490808 to -0.505439735863591
-0.740039564200648 to -0.744692541573432
-0.508574343749292 to -0.513227321122075
-0.512932873793848 to -0.517585851166631
-0.974830396004923 to -0.979483373377707
-0.381134308406121 to -0.385787285778905
-0.813436246939906 to -0.81808922431269
Changing layer 5's weights from 
-0.225631273814446 to -0.23028425118723
-0.667704797812708 to -0.672357775185492
-0.242516435214287 to -0.247169412587071
-1.17802861658574 to -1.18268159395852
-0.640759802886256 to -0.64541278025904
-1.03922774595858 to -1.04388072333136
-0.739262438842066 to -0.74391541621485
-0.666816211768397 to -0.671469189141181
-0.277214206286675 to -0.281867183659459
-0.283722318240412 to -0.288375295613196
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0339522117150416 to -0.0386051890878254
-0.433987175799668 to -0.438640153172451
-0.716507872320476 to -0.72116084969326
-0.565002506352723 to -0.569655483725507
Changing layer 0's weights from 
-0.47546372554102 to -0.480116702913803
-0.847390182230856 to -0.85204315960364
-1.08173944792547 to -1.08639242529826
-1.04718019804754 to -1.05183317542033
-0.299312628720189 to -0.303965606092972
-0.981954581950095 to -0.986607559322879
-0.828124858353522 to -0.832777835726306
-0.855579264376547 to -0.860232241749331
-1.0930711253623 to -1.09772410273509
-1.1368211804966 to -1.14147415786938
Changing layer 1's weights from 
-1.13172426811256 to -1.13637724548534
-0.530555583451178 to -0.535208560823961
-0.656258322213081 to -0.660911299585865
-0.666163660500434 to -0.670816637873218
-0.851216442797568 to -0.855869420170352
-0.989454276774314 to -0.994107254147098
-0.718735553238776 to -0.72338853061156
-1.10453858217993 to -1.10919155955271
-0.692875601265814 to -0.697528578638598
-1.05552246413031 to -1.06017544150309
Changing layer 2's weights from 
-0.246334232304479 to -0.250987209677262
-0.800387717698005 to -0.805040695070789
-0.515798427079107 to -0.52045140445189
-0.344598330471898 to -0.349251307844681
-0.521851040337469 to -0.526504017710252
-0.72786066672602 to -0.732513644098804
-0.209047950718785 to -0.213700928091568
-0.281469978306676 to -0.286122955679459
-1.01346827289858 to -1.01812125027136
-0.968510754321006 to -0.97316373169379
Changing layer 3's weights from 
-0.279384530995275 to -0.284037508368058
-0.542937613938238 to -0.547590591311022
-0.300327695820714 to -0.304980673193497
-1.20776692757686 to -1.21241990494964
-0.423003591511632 to -0.427656568884415
-0.55325219771662 to -0.557905175089404
-0.88343704483309 to -0.888090022205874
-0.424924172375585 to -0.429577149748368
-0.756620146248725 to -0.761273123621509
-0.283790267918493 to -0.288443245291276
Changing layer 4's weights from 
-0.383064068768407 to -0.38771704614119
-0.591896869156745 to -0.596549846529529
-0.792374469254401 to -0.797027446627185
-0.505439735863591 to -0.510092713236375
-0.744692541573432 to -0.749345518946216
-0.513227321122075 to -0.517880298494859
-0.517585851166631 to -0.522238828539415
-0.979483373377707 to -0.984136350750491
-0.385787285778905 to -0.390440263151688
-0.81808922431269 to -0.822742201685474
Changing layer 5's weights from 
-0.23028425118723 to -0.234937228560013
-0.672357775185492 to -0.677010752558276
-0.247169412587071 to -0.251822389959854
-1.18268159395852 to -1.1873345713313
-0.64541278025904 to -0.650065757631824
-1.04388072333136 to -1.04853370070414
-0.74391541621485 to -0.748568393587634
-0.671469189141181 to -0.676122166513965
-0.281867183659459 to -0.286520161032242
-0.288375295613196 to -0.293028272985979
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0386051890878254 to -0.0432581664606092
-0.438640153172451 to -0.443293130545235
-0.72116084969326 to -0.725813827066043
-0.569655483725507 to -0.574308461098291
Changing layer 0's weights from 
-0.480116702913803 to -0.484769680286587
-0.85204315960364 to -0.856696136976423
-1.08639242529826 to -1.09104540267104
-1.05183317542033 to -1.05648615279311
-0.303965606092972 to -0.308618583465756
-0.986607559322879 to -0.991260536695663
-0.832777835726306 to -0.83743081309909
-0.860232241749331 to -0.864885219122114
-1.09772410273509 to -1.10237708010787
-1.14147415786938 to -1.14612713524217
Changing layer 1's weights from 
-1.13637724548534 to -1.14103022285813
-0.535208560823961 to -0.539861538196745
-0.660911299585865 to -0.665564276958649
-0.670816637873218 to -0.675469615246001
-0.855869420170352 to -0.860522397543136
-0.994107254147098 to -0.998760231519881
-0.72338853061156 to -0.728041507984344
-1.10919155955271 to -1.1138445369255
-0.697528578638598 to -0.702181556011381
-1.06017544150309 to -1.06482841887588
Changing layer 2's weights from 
-0.250987209677262 to -0.255640187050046
-0.805040695070789 to -0.809693672443572
-0.52045140445189 to -0.525104381824674
-0.349251307844681 to -0.353904285217465
-0.526504017710252 to -0.531156995083036
-0.732513644098804 to -0.737166621471588
-0.213700928091568 to -0.218353905464352
-0.286122955679459 to -0.290775933052243
-1.01812125027136 to -1.02277422764415
-0.97316373169379 to -0.977816709066573
Changing layer 3's weights from 
-0.284037508368058 to -0.288690485740842
-0.547590591311022 to -0.552243568683805
-0.304980673193497 to -0.309633650566281
-1.21241990494964 to -1.21707288232243
-0.427656568884415 to -0.432309546257199
-0.557905175089404 to -0.562558152462188
-0.888090022205874 to -0.892742999578657
-0.429577149748368 to -0.434230127121152
-0.761273123621509 to -0.765926100994292
-0.288443245291276 to -0.29309622266406
Changing layer 4's weights from 
-0.38771704614119 to -0.392370023513974
-0.596549846529529 to -0.601202823902312
-0.797027446627185 to -0.801680423999969
-0.510092713236375 to -0.514745690609159
-0.749345518946216 to -0.753998496319
-0.517880298494859 to -0.522533275867643
-0.522238828539415 to -0.526891805912199
-0.984136350750491 to -0.988789328123274
-0.390440263151688 to -0.395093240524472
-0.822742201685474 to -0.827395179058257
Changing layer 5's weights from 
-0.234937228560013 to -0.239590205932797
-0.677010752558276 to -0.68166372993106
-0.251822389959854 to -0.256475367332638
-1.1873345713313 to -1.19198754870409
-0.650065757631824 to -0.654718735004608
-1.04853370070414 to -1.05318667807693
-0.748568393587634 to -0.753221370960418
-0.676122166513965 to -0.680775143886748
-0.286520161032242 to -0.291173138405026
-0.293028272985979 to -0.297681250358763
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0432581664606092 to -0.0479111438333931
-0.443293130545235 to -0.447946107918019
-0.725813827066043 to -0.730466804438827
-0.574308461098291 to -0.578961438471075
Changing layer 0's weights from 
-0.484769680286587 to -0.489422657659371
-0.856696136976423 to -0.861349114349207
-1.09104540267104 to -1.09569838004383
-1.05648615279311 to -1.06113913016589
-0.308618583465756 to -0.31327156083854
-0.991260536695663 to -0.995913514068446
-0.83743081309909 to -0.842083790471873
-0.864885219122114 to -0.869538196494898
-1.10237708010787 to -1.10703005748065
-1.14612713524217 to -1.15078011261495
Changing layer 1's weights from 
-1.14103022285813 to -1.14568320023091
-0.539861538196745 to -0.544514515569529
-0.665564276958649 to -0.670217254331432
-0.675469615246001 to -0.680122592618785
-0.860522397543136 to -0.865175374915919
-0.998760231519881 to -1.00341320889267
-0.728041507984344 to -0.732694485357127
-1.1138445369255 to -1.11849751429828
-0.702181556011381 to -0.706834533384165
-1.06482841887588 to -1.06948139624866
Changing layer 2's weights from 
-0.255640187050046 to -0.26029316442283
-0.809693672443572 to -0.814346649816356
-0.525104381824674 to -0.529757359197458
-0.353904285217465 to -0.358557262590249
-0.531156995083036 to -0.53580997245582
-0.737166621471588 to -0.741819598844371
-0.218353905464352 to -0.223006882837136
-0.290775933052243 to -0.295428910425027
-1.02277422764415 to -1.02742720501693
-0.977816709066573 to -0.982469686439357
Changing layer 3's weights from 
-0.288690485740842 to -0.293343463113626
-0.552243568683805 to -0.556896546056589
-0.309633650566281 to -0.314286627939065
-1.21707288232243 to -1.22172585969521
-0.432309546257199 to -0.436962523629983
-0.562558152462188 to -0.567211129834971
-0.892742999578657 to -0.897395976951441
-0.434230127121152 to -0.438883104493936
-0.765926100994292 to -0.770579078367076
-0.29309622266406 to -0.297749200036844
Changing layer 4's weights from 
-0.392370023513974 to -0.397023000886758
-0.601202823902312 to -0.605855801275096
-0.801680423999969 to -0.806333401372752
-0.514745690609159 to -0.519398667981943
-0.753998496319 to -0.758651473691783
-0.522533275867643 to -0.527186253240427
-0.526891805912199 to -0.531544783284983
-0.988789328123274 to -0.993442305496058
-0.395093240524472 to -0.399746217897256
-0.827395179058257 to -0.832048156431041
Changing layer 5's weights from 
-0.239590205932797 to -0.244243183305581
-0.68166372993106 to -0.686316707303843
-0.256475367332638 to -0.261128344705422
-1.19198754870409 to -1.19664052607687
-0.654718735004608 to -0.659371712377391
-1.05318667807693 to -1.05783965544971
-0.753221370960418 to -0.757874348333201
-0.680775143886748 to -0.685428121259532
-0.291173138405026 to -0.29582611577781
-0.297681250358763 to -0.302334227731547
10/5/2016 1:47:34 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0479111438333931 to -0.0525641212061769
-0.447946107918019 to -0.452599085290803
-0.730466804438827 to -0.735119781811611
-0.578961438471075 to -0.583614415843858
Changing layer 0's weights from 
-0.489422657659371 to -0.494075635032155
-0.861349114349207 to -0.866002091721991
-1.09569838004383 to -1.10035135741661
-1.06113913016589 to -1.06579210753868
-0.31327156083854 to -0.317924538211324
-0.995913514068446 to -1.00056649144123
-0.842083790471873 to -0.846736767844657
-0.869538196494898 to -0.874191173867682
-1.10703005748065 to -1.11168303485344
-1.15078011261495 to -1.15543308998773
Changing layer 1's weights from 
-1.14568320023091 to -1.15033617760369
-0.544514515569529 to -0.549167492942313
-0.670217254331432 to -0.674870231704216
-0.680122592618785 to -0.684775569991569
-0.865175374915919 to -0.869828352288703
-1.00341320889267 to -1.00806618626545
-0.732694485357127 to -0.737347462729911
-1.11849751429828 to -1.12315049167107
-0.706834533384165 to -0.711487510756949
-1.06948139624866 to -1.07413437362144
Changing layer 2's weights from 
-0.26029316442283 to -0.264946141795614
-0.814346649816356 to -0.81899962718914
-0.529757359197458 to -0.534410336570242
-0.358557262590249 to -0.363210239963033
-0.53580997245582 to -0.540462949828604
-0.741819598844371 to -0.746472576217155
-0.223006882837136 to -0.22765986020992
-0.295428910425027 to -0.300081887797811
-1.02742720501693 to -1.03208018238972
-0.982469686439357 to -0.987122663812141
Changing layer 3's weights from 
-0.293343463113626 to -0.29799644048641
-0.556896546056589 to -0.561549523429373
-0.314286627939065 to -0.318939605311849
-1.22172585969521 to -1.22637883706799
-0.436962523629983 to -0.441615501002767
-0.567211129834971 to -0.571864107207755
-0.897395976951441 to -0.902048954324225
-0.438883104493936 to -0.44353608186672
-0.770579078367076 to -0.77523205573986
-0.297749200036844 to -0.302402177409628
Changing layer 4's weights from 
-0.397023000886758 to -0.401675978259542
-0.605855801275096 to -0.61050877864788
-0.806333401372752 to -0.810986378745536
-0.519398667981943 to -0.524051645354727
-0.758651473691783 to -0.763304451064567
-0.527186253240427 to -0.531839230613211
-0.531544783284983 to -0.536197760657767
-0.993442305496058 to -0.998095282868842
-0.399746217897256 to -0.40439919527004
-0.832048156431041 to -0.836701133803825
Changing layer 5's weights from 
-0.244243183305581 to -0.248896160678365
-0.686316707303843 to -0.690969684676627
-0.261128344705422 to -0.265781322078206
-1.19664052607687 to -1.20129350344965
-0.659371712377391 to -0.664024689750175
-1.05783965544971 to -1.06249263282249
-0.757874348333201 to -0.762527325705985
-0.685428121259532 to -0.690081098632316
-0.29582611577781 to -0.300479093150594
-0.302334227731547 to -0.306987205104331
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0525641212061769 to -0.0572170985789607
-0.452599085290803 to -0.457252062663587
-0.735119781811611 to -0.739772759184395
-0.583614415843858 to -0.588267393216642
Changing layer 0's weights from 
-0.494075635032155 to -0.498728612404939
-0.866002091721991 to -0.870655069094775
-1.10035135741661 to -1.10500433478939
-1.06579210753868 to -1.07044508491146
-0.317924538211324 to -0.322577515584108
-1.00056649144123 to -1.00521946881401
-0.846736767844657 to -0.851389745217441
-0.874191173867682 to -0.878844151240466
-1.11168303485344 to -1.11633601222622
-1.15543308998773 to -1.16008606736052
Changing layer 1's weights from 
-1.15033617760369 to -1.15498915497648
-0.549167492942313 to -0.553820470315097
-0.674870231704216 to -0.679523209077
-0.684775569991569 to -0.689428547364353
-0.869828352288703 to -0.874481329661487
-1.00806618626545 to -1.01271916363823
-0.737347462729911 to -0.742000440102695
-1.12315049167107 to -1.12780346904385
-0.711487510756949 to -0.716140488129733
-1.07413437362144 to -1.07878735099423
Changing layer 2's weights from 
-0.264946141795614 to -0.269599119168398
-0.81899962718914 to -0.823652604561924
-0.534410336570242 to -0.539063313943026
-0.363210239963033 to -0.367863217335817
-0.540462949828604 to -0.545115927201388
-0.746472576217155 to -0.751125553589939
-0.22765986020992 to -0.232312837582704
-0.300081887797811 to -0.304734865170595
-1.03208018238972 to -1.0367331597625
-0.987122663812141 to -0.991775641184925
Changing layer 3's weights from 
-0.29799644048641 to -0.302649417859194
-0.561549523429373 to -0.566202500802157
-0.318939605311849 to -0.323592582684633
-1.22637883706799 to -1.23103181444078
-0.441615501002767 to -0.446268478375551
-0.571864107207755 to -0.576517084580539
-0.902048954324225 to -0.906701931697009
-0.44353608186672 to -0.448189059239504
-0.77523205573986 to -0.779885033112644
-0.302402177409628 to -0.307055154782412
Changing layer 4's weights from 
-0.401675978259542 to -0.406328955632326
-0.61050877864788 to -0.615161756020664
-0.810986378745536 to -0.81563935611832
-0.524051645354727 to -0.528704622727511
-0.763304451064567 to -0.767957428437351
-0.531839230613211 to -0.536492207985995
-0.536197760657767 to -0.540850738030551
-0.998095282868842 to -1.00274826024163
-0.40439919527004 to -0.409052172642824
-0.836701133803825 to -0.841354111176609
Changing layer 5's weights from 
-0.248896160678365 to -0.253549138051149
-0.690969684676627 to -0.695622662049411
-0.265781322078206 to -0.27043429945099
-1.20129350344965 to -1.20594648082244
-0.664024689750175 to -0.668677667122959
-1.06249263282249 to -1.06714561019528
-0.762527325705985 to -0.767180303078769
-0.690081098632316 to -0.6947340760051
-0.300479093150594 to -0.305132070523378
-0.306987205104331 to -0.311640182477115
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0572170985789607 to -0.0618700759517445
-0.457252062663587 to -0.46190504003637
-0.739772759184395 to -0.744425736557179
-0.588267393216642 to -0.592920370589426
Changing layer 0's weights from 
-0.498728612404939 to -0.503381589777722
-0.870655069094775 to -0.875308046467559
-1.10500433478939 to -1.10965731216218
-1.07044508491146 to -1.07509806228425
-0.322577515584108 to -0.327230492956891
-1.00521946881401 to -1.0098724461868
-0.851389745217441 to -0.856042722590225
-0.878844151240466 to -0.88349712861325
-1.11633601222622 to -1.120988989599
-1.16008606736052 to -1.1647390447333
Changing layer 1's weights from 
-1.15498915497648 to -1.15964213234926
-0.553820470315097 to -0.55847344768788
-0.679523209077 to -0.684176186449784
-0.689428547364353 to -0.694081524737137
-0.874481329661487 to -0.879134307034271
-1.01271916363823 to -1.01737214101102
-0.742000440102695 to -0.746653417475479
-1.12780346904385 to -1.13245644641663
-0.716140488129733 to -0.720793465502517
-1.07878735099423 to -1.08344032836701
Changing layer 2's weights from 
-0.269599119168398 to -0.274252096541181
-0.823652604561924 to -0.828305581934708
-0.539063313943026 to -0.543716291315809
-0.367863217335817 to -0.3725161947086
-0.545115927201388 to -0.549768904574171
-0.751125553589939 to -0.755778530962723
-0.232312837582704 to -0.236965814955487
-0.304734865170595 to -0.309387842543378
-1.0367331597625 to -1.04138613713528
-0.991775641184925 to -0.996428618557709
Changing layer 3's weights from 
-0.302649417859194 to -0.307302395231977
-0.566202500802157 to -0.570855478174941
-0.323592582684633 to -0.328245560057416
-1.23103181444078 to -1.23568479181356
-0.446268478375551 to -0.450921455748334
-0.576517084580539 to -0.581170061953323
-0.906701931697009 to -0.911354909069793
-0.448189059239504 to -0.452842036612287
-0.779885033112644 to -0.784538010485428
-0.307055154782412 to -0.311708132155195
Changing layer 4's weights from 
-0.406328955632326 to -0.410981933005109
-0.615161756020664 to -0.619814733393448
-0.81563935611832 to -0.820292333491104
-0.528704622727511 to -0.533357600100294
-0.767957428437351 to -0.772610405810135
-0.536492207985995 to -0.541145185358778
-0.540850738030551 to -0.545503715403334
-1.00274826024163 to -1.00740123761441
-0.409052172642824 to -0.413705150015607
-0.841354111176609 to -0.846007088549393
Changing layer 5's weights from 
-0.253549138051149 to -0.258202115423932
-0.695622662049411 to -0.700275639422195
-0.27043429945099 to -0.275087276823773
-1.20594648082244 to -1.21059945819522
-0.668677667122959 to -0.673330644495743
-1.06714561019528 to -1.07179858756806
-0.767180303078769 to -0.771833280451553
-0.6947340760051 to -0.699387053377884
-0.305132070523378 to -0.309785047896161
-0.311640182477115 to -0.316293159849898
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0618700759517445 to -0.0665230533245283
-0.46190504003637 to -0.466558017409154
-0.744425736557179 to -0.749078713929963
-0.592920370589426 to -0.59757334796221
Changing layer 0's weights from 
-0.503381589777722 to -0.508034567150506
-0.875308046467559 to -0.879961023840342
-1.10965731216218 to -1.11431028953496
-1.07509806228425 to -1.07975103965703
-0.327230492956891 to -0.331883470329675
-1.0098724461868 to -1.01452542355958
-0.856042722590225 to -0.860695699963009
-0.88349712861325 to -0.888150105986034
-1.120988989599 to -1.12564196697179
-1.1647390447333 to -1.16939202210609
Changing layer 1's weights from 
-1.15964213234926 to -1.16429510972205
-0.55847344768788 to -0.563126425060664
-0.684176186449784 to -0.688829163822568
-0.694081524737137 to -0.698734502109921
-0.879134307034271 to -0.883787284407055
-1.01737214101102 to -1.0220251183838
-0.746653417475479 to -0.751306394848263
-1.13245644641663 to -1.13710942378942
-0.720793465502517 to -0.7254464428753
-1.08344032836701 to -1.08809330573979
Changing layer 2's weights from 
-0.274252096541181 to -0.278905073913965
-0.828305581934708 to -0.832958559307492
-0.543716291315809 to -0.548369268688593
-0.3725161947086 to -0.377169172081384
-0.549768904574171 to -0.554421881946955
-0.755778530962723 to -0.760431508335507
-0.236965814955487 to -0.241618792328271
-0.309387842543378 to -0.314040819916162
-1.04138613713528 to -1.04603911450807
-0.996428618557709 to -1.00108159593049
Changing layer 3's weights from 
-0.307302395231977 to -0.311955372604761
-0.570855478174941 to -0.575508455547724
-0.328245560057416 to -0.3328985374302
-1.23568479181356 to -1.24033776918635
-0.450921455748334 to -0.455574433121118
-0.581170061953323 to -0.585823039326107
-0.911354909069793 to -0.916007886442576
-0.452842036612287 to -0.457495013985071
-0.784538010485428 to -0.789190987858212
-0.311708132155195 to -0.316361109527979
Changing layer 4's weights from 
-0.410981933005109 to -0.415634910377893
-0.619814733393448 to -0.624467710766231
-0.820292333491104 to -0.824945310863888
-0.533357600100294 to -0.538010577473078
-0.772610405810135 to -0.777263383182919
-0.541145185358778 to -0.545798162731562
-0.545503715403334 to -0.550156692776118
-1.00740123761441 to -1.01205421498719
-0.413705150015607 to -0.418358127388391
-0.846007088549393 to -0.850660065922176
Changing layer 5's weights from 
-0.258202115423932 to -0.262855092796716
-0.700275639422195 to -0.704928616794979
-0.275087276823773 to -0.279740254196557
-1.21059945819522 to -1.21525243556801
-0.673330644495743 to -0.677983621868527
-1.07179858756806 to -1.07645156494084
-0.771833280451553 to -0.776486257824337
-0.699387053377884 to -0.704040030750667
-0.309785047896161 to -0.314438025268945
-0.316293159849898 to -0.320946137222682
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0665230533245283 to -0.0711760306973121
-0.466558017409154 to -0.471210994781938
-0.749078713929963 to -0.753731691302746
-0.59757334796221 to -0.602226325334994
Changing layer 0's weights from 
-0.508034567150506 to -0.51268754452329
-0.879961023840342 to -0.884614001213126
-1.11431028953496 to -1.11896326690774
-1.07975103965703 to -1.08440401702981
-0.331883470329675 to -0.336536447702459
-1.01452542355958 to -1.01917840093236
-0.860695699963009 to -0.865348677335792
-0.888150105986034 to -0.892803083358817
-1.12564196697179 to -1.13029494434457
-1.16939202210609 to -1.17404499947887
Changing layer 1's weights from 
-1.16429510972205 to -1.16894808709483
-0.563126425060664 to -0.567779402433448
-0.688829163822568 to -0.693482141195351
-0.698734502109921 to -0.703387479482704
-0.883787284407055 to -0.888440261779838
-1.0220251183838 to -1.02667809575658
-0.751306394848263 to -0.755959372221046
-1.13710942378942 to -1.1417624011622
-0.7254464428753 to -0.730099420248084
-1.08809330573979 to -1.09274628311258
Changing layer 2's weights from 
-0.278905073913965 to -0.283558051286749
-0.832958559307492 to -0.837611536680275
-0.548369268688593 to -0.553022246061377
-0.377169172081384 to -0.381822149454168
-0.554421881946955 to -0.559074859319739
-0.760431508335507 to -0.76508448570829
-0.241618792328271 to -0.246271769701055
-0.314040819916162 to -0.318693797288946
-1.04603911450807 to -1.05069209188085
-1.00108159593049 to -1.00573457330328
Changing layer 3's weights from 
-0.311955372604761 to -0.316608349977545
-0.575508455547724 to -0.580161432920508
-0.3328985374302 to -0.337551514802984
-1.24033776918635 to -1.24499074655913
-0.455574433121118 to -0.460227410493902
-0.585823039326107 to -0.59047601669889
-0.916007886442576 to -0.92066086381536
-0.457495013985071 to -0.462147991357855
-0.789190987858212 to -0.793843965230995
-0.316361109527979 to -0.321014086900763
Changing layer 4's weights from 
-0.415634910377893 to -0.420287887750677
-0.624467710766231 to -0.629120688139015
-0.824945310863888 to -0.829598288236671
-0.538010577473078 to -0.542663554845862
-0.777263383182919 to -0.781916360555702
-0.545798162731562 to -0.550451140104346
-0.550156692776118 to -0.554809670148902
-1.01205421498719 to -1.01670719235998
-0.418358127388391 to -0.423011104761175
-0.850660065922176 to -0.85531304329496
Changing layer 5's weights from 
-0.262855092796716 to -0.2675080701695
-0.704928616794979 to -0.709581594167762
-0.279740254196557 to -0.284393231569341
-1.21525243556801 to -1.21990541294079
-0.677983621868527 to -0.68263659924131
-1.07645156494084 to -1.08110454231363
-0.776486257824337 to -0.78113923519712
-0.704040030750667 to -0.708693008123451
-0.314438025268945 to -0.319091002641729
-0.320946137222682 to -0.325599114595466
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0711760306973121 to -0.0758290080700959
-0.471210994781938 to -0.475863972154722
-0.753731691302746 to -0.75838466867553
-0.602226325334994 to -0.606879302707777
Changing layer 0's weights from 
-0.51268754452329 to -0.517340521896074
-0.884614001213126 to -0.88926697858591
-1.11896326690774 to -1.12361624428053
-1.08440401702981 to -1.0890569944026
-0.336536447702459 to -0.341189425075243
-1.01917840093236 to -1.02383137830515
-0.865348677335792 to -0.870001654708576
-0.892803083358817 to -0.897456060731601
-1.13029494434457 to -1.13494792171736
-1.17404499947887 to -1.17869797685165
Changing layer 1's weights from 
-1.16894808709483 to -1.17360106446761
-0.567779402433448 to -0.572432379806232
-0.693482141195351 to -0.698135118568135
-0.703387479482704 to -0.708040456855488
-0.888440261779838 to -0.893093239152622
-1.02667809575658 to -1.03133107312937
-0.755959372221046 to -0.76061234959383
-1.1417624011622 to -1.14641537853498
-0.730099420248084 to -0.734752397620868
-1.09274628311258 to -1.09739926048536
Changing layer 2's weights from 
-0.283558051286749 to -0.288211028659533
-0.837611536680275 to -0.842264514053059
-0.553022246061377 to -0.557675223434161
-0.381822149454168 to -0.386475126826952
-0.559074859319739 to -0.563727836692523
-0.76508448570829 to -0.769737463081074
-0.246271769701055 to -0.250924747073839
-0.318693797288946 to -0.32334677466173
-1.05069209188085 to -1.05534506925363
-1.00573457330328 to -1.01038755067606
Changing layer 3's weights from 
-0.316608349977545 to -0.321261327350329
-0.580161432920508 to -0.584814410293292
-0.337551514802984 to -0.342204492175768
-1.24499074655913 to -1.24964372393191
-0.460227410493902 to -0.464880387866686
-0.59047601669889 to -0.595128994071674
-0.92066086381536 to -0.925313841188144
-0.462147991357855 to -0.466800968730639
-0.793843965230995 to -0.798496942603779
-0.321014086900763 to -0.325667064273547
Changing layer 4's weights from 
-0.420287887750677 to -0.424940865123461
-0.629120688139015 to -0.633773665511799
-0.829598288236671 to -0.834251265609455
-0.542663554845862 to -0.547316532218646
-0.781916360555702 to -0.786569337928486
-0.550451140104346 to -0.55510411747713
-0.554809670148902 to -0.559462647521686
-1.01670719235998 to -1.02136016973276
-0.423011104761175 to -0.427664082133959
-0.85531304329496 to -0.859966020667744
Changing layer 5's weights from 
-0.2675080701695 to -0.272161047542284
-0.709581594167762 to -0.714234571540546
-0.284393231569341 to -0.289046208942125
-1.21990541294079 to -1.22455839031357
-0.68263659924131 to -0.687289576614094
-1.08110454231363 to -1.08575751968641
-0.78113923519712 to -0.785792212569904
-0.708693008123451 to -0.713345985496235
-0.319091002641729 to -0.323743980014513
-0.325599114595466 to -0.33025209196825
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0758290080700959 to -0.0804819854428797
-0.475863972154722 to -0.480516949527506
-0.75838466867553 to -0.763037646048314
-0.606879302707777 to -0.611532280080561
Changing layer 0's weights from 
-0.517340521896074 to -0.521993499268858
-0.88926697858591 to -0.893919955958694
-1.12361624428053 to -1.12826922165331
-1.0890569944026 to -1.09370997177538
-0.341189425075243 to -0.345842402448027
-1.02383137830515 to -1.02848435567793
-0.870001654708576 to -0.87465463208136
-0.897456060731601 to -0.902109038104385
-1.13494792171736 to -1.13960089909014
-1.17869797685165 to -1.18335095422444
Changing layer 1's weights from 
-1.17360106446761 to -1.1782540418404
-0.572432379806232 to -0.577085357179016
-0.698135118568135 to -0.702788095940919
-0.708040456855488 to -0.712693434228272
-0.893093239152622 to -0.897746216525406
-1.03133107312937 to -1.03598405050215
-0.76061234959383 to -0.765265326966614
-1.14641537853498 to -1.15106835590777
-0.734752397620868 to -0.739405374993652
-1.09739926048536 to -1.10205223785814
Changing layer 2's weights from 
-0.288211028659533 to -0.292864006032317
-0.842264514053059 to -0.846917491425843
-0.557675223434161 to -0.562328200806945
-0.386475126826952 to -0.391128104199736
-0.563727836692523 to -0.568380814065307
-0.769737463081074 to -0.774390440453858
-0.250924747073839 to -0.255577724446623
-0.32334677466173 to -0.327999752034514
-1.05534506925363 to -1.05999804662642
-1.01038755067606 to -1.01504052804884
Changing layer 3's weights from 
-0.321261327350329 to -0.325914304723113
-0.584814410293292 to -0.589467387666076
-0.342204492175768 to -0.346857469548552
-1.24964372393191 to -1.2542967013047
-0.464880387866686 to -0.46953336523947
-0.595128994071674 to -0.599781971444458
-0.925313841188144 to -0.929966818560928
-0.466800968730639 to -0.471453946103423
-0.798496942603779 to -0.803149919976563
-0.325667064273547 to -0.330320041646331
Changing layer 4's weights from 
-0.424940865123461 to -0.429593842496245
-0.633773665511799 to -0.638426642884583
-0.834251265609455 to -0.838904242982239
-0.547316532218646 to -0.55196950959143
-0.786569337928486 to -0.79122231530127
-0.55510411747713 to -0.559757094849914
-0.559462647521686 to -0.56411562489447
-1.02136016973276 to -1.02601314710554
-0.427664082133959 to -0.432317059506743
-0.859966020667744 to -0.864618998040528
Changing layer 5's weights from 
-0.272161047542284 to -0.276814024915068
-0.714234571540546 to -0.71888754891333
-0.289046208942125 to -0.293699186314909
-1.22455839031357 to -1.22921136768636
-0.687289576614094 to -0.691942553986878
-1.08575751968641 to -1.0904104970592
-0.785792212569904 to -0.790445189942688
-0.713345985496235 to -0.717998962869019
-0.323743980014513 to -0.328396957387297
-0.33025209196825 to -0.334905069341034
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0804819854428797 to -0.0851349628156635
-0.480516949527506 to -0.485169926900289
-0.763037646048314 to -0.767690623421098
-0.611532280080561 to -0.616185257453345
Changing layer 0's weights from 
-0.521993499268858 to -0.526646476641641
-0.893919955958694 to -0.898572933331478
-1.12826922165331 to -1.1329221990261
-1.09370997177538 to -1.09836294914816
-0.345842402448027 to -0.35049537982081
-1.02848435567793 to -1.03313733305072
-0.87465463208136 to -0.879307609454144
-0.902109038104385 to -0.906762015477169
-1.13960089909014 to -1.14425387646292
-1.18335095422444 to -1.18800393159722
Changing layer 1's weights from 
-1.1782540418404 to -1.18290701921318
-0.577085357179016 to -0.581738334551799
-0.702788095940919 to -0.707441073313703
-0.712693434228272 to -0.717346411601056
-0.897746216525406 to -0.90239919389819
-1.03598405050215 to -1.04063702787493
-0.765265326966614 to -0.769918304339398
-1.15106835590777 to -1.15572133328055
-0.739405374993652 to -0.744058352366436
-1.10205223785814 to -1.10670521523093
Changing layer 2's weights from 
-0.292864006032317 to -0.2975169834051
-0.846917491425843 to -0.851570468798627
-0.562328200806945 to -0.566981178179728
-0.391128104199736 to -0.395781081572519
-0.568380814065307 to -0.57303379143809
-0.774390440453858 to -0.779043417826642
-0.255577724446623 to -0.260230701819406
-0.327999752034514 to -0.332652729407297
-1.05999804662642 to -1.0646510239992
-1.01504052804884 to -1.01969350542163
Changing layer 3's weights from 
-0.325914304723113 to -0.330567282095896
-0.589467387666076 to -0.59412036503886
-0.346857469548552 to -0.351510446921335
-1.2542967013047 to -1.25894967867748
-0.46953336523947 to -0.474186342612253
-0.599781971444458 to -0.604434948817242
-0.929966818560928 to -0.934619795933712
-0.471453946103423 to -0.476106923476206
-0.803149919976563 to -0.807802897349347
-0.330320041646331 to -0.334973019019114
Changing layer 4's weights from 
-0.429593842496245 to -0.434246819869028
-0.638426642884583 to -0.643079620257367
-0.838904242982239 to -0.843557220355023
-0.55196950959143 to -0.556622486964213
-0.79122231530127 to -0.795875292674054
-0.559757094849914 to -0.564410072222697
-0.56411562489447 to -0.568768602267253
-1.02601314710554 to -1.03066612447833
-0.432317059506743 to -0.436970036879526
-0.864618998040528 to -0.869271975413312
Changing layer 5's weights from 
-0.276814024915068 to -0.281467002287851
-0.71888754891333 to -0.723540526286114
-0.293699186314909 to -0.298352163687692
-1.22921136768636 to -1.23386434505914
-0.691942553986878 to -0.696595531359662
-1.0904104970592 to -1.09506347443198
-0.790445189942688 to -0.795098167315472
-0.717998962869019 to -0.722651940241803
-0.328396957387297 to -0.33304993476008
-0.334905069341034 to -0.339558046713817
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0851349628156635 to -0.0897879401884473
-0.485169926900289 to -0.489822904273073
-0.767690623421098 to -0.772343600793882
-0.616185257453345 to -0.620838234826129
Changing layer 0's weights from 
-0.526646476641641 to -0.531299454014425
-0.898572933331478 to -0.903225910704262
-1.1329221990261 to -1.13757517639888
-1.09836294914816 to -1.10301592652095
-0.35049537982081 to -0.355148357193594
-1.03313733305072 to -1.0377903104235
-0.879307609454144 to -0.883960586826928
-0.906762015477169 to -0.911414992849953
-1.14425387646292 to -1.14890685383571
-1.18800393159722 to -1.19265690897
Changing layer 1's weights from 
-1.18290701921318 to -1.18755999658596
-0.581738334551799 to -0.586391311924583
-0.707441073313703 to -0.712094050686487
-0.717346411601056 to -0.72199938897384
-0.90239919389819 to -0.907052171270974
-1.04063702787493 to -1.04529000524772
-0.769918304339398 to -0.774571281712182
-1.15572133328055 to -1.16037431065334
-0.744058352366436 to -0.74871132973922
-1.10670521523093 to -1.11135819260371
Changing layer 2's weights from 
-0.2975169834051 to -0.302169960777884
-0.851570468798627 to -0.856223446171411
-0.566981178179728 to -0.571634155552512
-0.395781081572519 to -0.400434058945303
-0.57303379143809 to -0.577686768810874
-0.779043417826642 to -0.783696395199426
-0.260230701819406 to -0.26488367919219
-0.332652729407297 to -0.337305706780081
-1.0646510239992 to -1.06930400137199
-1.01969350542163 to -1.02434648279441
Changing layer 3's weights from 
-0.330567282095896 to -0.33522025946868
-0.59412036503886 to -0.598773342411643
-0.351510446921335 to -0.356163424294119
-1.25894967867748 to -1.26360265605026
-0.474186342612253 to -0.478839319985037
-0.604434948817242 to -0.609087926190026
-0.934619795933712 to -0.939272773306496
-0.476106923476206 to -0.48075990084899
-0.807802897349347 to -0.812455874722131
-0.334973019019114 to -0.339625996391898
Changing layer 4's weights from 
-0.434246819869028 to -0.438899797241812
-0.643079620257367 to -0.64773259763015
-0.843557220355023 to -0.848210197727807
-0.556622486964213 to -0.561275464336997
-0.795875292674054 to -0.800528270046838
-0.564410072222697 to -0.569063049595481
-0.568768602267253 to -0.573421579640037
-1.03066612447833 to -1.03531910185111
-0.436970036879526 to -0.44162301425231
-0.869271975413312 to -0.873924952786096
Changing layer 5's weights from 
-0.281467002287851 to -0.286119979660635
-0.723540526286114 to -0.728193503658898
-0.298352163687692 to -0.303005141060476
-1.23386434505914 to -1.23851732243192
-0.696595531359662 to -0.701248508732446
-1.09506347443198 to -1.09971645180476
-0.795098167315472 to -0.799751144688256
-0.722651940241803 to -0.727304917614586
-0.33304993476008 to -0.337702912132864
-0.339558046713817 to -0.344211024086601
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0897879401884473 to -0.0944409175612311
-0.489822904273073 to -0.494475881645857
-0.772343600793882 to -0.776996578166665
-0.620838234826129 to -0.625491212198913
Changing layer 0's weights from 
-0.531299454014425 to -0.535952431387209
-0.903225910704262 to -0.907878888077045
-1.13757517639888 to -1.14222815377166
-1.10301592652095 to -1.10766890389373
-0.355148357193594 to -0.359801334566378
-1.0377903104235 to -1.04244328779628
-0.883960586826928 to -0.888613564199711
-0.911414992849953 to -0.916067970222736
-1.14890685383571 to -1.15355983120849
-1.19265690897 to -1.19730988634279
Changing layer 1's weights from 
-1.18755999658596 to -1.19221297395875
-0.586391311924583 to -0.591044289297367
-0.712094050686487 to -0.71674702805927
-0.72199938897384 to -0.726652366346623
-0.907052171270974 to -0.911705148643757
-1.04529000524772 to -1.0499429826205
-0.774571281712182 to -0.779224259084965
-1.16037431065334 to -1.16502728802612
-0.74871132973922 to -0.753364307112003
-1.11135819260371 to -1.1160111699765
Changing layer 2's weights from 
-0.302169960777884 to -0.306822938150668
-0.856223446171411 to -0.860876423544194
-0.571634155552512 to -0.576287132925296
-0.400434058945303 to -0.405087036318087
-0.577686768810874 to -0.582339746183658
-0.783696395199426 to -0.788349372572209
-0.26488367919219 to -0.269536656564974
-0.337305706780081 to -0.341958684152865
-1.06930400137199 to -1.07395697874477
-1.02434648279441 to -1.02899946016719
Changing layer 3's weights from 
-0.33522025946868 to -0.339873236841464
-0.598773342411643 to -0.603426319784427
-0.356163424294119 to -0.360816401666903
-1.26360265605026 to -1.26825563342305
-0.478839319985037 to -0.483492297357821
-0.609087926190026 to -0.613740903562809
-0.939272773306496 to -0.943925750679279
-0.48075990084899 to -0.485412878221774
-0.812455874722131 to -0.817108852094914
-0.339625996391898 to -0.344278973764682
Changing layer 4's weights from 
-0.438899797241812 to -0.443552774614596
-0.64773259763015 to -0.652385575002934
-0.848210197727807 to -0.85286317510059
-0.561275464336997 to -0.565928441709781
-0.800528270046838 to -0.805181247419621
-0.569063049595481 to -0.573716026968265
-0.573421579640037 to -0.578074557012821
-1.03531910185111 to -1.0399720792239
-0.44162301425231 to -0.446275991625094
-0.873924952786096 to -0.878577930158879
Changing layer 5's weights from 
-0.286119979660635 to -0.290772957033419
-0.728193503658898 to -0.732846481031681
-0.303005141060476 to -0.30765811843326
-1.23851732243192 to -1.24317029980471
-0.701248508732446 to -0.705901486105229
-1.09971645180476 to -1.10436942917755
-0.799751144688256 to -0.804404122061039
-0.727304917614586 to -0.73195789498737
-0.337702912132864 to -0.342355889505648
-0.344211024086601 to -0.348864001459385
10/5/2016 1:47:35 PMStarting learning phase with deltaScore: 0
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0944409175612311 to -0.0990938949340149
-0.494475881645857 to -0.499128859018641
-0.776996578166665 to -0.781649555539449
-0.625491212198913 to -0.630144189571696
Changing layer 0's weights from 
-0.535952431387209 to -0.540605408759993
-0.907878888077045 to -0.912531865449829
-1.14222815377166 to -1.14688113114445
-1.10766890389373 to -1.11232188126652
-0.359801334566378 to -0.364454311939162
-1.04244328779628 to -1.04709626516907
-0.888613564199711 to -0.893266541572495
-0.916067970222736 to -0.92072094759552
-1.15355983120849 to -1.15821280858127
-1.19730988634279 to -1.20196286371557
Changing layer 1's weights from 
-1.19221297395875 to -1.19686595133153
-0.591044289297367 to -0.595697266670151
-0.71674702805927 to -0.721400005432054
-0.726652366346623 to -0.731305343719407
-0.911705148643757 to -0.916358126016541
-1.0499429826205 to -1.05459595999329
-0.779224259084965 to -0.783877236457749
-1.16502728802612 to -1.1696802653989
-0.753364307112003 to -0.758017284484787
-1.1160111699765 to -1.12066414734928
Changing layer 2's weights from 
-0.306822938150668 to -0.311475915523452
-0.860876423544194 to -0.865529400916978
-0.576287132925296 to -0.58094011029808
-0.405087036318087 to -0.409740013690871
-0.582339746183658 to -0.586992723556442
-0.788349372572209 to -0.793002349944993
-0.269536656564974 to -0.274189633937758
-0.341958684152865 to -0.346611661525649
-1.07395697874477 to -1.07860995611755
-1.02899946016719 to -1.03365243753998
Changing layer 3's weights from 
-0.339873236841464 to -0.344526214214248
-0.603426319784427 to -0.608079297157211
-0.360816401666903 to -0.365469379039687
-1.26825563342305 to -1.27290861079583
-0.483492297357821 to -0.488145274730605
-0.613740903562809 to -0.618393880935593
-0.943925750679279 to -0.948578728052063
-0.485412878221774 to -0.490065855594558
-0.817108852094914 to -0.821761829467698
-0.344278973764682 to -0.348931951137466
Changing layer 4's weights from 
-0.443552774614596 to -0.44820575198738
-0.652385575002934 to -0.657038552375718
-0.85286317510059 to -0.857516152473374
-0.565928441709781 to -0.570581419082565
-0.805181247419621 to -0.809834224792405
-0.573716026968265 to -0.578369004341049
-0.578074557012821 to -0.582727534385605
-1.0399720792239 to -1.04462505659668
-0.446275991625094 to -0.450928968997878
-0.878577930158879 to -0.883230907531663
Changing layer 5's weights from 
-0.290772957033419 to -0.295425934406203
-0.732846481031681 to -0.737499458404465
-0.30765811843326 to -0.312311095806044
-1.24317029980471 to -1.24782327717749
-0.705901486105229 to -0.710554463478013
-1.10436942917755 to -1.10902240655033
-0.804404122061039 to -0.809057099433823
-0.73195789498737 to -0.736610872360154
-0.342355889505648 to -0.347008866878432
-0.348864001459385 to -0.353516978832169
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.0990938949340149 to -0.103746872306799
-0.499128859018641 to -0.503781836391425
-0.781649555539449 to -0.786302532912233
-0.630144189571696 to -0.63479716694448
Changing layer 0's weights from 
-0.540605408759993 to -0.545258386132777
-0.912531865449829 to -0.917184842822613
-1.14688113114445 to -1.15153410851723
-1.11232188126652 to -1.1169748586393
-0.364454311939162 to -0.369107289311946
-1.04709626516907 to -1.05174924254185
-0.893266541572495 to -0.897919518945279
-0.92072094759552 to -0.925373924968304
-1.15821280858127 to -1.16286578595406
-1.20196286371557 to -1.20661584108836
Changing layer 1's weights from 
-1.19686595133153 to -1.20151892870432
-0.595697266670151 to -0.600350244042935
-0.721400005432054 to -0.726052982804838
-0.731305343719407 to -0.735958321092191
-0.916358126016541 to -0.921011103389325
-1.05459595999329 to -1.05924893736607
-0.783877236457749 to -0.788530213830533
-1.1696802653989 to -1.17433324277169
-0.758017284484787 to -0.762670261857571
-1.12066414734928 to -1.12531712472206
Changing layer 2's weights from 
-0.311475915523452 to -0.316128892896236
-0.865529400916978 to -0.870182378289762
-0.58094011029808 to -0.585593087670864
-0.409740013690871 to -0.414392991063655
-0.586992723556442 to -0.591645700929226
-0.793002349944993 to -0.797655327317777
-0.274189633937758 to -0.278842611310542
-0.346611661525649 to -0.351264638898433
-1.07860995611755 to -1.08326293349034
-1.03365243753998 to -1.03830541491276
Changing layer 3's weights from 
-0.344526214214248 to -0.349179191587032
-0.608079297157211 to -0.612732274529995
-0.365469379039687 to -0.370122356412471
-1.27290861079583 to -1.27756158816862
-0.488145274730605 to -0.492798252103389
-0.618393880935593 to -0.623046858308377
-0.948578728052063 to -0.953231705424847
-0.490065855594558 to -0.494718832967342
-0.821761829467698 to -0.826414806840482
-0.348931951137466 to -0.35358492851025
Changing layer 4's weights from 
-0.44820575198738 to -0.452858729360164
-0.657038552375718 to -0.661691529748502
-0.857516152473374 to -0.862169129846158
-0.570581419082565 to -0.575234396455349
-0.809834224792405 to -0.814487202165189
-0.578369004341049 to -0.583021981713833
-0.582727534385605 to -0.587380511758389
-1.04462505659668 to -1.04927803396946
-0.450928968997878 to -0.455581946370662
-0.883230907531663 to -0.887883884904447
Changing layer 5's weights from 
-0.295425934406203 to -0.300078911778987
-0.737499458404465 to -0.742152435777249
-0.312311095806044 to -0.316964073178828
-1.24782327717749 to -1.25247625455028
-0.710554463478013 to -0.715207440850797
-1.10902240655033 to -1.11367538392311
-0.809057099433823 to -0.813710076806607
-0.736610872360154 to -0.741263849732938
-0.347008866878432 to -0.351661844251216
-0.353516978832169 to -0.358169956204953
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.103746872306799 to -0.108399849679583
-0.503781836391425 to -0.508434813764208
-0.786302532912233 to -0.790955510285017
-0.63479716694448 to -0.639450144317264
Changing layer 0's weights from 
-0.545258386132777 to -0.54991136350556
-0.917184842822613 to -0.921837820195397
-1.15153410851723 to -1.15618708589001
-1.1169748586393 to -1.12162783601208
-0.369107289311946 to -0.373760266684729
-1.05174924254185 to -1.05640221991463
-0.897919518945279 to -0.902572496318063
-0.925373924968304 to -0.930026902341088
-1.16286578595406 to -1.16751876332684
-1.20661584108836 to -1.21126881846114
Changing layer 1's weights from 
-1.20151892870432 to -1.2061719060771
-0.600350244042935 to -0.605003221415719
-0.726052982804838 to -0.730705960177622
-0.735958321092191 to -0.740611298464975
-0.921011103389325 to -0.925664080762109
-1.05924893736607 to -1.06390191473885
-0.788530213830533 to -0.793183191203317
-1.17433324277169 to -1.17898622014447
-0.762670261857571 to -0.767323239230355
-1.12531712472206 to -1.12997010209485
Changing layer 2's weights from 
-0.316128892896236 to -0.320781870269019
-0.870182378289762 to -0.874835355662546
-0.585593087670864 to -0.590246065043647
-0.414392991063655 to -0.419045968436438
-0.591645700929226 to -0.596298678302009
-0.797655327317777 to -0.802308304690561
-0.278842611310542 to -0.283495588683325
-0.351264638898433 to -0.355917616271216
-1.08326293349034 to -1.08791591086312
-1.03830541491276 to -1.04295839228555
Changing layer 3's weights from 
-0.349179191587032 to -0.353832168959815
-0.612732274529995 to -0.617385251902779
-0.370122356412471 to -0.374775333785254
-1.27756158816862 to -1.2822145655414
-0.492798252103389 to -0.497451229476172
-0.623046858308377 to -0.627699835681161
-0.953231705424847 to -0.957884682797631
-0.494718832967342 to -0.499371810340125
-0.826414806840482 to -0.831067784213266
-0.35358492851025 to -0.358237905883033
Changing layer 4's weights from 
-0.452858729360164 to -0.457511706732947
-0.661691529748502 to -0.666344507121286
-0.862169129846158 to -0.866822107218942
-0.575234396455349 to -0.579887373828132
-0.814487202165189 to -0.819140179537973
-0.583021981713833 to -0.587674959086616
-0.587380511758389 to -0.592033489131172
-1.04927803396946 to -1.05393101134225
-0.455581946370662 to -0.460234923743445
-0.887883884904447 to -0.892536862277231
Changing layer 5's weights from 
-0.300078911778987 to -0.30473188915177
-0.742152435777249 to -0.746805413150033
-0.316964073178828 to -0.321617050551611
-1.25247625455028 to -1.25712923192306
-0.715207440850797 to -0.719860418223581
-1.11367538392311 to -1.1183283612959
-0.813710076806607 to -0.818363054179391
-0.741263849732938 to -0.745916827105722
-0.351661844251216 to -0.356314821623999
-0.358169956204953 to -0.362822933577736
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.108399849679583 to -0.113052827052366
-0.508434813764208 to -0.513087791136992
-0.790955510285017 to -0.795608487657801
-0.639450144317264 to -0.644103121690048
Changing layer 0's weights from 
-0.54991136350556 to -0.554564340878344
-0.921837820195397 to -0.926490797568181
-1.15618708589001 to -1.1608400632628
-1.12162783601208 to -1.12628081338487
-0.373760266684729 to -0.378413244057513
-1.05640221991463 to -1.06105519728742
-0.902572496318063 to -0.907225473690847
-0.930026902341088 to -0.934679879713872
-1.16751876332684 to -1.17217174069963
-1.21126881846114 to -1.21592179583392
Changing layer 1's weights from 
-1.2061719060771 to -1.21082488344988
-0.605003221415719 to -0.609656198788502
-0.730705960177622 to -0.735358937550406
-0.740611298464975 to -0.745264275837759
-0.925664080762109 to -0.930317058134893
-1.06390191473885 to -1.06855489211164
-0.793183191203317 to -0.797836168576101
-1.17898622014447 to -1.18363919751725
-0.767323239230355 to -0.771976216603139
-1.12997010209485 to -1.13462307946763
Changing layer 2's weights from 
-0.320781870269019 to -0.325434847641803
-0.874835355662546 to -0.87948833303533
-0.590246065043647 to -0.594899042416431
-0.419045968436438 to -0.423698945809222
-0.596298678302009 to -0.600951655674793
-0.802308304690561 to -0.806961282063345
-0.283495588683325 to -0.288148566056109
-0.355917616271216 to -0.360570593644
-1.08791591086312 to -1.0925688882359
-1.04295839228555 to -1.04761136965833
Changing layer 3's weights from 
-0.353832168959815 to -0.358485146332599
-0.617385251902779 to -0.622038229275562
-0.374775333785254 to -0.379428311158038
-1.2822145655414 to -1.28686754291418
-0.497451229476172 to -0.502104206848956
-0.627699835681161 to -0.632352813053945
-0.957884682797631 to -0.962537660170415
-0.499371810340125 to -0.504024787712909
-0.831067784213266 to -0.83572076158605
-0.358237905883033 to -0.362890883255817
Changing layer 4's weights from 
-0.457511706732947 to -0.462164684105731
-0.666344507121286 to -0.670997484494069
-0.866822107218942 to -0.871475084591726
-0.579887373828132 to -0.584540351200916
-0.819140179537973 to -0.823793156910757
-0.587674959086616 to -0.5923279364594
-0.592033489131172 to -0.596686466503956
-1.05393101134225 to -1.05858398871503
-0.460234923743445 to -0.464887901116229
-0.892536862277231 to -0.897189839650015
Changing layer 5's weights from 
-0.30473188915177 to -0.309384866524554
-0.746805413150033 to -0.751458390522817
-0.321617050551611 to -0.326270027924395
-1.25712923192306 to -1.26178220929584
-0.719860418223581 to -0.724513395596365
-1.1183283612959 to -1.12298133866868
-0.818363054179391 to -0.823016031552175
-0.745916827105722 to -0.750569804478506
-0.356314821623999 to -0.360967798996783
-0.362822933577736 to -0.36747591095052
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.113052827052366 to -0.11770580442515
-0.513087791136992 to -0.517740768509776
-0.795608487657801 to -0.800261465030584
-0.644103121690048 to -0.648756099062832
Changing layer 0's weights from 
-0.554564340878344 to -0.559217318251128
-0.926490797568181 to -0.931143774940964
-1.1608400632628 to -1.16549304063558
-1.12628081338487 to -1.13093379075765
-0.378413244057513 to -0.383066221430297
-1.06105519728742 to -1.0657081746602
-0.907225473690847 to -0.91187845106363
-0.934679879713872 to -0.939332857086655
-1.17217174069963 to -1.17682471807241
-1.21592179583392 to -1.22057477320671
Changing layer 1's weights from 
-1.21082488344988 to -1.21547786082267
-0.609656198788502 to -0.614309176161286
-0.735358937550406 to -0.740011914923189
-0.745264275837759 to -0.749917253210542
-0.930317058134893 to -0.934970035507676
-1.06855489211164 to -1.07320786948442
-0.797836168576101 to -0.802489145948884
-1.18363919751725 to -1.18829217489004
-0.771976216603139 to -0.776629193975922
-1.13462307946763 to -1.13927605684041
Changing layer 2's weights from 
-0.325434847641803 to -0.330087825014587
-0.87948833303533 to -0.884141310408113
-0.594899042416431 to -0.599552019789215
-0.423698945809222 to -0.428351923182006
-0.600951655674793 to -0.605604633047577
-0.806961282063345 to -0.811614259436128
-0.288148566056109 to -0.292801543428893
-0.360570593644 to -0.365223571016784
-1.0925688882359 to -1.09722186560869
-1.04761136965833 to -1.05226434703111
Changing layer 3's weights from 
-0.358485146332599 to -0.363138123705383
-0.622038229275562 to -0.626691206648346
-0.379428311158038 to -0.384081288530822
-1.28686754291418 to -1.29152052028697
-0.502104206848956 to -0.50675718422174
-0.632352813053945 to -0.637005790426728
-0.962537660170415 to -0.967190637543198
-0.504024787712909 to -0.508677765085693
-0.83572076158605 to -0.840373738958833
-0.362890883255817 to -0.367543860628601
Changing layer 4's weights from 
-0.462164684105731 to -0.466817661478515
-0.670997484494069 to -0.675650461866853
-0.871475084591726 to -0.876128061964509
-0.584540351200916 to -0.5891933285737
-0.823793156910757 to -0.82844613428354
-0.5923279364594 to -0.596980913832184
-0.596686466503956 to -0.60133944387674
-1.05858398871503 to -1.06323696608781
-0.464887901116229 to -0.469540878489013
-0.897189839650015 to -0.901842817022798
Changing layer 5's weights from 
-0.309384866524554 to -0.314037843897338
-0.751458390522817 to -0.7561113678956
-0.326270027924395 to -0.330923005297179
-1.26178220929584 to -1.26643518666863
-0.724513395596365 to -0.729166372969148
-1.12298133866868 to -1.12763431604147
-0.823016031552175 to -0.827669008924958
-0.750569804478506 to -0.755222781851289
-0.360967798996783 to -0.365620776369567
-0.36747591095052 to -0.372128888323304
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.11770580442515 to -0.122358781797934
-0.517740768509776 to -0.52239374588256
-0.800261465030584 to -0.804914442403368
-0.648756099062832 to -0.653409076435616
Changing layer 0's weights from 
-0.559217318251128 to -0.563870295623912
-0.931143774940964 to -0.935796752313748
-1.16549304063558 to -1.17014601800836
-1.13093379075765 to -1.13558676813043
-0.383066221430297 to -0.387719198803081
-1.0657081746602 to -1.07036115203299
-0.91187845106363 to -0.916531428436414
-0.939332857086655 to -0.943985834459439
-1.17682471807241 to -1.18147769544519
-1.22057477320671 to -1.22522775057949
Changing layer 1's weights from 
-1.21547786082267 to -1.22013083819545
-0.614309176161286 to -0.61896215353407
-0.740011914923189 to -0.744664892295973
-0.749917253210542 to -0.754570230583326
-0.934970035507676 to -0.93962301288046
-1.07320786948442 to -1.0778608468572
-0.802489145948884 to -0.807142123321668
-1.18829217489004 to -1.19294515226282
-0.776629193975922 to -0.781282171348706
-1.13927605684041 to -1.1439290342132
Changing layer 2's weights from 
-0.330087825014587 to -0.334740802387371
-0.884141310408113 to -0.888794287780897
-0.599552019789215 to -0.604204997161999
-0.428351923182006 to -0.43300490055479
-0.605604633047577 to -0.610257610420361
-0.811614259436128 to -0.816267236808912
-0.292801543428893 to -0.297454520801677
-0.365223571016784 to -0.369876548389568
-1.09722186560869 to -1.10187484298147
-1.05226434703111 to -1.0569173244039
Changing layer 3's weights from 
-0.363138123705383 to -0.367791101078167
-0.626691206648346 to -0.63134418402113
-0.384081288530822 to -0.388734265903606
-1.29152052028697 to -1.29617349765975
-0.50675718422174 to -0.511410161594524
-0.637005790426728 to -0.641658767799512
-0.967190637543198 to -0.971843614915982
-0.508677765085693 to -0.513330742458477
-0.840373738958833 to -0.845026716331617
-0.367543860628601 to -0.372196838001385
Changing layer 4's weights from 
-0.466817661478515 to -0.471470638851299
-0.675650461866853 to -0.680303439239637
-0.876128061964509 to -0.880781039337293
-0.5891933285737 to -0.593846305946484
-0.82844613428354 to -0.833099111656324
-0.596980913832184 to -0.601633891204968
-0.60133944387674 to -0.605992421249524
-1.06323696608781 to -1.0678899434606
-0.469540878489013 to -0.474193855861797
-0.901842817022798 to -0.906495794395582
Changing layer 5's weights from 
-0.314037843897338 to -0.318690821270122
-0.7561113678956 to -0.760764345268384
-0.330923005297179 to -0.335575982669963
-1.26643518666863 to -1.27108816404141
-0.729166372969148 to -0.733819350341932
-1.12763431604147 to -1.13228729341425
-0.827669008924958 to -0.832321986297742
-0.755222781851289 to -0.759875759224073
-0.365620776369567 to -0.370273753742351
-0.372128888323304 to -0.376781865696088
Trying to learn from memory 10, 0, -0.2
sum 0.0547570954096846 distri -0.0985214975462077
Using diff 0.139589319103471 and condRate 0.166666666666667
Changed category 0 weights from 
-0.122358781797934 to -0.127011759170718
-0.52239374588256 to -0.527046723255344
-0.804914442403368 to -0.809567419776152
-0.653409076435616 to -0.658062053808399
Changing layer 0's weights from 
-0.563870295623912 to -0.568523272996696
-0.935796752313748 to -0.940449729686532
-1.17014601800836 to -1.17479899538115
-1.13558676813043 to -1.14023974550322
-0.387719198803081 to -0.392372176175865
-1.07036115203299 to -1.07501412940577
-0.916531428436414 to -0.921184405809198
-0.943985834459439 to -0.948638811832223
-1.18147769544519 to -1.18613067281798
-1.22522775057949 to -1.22988072795227
Changing layer 1's weights from 
-1.22013083819545 to -1.22478381556823
-0.61896215353407 to -0.623615130906854
-0.744664892295973 to -0.749317869668757
-0.754570230583326 to -0.75922320795611
-0.93962301288046 to -0.944275990253244
-1.0778608468572 to -1.08251382422999
-0.807142123321668 to -0.811795100694452
-1.19294515226282 to -1.1975981296356
-0.781282171348706 to -0.78593514872149
-1.1439290342132 to -1.14858201158598
Changing layer 2's weights from 
-0.334740802387371 to -0.339393779760155
-0.888794287780897 to -0.893447265153681
-0.604204997161999 to -0.608857974534783
-0.43300490055479 to -0.437657877927574
-0.610257610420361 to -0.614910587793145
-0.816267236808912 to -0.820920214181696
-0.297454520801677 to -0.302107498174461
-0.369876548389568 to -0.374529525762352
-1.10187484298147 to -1.10652782035425
-1.0569173244039 to -1.06157030177668
Changing layer 3's weights from 
-0.367791101078167 to -0.372444078450951
-0.63134418402113 to -0.635997161393914
-0.388734265903606 to -0.39338724327639
-1.29617349765975 to -1.30082647503253
-0.511410161594524 to -0.516063138967308
-0.641658767799512 to -0.646311745172296
-0.971843614915982 to -0.976496592288766
-0.513330742458477 to -0.517983719831261
-0.845026716331617 to -0.849679693704401
-0.372196838001385 to -0.376849815374169
Changing layer 4's weights from 
-0.471470638851299 to -0.476123616224083
-0.680303439239637 to -0.684956416612421
-0.880781039337293 to -0.885434016710077
-0.593846305946484 to -0.598499283319268
-0.833099111656324 to -0.837752089029108
-0.601633891204968 to -0.606286868577752
-0.605992421249524 to -0.610645398622308
-1.0678899434606 to -1.07254292083338
-0.474193855861797 to -0.478846833234581
-0.906495794395582 to -0.911148771768366
Changing layer 5's weights from 
-0.318690821270122 to -0.323343798642906
-0.760764345268384 to -0.765417322641168
-0.335575982669963 to -0.340228960042747
-1.27108816404141 to -1.27574114141419
-0.733819350341932 to -0.738472327714716
-1.13228729341425 to -1.13694027078703
-0.832321986297742 to -0.836974963670526
-0.759875759224073 to -0.764528736596857
-0.370273753742351 to -0.374926731115135
-0.376781865696088 to -0.381434843068872
10/5/2016 1:47:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:35 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:36 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:37 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:38 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:39 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:40 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:41 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:42 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:43 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:44 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:45 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:46 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:47 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:48 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:49 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:50 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:51 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:52 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:53 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:54 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:55 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:56 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:57 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:58 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:47:59 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:00 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:01 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:02 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:03 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:04 PMStarting learning phase with deltaScore: 0
10/5/2016 1:48:04 PMStarting learning phase with deltaScore: 0
